2023-05-10 09:40:36.279: my pid: 7440
2023-05-10 09:40:36.279: model: model.general_recommender.SGL
2023-05-10 09:40:36.279: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:40:36.279: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:42:11.054: my pid: 11380
2023-05-10 09:42:11.054: model: model.general_recommender.SGL
2023-05-10 09:42:11.055: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:42:11.055: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:43:49.108: my pid: 3616
2023-05-10 09:43:49.108: model: model.general_recommender.SGL
2023-05-10 09:43:49.108: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:43:49.108: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:44:32.875: my pid: 14368
2023-05-10 09:44:32.875: model: model.general_recommender.SGL
2023-05-10 09:44:32.875: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:44:32.875: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:47:16.740: my pid: 868
2023-05-10 09:47:16.740: model: model.general_recommender.SGL
2023-05-10 09:47:16.740: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:47:16.740: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:47:55.148: my pid: 4276
2023-05-10 09:47:55.148: model: model.general_recommender.SGL
2023-05-10 09:47:55.148: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:47:55.148: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:52:54.228: my pid: 2852
2023-05-10 09:52:54.229: model: model.general_recommender.SGL
2023-05-10 09:52:54.229: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:52:54.229: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:52:57.642: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 09:53:16.833: my pid: 11940
2023-05-10 09:53:16.833: model: model.general_recommender.SGL
2023-05-10 09:53:16.833: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 09:53:16.833: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 09:53:20.213: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 09:53:40.669: [iter 1 : loss : 1.1346 = 0.6931 + 0.4416 + 0.0000, time: 20.454608]
2023-05-10 09:53:40.951: epoch 1:	0.00129553  	0.00252832  	0.00214150  
2023-05-10 09:53:40.951: Find a better model.
2023-05-10 09:54:00.933: [iter 2 : loss : 1.1357 = 0.6930 + 0.4427 + 0.0000, time: 19.978741]
2023-05-10 09:54:01.239: epoch 2:	0.00184335  	0.00313036  	0.00279487  
2023-05-10 09:54:01.239: Find a better model.
2023-05-10 09:54:21.421: [iter 3 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 20.177733]
2023-05-10 09:54:21.714: epoch 3:	0.00214688  	0.00410260  	0.00337225  
2023-05-10 09:54:21.714: Find a better model.
2023-05-10 09:54:41.713: [iter 4 : loss : 1.1365 = 0.6928 + 0.4437 + 0.0000, time: 19.994840]
2023-05-10 09:54:42.003: epoch 4:	0.00246521  	0.00427794  	0.00374260  
2023-05-10 09:54:42.003: Find a better model.
2023-05-10 09:55:02.090: [iter 5 : loss : 1.1357 = 0.6927 + 0.4430 + 0.0000, time: 20.082347]
2023-05-10 09:55:02.386: epoch 5:	0.00306485  	0.00562224  	0.00484647  
2023-05-10 09:55:02.386: Find a better model.
2023-05-10 09:55:22.452: [iter 6 : loss : 1.1373 = 0.6926 + 0.4447 + 0.0000, time: 20.062308]
2023-05-10 09:55:22.743: epoch 6:	0.00350163  	0.00661871  	0.00550632  
2023-05-10 09:55:22.743: Find a better model.
2023-05-10 09:55:43.018: [iter 7 : loss : 1.1360 = 0.6923 + 0.4436 + 0.0000, time: 20.270575]
2023-05-10 09:55:43.316: epoch 7:	0.00443441  	0.00893492  	0.00741626  
2023-05-10 09:55:43.316: Find a better model.
2023-05-10 09:56:03.473: [iter 8 : loss : 1.1378 = 0.6921 + 0.4457 + 0.0000, time: 20.153940]
2023-05-10 09:56:03.756: epoch 8:	0.00456026  	0.00881033  	0.00790860  
2023-05-10 09:56:24.016: [iter 9 : loss : 1.1362 = 0.6917 + 0.4444 + 0.0000, time: 20.256725]
2023-05-10 09:56:24.312: epoch 9:	0.00581135  	0.01279615  	0.01072612  
2023-05-10 09:56:24.312: Find a better model.
2023-05-10 09:56:44.059: [iter 10 : loss : 1.1382 = 0.6912 + 0.4470 + 0.0000, time: 19.744093]
2023-05-10 09:56:44.348: epoch 10:	0.00619631  	0.01270924  	0.01079337  
2023-05-10 09:57:04.363: [iter 11 : loss : 1.1357 = 0.6904 + 0.4453 + 0.0000, time: 20.011944]
2023-05-10 09:57:04.661: epoch 11:	0.00746221  	0.01654543  	0.01441363  
2023-05-10 09:57:04.661: Find a better model.
2023-05-10 09:57:24.456: [iter 12 : loss : 1.1377 = 0.6891 + 0.4486 + 0.0000, time: 19.789619]
2023-05-10 09:57:24.733: epoch 12:	0.00875033  	0.01898286  	0.01685714  
2023-05-10 09:57:24.733: Find a better model.
2023-05-10 09:57:46.995: [iter 13 : loss : 1.1337 = 0.6874 + 0.4462 + 0.0000, time: 22.258844]
2023-05-10 09:57:47.282: epoch 13:	0.01063072  	0.02499772  	0.02210773  
2023-05-10 09:57:47.282: Find a better model.
2023-05-10 09:58:06.851: [iter 14 : loss : 1.1357 = 0.6851 + 0.4506 + 0.0001, time: 19.563757]
2023-05-10 09:58:07.121: epoch 14:	0.01294789  	0.03047851  	0.02723709  
2023-05-10 09:58:07.121: Find a better model.
2023-05-10 09:58:26.792: [iter 15 : loss : 1.1289 = 0.6815 + 0.4473 + 0.0001, time: 19.667422]
2023-05-10 09:58:27.061: epoch 15:	0.01536872  	0.03745213  	0.03376215  
2023-05-10 09:58:27.061: Find a better model.
2023-05-10 09:58:46.627: [iter 16 : loss : 1.1289 = 0.6757 + 0.4531 + 0.0001, time: 19.561875]
2023-05-10 09:58:46.899: epoch 16:	0.01816711  	0.04416107  	0.03989286  
2023-05-10 09:58:46.899: Find a better model.
2023-05-10 09:59:06.546: [iter 17 : loss : 1.1154 = 0.6665 + 0.4488 + 0.0002, time: 19.643266]
2023-05-10 09:59:06.815: epoch 17:	0.02102477  	0.05374842  	0.04726898  
2023-05-10 09:59:06.815: Find a better model.
2023-05-10 09:59:26.407: [iter 18 : loss : 1.1077 = 0.6510 + 0.4565 + 0.0002, time: 19.586565]
2023-05-10 09:59:26.675: epoch 18:	0.02361590  	0.06136737  	0.05321244  
2023-05-10 09:59:26.675: Find a better model.
2023-05-10 09:59:46.330: [iter 19 : loss : 1.0790 = 0.6273 + 0.4514 + 0.0003, time: 19.649650]
2023-05-10 09:59:46.594: epoch 19:	0.02587390  	0.06824511  	0.05825581  
2023-05-10 09:59:46.594: Find a better model.
2023-05-10 10:00:05.987: [iter 20 : loss : 1.0555 = 0.5938 + 0.4612 + 0.0005, time: 19.388885]
2023-05-10 10:00:06.267: epoch 20:	0.02708064  	0.07225475  	0.06082251  
2023-05-10 10:00:06.267: Find a better model.
2023-05-10 10:00:25.910: [iter 21 : loss : 1.0075 = 0.5506 + 0.4563 + 0.0007, time: 19.639280]
2023-05-10 10:00:26.172: epoch 21:	0.02817630  	0.07560564  	0.06316067  
2023-05-10 10:00:26.172: Find a better model.
2023-05-10 10:00:45.792: [iter 22 : loss : 0.9692 = 0.5006 + 0.4677 + 0.0009, time: 19.605845]
2023-05-10 10:00:46.059: epoch 22:	0.02833918  	0.07673719  	0.06363038  
2023-05-10 10:00:46.059: Find a better model.
2023-05-10 10:01:05.759: [iter 23 : loss : 0.9129 = 0.4491 + 0.4626 + 0.0012, time: 19.694662]
2023-05-10 10:01:06.027: epoch 23:	0.02894625  	0.07842416  	0.06484257  
2023-05-10 10:01:06.027: Find a better model.
2023-05-10 10:01:25.586: [iter 24 : loss : 0.8763 = 0.4011 + 0.4737 + 0.0015, time: 19.554369]
2023-05-10 10:01:25.852: epoch 24:	0.02903509  	0.07922227  	0.06516071  
2023-05-10 10:01:25.852: Find a better model.
2023-05-10 10:01:45.665: [iter 25 : loss : 0.8269 = 0.3580 + 0.4670 + 0.0018, time: 19.808476]
2023-05-10 10:01:45.931: epoch 25:	0.02915354  	0.07967830  	0.06534810  
2023-05-10 10:01:45.931: Find a better model.
2023-05-10 10:02:05.535: [iter 26 : loss : 0.8009 = 0.3220 + 0.4767 + 0.0021, time: 19.600613]
2023-05-10 10:02:05.820: epoch 26:	0.02937563  	0.08089937  	0.06589705  
2023-05-10 10:02:05.820: Find a better model.
2023-05-10 10:02:25.312: [iter 27 : loss : 0.7623 = 0.2910 + 0.4689 + 0.0024, time: 19.487719]
2023-05-10 10:02:25.579: epoch 27:	0.02949408  	0.08129591  	0.06608542  
2023-05-10 10:02:25.579: Find a better model.
2023-05-10 10:02:45.153: [iter 28 : loss : 0.7452 = 0.2653 + 0.4773 + 0.0026, time: 19.570507]
2023-05-10 10:02:45.424: epoch 28:	0.02942006  	0.08154182  	0.06623196  
2023-05-10 10:02:45.424: Find a better model.
2023-05-10 10:03:05.105: [iter 29 : loss : 0.7155 = 0.2440 + 0.4686 + 0.0029, time: 19.677775]
2023-05-10 10:03:05.378: epoch 29:	0.02961994  	0.08209860  	0.06661403  
2023-05-10 10:03:05.378: Find a better model.
2023-05-10 10:03:24.913: [iter 30 : loss : 0.7044 = 0.2247 + 0.4765 + 0.0032, time: 19.530497]
2023-05-10 10:03:25.202: epoch 30:	0.02977541  	0.08235595  	0.06686135  
2023-05-10 10:03:25.202: Find a better model.
2023-05-10 10:03:44.851: [iter 31 : loss : 0.6798 = 0.2087 + 0.4676 + 0.0034, time: 19.644345]
2023-05-10 10:03:45.112: epoch 31:	0.02990868  	0.08296822  	0.06730115  
2023-05-10 10:03:45.112: Find a better model.
2023-05-10 10:04:04.728: [iter 32 : loss : 0.6736 = 0.1949 + 0.4751 + 0.0036, time: 19.612834]
2023-05-10 10:04:05.014: epoch 32:	0.02981984  	0.08239195  	0.06712474  
2023-05-10 10:04:24.684: [iter 33 : loss : 0.6534 = 0.1833 + 0.4662 + 0.0038, time: 19.665707]
2023-05-10 10:04:24.968: epoch 33:	0.02987167  	0.08268170  	0.06728562  
2023-05-10 10:04:44.648: [iter 34 : loss : 0.6502 = 0.1724 + 0.4737 + 0.0041, time: 19.675991]
2023-05-10 10:04:44.908: epoch 34:	0.02993089  	0.08267540  	0.06728508  
2023-05-10 10:05:04.611: [iter 35 : loss : 0.6311 = 0.1621 + 0.4647 + 0.0043, time: 19.700070]
2023-05-10 10:05:04.889: epoch 35:	0.03000492  	0.08303433  	0.06744335  
2023-05-10 10:05:04.889: Find a better model.
2023-05-10 10:05:27.392: [iter 36 : loss : 0.6308 = 0.1540 + 0.4723 + 0.0045, time: 22.499063]
2023-05-10 10:05:27.656: epoch 36:	0.03013078  	0.08352561  	0.06768014  
2023-05-10 10:05:27.657: Find a better model.
2023-05-10 10:05:47.633: [iter 37 : loss : 0.6145 = 0.1464 + 0.4634 + 0.0047, time: 19.973595]
2023-05-10 10:05:47.895: epoch 37:	0.03018260  	0.08399105  	0.06788095  
2023-05-10 10:05:47.896: Find a better model.
2023-05-10 10:06:07.717: [iter 38 : loss : 0.6153 = 0.1393 + 0.4711 + 0.0049, time: 19.817322]
2023-05-10 10:06:08.011: epoch 38:	0.03009376  	0.08367826  	0.06778461  
2023-05-10 10:06:27.885: [iter 39 : loss : 0.6009 = 0.1335 + 0.4623 + 0.0050, time: 19.871627]
2023-05-10 10:06:28.148: epoch 39:	0.03009376  	0.08407596  	0.06777658  
2023-05-10 10:06:28.148: Find a better model.
2023-05-10 10:06:48.069: [iter 40 : loss : 0.6022 = 0.1271 + 0.4699 + 0.0052, time: 19.916791]
2023-05-10 10:06:48.425: epoch 40:	0.02997530  	0.08344027  	0.06772754  
2023-05-10 10:07:08.157: [iter 41 : loss : 0.5879 = 0.1214 + 0.4611 + 0.0054, time: 19.727708]
2023-05-10 10:07:08.445: epoch 41:	0.03010855  	0.08344973  	0.06786226  
2023-05-10 10:07:28.053: [iter 42 : loss : 0.5916 = 0.1170 + 0.4691 + 0.0056, time: 19.603466]
2023-05-10 10:07:28.352: epoch 42:	0.03001230  	0.08270042  	0.06751042  
2023-05-10 10:07:48.179: [iter 43 : loss : 0.5784 = 0.1125 + 0.4601 + 0.0057, time: 19.823267]
2023-05-10 10:07:48.451: epoch 43:	0.03005672  	0.08298060  	0.06767274  
2023-05-10 10:08:08.040: [iter 44 : loss : 0.5829 = 0.1090 + 0.4680 + 0.0059, time: 19.585740]
2023-05-10 10:08:08.338: epoch 44:	0.02999749  	0.08254768  	0.06746451  
2023-05-10 10:08:28.440: [iter 45 : loss : 0.5704 = 0.1050 + 0.4593 + 0.0060, time: 20.097333]
2023-05-10 10:08:28.721: epoch 45:	0.02987903  	0.08203048  	0.06719852  
2023-05-10 10:08:48.699: [iter 46 : loss : 0.5747 = 0.1011 + 0.4674 + 0.0062, time: 19.972962]
2023-05-10 10:08:48.968: epoch 46:	0.02970877  	0.08140919  	0.06695744  
2023-05-10 10:09:09.088: [iter 47 : loss : 0.5626 = 0.0977 + 0.4586 + 0.0063, time: 20.114922]
2023-05-10 10:09:09.376: epoch 47:	0.02971618  	0.08142395  	0.06688894  
2023-05-10 10:09:29.219: [iter 48 : loss : 0.5680 = 0.0949 + 0.4666 + 0.0065, time: 19.838897]
2023-05-10 10:09:29.486: epoch 48:	0.02981242  	0.08162269  	0.06705566  
2023-05-10 10:09:49.986: [iter 49 : loss : 0.5567 = 0.0921 + 0.4579 + 0.0066, time: 20.496668]
2023-05-10 10:09:50.270: epoch 49:	0.02976800  	0.08168207  	0.06707793  
2023-05-10 10:10:10.114: [iter 50 : loss : 0.5623 = 0.0896 + 0.4660 + 0.0068, time: 19.839011]
2023-05-10 10:10:10.411: epoch 50:	0.02975319  	0.08122769  	0.06693742  
2023-05-10 10:11:26.680: my pid: 6832
2023-05-10 10:11:26.680: model: model.general_recommender.SGL
2023-05-10 10:11:26.680: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 10:11:26.680: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 10:11:30.005: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 10:27:57.854: my pid: 8468
2023-05-10 10:27:57.854: model: model.general_recommender.SGL
2023-05-10 10:27:57.854: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 10:27:57.854: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 10:28:01.246: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 10:28:22.127: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.880925]
2023-05-10 10:28:22.388: epoch 1:	0.00129553  	0.00284991  	0.00215294  
2023-05-10 10:28:22.389: Find a better model.
2023-05-10 10:28:43.245: [iter 2 : loss : 1.1362 = 0.6930 + 0.4432 + 0.0000, time: 20.852512]
2023-05-10 10:28:43.528: epoch 2:	0.00176192  	0.00303629  	0.00267748  
2023-05-10 10:28:43.528: Find a better model.
2023-05-10 10:29:04.580: [iter 3 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 21.048411]
2023-05-10 10:29:04.880: epoch 3:	0.00210986  	0.00447172  	0.00348037  
2023-05-10 10:29:04.880: Find a better model.
2023-05-10 10:29:25.848: [iter 4 : loss : 1.1370 = 0.6928 + 0.4442 + 0.0000, time: 20.962304]
2023-05-10 10:29:26.167: epoch 4:	0.00225052  	0.00420337  	0.00343037  
2023-05-10 10:29:47.133: [iter 5 : loss : 1.1356 = 0.6927 + 0.4429 + 0.0000, time: 20.963212]
2023-05-10 10:29:47.420: epoch 5:	0.00295381  	0.00617310  	0.00495353  
2023-05-10 10:29:47.421: Find a better model.
2023-05-10 10:30:08.581: [iter 6 : loss : 1.1379 = 0.6926 + 0.4453 + 0.0000, time: 21.156433]
2023-05-10 10:30:08.888: epoch 6:	0.00307225  	0.00558733  	0.00461113  
2023-05-10 10:30:29.953: [iter 7 : loss : 1.1361 = 0.6924 + 0.4437 + 0.0000, time: 21.061081]
2023-05-10 10:30:30.263: epoch 7:	0.00390880  	0.00809255  	0.00644328  
2023-05-10 10:30:30.263: Find a better model.
2023-05-10 10:30:51.172: [iter 8 : loss : 1.1384 = 0.6922 + 0.4462 + 0.0000, time: 20.905476]
2023-05-10 10:30:51.454: epoch 8:	0.00383476  	0.00688732  	0.00617257  
2023-05-10 10:31:12.361: [iter 9 : loss : 1.1362 = 0.6918 + 0.4444 + 0.0000, time: 20.903373]
2023-05-10 10:31:12.647: epoch 9:	0.00541900  	0.01268995  	0.00999855  
2023-05-10 10:31:12.647: Find a better model.
2023-05-10 10:31:34.268: [iter 10 : loss : 1.1388 = 0.6913 + 0.4474 + 0.0000, time: 21.616687]
2023-05-10 10:31:34.551: epoch 10:	0.00557446  	0.01233494  	0.01027359  
2023-05-10 10:31:56.172: [iter 11 : loss : 1.1358 = 0.6906 + 0.4452 + 0.0000, time: 21.616777]
2023-05-10 10:31:56.463: epoch 11:	0.00675893  	0.01596800  	0.01311174  
2023-05-10 10:31:56.463: Find a better model.
2023-05-10 10:32:18.221: [iter 12 : loss : 1.1383 = 0.6894 + 0.4489 + 0.0000, time: 21.753705]
2023-05-10 10:32:18.508: epoch 12:	0.00744000  	0.01740389  	0.01467035  
2023-05-10 10:32:18.508: Find a better model.
2023-05-10 10:32:39.489: [iter 13 : loss : 1.1340 = 0.6879 + 0.4461 + 0.0000, time: 20.976727]
2023-05-10 10:32:39.767: epoch 13:	0.00955725  	0.02323831  	0.01973217  
2023-05-10 10:32:39.767: Find a better model.
2023-05-10 10:33:00.519: [iter 14 : loss : 1.1367 = 0.6858 + 0.4508 + 0.0001, time: 20.749182]
2023-05-10 10:33:00.797: epoch 14:	0.01114893  	0.02698106  	0.02330431  
2023-05-10 10:33:00.797: Find a better model.
2023-05-10 10:33:21.670: [iter 15 : loss : 1.1300 = 0.6828 + 0.4471 + 0.0001, time: 20.867805]
2023-05-10 10:33:21.947: epoch 15:	0.01391029  	0.03416349  	0.02979587  
2023-05-10 10:33:21.947: Find a better model.
2023-05-10 10:33:42.545: [iter 16 : loss : 1.1312 = 0.6780 + 0.4531 + 0.0001, time: 20.594759]
2023-05-10 10:33:42.815: epoch 16:	0.01676792  	0.04234220  	0.03645165  
2023-05-10 10:33:42.815: Find a better model.
2023-05-10 10:34:03.640: [iter 17 : loss : 1.1189 = 0.6704 + 0.4484 + 0.0001, time: 20.820937]
2023-05-10 10:34:03.931: epoch 17:	0.02021780  	0.05127982  	0.04466984  
2023-05-10 10:34:03.931: Find a better model.
2023-05-10 10:34:24.509: [iter 18 : loss : 1.1144 = 0.6577 + 0.4566 + 0.0002, time: 20.573117]
2023-05-10 10:34:24.782: epoch 18:	0.02292741  	0.05792253  	0.05110826  
2023-05-10 10:34:24.782: Find a better model.
2023-05-10 10:34:45.618: [iter 19 : loss : 1.0887 = 0.6378 + 0.4506 + 0.0003, time: 20.831989]
2023-05-10 10:34:45.911: epoch 19:	0.02550372  	0.06524660  	0.05665000  
2023-05-10 10:34:45.911: Find a better model.
2023-05-10 10:35:06.530: [iter 20 : loss : 1.0703 = 0.6088 + 0.4611 + 0.0004, time: 20.612226]
2023-05-10 10:35:06.826: epoch 20:	0.02691775  	0.07020339  	0.06003452  
2023-05-10 10:35:06.826: Find a better model.
2023-05-10 10:35:27.429: [iter 21 : loss : 1.0255 = 0.5696 + 0.4552 + 0.0006, time: 20.598787]
2023-05-10 10:35:27.702: epoch 21:	0.02809486  	0.07357270  	0.06245583  
2023-05-10 10:35:27.702: Find a better model.
2023-05-10 10:35:48.070: [iter 22 : loss : 0.9907 = 0.5224 + 0.4674 + 0.0008, time: 20.364403]
2023-05-10 10:35:48.352: epoch 22:	0.02830216  	0.07517268  	0.06331614  
2023-05-10 10:35:48.352: Find a better model.
2023-05-10 10:36:09.006: [iter 23 : loss : 0.9343 = 0.4719 + 0.4613 + 0.0011, time: 20.649204]
2023-05-10 10:36:09.296: epoch 23:	0.02880558  	0.07662462  	0.06418413  
2023-05-10 10:36:09.296: Find a better model.
2023-05-10 10:36:29.641: [iter 24 : loss : 0.8978 = 0.4224 + 0.4740 + 0.0014, time: 20.340651]
2023-05-10 10:36:29.907: epoch 24:	0.02879077  	0.07689172  	0.06451464  
2023-05-10 10:36:29.907: Find a better model.
2023-05-10 10:36:50.364: [iter 25 : loss : 0.8454 = 0.3772 + 0.4665 + 0.0017, time: 20.452551]
2023-05-10 10:36:50.645: epoch 25:	0.02910911  	0.07790980  	0.06518448  
2023-05-10 10:36:50.645: Find a better model.
2023-05-10 10:37:11.223: [iter 26 : loss : 0.8183 = 0.3387 + 0.4777 + 0.0020, time: 20.575147]
2023-05-10 10:37:11.508: epoch 26:	0.02902030  	0.07856417  	0.06526928  
2023-05-10 10:37:11.508: Find a better model.
2023-05-10 10:37:32.385: [iter 27 : loss : 0.7762 = 0.3051 + 0.4688 + 0.0023, time: 20.873180]
2023-05-10 10:37:32.660: epoch 27:	0.02928681  	0.07929193  	0.06559582  
2023-05-10 10:37:32.661: Find a better model.
2023-05-10 10:37:56.214: [iter 28 : loss : 0.7590 = 0.2776 + 0.4789 + 0.0025, time: 23.547855]
2023-05-10 10:37:56.492: epoch 28:	0.02928683  	0.07948270  	0.06554232  
2023-05-10 10:37:56.492: Find a better model.
2023-05-10 10:38:17.359: [iter 29 : loss : 0.7259 = 0.2543 + 0.4688 + 0.0028, time: 20.863285]
2023-05-10 10:38:17.631: epoch 29:	0.02937566  	0.07983582  	0.06593981  
2023-05-10 10:38:17.631: Find a better model.
2023-05-10 10:38:38.038: [iter 30 : loss : 0.7147 = 0.2337 + 0.4779 + 0.0030, time: 20.404704]
2023-05-10 10:38:38.322: epoch 30:	0.02945710  	0.08052348  	0.06623834  
2023-05-10 10:38:38.323: Find a better model.
2023-05-10 10:38:58.969: [iter 31 : loss : 0.6876 = 0.2165 + 0.4678 + 0.0033, time: 20.641777]
2023-05-10 10:38:59.232: epoch 31:	0.02954593  	0.08101603  	0.06662248  
2023-05-10 10:38:59.232: Find a better model.
2023-05-10 10:39:19.813: [iter 32 : loss : 0.6816 = 0.2015 + 0.4766 + 0.0035, time: 20.577190]
2023-05-10 10:39:20.103: epoch 32:	0.02948670  	0.08075071  	0.06662828  
2023-05-10 10:39:40.948: [iter 33 : loss : 0.6596 = 0.1895 + 0.4663 + 0.0037, time: 20.841963]
2023-05-10 10:39:41.227: epoch 33:	0.02953113  	0.08083817  	0.06690628  
2023-05-10 10:40:01.772: [iter 34 : loss : 0.6569 = 0.1777 + 0.4752 + 0.0040, time: 20.538036]
2023-05-10 10:40:02.035: epoch 34:	0.02960517  	0.08031413  	0.06675012  
2023-05-10 10:40:22.534: [iter 35 : loss : 0.6358 = 0.1669 + 0.4648 + 0.0042, time: 20.494543]
2023-05-10 10:40:22.795: epoch 35:	0.02957555  	0.08016634  	0.06677988  
2023-05-10 10:40:43.556: [iter 36 : loss : 0.6366 = 0.1584 + 0.4739 + 0.0044, time: 20.757424]
2023-05-10 10:40:43.837: epoch 36:	0.02948670  	0.08006749  	0.06682614  
2023-05-10 10:41:04.494: [iter 37 : loss : 0.6188 = 0.1506 + 0.4636 + 0.0046, time: 20.652026]
2023-05-10 10:41:04.754: epoch 37:	0.02957554  	0.08008287  	0.06673148  
2023-05-10 10:41:25.404: [iter 38 : loss : 0.6202 = 0.1427 + 0.4727 + 0.0048, time: 20.645201]
2023-05-10 10:41:25.675: epoch 38:	0.02962736  	0.08044634  	0.06702482  
2023-05-10 10:41:46.332: [iter 39 : loss : 0.6040 = 0.1367 + 0.4623 + 0.0050, time: 20.652435]
2023-05-10 10:41:46.606: epoch 39:	0.02967178  	0.08043361  	0.06712769  
2023-05-10 10:42:07.131: [iter 40 : loss : 0.6065 = 0.1300 + 0.4713 + 0.0051, time: 20.520416]
2023-05-10 10:42:07.391: epoch 40:	0.02975320  	0.08060785  	0.06706086  
2023-05-10 10:42:28.067: [iter 41 : loss : 0.5909 = 0.1245 + 0.4611 + 0.0053, time: 20.671754]
2023-05-10 10:42:28.328: epoch 41:	0.02962734  	0.08043976  	0.06685216  
2023-05-10 10:42:48.892: [iter 42 : loss : 0.5954 = 0.1193 + 0.4706 + 0.0055, time: 20.560362]
2023-05-10 10:42:49.150: epoch 42:	0.02956811  	0.08013757  	0.06681108  
2023-05-10 10:43:10.028: [iter 43 : loss : 0.5804 = 0.1145 + 0.4602 + 0.0056, time: 20.873631]
2023-05-10 10:43:10.307: epoch 43:	0.02966437  	0.08026985  	0.06689017  
2023-05-10 10:43:31.095: [iter 44 : loss : 0.5865 = 0.1111 + 0.4696 + 0.0058, time: 20.784451]
2023-05-10 10:43:31.371: epoch 44:	0.02960513  	0.07977501  	0.06682383  
2023-05-10 10:43:52.413: [iter 45 : loss : 0.5720 = 0.1066 + 0.4594 + 0.0060, time: 21.037604]
2023-05-10 10:43:52.689: epoch 45:	0.02951630  	0.07940497  	0.06670895  
2023-05-10 10:44:13.419: [iter 46 : loss : 0.5779 = 0.1030 + 0.4687 + 0.0061, time: 20.726971]
2023-05-10 10:44:13.696: epoch 46:	0.02947928  	0.07919227  	0.06660757  
2023-05-10 10:44:36.586: [iter 47 : loss : 0.5641 = 0.0993 + 0.4586 + 0.0063, time: 22.884682]
2023-05-10 10:44:36.848: epoch 47:	0.02946447  	0.07945067  	0.06658729  
2023-05-10 10:44:57.933: [iter 48 : loss : 0.5706 = 0.0962 + 0.4680 + 0.0064, time: 21.082475]
2023-05-10 10:44:58.210: epoch 48:	0.02947927  	0.07924582  	0.06646653  
2023-05-10 10:45:19.051: [iter 49 : loss : 0.5579 = 0.0934 + 0.4579 + 0.0066, time: 20.836768]
2023-05-10 10:45:19.314: epoch 49:	0.02933862  	0.07865047  	0.06616128  
2023-05-10 10:45:40.537: [iter 50 : loss : 0.5648 = 0.0907 + 0.4674 + 0.0067, time: 21.220785]
2023-05-10 10:45:40.815: epoch 50:	0.02938303  	0.07871215  	0.06619582  
2023-05-10 10:46:23.956: my pid: 11804
2023-05-10 10:46:23.957: model: model.general_recommender.SGL
2023-05-10 10:46:23.957: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 10:46:23.957: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 10:46:27.345: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 10:46:48.801: [iter 1 : loss : 1.1340 = 0.6931 + 0.4409 + 0.0000, time: 21.454921]
2023-05-10 10:46:49.072: epoch 1:	0.00142878  	0.00268488  	0.00224234  
2023-05-10 10:46:49.072: Find a better model.
2023-05-10 10:47:10.824: [iter 2 : loss : 1.1349 = 0.6930 + 0.4419 + 0.0000, time: 21.748073]
2023-05-10 10:47:11.109: epoch 2:	0.00181374  	0.00316365  	0.00277089  
2023-05-10 10:47:11.109: Find a better model.
2023-05-10 10:47:33.186: [iter 3 : loss : 1.1336 = 0.6930 + 0.4407 + 0.0000, time: 22.074345]
2023-05-10 10:47:33.478: epoch 3:	0.00228013  	0.00483257  	0.00391506  
2023-05-10 10:47:33.478: Find a better model.
2023-05-10 10:47:54.993: [iter 4 : loss : 1.1354 = 0.6929 + 0.4425 + 0.0000, time: 21.511750]
2023-05-10 10:47:55.282: epoch 4:	0.00234676  	0.00422081  	0.00370860  
2023-05-10 10:48:16.899: [iter 5 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 21.614132]
2023-05-10 10:48:17.184: epoch 5:	0.00314628  	0.00615823  	0.00508319  
2023-05-10 10:48:17.184: Find a better model.
2023-05-10 10:48:38.610: [iter 6 : loss : 1.1360 = 0.6926 + 0.4433 + 0.0000, time: 21.423002]
2023-05-10 10:48:38.918: epoch 6:	0.00344241  	0.00596352  	0.00509767  
2023-05-10 10:49:00.330: [iter 7 : loss : 1.1341 = 0.6924 + 0.4417 + 0.0000, time: 21.407020]
2023-05-10 10:49:00.614: epoch 7:	0.00478234  	0.00958399  	0.00797176  
2023-05-10 10:49:00.614: Find a better model.
2023-05-10 10:49:22.033: [iter 8 : loss : 1.1364 = 0.6922 + 0.4441 + 0.0000, time: 21.414949]
2023-05-10 10:49:22.339: epoch 8:	0.00525613  	0.00972917  	0.00851019  
2023-05-10 10:49:22.339: Find a better model.
2023-05-10 10:49:43.925: [iter 9 : loss : 1.1341 = 0.6919 + 0.4422 + 0.0000, time: 21.583054]
2023-05-10 10:49:44.202: epoch 9:	0.00640359  	0.01418606  	0.01189766  
2023-05-10 10:49:44.202: Find a better model.
2023-05-10 10:50:05.753: [iter 10 : loss : 1.1367 = 0.6914 + 0.4452 + 0.0000, time: 21.547446]
2023-05-10 10:50:06.057: epoch 10:	0.00658126  	0.01365523  	0.01203232  
2023-05-10 10:50:27.682: [iter 11 : loss : 1.1336 = 0.6907 + 0.4428 + 0.0000, time: 21.621570]
2023-05-10 10:50:27.980: epoch 11:	0.00786937  	0.01748804  	0.01503946  
2023-05-10 10:50:27.980: Find a better model.
2023-05-10 10:50:49.139: [iter 12 : loss : 1.1362 = 0.6896 + 0.4465 + 0.0000, time: 21.155113]
2023-05-10 10:50:49.432: epoch 12:	0.00877253  	0.01917239  	0.01695274  
2023-05-10 10:50:49.432: Find a better model.
2023-05-10 10:51:10.864: [iter 13 : loss : 1.1314 = 0.6879 + 0.4435 + 0.0000, time: 21.427011]
2023-05-10 10:51:11.144: epoch 13:	0.01037160  	0.02458162  	0.02165328  
2023-05-10 10:51:11.144: Find a better model.
2023-05-10 10:51:32.144: [iter 14 : loss : 1.1340 = 0.6854 + 0.4486 + 0.0001, time: 20.996409]
2023-05-10 10:51:32.442: epoch 14:	0.01262215  	0.02968082  	0.02638903  
2023-05-10 10:51:32.442: Find a better model.
2023-05-10 10:51:53.901: [iter 15 : loss : 1.1262 = 0.6818 + 0.4443 + 0.0001, time: 21.455051]
2023-05-10 10:51:54.184: epoch 15:	0.01505780  	0.03790881  	0.03313944  
2023-05-10 10:51:54.184: Find a better model.
2023-05-10 10:52:15.456: [iter 16 : loss : 1.1268 = 0.6756 + 0.4511 + 0.0001, time: 21.268842]
2023-05-10 10:52:15.753: epoch 16:	0.01770072  	0.04477692  	0.03983301  
2023-05-10 10:52:15.753: Find a better model.
2023-05-10 10:52:37.101: [iter 17 : loss : 1.1113 = 0.6656 + 0.4455 + 0.0002, time: 21.343664]
2023-05-10 10:52:37.396: epoch 17:	0.02018080  	0.05160011  	0.04600689  
2023-05-10 10:52:37.396: Find a better model.
2023-05-10 10:52:58.679: [iter 18 : loss : 1.1035 = 0.6486 + 0.4547 + 0.0002, time: 21.279910]
2023-05-10 10:52:58.982: epoch 18:	0.02293481  	0.05915971  	0.05148443  
2023-05-10 10:52:58.982: Find a better model.
2023-05-10 10:53:20.058: [iter 19 : loss : 1.0711 = 0.6227 + 0.4481 + 0.0004, time: 21.070929]
2023-05-10 10:53:20.353: epoch 19:	0.02501514  	0.06571865  	0.05620712  
2023-05-10 10:53:20.353: Find a better model.
2023-05-10 10:53:41.655: [iter 20 : loss : 1.0464 = 0.5864 + 0.4595 + 0.0005, time: 21.298587]
2023-05-10 10:53:41.928: epoch 20:	0.02647356  	0.07023587  	0.05962942  
2023-05-10 10:53:41.928: Find a better model.
2023-05-10 10:54:03.246: [iter 21 : loss : 0.9943 = 0.5406 + 0.4530 + 0.0007, time: 21.315824]
2023-05-10 10:54:03.515: epoch 21:	0.02785058  	0.07407399  	0.06212694  
2023-05-10 10:54:03.515: Find a better model.
2023-05-10 10:54:24.633: [iter 22 : loss : 0.9564 = 0.4894 + 0.4660 + 0.0010, time: 21.113634]
2023-05-10 10:54:24.923: epoch 22:	0.02832437  	0.07665676  	0.06354594  
2023-05-10 10:54:24.923: Find a better model.
2023-05-10 10:54:46.580: [iter 23 : loss : 0.8984 = 0.4380 + 0.4591 + 0.0013, time: 21.652852]
2023-05-10 10:54:46.866: epoch 23:	0.02878337  	0.07801282  	0.06428794  
2023-05-10 10:54:46.867: Find a better model.
2023-05-10 10:55:08.391: [iter 24 : loss : 0.8641 = 0.3909 + 0.4716 + 0.0016, time: 21.520229]
2023-05-10 10:55:08.680: epoch 24:	0.02910173  	0.07940014  	0.06480922  
2023-05-10 10:55:08.680: Find a better model.
2023-05-10 10:55:30.187: [iter 25 : loss : 0.8142 = 0.3492 + 0.4632 + 0.0019, time: 21.504126]
2023-05-10 10:55:30.470: epoch 25:	0.02939044  	0.08048395  	0.06526852  
2023-05-10 10:55:30.470: Find a better model.
2023-05-10 10:55:51.833: [iter 26 : loss : 0.7908 = 0.3145 + 0.4742 + 0.0022, time: 21.358754]
2023-05-10 10:55:52.116: epoch 26:	0.02916096  	0.08005957  	0.06518850  
2023-05-10 10:56:13.742: [iter 27 : loss : 0.7516 = 0.2844 + 0.4648 + 0.0025, time: 21.623116]
2023-05-10 10:56:14.022: epoch 27:	0.02935344  	0.08056888  	0.06555322  
2023-05-10 10:56:14.022: Find a better model.
2023-05-10 10:56:35.405: [iter 28 : loss : 0.7371 = 0.2598 + 0.4746 + 0.0027, time: 21.379164]
2023-05-10 10:56:35.684: epoch 28:	0.02937565  	0.08087348  	0.06572763  
2023-05-10 10:56:35.684: Find a better model.
2023-05-10 10:56:57.352: [iter 29 : loss : 0.7062 = 0.2388 + 0.4644 + 0.0030, time: 21.663615]
2023-05-10 10:56:57.632: epoch 29:	0.02948670  	0.08114699  	0.06605575  
2023-05-10 10:56:57.632: Find a better model.
2023-05-10 10:57:19.188: [iter 30 : loss : 0.6970 = 0.2201 + 0.4737 + 0.0032, time: 21.551399]
2023-05-10 10:57:19.468: epoch 30:	0.02949410  	0.08123101  	0.06595276  
2023-05-10 10:57:19.468: Find a better model.
2023-05-10 10:57:40.983: [iter 31 : loss : 0.6716 = 0.2047 + 0.4634 + 0.0035, time: 21.512004]
2023-05-10 10:57:41.267: epoch 31:	0.02948670  	0.08175768  	0.06622872  
2023-05-10 10:57:41.267: Find a better model.
2023-05-10 10:58:05.207: [iter 32 : loss : 0.6677 = 0.1916 + 0.4723 + 0.0037, time: 23.936861]
2023-05-10 10:58:05.490: epoch 32:	0.02959035  	0.08173075  	0.06631657  
2023-05-10 10:58:27.335: [iter 33 : loss : 0.6462 = 0.1802 + 0.4621 + 0.0039, time: 21.841702]
2023-05-10 10:58:27.601: epoch 33:	0.02959776  	0.08148003  	0.06644751  
2023-05-10 10:58:48.971: [iter 34 : loss : 0.6447 = 0.1695 + 0.4710 + 0.0041, time: 21.366042]
2023-05-10 10:58:49.252: epoch 34:	0.02973102  	0.08155969  	0.06680386  
2023-05-10 10:59:10.827: [iter 35 : loss : 0.6243 = 0.1593 + 0.4607 + 0.0044, time: 21.571041]
2023-05-10 10:59:11.108: epoch 35:	0.02986427  	0.08203676  	0.06692606  
2023-05-10 10:59:11.108: Find a better model.
2023-05-10 10:59:32.556: [iter 36 : loss : 0.6258 = 0.1515 + 0.4697 + 0.0045, time: 21.443016]
2023-05-10 10:59:32.829: epoch 36:	0.02996052  	0.08214694  	0.06689382  
2023-05-10 10:59:32.829: Find a better model.
2023-05-10 10:59:54.518: [iter 37 : loss : 0.6085 = 0.1443 + 0.4595 + 0.0047, time: 21.683869]
2023-05-10 10:59:54.797: epoch 37:	0.02990129  	0.08176512  	0.06688256  
2023-05-10 11:00:16.161: [iter 38 : loss : 0.6109 = 0.1374 + 0.4686 + 0.0049, time: 21.359766]
2023-05-10 11:00:16.443: epoch 38:	0.02988648  	0.08178711  	0.06692399  
2023-05-10 11:00:37.897: [iter 39 : loss : 0.5949 = 0.1314 + 0.4583 + 0.0051, time: 21.447552]
2023-05-10 11:00:38.170: epoch 39:	0.02992349  	0.08194813  	0.06701279  
2023-05-10 11:00:59.499: [iter 40 : loss : 0.5982 = 0.1256 + 0.4673 + 0.0053, time: 21.325768]
2023-05-10 11:00:59.777: epoch 40:	0.02990127  	0.08198448  	0.06686638  
2023-05-10 11:01:21.118: [iter 41 : loss : 0.5825 = 0.1198 + 0.4572 + 0.0055, time: 21.336420]
2023-05-10 11:01:21.382: epoch 41:	0.02989387  	0.08178078  	0.06693617  
2023-05-10 11:01:42.894: [iter 42 : loss : 0.5873 = 0.1152 + 0.4665 + 0.0056, time: 21.509418]
2023-05-10 11:01:43.170: epoch 42:	0.02987167  	0.08167460  	0.06689629  
2023-05-10 11:02:04.831: [iter 43 : loss : 0.5730 = 0.1108 + 0.4564 + 0.0058, time: 21.656948]
2023-05-10 11:02:05.117: epoch 43:	0.02981244  	0.08173986  	0.06701525  
2023-05-10 11:02:26.284: [iter 44 : loss : 0.5792 = 0.1076 + 0.4656 + 0.0060, time: 21.162474]
2023-05-10 11:02:26.546: epoch 44:	0.02973842  	0.08138566  	0.06676797  
2023-05-10 11:02:48.214: [iter 45 : loss : 0.5650 = 0.1034 + 0.4555 + 0.0061, time: 21.665320]
2023-05-10 11:02:48.497: epoch 45:	0.02953852  	0.08055554  	0.06635958  
2023-05-10 11:03:09.938: [iter 46 : loss : 0.5710 = 0.0998 + 0.4650 + 0.0063, time: 21.436419]
2023-05-10 11:03:10.218: epoch 46:	0.02950891  	0.08036270  	0.06630998  
2023-05-10 11:03:31.849: [iter 47 : loss : 0.5577 = 0.0965 + 0.4548 + 0.0064, time: 21.627236]
2023-05-10 11:03:32.157: epoch 47:	0.02948669  	0.08017523  	0.06616426  
2023-05-10 11:03:53.436: [iter 48 : loss : 0.5642 = 0.0935 + 0.4641 + 0.0066, time: 21.274998]
2023-05-10 11:03:53.696: epoch 48:	0.02934602  	0.07981326  	0.06603077  
2023-05-10 11:04:15.212: [iter 49 : loss : 0.5520 = 0.0911 + 0.4541 + 0.0067, time: 21.512049]
2023-05-10 11:04:15.490: epoch 49:	0.02930900  	0.07957049  	0.06593677  
2023-05-10 11:04:37.016: [iter 50 : loss : 0.5586 = 0.0883 + 0.4634 + 0.0069, time: 21.522977]
2023-05-10 11:04:37.295: epoch 50:	0.02920535  	0.07915547  	0.06574345  
2023-05-10 11:04:58.989: [iter 51 : loss : 0.5458 = 0.0854 + 0.4535 + 0.0070, time: 21.689927]
2023-05-10 11:04:59.266: epoch 51:	0.02930900  	0.07929964  	0.06588717  
2023-05-10 11:05:22.725: [iter 52 : loss : 0.5526 = 0.0825 + 0.4629 + 0.0071, time: 23.455151]
2023-05-10 11:05:22.991: epoch 52:	0.02918314  	0.07901949  	0.06556746  
2023-05-10 11:05:44.670: [iter 53 : loss : 0.5416 = 0.0813 + 0.4531 + 0.0073, time: 21.673714]
2023-05-10 11:05:44.935: epoch 53:	0.02919055  	0.07908752  	0.06557407  
2023-05-10 11:08:33.633: my pid: 16084
2023-05-10 11:08:33.633: model: model.general_recommender.SGL
2023-05-10 11:08:33.633: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 11:08:33.633: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 11:08:36.957: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 11:08:57.101: [iter 1 : loss : 1.1361 = 0.6931 + 0.4430 + 0.0000, time: 20.144134]
2023-05-10 11:08:57.364: epoch 1:	0.00116968  	0.00220346  	0.00188284  
2023-05-10 11:08:57.364: Find a better model.
2023-05-10 11:09:17.830: [iter 2 : loss : 1.1374 = 0.6930 + 0.4444 + 0.0000, time: 20.463186]
2023-05-10 11:09:18.139: epoch 2:	0.00160646  	0.00246514  	0.00226749  
2023-05-10 11:09:18.140: Find a better model.
2023-05-10 11:09:38.695: [iter 3 : loss : 1.1373 = 0.6929 + 0.4444 + 0.0000, time: 20.551730]
2023-05-10 11:09:38.991: epoch 3:	0.00230234  	0.00413106  	0.00348338  
2023-05-10 11:09:38.992: Find a better model.
2023-05-10 11:09:59.203: [iter 4 : loss : 1.1389 = 0.6928 + 0.4461 + 0.0000, time: 20.208173]
2023-05-10 11:09:59.503: epoch 4:	0.00243559  	0.00386241  	0.00360802  
2023-05-10 11:10:19.869: [iter 5 : loss : 1.1385 = 0.6926 + 0.4458 + 0.0000, time: 20.362061]
2023-05-10 11:10:20.184: epoch 5:	0.00320551  	0.00611084  	0.00501852  
2023-05-10 11:10:20.184: Find a better model.
2023-05-10 11:10:40.564: [iter 6 : loss : 1.1400 = 0.6925 + 0.4475 + 0.0000, time: 20.375831]
2023-05-10 11:10:40.855: epoch 6:	0.00330175  	0.00633239  	0.00536532  
2023-05-10 11:10:40.855: Find a better model.
2023-05-10 11:11:01.342: [iter 7 : loss : 1.1392 = 0.6922 + 0.4469 + 0.0000, time: 20.484004]
2023-05-10 11:11:01.644: epoch 7:	0.00411608  	0.00878381  	0.00720653  
2023-05-10 11:11:01.644: Find a better model.
2023-05-10 11:11:22.202: [iter 8 : loss : 1.1410 = 0.6920 + 0.4490 + 0.0000, time: 20.554098]
2023-05-10 11:11:22.510: epoch 8:	0.00456026  	0.00854952  	0.00736169  
2023-05-10 11:11:43.119: [iter 9 : loss : 1.1397 = 0.6916 + 0.4481 + 0.0000, time: 20.605301]
2023-05-10 11:11:43.433: epoch 9:	0.00528575  	0.01138350  	0.00941749  
2023-05-10 11:11:43.433: Find a better model.
2023-05-10 11:12:04.014: [iter 10 : loss : 1.1416 = 0.6910 + 0.4506 + 0.0000, time: 20.577216]
2023-05-10 11:12:04.316: epoch 10:	0.00549303  	0.01130291  	0.00965503  
2023-05-10 11:12:24.769: [iter 11 : loss : 1.1394 = 0.6901 + 0.4493 + 0.0000, time: 20.450429]
2023-05-10 11:12:25.090: epoch 11:	0.00684037  	0.01603441  	0.01357087  
2023-05-10 11:12:25.090: Find a better model.
2023-05-10 11:12:45.515: [iter 12 : loss : 1.1415 = 0.6890 + 0.4525 + 0.0000, time: 20.420998]
2023-05-10 11:12:45.808: epoch 12:	0.00781016  	0.01739744  	0.01562780  
2023-05-10 11:12:45.808: Find a better model.
2023-05-10 11:13:06.229: [iter 13 : loss : 1.1381 = 0.6875 + 0.4505 + 0.0000, time: 20.417479]
2023-05-10 11:13:06.512: epoch 13:	0.00963868  	0.02290970  	0.02012798  
2023-05-10 11:13:06.512: Find a better model.
2023-05-10 11:13:26.974: [iter 14 : loss : 1.1402 = 0.6856 + 0.4546 + 0.0001, time: 20.457413]
2023-05-10 11:13:27.281: epoch 14:	0.01125257  	0.02738532  	0.02427209  
2023-05-10 11:13:27.281: Find a better model.
2023-05-10 11:13:47.425: [iter 15 : loss : 1.1347 = 0.6827 + 0.4520 + 0.0001, time: 20.140039]
2023-05-10 11:13:47.707: epoch 15:	0.01382886  	0.03499061  	0.03060703  
2023-05-10 11:13:47.707: Find a better model.
2023-05-10 11:14:07.719: [iter 16 : loss : 1.1355 = 0.6783 + 0.4571 + 0.0001, time: 20.009681]
2023-05-10 11:14:07.996: epoch 16:	0.01655322  	0.04118089  	0.03665668  
2023-05-10 11:14:07.997: Find a better model.
2023-05-10 11:14:27.817: [iter 17 : loss : 1.1254 = 0.6716 + 0.4536 + 0.0001, time: 19.815296]
2023-05-10 11:14:28.116: epoch 17:	0.01929979  	0.05001312  	0.04382328  
2023-05-10 11:14:28.116: Find a better model.
2023-05-10 11:14:47.885: [iter 18 : loss : 1.1214 = 0.6608 + 0.4604 + 0.0002, time: 19.765337]
2023-05-10 11:14:48.163: epoch 18:	0.02206122  	0.05615005  	0.04934201  
2023-05-10 11:14:48.163: Find a better model.
2023-05-10 11:15:08.033: [iter 19 : loss : 1.1002 = 0.6440 + 0.4560 + 0.0003, time: 19.865636]
2023-05-10 11:15:08.322: epoch 19:	0.02472640  	0.06426605  	0.05522583  
2023-05-10 11:15:08.322: Find a better model.
2023-05-10 11:15:28.060: [iter 20 : loss : 1.0842 = 0.6193 + 0.4645 + 0.0004, time: 19.734713]
2023-05-10 11:15:28.349: epoch 20:	0.02616263  	0.06827565  	0.05903405  
2023-05-10 11:15:28.349: Find a better model.
2023-05-10 11:15:48.390: [iter 21 : loss : 1.0457 = 0.5850 + 0.4602 + 0.0005, time: 20.038182]
2023-05-10 11:15:48.671: epoch 21:	0.02738415  	0.07198498  	0.06172284  
2023-05-10 11:15:48.671: Find a better model.
2023-05-10 11:16:08.692: [iter 22 : loss : 1.0135 = 0.5420 + 0.4707 + 0.0007, time: 20.017486]
2023-05-10 11:16:08.994: epoch 22:	0.02803563  	0.07379631  	0.06289479  
2023-05-10 11:16:08.994: Find a better model.
2023-05-10 11:16:28.974: [iter 23 : loss : 0.9607 = 0.4934 + 0.4663 + 0.0010, time: 19.975837]
2023-05-10 11:16:29.273: epoch 23:	0.02883519  	0.07649212  	0.06410386  
2023-05-10 11:16:29.273: Find a better model.
2023-05-10 11:16:49.057: [iter 24 : loss : 0.9225 = 0.4439 + 0.4774 + 0.0012, time: 19.779605]
2023-05-10 11:16:49.337: epoch 24:	0.02903507  	0.07708987  	0.06471297  
2023-05-10 11:16:49.337: Find a better model.
2023-05-10 11:17:09.333: [iter 25 : loss : 0.8707 = 0.3973 + 0.4719 + 0.0015, time: 19.991406]
2023-05-10 11:17:09.603: epoch 25:	0.02928679  	0.07811452  	0.06521115  
2023-05-10 11:17:09.603: Find a better model.
2023-05-10 11:17:31.271: [iter 26 : loss : 0.8399 = 0.3563 + 0.4817 + 0.0018, time: 21.664304]
2023-05-10 11:17:31.534: epoch 26:	0.02935343  	0.07836495  	0.06540830  
2023-05-10 11:17:31.534: Find a better model.
2023-05-10 11:17:51.523: [iter 27 : loss : 0.7978 = 0.3208 + 0.4749 + 0.0021, time: 19.984507]
2023-05-10 11:17:51.786: epoch 27:	0.02947188  	0.07904589  	0.06595955  
2023-05-10 11:17:51.786: Find a better model.
2023-05-10 11:18:11.597: [iter 28 : loss : 0.7764 = 0.2907 + 0.4833 + 0.0024, time: 19.807034]
2023-05-10 11:18:11.859: epoch 28:	0.02938306  	0.07925302  	0.06598519  
2023-05-10 11:18:11.859: Find a better model.
2023-05-10 11:18:31.710: [iter 29 : loss : 0.7435 = 0.2656 + 0.4753 + 0.0026, time: 19.846578]
2023-05-10 11:18:31.970: epoch 29:	0.02955332  	0.08005954  	0.06638230  
2023-05-10 11:18:31.970: Find a better model.
2023-05-10 11:18:51.956: [iter 30 : loss : 0.7293 = 0.2437 + 0.4827 + 0.0029, time: 19.982127]
2023-05-10 11:18:52.219: epoch 30:	0.02952372  	0.07991046  	0.06633367  
2023-05-10 11:19:12.285: [iter 31 : loss : 0.7027 = 0.2252 + 0.4743 + 0.0032, time: 20.060891]
2023-05-10 11:19:12.547: epoch 31:	0.02954594  	0.07990779  	0.06647313  
2023-05-10 11:19:32.349: [iter 32 : loss : 0.6943 = 0.2096 + 0.4813 + 0.0034, time: 19.797990]
2023-05-10 11:19:32.611: epoch 32:	0.02963477  	0.08033813  	0.06653231  
2023-05-10 11:19:32.611: Find a better model.
2023-05-10 11:19:52.664: [iter 33 : loss : 0.6726 = 0.1960 + 0.4730 + 0.0036, time: 20.049903]
2023-05-10 11:19:52.925: epoch 33:	0.02971623  	0.08057756  	0.06664323  
2023-05-10 11:19:52.925: Find a better model.
2023-05-10 11:20:12.762: [iter 34 : loss : 0.6676 = 0.1838 + 0.4799 + 0.0038, time: 19.832640]
2023-05-10 11:20:13.023: epoch 34:	0.02977545  	0.08069727  	0.06678529  
2023-05-10 11:20:13.024: Find a better model.
2023-05-10 11:20:32.851: [iter 35 : loss : 0.6478 = 0.1724 + 0.4713 + 0.0041, time: 19.824646]
2023-05-10 11:20:33.112: epoch 35:	0.02976063  	0.08068826  	0.06689188  
2023-05-10 11:20:52.971: [iter 36 : loss : 0.6461 = 0.1632 + 0.4786 + 0.0043, time: 19.854556]
2023-05-10 11:20:53.232: epoch 36:	0.02985688  	0.08130655  	0.06709305  
2023-05-10 11:20:53.232: Find a better model.
2023-05-10 11:21:13.085: [iter 37 : loss : 0.6293 = 0.1549 + 0.4699 + 0.0045, time: 19.849305]
2023-05-10 11:21:13.358: epoch 37:	0.02995311  	0.08151957  	0.06726933  
2023-05-10 11:21:13.358: Find a better model.
2023-05-10 11:21:33.126: [iter 38 : loss : 0.6287 = 0.1469 + 0.4772 + 0.0046, time: 19.764844]
2023-05-10 11:21:33.400: epoch 38:	0.02998271  	0.08181118  	0.06737958  
2023-05-10 11:21:33.400: Find a better model.
2023-05-10 11:21:53.229: [iter 39 : loss : 0.6142 = 0.1407 + 0.4687 + 0.0048, time: 19.824672]
2023-05-10 11:21:53.493: epoch 39:	0.03007896  	0.08188232  	0.06737501  
2023-05-10 11:21:53.493: Find a better model.
2023-05-10 11:22:13.085: [iter 40 : loss : 0.6146 = 0.1337 + 0.4760 + 0.0050, time: 19.587457]
2023-05-10 11:22:13.358: epoch 40:	0.03005676  	0.08205825  	0.06756254  
2023-05-10 11:22:13.358: Find a better model.
2023-05-10 11:22:33.043: [iter 41 : loss : 0.6002 = 0.1275 + 0.4675 + 0.0052, time: 19.681138]
2023-05-10 11:22:33.328: epoch 41:	0.03003455  	0.08187116  	0.06750007  
2023-05-10 11:22:53.090: [iter 42 : loss : 0.6026 = 0.1222 + 0.4750 + 0.0054, time: 19.757903]
2023-05-10 11:22:53.362: epoch 42:	0.03010118  	0.08209106  	0.06754857  
2023-05-10 11:22:53.362: Find a better model.
2023-05-10 11:23:13.194: [iter 43 : loss : 0.5889 = 0.1169 + 0.4665 + 0.0055, time: 19.826661]
2023-05-10 11:23:13.461: epoch 43:	0.03015300  	0.08217789  	0.06766064  
2023-05-10 11:23:13.461: Find a better model.
2023-05-10 11:23:32.992: [iter 44 : loss : 0.5936 = 0.1139 + 0.4740 + 0.0057, time: 19.526244]
2023-05-10 11:23:33.251: epoch 44:	0.03010859  	0.08218630  	0.06769146  
2023-05-10 11:23:33.251: Find a better model.
2023-05-10 11:23:52.450: [iter 45 : loss : 0.5805 = 0.1091 + 0.4655 + 0.0059, time: 19.194621]
2023-05-10 11:23:52.710: epoch 45:	0.02998274  	0.08182760  	0.06734852  
2023-05-10 11:24:11.958: [iter 46 : loss : 0.5846 = 0.1053 + 0.4732 + 0.0060, time: 19.244061]
2023-05-10 11:24:12.220: epoch 46:	0.02986429  	0.08162591  	0.06713288  
2023-05-10 11:24:31.458: [iter 47 : loss : 0.5721 = 0.1012 + 0.4648 + 0.0062, time: 19.233422]
2023-05-10 11:24:31.717: epoch 47:	0.02981246  	0.08135932  	0.06719814  
2023-05-10 11:24:50.780: [iter 48 : loss : 0.5771 = 0.0984 + 0.4724 + 0.0063, time: 19.058681]
2023-05-10 11:24:51.047: epoch 48:	0.02983466  	0.08106162  	0.06725857  
2023-05-10 11:25:10.256: [iter 49 : loss : 0.5655 = 0.0950 + 0.4640 + 0.0065, time: 19.205916]
2023-05-10 11:25:10.540: epoch 49:	0.02984946  	0.08092103  	0.06708157  
2023-05-10 11:25:29.510: [iter 50 : loss : 0.5710 = 0.0925 + 0.4719 + 0.0066, time: 18.965145]
2023-05-10 11:25:29.781: epoch 50:	0.02968658  	0.08088278  	0.06698125  
2023-05-10 11:25:49.045: [iter 51 : loss : 0.5593 = 0.0892 + 0.4633 + 0.0068, time: 19.261286]
2023-05-10 11:25:49.313: epoch 51:	0.02968658  	0.08079416  	0.06697989  
2023-05-10 11:26:08.335: [iter 52 : loss : 0.5645 = 0.0865 + 0.4711 + 0.0069, time: 19.018816]
2023-05-10 11:26:08.602: epoch 52:	0.02963476  	0.08056936  	0.06685091  
2023-05-10 11:26:27.646: [iter 53 : loss : 0.5552 = 0.0854 + 0.4628 + 0.0070, time: 19.039988]
2023-05-10 11:26:27.913: epoch 53:	0.02952371  	0.08055621  	0.06664622  
2023-05-10 11:26:47.116: [iter 54 : loss : 0.5604 = 0.0827 + 0.4705 + 0.0072, time: 19.200211]
2023-05-10 11:26:47.385: epoch 54:	0.02964215  	0.08062622  	0.06667428  
2023-05-10 11:27:06.417: [iter 55 : loss : 0.5498 = 0.0803 + 0.4622 + 0.0073, time: 19.028043]
2023-05-10 11:27:06.684: epoch 55:	0.02947188  	0.08031822  	0.06643447  
2023-05-10 11:27:25.876: [iter 56 : loss : 0.5551 = 0.0778 + 0.4699 + 0.0074, time: 19.186550]
2023-05-10 11:27:26.143: epoch 56:	0.02949409  	0.08029304  	0.06641297  
2023-05-10 11:27:45.378: [iter 57 : loss : 0.5452 = 0.0759 + 0.4617 + 0.0075, time: 19.230978]
2023-05-10 11:27:45.649: epoch 57:	0.02947188  	0.08025536  	0.06627764  
2023-05-10 11:28:05.067: [iter 58 : loss : 0.5513 = 0.0741 + 0.4696 + 0.0077, time: 19.413316]
2023-05-10 11:28:05.332: epoch 58:	0.02936083  	0.08021726  	0.06623441  
2023-05-10 11:28:24.962: [iter 59 : loss : 0.5412 = 0.0722 + 0.4613 + 0.0078, time: 19.625447]
2023-05-10 11:28:25.229: epoch 59:	0.02941266  	0.08004509  	0.06621347  
2023-05-10 11:28:44.651: [iter 60 : loss : 0.5477 = 0.0707 + 0.4690 + 0.0079, time: 19.416773]
2023-05-10 11:28:44.917: epoch 60:	0.02931642  	0.08005702  	0.06600496  
2023-05-10 11:29:04.526: [iter 61 : loss : 0.5382 = 0.0691 + 0.4610 + 0.0080, time: 19.605489]
2023-05-10 11:29:04.795: epoch 61:	0.02936825  	0.07996997  	0.06591616  
2023-05-10 11:29:24.208: [iter 62 : loss : 0.5454 = 0.0682 + 0.4690 + 0.0081, time: 19.407193]
2023-05-10 11:29:24.477: epoch 62:	0.02930163  	0.07977439  	0.06586216  
2023-05-10 11:29:44.115: [iter 63 : loss : 0.5356 = 0.0668 + 0.4606 + 0.0082, time: 19.635441]
2023-05-10 11:29:44.381: epoch 63:	0.02926461  	0.07961808  	0.06575269  
2023-05-10 11:30:03.883: [iter 64 : loss : 0.5425 = 0.0655 + 0.4686 + 0.0083, time: 19.497818]
2023-05-10 11:30:04.150: epoch 64:	0.02919057  	0.07958335  	0.06566996  
2023-05-10 11:30:23.710: [iter 65 : loss : 0.5323 = 0.0636 + 0.4603 + 0.0085, time: 19.555619]
2023-05-10 11:30:23.979: epoch 65:	0.02915355  	0.07970308  	0.06565236  
2023-05-10 11:30:43.453: [iter 66 : loss : 0.5395 = 0.0627 + 0.4683 + 0.0086, time: 19.469901]
2023-05-10 11:30:43.719: epoch 66:	0.02907952  	0.07938533  	0.06548601  
2023-05-10 11:31:03.310: [iter 67 : loss : 0.5302 = 0.0616 + 0.4600 + 0.0087, time: 19.586564]
2023-05-10 11:31:03.577: epoch 67:	0.02907212  	0.07917286  	0.06532612  
2023-05-10 11:31:22.997: [iter 68 : loss : 0.5375 = 0.0607 + 0.4681 + 0.0088, time: 19.416151]
2023-05-10 11:31:23.261: epoch 68:	0.02890924  	0.07868351  	0.06496122  
2023-05-10 11:31:42.883: [iter 69 : loss : 0.5279 = 0.0594 + 0.4596 + 0.0089, time: 19.619514]
2023-05-10 11:31:43.150: epoch 69:	0.02874637  	0.07818029  	0.06464232  
2023-05-10 11:31:43.150: Early stopping is trigger at epoch: 69
2023-05-10 11:31:43.150: best_result@epoch 44:

2023-05-10 11:31:43.150: 		0.0301      	0.0822      	0.0677      
2023-05-10 14:15:03.301: my pid: 6676
2023-05-10 14:15:03.301: model: model.general_recommender.SGL
2023-05-10 14:15:03.301: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 14:15:03.301: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 14:15:06.566: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 14:15:27.972: [iter 1 : loss : 1.1334 = 0.6931 + 0.4404 + 0.0000, time: 21.405750]
2023-05-10 14:15:28.239: epoch 1:	0.00133254  	0.00238867  	0.00205833  
2023-05-10 14:15:28.239: Find a better model.
2023-05-10 14:15:49.699: [iter 2 : loss : 1.1344 = 0.6930 + 0.4414 + 0.0000, time: 21.457206]
2023-05-10 14:15:49.985: epoch 2:	0.00222831  	0.00358197  	0.00292823  
2023-05-10 14:15:49.985: Find a better model.
2023-05-10 14:16:11.251: [iter 3 : loss : 1.1325 = 0.6930 + 0.4396 + 0.0000, time: 21.260939]
2023-05-10 14:16:11.532: epoch 3:	0.00277613  	0.00514852  	0.00432436  
2023-05-10 14:16:11.532: Find a better model.
2023-05-10 14:16:32.486: [iter 4 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 20.950679]
2023-05-10 14:16:32.768: epoch 4:	0.00312408  	0.00542006  	0.00468817  
2023-05-10 14:16:32.769: Find a better model.
2023-05-10 14:16:54.054: [iter 5 : loss : 1.1327 = 0.6928 + 0.4399 + 0.0000, time: 21.281455]
2023-05-10 14:16:54.333: epoch 5:	0.00367190  	0.00738352  	0.00620291  
2023-05-10 14:16:54.333: Find a better model.
2023-05-10 14:17:15.449: [iter 6 : loss : 1.1351 = 0.6927 + 0.4424 + 0.0000, time: 21.112926]
2023-05-10 14:17:15.729: epoch 6:	0.00415309  	0.00713159  	0.00629417  
2023-05-10 14:17:36.839: [iter 7 : loss : 1.1327 = 0.6925 + 0.4402 + 0.0000, time: 21.106475]
2023-05-10 14:17:37.131: epoch 7:	0.00484157  	0.00965185  	0.00808930  
2023-05-10 14:17:37.131: Find a better model.
2023-05-10 14:17:58.079: [iter 8 : loss : 1.1354 = 0.6923 + 0.4430 + 0.0000, time: 20.945618]
2023-05-10 14:17:58.357: epoch 8:	0.00560407  	0.01085903  	0.00930253  
2023-05-10 14:17:58.357: Find a better model.
2023-05-10 14:18:19.400: [iter 9 : loss : 1.1325 = 0.6920 + 0.4405 + 0.0000, time: 21.039148]
2023-05-10 14:18:19.676: epoch 9:	0.00672192  	0.01459558  	0.01258298  
2023-05-10 14:18:19.676: Find a better model.
2023-05-10 14:18:40.668: [iter 10 : loss : 1.1356 = 0.6916 + 0.4440 + 0.0000, time: 20.988736]
2023-05-10 14:18:40.943: epoch 10:	0.00741039  	0.01437088  	0.01320903  
2023-05-10 14:19:02.028: [iter 11 : loss : 1.1319 = 0.6911 + 0.4408 + 0.0000, time: 21.081199]
2023-05-10 14:19:02.316: epoch 11:	0.00846901  	0.01977865  	0.01671003  
2023-05-10 14:19:02.316: Find a better model.
2023-05-10 14:19:23.209: [iter 12 : loss : 1.1353 = 0.6902 + 0.4451 + 0.0000, time: 20.889323]
2023-05-10 14:19:23.485: epoch 12:	0.00959426  	0.02123174  	0.01897126  
2023-05-10 14:19:23.485: Find a better model.
2023-05-10 14:19:46.794: [iter 13 : loss : 1.1300 = 0.6887 + 0.4412 + 0.0000, time: 23.305663]
2023-05-10 14:19:47.050: epoch 13:	0.01048265  	0.02450534  	0.02188199  
2023-05-10 14:19:47.050: Find a better model.
2023-05-10 14:20:07.816: [iter 14 : loss : 1.1333 = 0.6863 + 0.4470 + 0.0000, time: 20.762080]
2023-05-10 14:20:08.074: epoch 14:	0.01283683  	0.02918571  	0.02627990  
2023-05-10 14:20:08.074: Find a better model.
2023-05-10 14:20:28.966: [iter 15 : loss : 1.1248 = 0.6830 + 0.4418 + 0.0001, time: 20.888160]
2023-05-10 14:20:29.223: epoch 15:	0.01456918  	0.03613860  	0.03152433  
2023-05-10 14:20:29.223: Find a better model.
2023-05-10 14:20:50.172: [iter 16 : loss : 1.1269 = 0.6772 + 0.4496 + 0.0001, time: 20.944165]
2023-05-10 14:20:50.437: epoch 16:	0.01797463  	0.04469442  	0.03959174  
2023-05-10 14:20:50.438: Find a better model.
2023-05-10 14:21:11.345: [iter 17 : loss : 1.1107 = 0.6679 + 0.4427 + 0.0001, time: 20.904154]
2023-05-10 14:21:11.602: epoch 17:	0.02040290  	0.05255302  	0.04570975  
2023-05-10 14:21:11.602: Find a better model.
2023-05-10 14:21:32.348: [iter 18 : loss : 1.1046 = 0.6515 + 0.4529 + 0.0002, time: 20.742582]
2023-05-10 14:21:32.606: epoch 18:	0.02325315  	0.06050929  	0.05247523  
2023-05-10 14:21:32.606: Find a better model.
2023-05-10 14:21:53.540: [iter 19 : loss : 1.0717 = 0.6263 + 0.4450 + 0.0003, time: 20.931262]
2023-05-10 14:21:53.798: epoch 19:	0.02551855  	0.06690495  	0.05752153  
2023-05-10 14:21:53.798: Find a better model.
2023-05-10 14:22:14.367: [iter 20 : loss : 1.0484 = 0.5905 + 0.4574 + 0.0005, time: 20.565551]
2023-05-10 14:22:14.624: epoch 20:	0.02691775  	0.07165802  	0.06096304  
2023-05-10 14:22:14.624: Find a better model.
2023-05-10 14:22:35.510: [iter 21 : loss : 0.9955 = 0.5450 + 0.4497 + 0.0007, time: 20.883106]
2023-05-10 14:22:35.766: epoch 21:	0.02785058  	0.07447643  	0.06287848  
2023-05-10 14:22:35.766: Find a better model.
2023-05-10 14:22:56.346: [iter 22 : loss : 0.9580 = 0.4937 + 0.4633 + 0.0010, time: 20.576520]
2023-05-10 14:22:56.601: epoch 22:	0.02829478  	0.07609375  	0.06360670  
2023-05-10 14:22:56.601: Find a better model.
2023-05-10 14:23:17.319: [iter 23 : loss : 0.8990 = 0.4420 + 0.4557 + 0.0013, time: 20.714684]
2023-05-10 14:23:17.571: epoch 23:	0.02895367  	0.07831584  	0.06467014  
2023-05-10 14:23:17.571: Find a better model.
2023-05-10 14:23:38.302: [iter 24 : loss : 0.8646 = 0.3944 + 0.4687 + 0.0016, time: 20.726824]
2023-05-10 14:23:38.552: epoch 24:	0.02908693  	0.07905245  	0.06478943  
2023-05-10 14:23:38.552: Find a better model.
2023-05-10 14:23:59.346: [iter 25 : loss : 0.8135 = 0.3517 + 0.4600 + 0.0019, time: 20.788102]
2023-05-10 14:23:59.596: epoch 25:	0.02947190  	0.08012220  	0.06519239  
2023-05-10 14:23:59.596: Find a better model.
2023-05-10 14:24:20.313: [iter 26 : loss : 0.7902 = 0.3166 + 0.4714 + 0.0022, time: 20.713695]
2023-05-10 14:24:20.567: epoch 26:	0.02937566  	0.08022706  	0.06542431  
2023-05-10 14:24:20.568: Find a better model.
2023-05-10 14:24:41.307: [iter 27 : loss : 0.7502 = 0.2861 + 0.4616 + 0.0024, time: 20.735450]
2023-05-10 14:24:41.554: epoch 27:	0.02962738  	0.08160444  	0.06600697  
2023-05-10 14:24:41.555: Find a better model.
2023-05-10 14:25:02.294: [iter 28 : loss : 0.7357 = 0.2611 + 0.4719 + 0.0027, time: 20.735580]
2023-05-10 14:25:02.545: epoch 28:	0.02969400  	0.08165988  	0.06607299  
2023-05-10 14:25:02.546: Find a better model.
2023-05-10 14:25:23.287: [iter 29 : loss : 0.7044 = 0.2400 + 0.4614 + 0.0030, time: 20.737647]
2023-05-10 14:25:23.537: epoch 29:	0.02984947  	0.08214773  	0.06648140  
2023-05-10 14:25:23.537: Find a better model.
2023-05-10 14:25:44.253: [iter 30 : loss : 0.6959 = 0.2214 + 0.4713 + 0.0032, time: 20.712601]
2023-05-10 14:25:44.503: epoch 30:	0.02979024  	0.08203210  	0.06668039  
2023-05-10 14:26:05.275: [iter 31 : loss : 0.6696 = 0.2056 + 0.4605 + 0.0035, time: 20.767943]
2023-05-10 14:26:05.526: epoch 31:	0.02996053  	0.08282589  	0.06706075  
2023-05-10 14:26:05.526: Find a better model.
2023-05-10 14:26:26.089: [iter 32 : loss : 0.6658 = 0.1920 + 0.4701 + 0.0037, time: 20.559463]
2023-05-10 14:26:26.352: epoch 32:	0.02998273  	0.08251075  	0.06716446  
2023-05-10 14:26:47.250: [iter 33 : loss : 0.6438 = 0.1806 + 0.4593 + 0.0039, time: 20.893562]
2023-05-10 14:26:47.500: epoch 33:	0.02996792  	0.08277628  	0.06726126  
2023-05-10 14:27:08.452: [iter 34 : loss : 0.6428 = 0.1701 + 0.4686 + 0.0041, time: 20.949290]
2023-05-10 14:27:08.705: epoch 34:	0.03002714  	0.08270309  	0.06732446  
2023-05-10 14:27:29.634: [iter 35 : loss : 0.6223 = 0.1600 + 0.4579 + 0.0044, time: 20.925660]
2023-05-10 14:27:29.881: epoch 35:	0.03004195  	0.08268587  	0.06735017  
2023-05-10 14:27:50.828: [iter 36 : loss : 0.6240 = 0.1522 + 0.4673 + 0.0045, time: 20.942110]
2023-05-10 14:27:51.075: epoch 36:	0.02999753  	0.08251281  	0.06719369  
2023-05-10 14:28:12.234: [iter 37 : loss : 0.6062 = 0.1447 + 0.4567 + 0.0047, time: 21.153449]
2023-05-10 14:28:12.488: epoch 37:	0.03001233  	0.08279818  	0.06730154  
2023-05-10 14:28:33.419: [iter 38 : loss : 0.6087 = 0.1376 + 0.4662 + 0.0049, time: 20.925958]
2023-05-10 14:28:33.668: epoch 38:	0.02997531  	0.08264442  	0.06725933  
2023-05-10 14:28:54.792: [iter 39 : loss : 0.5925 = 0.1318 + 0.4555 + 0.0051, time: 21.120425]
2023-05-10 14:28:55.044: epoch 39:	0.02991609  	0.08259431  	0.06729564  
2023-05-10 14:29:16.015: [iter 40 : loss : 0.5962 = 0.1258 + 0.4650 + 0.0053, time: 20.967350]
2023-05-10 14:29:16.264: epoch 40:	0.02993830  	0.08234012  	0.06727990  
2023-05-10 14:29:37.215: [iter 41 : loss : 0.5802 = 0.1202 + 0.4545 + 0.0055, time: 20.946440]
2023-05-10 14:29:37.468: epoch 41:	0.02992349  	0.08219237  	0.06722655  
2023-05-10 14:29:58.571: [iter 42 : loss : 0.5853 = 0.1154 + 0.4643 + 0.0056, time: 21.099508]
2023-05-10 14:29:58.821: epoch 42:	0.02990129  	0.08217239  	0.06732056  
2023-05-10 14:30:19.763: [iter 43 : loss : 0.5707 = 0.1112 + 0.4537 + 0.0058, time: 20.937430]
2023-05-10 14:30:20.014: epoch 43:	0.02993089  	0.08191707  	0.06734870  
2023-05-10 14:30:40.769: [iter 44 : loss : 0.5773 = 0.1080 + 0.4633 + 0.0060, time: 20.751187]
2023-05-10 14:30:41.018: epoch 44:	0.02986426  	0.08150069  	0.06720997  
2023-05-10 14:31:01.958: [iter 45 : loss : 0.5625 = 0.1035 + 0.4528 + 0.0061, time: 20.936923]
2023-05-10 14:31:02.206: epoch 45:	0.02976061  	0.08099807  	0.06713110  
2023-05-10 14:31:23.143: [iter 46 : loss : 0.5692 = 0.1002 + 0.4627 + 0.0063, time: 20.934073]
2023-05-10 14:31:23.400: epoch 46:	0.02964216  	0.08091141  	0.06693371  
2023-05-10 14:31:44.333: [iter 47 : loss : 0.5553 = 0.0968 + 0.4520 + 0.0064, time: 20.928862]
2023-05-10 14:31:44.581: epoch 47:	0.02965695  	0.08074072  	0.06689671  
2023-05-10 14:32:05.531: [iter 48 : loss : 0.5622 = 0.0937 + 0.4619 + 0.0066, time: 20.946113]
2023-05-10 14:32:05.780: epoch 48:	0.02967917  	0.08083660  	0.06700792  
2023-05-10 14:32:26.726: [iter 49 : loss : 0.5492 = 0.0910 + 0.4514 + 0.0067, time: 20.941142]
2023-05-10 14:32:26.974: epoch 49:	0.02963475  	0.08060330  	0.06698155  
2023-05-10 14:32:47.775: [iter 50 : loss : 0.5566 = 0.0883 + 0.4614 + 0.0069, time: 20.796392]
2023-05-10 14:32:48.025: epoch 50:	0.02960514  	0.08008332  	0.06676377  
2023-05-10 14:33:08.949: [iter 51 : loss : 0.5434 = 0.0856 + 0.4508 + 0.0070, time: 20.919620]
2023-05-10 14:33:09.198: epoch 51:	0.02953111  	0.08010211  	0.06672197  
2023-05-10 14:33:30.116: [iter 52 : loss : 0.5511 = 0.0831 + 0.4609 + 0.0071, time: 20.914007]
2023-05-10 14:33:30.377: epoch 52:	0.02944227  	0.07950339  	0.06649692  
2023-05-10 14:33:51.322: [iter 53 : loss : 0.5394 = 0.0818 + 0.4503 + 0.0073, time: 20.940796]
2023-05-10 14:33:51.571: epoch 53:	0.02922018  	0.07921806  	0.06627651  
2023-05-10 14:34:12.311: [iter 54 : loss : 0.5470 = 0.0793 + 0.4602 + 0.0074, time: 20.736110]
2023-05-10 14:34:12.558: epoch 54:	0.02924239  	0.07917202  	0.06619911  
2023-05-10 14:34:33.510: [iter 55 : loss : 0.5347 = 0.0773 + 0.4498 + 0.0075, time: 20.949009]
2023-05-10 14:34:33.761: epoch 55:	0.02919055  	0.07900319  	0.06610262  
2023-05-10 14:34:54.496: [iter 56 : loss : 0.5425 = 0.0750 + 0.4598 + 0.0077, time: 20.731326]
2023-05-10 14:34:54.746: epoch 56:	0.02902768  	0.07863857  	0.06586600  
2023-05-10 14:34:54.746: Early stopping is trigger at epoch: 56
2023-05-10 14:34:54.746: best_result@epoch 31:

2023-05-10 14:34:54.746: 		0.0300      	0.0828      	0.0671      
2023-05-10 19:57:09.217: my pid: 15868
2023-05-10 19:57:09.217: model: model.general_recommender.SGL
2023-05-10 19:57:09.217: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 19:57:09.217: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 19:57:12.518: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 19:57:34.520: [iter 1 : loss : 1.1328 = 0.6931 + 0.4397 + 0.0000, time: 22.001168]
2023-05-10 19:57:34.785: epoch 1:	0.00136216  	0.00261748  	0.00224492  
2023-05-10 19:57:34.785: Find a better model.
2023-05-10 19:57:56.479: [iter 2 : loss : 1.1340 = 0.6930 + 0.4409 + 0.0000, time: 21.691398]
2023-05-10 19:57:56.766: epoch 2:	0.00229494  	0.00394953  	0.00355604  
2023-05-10 19:57:56.766: Find a better model.
2023-05-10 19:58:18.886: [iter 3 : loss : 1.1315 = 0.6930 + 0.4385 + 0.0000, time: 22.115197]
2023-05-10 19:58:19.170: epoch 3:	0.00273912  	0.00518977  	0.00436528  
2023-05-10 19:58:19.170: Find a better model.
2023-05-10 19:58:41.025: [iter 4 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.851172]
2023-05-10 19:58:41.314: epoch 4:	0.00313888  	0.00535867  	0.00480147  
2023-05-10 19:58:41.314: Find a better model.
2023-05-10 19:59:03.080: [iter 5 : loss : 1.1315 = 0.6928 + 0.4386 + 0.0000, time: 21.761711]
2023-05-10 19:59:03.362: epoch 5:	0.00357566  	0.00723602  	0.00603563  
2023-05-10 19:59:03.363: Find a better model.
2023-05-10 19:59:25.212: [iter 6 : loss : 1.1343 = 0.6927 + 0.4415 + 0.0000, time: 21.845977]
2023-05-10 19:59:25.505: epoch 6:	0.00417530  	0.00693176  	0.00620835  
2023-05-10 19:59:47.477: [iter 7 : loss : 1.1314 = 0.6926 + 0.4388 + 0.0000, time: 21.966955]
2023-05-10 19:59:47.761: epoch 7:	0.00495261  	0.00974878  	0.00828996  
2023-05-10 19:59:47.761: Find a better model.
2023-05-10 20:00:09.629: [iter 8 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 21.864760]
2023-05-10 20:00:09.912: epoch 8:	0.00572992  	0.01047977  	0.00930929  
2023-05-10 20:00:09.912: Find a better model.
2023-05-10 20:00:31.853: [iter 9 : loss : 1.1311 = 0.6922 + 0.4389 + 0.0000, time: 21.937476]
2023-05-10 20:00:32.134: epoch 9:	0.00638138  	0.01382657  	0.01154664  
2023-05-10 20:00:32.134: Find a better model.
2023-05-10 20:00:53.795: [iter 10 : loss : 1.1347 = 0.6919 + 0.4429 + 0.0000, time: 21.657166]
2023-05-10 20:00:54.079: epoch 10:	0.00768430  	0.01594575  	0.01353488  
2023-05-10 20:00:54.079: Find a better model.
2023-05-10 20:01:16.005: [iter 11 : loss : 1.1305 = 0.6914 + 0.4391 + 0.0000, time: 21.923333]
2023-05-10 20:01:16.273: epoch 11:	0.00859485  	0.02083457  	0.01682930  
2023-05-10 20:01:16.273: Find a better model.
2023-05-10 20:01:37.954: [iter 12 : loss : 1.1345 = 0.6907 + 0.4438 + 0.0000, time: 21.675555]
2023-05-10 20:01:38.236: epoch 12:	0.01025315  	0.02257158  	0.01969617  
2023-05-10 20:01:38.236: Find a better model.
2023-05-10 20:02:00.231: [iter 13 : loss : 1.1289 = 0.6896 + 0.4393 + 0.0000, time: 21.992443]
2023-05-10 20:02:00.512: epoch 13:	0.01098606  	0.02744655  	0.02287450  
2023-05-10 20:02:00.512: Find a better model.
2023-05-10 20:02:22.359: [iter 14 : loss : 1.1330 = 0.6875 + 0.4455 + 0.0000, time: 21.841952]
2023-05-10 20:02:22.638: epoch 14:	0.01322180  	0.03117126  	0.02702665  
2023-05-10 20:02:22.638: Find a better model.
2023-05-10 20:02:44.775: [iter 15 : loss : 1.1241 = 0.6844 + 0.4396 + 0.0001, time: 22.132429]
2023-05-10 20:02:45.059: epoch 15:	0.01407317  	0.03633801  	0.03099848  
2023-05-10 20:02:45.059: Find a better model.
2023-05-10 20:03:06.959: [iter 16 : loss : 1.1271 = 0.6791 + 0.4480 + 0.0001, time: 21.894795]
2023-05-10 20:03:07.235: epoch 16:	0.01750824  	0.04327185  	0.03822210  
2023-05-10 20:03:07.236: Find a better model.
2023-05-10 20:03:29.359: [iter 17 : loss : 1.1112 = 0.6706 + 0.4404 + 0.0001, time: 22.120189]
2023-05-10 20:03:29.635: epoch 17:	0.01918136  	0.04999315  	0.04342072  
2023-05-10 20:03:29.636: Find a better model.
2023-05-10 20:03:51.517: [iter 18 : loss : 1.1064 = 0.6551 + 0.4511 + 0.0002, time: 21.877834]
2023-05-10 20:03:51.796: epoch 18:	0.02253506  	0.05757657  	0.05023142  
2023-05-10 20:03:51.796: Find a better model.
2023-05-10 20:04:13.947: [iter 19 : loss : 1.0737 = 0.6309 + 0.4425 + 0.0003, time: 22.147151]
2023-05-10 20:04:14.227: epoch 19:	0.02442288  	0.06385564  	0.05529982  
2023-05-10 20:04:14.227: Find a better model.
2023-05-10 20:04:36.284: [iter 20 : loss : 1.0511 = 0.5953 + 0.4553 + 0.0005, time: 22.053216]
2023-05-10 20:04:36.566: epoch 20:	0.02617006  	0.06898074  	0.05937809  
2023-05-10 20:04:36.566: Find a better model.
2023-05-10 20:04:58.540: [iter 21 : loss : 0.9977 = 0.5498 + 0.4472 + 0.0007, time: 21.968508]
2023-05-10 20:04:58.819: epoch 21:	0.02725833  	0.07250699  	0.06180122  
2023-05-10 20:04:58.819: Find a better model.
2023-05-10 20:05:20.677: [iter 22 : loss : 0.9595 = 0.4978 + 0.4608 + 0.0009, time: 21.854882]
2023-05-10 20:05:20.954: epoch 22:	0.02808750  	0.07490510  	0.06339718  
2023-05-10 20:05:20.954: Find a better model.
2023-05-10 20:05:42.942: [iter 23 : loss : 0.8997 = 0.4452 + 0.4532 + 0.0012, time: 21.985462]
2023-05-10 20:05:43.217: epoch 23:	0.02859832  	0.07640113  	0.06418072  
2023-05-10 20:05:43.217: Find a better model.
2023-05-10 20:06:05.065: [iter 24 : loss : 0.8641 = 0.3967 + 0.4659 + 0.0015, time: 21.843935]
2023-05-10 20:06:05.339: epoch 24:	0.02867236  	0.07714865  	0.06441935  
2023-05-10 20:06:05.339: Find a better model.
2023-05-10 20:06:27.319: [iter 25 : loss : 0.8128 = 0.3534 + 0.4576 + 0.0018, time: 21.977066]
2023-05-10 20:06:27.590: epoch 25:	0.02891666  	0.07779291  	0.06492593  
2023-05-10 20:06:27.590: Find a better model.
2023-05-10 20:06:49.462: [iter 26 : loss : 0.7884 = 0.3175 + 0.4687 + 0.0021, time: 21.867872]
2023-05-10 20:06:49.735: epoch 26:	0.02914617  	0.07876735  	0.06526705  
2023-05-10 20:06:49.735: Find a better model.
2023-05-10 20:07:11.705: [iter 27 : loss : 0.7485 = 0.2869 + 0.4592 + 0.0024, time: 21.965874]
2023-05-10 20:07:11.977: epoch 27:	0.02933125  	0.07964789  	0.06551699  
2023-05-10 20:07:11.977: Find a better model.
2023-05-10 20:07:33.834: [iter 28 : loss : 0.7332 = 0.2613 + 0.4692 + 0.0027, time: 21.853884]
2023-05-10 20:07:34.106: epoch 28:	0.02936087  	0.07969677  	0.06560883  
2023-05-10 20:07:34.106: Find a better model.
2023-05-10 20:07:56.281: [iter 29 : loss : 0.7022 = 0.2401 + 0.4591 + 0.0030, time: 22.170883]
2023-05-10 20:07:56.553: epoch 29:	0.02948672  	0.07995138  	0.06581727  
2023-05-10 20:07:56.553: Find a better model.
2023-05-10 20:08:18.409: [iter 30 : loss : 0.6931 = 0.2213 + 0.4686 + 0.0032, time: 21.852945]
2023-05-10 20:08:18.685: epoch 30:	0.02942749  	0.07998867  	0.06580995  
2023-05-10 20:08:18.685: Find a better model.
2023-05-10 20:08:40.662: [iter 31 : loss : 0.6672 = 0.2054 + 0.4583 + 0.0035, time: 21.972062]
2023-05-10 20:08:40.932: epoch 31:	0.02951634  	0.08008247  	0.06602027  
2023-05-10 20:08:40.933: Find a better model.
2023-05-10 20:09:02.994: [iter 32 : loss : 0.6632 = 0.1921 + 0.4674 + 0.0037, time: 22.058746]
2023-05-10 20:09:03.267: epoch 32:	0.02944230  	0.08000758  	0.06589679  
2023-05-10 20:09:25.436: [iter 33 : loss : 0.6419 = 0.1809 + 0.4571 + 0.0039, time: 22.165859]
2023-05-10 20:09:25.711: epoch 33:	0.02955336  	0.08010054  	0.06615264  
2023-05-10 20:09:25.711: Find a better model.
2023-05-10 20:09:47.796: [iter 34 : loss : 0.6402 = 0.1700 + 0.4661 + 0.0041, time: 22.081148]
2023-05-10 20:09:48.069: epoch 34:	0.02950895  	0.08005407  	0.06618798  
2023-05-10 20:10:10.430: [iter 35 : loss : 0.6200 = 0.1600 + 0.4557 + 0.0043, time: 22.356192]
2023-05-10 20:10:10.705: epoch 35:	0.02963480  	0.08011277  	0.06620166  
2023-05-10 20:10:10.706: Find a better model.
2023-05-10 20:10:32.776: [iter 36 : loss : 0.6216 = 0.1521 + 0.4650 + 0.0045, time: 22.067177]
2023-05-10 20:10:33.051: epoch 36:	0.02959778  	0.08027502  	0.06638825  
2023-05-10 20:10:33.051: Find a better model.
2023-05-10 20:10:55.419: [iter 37 : loss : 0.6040 = 0.1446 + 0.4546 + 0.0047, time: 22.365194]
2023-05-10 20:10:55.692: epoch 37:	0.02958298  	0.08005130  	0.06639989  
2023-05-10 20:11:17.749: [iter 38 : loss : 0.6067 = 0.1380 + 0.4638 + 0.0049, time: 22.051579]
2023-05-10 20:11:18.020: epoch 38:	0.02951635  	0.07994194  	0.06651708  
2023-05-10 20:11:40.028: [iter 39 : loss : 0.5902 = 0.1317 + 0.4534 + 0.0051, time: 22.004377]
2023-05-10 20:11:40.297: epoch 39:	0.02962000  	0.08039632  	0.06676446  
2023-05-10 20:11:40.297: Find a better model.
2023-05-10 20:12:02.346: [iter 40 : loss : 0.5938 = 0.1259 + 0.4627 + 0.0053, time: 22.044804]
2023-05-10 20:12:02.618: epoch 40:	0.02958298  	0.08031598  	0.06664919  
2023-05-10 20:12:24.981: [iter 41 : loss : 0.5780 = 0.1201 + 0.4524 + 0.0055, time: 22.358221]
2023-05-10 20:12:25.253: epoch 41:	0.02964960  	0.08016120  	0.06653482  
2023-05-10 20:12:47.532: [iter 42 : loss : 0.5829 = 0.1153 + 0.4619 + 0.0056, time: 22.275491]
2023-05-10 20:12:47.804: epoch 42:	0.02953116  	0.08003505  	0.06657030  
2023-05-10 20:13:10.192: [iter 43 : loss : 0.5685 = 0.1111 + 0.4516 + 0.0058, time: 22.384155]
2023-05-10 20:13:10.465: epoch 43:	0.02950896  	0.07958598  	0.06626292  
2023-05-10 20:13:32.709: [iter 44 : loss : 0.5753 = 0.1082 + 0.4611 + 0.0060, time: 22.240594]
2023-05-10 20:13:32.982: epoch 44:	0.02957559  	0.07987325  	0.06644189  
2023-05-10 20:13:55.546: [iter 45 : loss : 0.5605 = 0.1036 + 0.4507 + 0.0061, time: 22.559748]
2023-05-10 20:13:55.816: epoch 45:	0.02946454  	0.07962651  	0.06625485  
2023-05-10 20:14:17.900: [iter 46 : loss : 0.5670 = 0.1002 + 0.4605 + 0.0063, time: 22.080169]
2023-05-10 20:14:18.171: epoch 46:	0.02939791  	0.07931726  	0.06609964  
2023-05-10 20:14:40.337: [iter 47 : loss : 0.5531 = 0.0967 + 0.4500 + 0.0064, time: 22.162040]
2023-05-10 20:14:40.609: epoch 47:	0.02936830  	0.07899456  	0.06613693  
2023-05-10 20:15:02.906: [iter 48 : loss : 0.5603 = 0.0939 + 0.4598 + 0.0066, time: 22.292462]
2023-05-10 20:15:03.175: epoch 48:	0.02941271  	0.07896426  	0.06615508  
2023-05-10 20:15:25.519: [iter 49 : loss : 0.5471 = 0.0910 + 0.4494 + 0.0067, time: 22.340310]
2023-05-10 20:15:25.789: epoch 49:	0.02935348  	0.07873271  	0.06600853  
2023-05-10 20:15:47.888: [iter 50 : loss : 0.5546 = 0.0885 + 0.4592 + 0.0069, time: 22.095178]
2023-05-10 20:15:48.158: epoch 50:	0.02921281  	0.07819689  	0.06576020  
2023-05-10 20:16:10.714: [iter 51 : loss : 0.5413 = 0.0855 + 0.4488 + 0.0070, time: 22.551638]
2023-05-10 20:16:10.986: epoch 51:	0.02927944  	0.07855368  	0.06581981  
2023-05-10 20:16:33.280: [iter 52 : loss : 0.5488 = 0.0830 + 0.4587 + 0.0071, time: 22.290452]
2023-05-10 20:16:33.553: epoch 52:	0.02917581  	0.07790825  	0.06574904  
2023-05-10 20:16:55.887: [iter 53 : loss : 0.5371 = 0.0816 + 0.4483 + 0.0073, time: 22.329815]
2023-05-10 20:16:56.158: epoch 53:	0.02921282  	0.07763068  	0.06567482  
2023-05-10 20:17:18.632: [iter 54 : loss : 0.5448 = 0.0793 + 0.4581 + 0.0074, time: 22.471846]
2023-05-10 20:17:18.908: epoch 54:	0.02927945  	0.07795259  	0.06573160  
2023-05-10 20:17:41.463: [iter 55 : loss : 0.5324 = 0.0771 + 0.4477 + 0.0075, time: 22.551556]
2023-05-10 20:17:41.733: epoch 55:	0.02915359  	0.07780506  	0.06565765  
2023-05-10 20:18:03.833: [iter 56 : loss : 0.5402 = 0.0749 + 0.4577 + 0.0077, time: 22.097093]
2023-05-10 20:18:04.101: epoch 56:	0.02913879  	0.07728773  	0.06549094  
2023-05-10 20:18:26.655: [iter 57 : loss : 0.5280 = 0.0729 + 0.4473 + 0.0078, time: 22.548571]
2023-05-10 20:18:26.927: epoch 57:	0.02906476  	0.07720684  	0.06537150  
2023-05-10 20:18:49.199: [iter 58 : loss : 0.5369 = 0.0715 + 0.4575 + 0.0079, time: 22.269515]
2023-05-10 20:18:49.470: epoch 58:	0.02906476  	0.07725034  	0.06528861  
2023-05-10 20:19:11.879: [iter 59 : loss : 0.5251 = 0.0701 + 0.4470 + 0.0080, time: 22.405101]
2023-05-10 20:19:12.151: epoch 59:	0.02902774  	0.07694359  	0.06522666  
2023-05-10 20:19:34.586: [iter 60 : loss : 0.5336 = 0.0684 + 0.4571 + 0.0081, time: 22.430988]
2023-05-10 20:19:34.858: epoch 60:	0.02895371  	0.07676517  	0.06496303  
2023-05-10 20:19:57.257: [iter 61 : loss : 0.5220 = 0.0672 + 0.4466 + 0.0083, time: 22.396491]
2023-05-10 20:19:57.525: epoch 61:	0.02887227  	0.07648360  	0.06479044  
2023-05-10 20:20:19.764: [iter 62 : loss : 0.5309 = 0.0657 + 0.4568 + 0.0084, time: 22.234147]
2023-05-10 20:20:20.034: epoch 62:	0.02883525  	0.07617451  	0.06473656  
2023-05-10 20:20:42.615: [iter 63 : loss : 0.5192 = 0.0645 + 0.4462 + 0.0085, time: 22.571503]
2023-05-10 20:20:42.886: epoch 63:	0.02887966  	0.07623567  	0.06479307  
2023-05-10 20:21:05.179: [iter 64 : loss : 0.5284 = 0.0633 + 0.4565 + 0.0086, time: 22.289463]
2023-05-10 20:21:05.450: epoch 64:	0.02888706  	0.07628699  	0.06487141  
2023-05-10 20:21:05.450: Early stopping is trigger at epoch: 64
2023-05-10 20:21:05.450: best_result@epoch 39:

2023-05-10 20:21:05.450: 		0.0296      	0.0804      	0.0668      
2023-05-10 20:40:38.927: my pid: 12812
2023-05-10 20:40:38.927: model: model.general_recommender.SGL
2023-05-10 20:40:38.927: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 20:40:38.927: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 20:40:42.194: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 20:41:04.643: [iter 1 : loss : 1.1319 = 0.6931 + 0.4388 + 0.0000, time: 22.448239]
2023-05-10 20:41:04.916: epoch 1:	0.00193219  	0.00371142  	0.00325918  
2023-05-10 20:41:04.916: Find a better model.
2023-05-10 20:41:27.766: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 22.845504]
2023-05-10 20:41:28.054: epoch 2:	0.00204324  	0.00374709  	0.00321702  
2023-05-10 20:41:28.054: Find a better model.
2023-05-10 20:41:51.027: [iter 3 : loss : 1.1306 = 0.6930 + 0.4376 + 0.0000, time: 22.970206]
2023-05-10 20:41:51.315: epoch 3:	0.00259106  	0.00504820  	0.00442877  
2023-05-10 20:41:51.315: Find a better model.
2023-05-10 20:42:13.962: [iter 4 : loss : 1.1336 = 0.6929 + 0.4407 + 0.0000, time: 22.642838]
2023-05-10 20:42:14.239: epoch 4:	0.00263548  	0.00432770  	0.00419101  
2023-05-10 20:42:37.032: [iter 5 : loss : 1.1304 = 0.6929 + 0.4376 + 0.0000, time: 22.789138]
2023-05-10 20:42:37.320: epoch 5:	0.00326473  	0.00703408  	0.00532811  
2023-05-10 20:42:37.320: Find a better model.
2023-05-10 20:42:59.797: [iter 6 : loss : 1.1338 = 0.6928 + 0.4410 + 0.0000, time: 22.474077]
2023-05-10 20:43:00.091: epoch 6:	0.00355345  	0.00623912  	0.00566550  
2023-05-10 20:43:22.809: [iter 7 : loss : 1.1303 = 0.6927 + 0.4376 + 0.0000, time: 22.713630]
2023-05-10 20:43:23.093: epoch 7:	0.00443440  	0.00951212  	0.00773174  
2023-05-10 20:43:23.093: Find a better model.
2023-05-10 20:43:45.572: [iter 8 : loss : 1.1340 = 0.6925 + 0.4415 + 0.0000, time: 22.476423]
2023-05-10 20:43:45.852: epoch 8:	0.00515249  	0.00950899  	0.00875610  
2023-05-10 20:44:08.408: [iter 9 : loss : 1.1299 = 0.6923 + 0.4376 + 0.0000, time: 22.550956]
2023-05-10 20:44:08.719: epoch 9:	0.00607785  	0.01378075  	0.01122088  
2023-05-10 20:44:08.720: Find a better model.
2023-05-10 20:44:30.914: [iter 10 : loss : 1.1341 = 0.6920 + 0.4421 + 0.0000, time: 22.190131]
2023-05-10 20:44:31.203: epoch 10:	0.00719570  	0.01448336  	0.01318321  
2023-05-10 20:44:31.203: Find a better model.
2023-05-10 20:44:53.604: [iter 11 : loss : 1.1293 = 0.6917 + 0.4376 + 0.0000, time: 22.397560]
2023-05-10 20:44:53.895: epoch 11:	0.00784716  	0.01958814  	0.01582564  
2023-05-10 20:44:53.895: Find a better model.
2023-05-10 20:45:16.147: [iter 12 : loss : 1.1340 = 0.6910 + 0.4429 + 0.0000, time: 22.249146]
2023-05-10 20:45:16.426: epoch 12:	0.00992741  	0.02319122  	0.01932519  
2023-05-10 20:45:16.426: Find a better model.
2023-05-10 20:45:38.763: [iter 13 : loss : 1.1280 = 0.6902 + 0.4377 + 0.0000, time: 22.332203]
2023-05-10 20:45:39.046: epoch 13:	0.01068994  	0.02860584  	0.02281144  
2023-05-10 20:45:39.046: Find a better model.
2023-05-10 20:46:01.299: [iter 14 : loss : 1.1329 = 0.6886 + 0.4443 + 0.0000, time: 22.249564]
2023-05-10 20:46:01.582: epoch 14:	0.01340688  	0.03359057  	0.02796745  
2023-05-10 20:46:01.582: Find a better model.
2023-05-10 20:46:24.152: [iter 15 : loss : 1.1241 = 0.6861 + 0.4380 + 0.0000, time: 22.564554]
2023-05-10 20:46:24.434: epoch 15:	0.01310336  	0.03484108  	0.02913822  
2023-05-10 20:46:24.434: Find a better model.
2023-05-10 20:46:46.689: [iter 16 : loss : 1.1279 = 0.6812 + 0.4466 + 0.0001, time: 22.252048]
2023-05-10 20:46:46.959: epoch 16:	0.01684937  	0.04152375  	0.03649815  
2023-05-10 20:46:46.959: Find a better model.
2023-05-10 20:47:09.324: [iter 17 : loss : 1.1126 = 0.6739 + 0.4386 + 0.0001, time: 22.362202]
2023-05-10 20:47:09.608: epoch 17:	0.01765632  	0.04690685  	0.04014226  
2023-05-10 20:47:09.608: Find a better model.
2023-05-10 20:47:32.056: [iter 18 : loss : 1.1097 = 0.6600 + 0.4496 + 0.0002, time: 22.445143]
2023-05-10 20:47:32.343: epoch 18:	0.02164667  	0.05580604  	0.04858473  
2023-05-10 20:47:32.343: Find a better model.
2023-05-10 20:47:55.094: [iter 19 : loss : 1.0787 = 0.6379 + 0.4405 + 0.0003, time: 22.747971]
2023-05-10 20:47:55.374: epoch 19:	0.02335681  	0.06164169  	0.05348402  
2023-05-10 20:47:55.374: Find a better model.
2023-05-10 20:48:17.859: [iter 20 : loss : 1.0580 = 0.6042 + 0.4534 + 0.0004, time: 22.480772]
2023-05-10 20:48:18.135: epoch 20:	0.02528908  	0.06751476  	0.05808151  
2023-05-10 20:48:18.135: Find a better model.
2023-05-10 20:48:40.880: [iter 21 : loss : 1.0058 = 0.5602 + 0.4450 + 0.0006, time: 22.740927]
2023-05-10 20:48:41.158: epoch 21:	0.02671788  	0.07180116  	0.06114691  
2023-05-10 20:48:41.158: Find a better model.
2023-05-10 20:49:03.618: [iter 22 : loss : 0.9678 = 0.5085 + 0.4585 + 0.0009, time: 22.455850]
2023-05-10 20:49:03.897: epoch 22:	0.02761370  	0.07533737  	0.06283557  
2023-05-10 20:49:03.897: Find a better model.
2023-05-10 20:49:26.463: [iter 23 : loss : 0.9075 = 0.4552 + 0.4511 + 0.0012, time: 22.561518]
2023-05-10 20:49:26.743: epoch 23:	0.02847246  	0.07729902  	0.06389782  
2023-05-10 20:49:26.744: Find a better model.
2023-05-10 20:49:49.223: [iter 24 : loss : 0.8708 = 0.4058 + 0.4635 + 0.0015, time: 22.474967]
2023-05-10 20:49:49.498: epoch 24:	0.02872418  	0.07832696  	0.06448019  
2023-05-10 20:49:49.498: Find a better model.
2023-05-10 20:50:12.269: [iter 25 : loss : 0.8188 = 0.3613 + 0.4557 + 0.0018, time: 22.766963]
2023-05-10 20:50:12.551: epoch 25:	0.02903512  	0.07910808  	0.06515729  
2023-05-10 20:50:12.551: Find a better model.
2023-05-10 20:50:35.021: [iter 26 : loss : 0.7926 = 0.3241 + 0.4663 + 0.0021, time: 22.465937]
2023-05-10 20:50:35.293: epoch 26:	0.02917579  	0.07965694  	0.06509982  
2023-05-10 20:50:35.293: Find a better model.
2023-05-10 20:50:58.041: [iter 27 : loss : 0.7525 = 0.2926 + 0.4575 + 0.0024, time: 22.743928]
2023-05-10 20:50:58.315: epoch 27:	0.02949412  	0.08049714  	0.06547263  
2023-05-10 20:50:58.315: Find a better model.
2023-05-10 20:51:20.967: [iter 28 : loss : 0.7360 = 0.2663 + 0.4670 + 0.0026, time: 22.649283]
2023-05-10 20:51:21.240: epoch 28:	0.02961257  	0.08122075  	0.06576248  
2023-05-10 20:51:21.241: Find a better model.
2023-05-10 20:51:44.029: [iter 29 : loss : 0.7048 = 0.2443 + 0.4576 + 0.0029, time: 22.784892]
2023-05-10 20:51:44.303: epoch 29:	0.02970881  	0.08144548  	0.06635997  
2023-05-10 20:51:44.303: Find a better model.
2023-05-10 20:52:06.945: [iter 30 : loss : 0.6947 = 0.2250 + 0.4665 + 0.0032, time: 22.636615]
2023-05-10 20:52:07.220: epoch 30:	0.02960516  	0.08093257  	0.06613007  
2023-05-10 20:52:30.043: [iter 31 : loss : 0.6690 = 0.2088 + 0.4568 + 0.0034, time: 22.818165]
2023-05-10 20:52:30.324: epoch 31:	0.02954594  	0.08104789  	0.06629913  
2023-05-10 20:52:52.966: [iter 32 : loss : 0.6642 = 0.1951 + 0.4654 + 0.0037, time: 22.638261]
2023-05-10 20:52:53.240: epoch 32:	0.02960516  	0.08112687  	0.06630914  
2023-05-10 20:53:16.193: [iter 33 : loss : 0.6428 = 0.1833 + 0.4556 + 0.0039, time: 22.950369]
2023-05-10 20:53:16.466: epoch 33:	0.02973842  	0.08135881  	0.06662562  
2023-05-10 20:53:39.164: [iter 34 : loss : 0.6407 = 0.1724 + 0.4642 + 0.0041, time: 22.694075]
2023-05-10 20:53:39.435: epoch 34:	0.02981245  	0.08197983  	0.06693217  
2023-05-10 20:53:39.435: Find a better model.
2023-05-10 20:54:02.385: [iter 35 : loss : 0.6207 = 0.1621 + 0.4543 + 0.0043, time: 22.946196]
2023-05-10 20:54:02.660: epoch 35:	0.02989388  	0.08257391  	0.06728064  
2023-05-10 20:54:02.660: Find a better model.
2023-05-10 20:54:25.509: [iter 36 : loss : 0.6217 = 0.1541 + 0.4631 + 0.0045, time: 22.845619]
2023-05-10 20:54:25.783: epoch 36:	0.02987168  	0.08198477  	0.06718776  
2023-05-10 20:54:48.774: [iter 37 : loss : 0.6045 = 0.1466 + 0.4532 + 0.0047, time: 22.986822]
2023-05-10 20:54:49.049: epoch 37:	0.02996051  	0.08190876  	0.06730608  
2023-05-10 20:55:11.749: [iter 38 : loss : 0.6062 = 0.1394 + 0.4619 + 0.0049, time: 22.696128]
2023-05-10 20:55:12.026: epoch 38:	0.02984207  	0.08212423  	0.06733212  
2023-05-10 20:55:34.982: [iter 39 : loss : 0.5904 = 0.1333 + 0.4520 + 0.0051, time: 22.952197]
2023-05-10 20:55:35.254: epoch 39:	0.02994570  	0.08209101  	0.06759840  
2023-05-10 20:55:58.279: [iter 40 : loss : 0.5934 = 0.1273 + 0.4609 + 0.0053, time: 23.021002]
2023-05-10 20:55:58.551: epoch 40:	0.02976063  	0.08166594  	0.06739507  
2023-05-10 20:56:21.528: [iter 41 : loss : 0.5780 = 0.1215 + 0.4510 + 0.0054, time: 22.973128]
2023-05-10 20:56:21.801: epoch 41:	0.02973100  	0.08152904  	0.06724292  
2023-05-10 20:56:44.666: [iter 42 : loss : 0.5826 = 0.1169 + 0.4601 + 0.0056, time: 22.860494]
2023-05-10 20:56:44.939: epoch 42:	0.02956073  	0.08124014  	0.06713808  
2023-05-10 20:57:08.312: [iter 43 : loss : 0.5683 = 0.1123 + 0.4502 + 0.0058, time: 23.368027]
2023-05-10 20:57:08.583: epoch 43:	0.02947190  	0.08095716  	0.06715106  
2023-05-10 20:57:31.480: [iter 44 : loss : 0.5743 = 0.1090 + 0.4593 + 0.0059, time: 22.893516]
2023-05-10 20:57:31.753: epoch 44:	0.02954593  	0.08107455  	0.06707515  
2023-05-10 20:57:54.897: [iter 45 : loss : 0.5602 = 0.1047 + 0.4493 + 0.0061, time: 23.140606]
2023-05-10 20:57:55.168: epoch 45:	0.02958294  	0.08093838  	0.06712835  
2023-05-10 20:58:18.422: [iter 46 : loss : 0.5661 = 0.1012 + 0.4587 + 0.0062, time: 23.250345]
2023-05-10 20:58:18.698: epoch 46:	0.02956072  	0.08056586  	0.06688242  
2023-05-10 20:58:41.717: [iter 47 : loss : 0.5526 = 0.0976 + 0.4486 + 0.0064, time: 23.015491]
2023-05-10 20:58:41.989: epoch 47:	0.02969397  	0.08104891  	0.06694813  
2023-05-10 20:59:05.013: [iter 48 : loss : 0.5595 = 0.0950 + 0.4580 + 0.0065, time: 23.021148]
2023-05-10 20:59:05.286: epoch 48:	0.02967916  	0.08094108  	0.06697446  
2023-05-10 20:59:28.507: [iter 49 : loss : 0.5465 = 0.0919 + 0.4479 + 0.0067, time: 23.216314]
2023-05-10 20:59:28.779: epoch 49:	0.02961253  	0.08083271  	0.06704924  
2023-05-10 20:59:51.630: [iter 50 : loss : 0.5536 = 0.0894 + 0.4574 + 0.0068, time: 22.847553]
2023-05-10 20:59:51.903: epoch 50:	0.02957551  	0.08054174  	0.06699113  
2023-05-10 21:00:15.057: [iter 51 : loss : 0.5409 = 0.0866 + 0.4473 + 0.0070, time: 23.148628]
2023-05-10 21:00:15.329: epoch 51:	0.02943486  	0.08037434  	0.06680654  
2023-05-10 21:00:38.411: [iter 52 : loss : 0.5477 = 0.0837 + 0.4569 + 0.0071, time: 23.078885]
2023-05-10 21:00:38.679: epoch 52:	0.02944226  	0.08022982  	0.06688756  
2023-05-10 21:01:01.840: [iter 53 : loss : 0.5366 = 0.0825 + 0.4468 + 0.0072, time: 23.156890]
2023-05-10 21:01:02.113: epoch 53:	0.02939044  	0.08010954  	0.06680591  
2023-05-10 21:01:24.976: [iter 54 : loss : 0.5437 = 0.0799 + 0.4564 + 0.0074, time: 22.859816]
2023-05-10 21:01:25.247: epoch 54:	0.02929419  	0.07953556  	0.06647923  
2023-05-10 21:01:48.424: [iter 55 : loss : 0.5316 = 0.0779 + 0.4462 + 0.0075, time: 23.174450]
2023-05-10 21:01:48.698: epoch 55:	0.02936082  	0.07939970  	0.06662069  
2023-05-10 21:02:11.607: [iter 56 : loss : 0.5392 = 0.0757 + 0.4559 + 0.0076, time: 22.906356]
2023-05-10 21:02:11.881: epoch 56:	0.02930159  	0.07929533  	0.06657215  
2023-05-10 21:02:35.031: [iter 57 : loss : 0.5275 = 0.0739 + 0.4458 + 0.0077, time: 23.144644]
2023-05-10 21:02:35.306: epoch 57:	0.02929418  	0.07897526  	0.06637903  
2023-05-10 21:02:58.335: [iter 58 : loss : 0.5359 = 0.0723 + 0.4558 + 0.0079, time: 23.024014]
2023-05-10 21:02:58.605: epoch 58:	0.02913131  	0.07846829  	0.06597367  
2023-05-10 21:03:21.621: [iter 59 : loss : 0.5242 = 0.0707 + 0.4455 + 0.0080, time: 23.013219]
2023-05-10 21:03:21.893: epoch 59:	0.02909430  	0.07826487  	0.06602325  
2023-05-10 21:03:46.971: [iter 60 : loss : 0.5325 = 0.0690 + 0.4554 + 0.0081, time: 25.074049]
2023-05-10 21:03:47.255: epoch 60:	0.02904988  	0.07789147  	0.06583796  
2023-05-10 21:03:47.255: Early stopping is trigger at epoch: 60
2023-05-10 21:03:47.255: best_result@epoch 35:

2023-05-10 21:03:47.255: 		0.0299      	0.0826      	0.0673      
2023-05-10 21:05:15.319: my pid: 8384
2023-05-10 21:05:15.319: model: model.general_recommender.SGL
2023-05-10 21:05:15.319: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 21:05:15.319: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 21:05:18.586: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 21:05:38.891: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.303829]
2023-05-10 21:05:39.157: epoch 1:	0.00144359  	0.00287857  	0.00246503  
2023-05-10 21:05:39.157: Find a better model.
2023-05-10 21:05:59.567: [iter 2 : loss : 1.1357 = 0.6930 + 0.4426 + 0.0000, time: 20.406689]
2023-05-10 21:05:59.856: epoch 2:	0.00178413  	0.00287936  	0.00250725  
2023-05-10 21:05:59.856: Find a better model.
2023-05-10 21:06:20.469: [iter 3 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 20.610048]
2023-05-10 21:06:20.774: epoch 3:	0.00224312  	0.00401966  	0.00352433  
2023-05-10 21:06:20.775: Find a better model.
2023-05-10 21:06:41.151: [iter 4 : loss : 1.1366 = 0.6928 + 0.4437 + 0.0000, time: 20.373743]
2023-05-10 21:06:41.446: epoch 4:	0.00230974  	0.00342352  	0.00315415  
2023-05-10 21:07:02.073: [iter 5 : loss : 1.1356 = 0.6927 + 0.4429 + 0.0000, time: 20.623731]
2023-05-10 21:07:02.365: epoch 5:	0.00310187  	0.00679616  	0.00512522  
2023-05-10 21:07:02.365: Find a better model.
2023-05-10 21:07:22.527: [iter 6 : loss : 1.1373 = 0.6926 + 0.4447 + 0.0000, time: 20.158945]
2023-05-10 21:07:22.827: epoch 6:	0.00333876  	0.00630788  	0.00507496  
2023-05-10 21:07:43.445: [iter 7 : loss : 1.1360 = 0.6924 + 0.4437 + 0.0000, time: 20.614399]
2023-05-10 21:07:43.759: epoch 7:	0.00440479  	0.00976724  	0.00781802  
2023-05-10 21:07:43.759: Find a better model.
2023-05-10 21:08:04.316: [iter 8 : loss : 1.1379 = 0.6921 + 0.4457 + 0.0000, time: 20.552928]
2023-05-10 21:08:04.608: epoch 8:	0.00481936  	0.01062134  	0.00886752  
2023-05-10 21:08:04.608: Find a better model.
2023-05-10 21:08:25.230: [iter 9 : loss : 1.1362 = 0.6917 + 0.4444 + 0.0000, time: 20.619508]
2023-05-10 21:08:25.522: epoch 9:	0.00612968  	0.01389587  	0.01174740  
2023-05-10 21:08:25.522: Find a better model.
2023-05-10 21:08:46.090: [iter 10 : loss : 1.1382 = 0.6912 + 0.4470 + 0.0000, time: 20.563725]
2023-05-10 21:08:46.384: epoch 10:	0.00635177  	0.01403054  	0.01156042  
2023-05-10 21:08:46.384: Find a better model.
2023-05-10 21:09:07.025: [iter 11 : loss : 1.1357 = 0.6904 + 0.4453 + 0.0000, time: 20.637575]
2023-05-10 21:09:07.315: epoch 11:	0.00763988  	0.01769573  	0.01480544  
2023-05-10 21:09:07.315: Find a better model.
2023-05-10 21:09:27.852: [iter 12 : loss : 1.1377 = 0.6891 + 0.4485 + 0.0000, time: 20.533390]
2023-05-10 21:09:28.129: epoch 12:	0.00849862  	0.02009019  	0.01731148  
2023-05-10 21:09:28.129: Find a better model.
2023-05-10 21:09:48.812: [iter 13 : loss : 1.1337 = 0.6874 + 0.4463 + 0.0000, time: 20.678893]
2023-05-10 21:09:49.108: epoch 13:	0.01037159  	0.02518463  	0.02183973  
2023-05-10 21:09:49.108: Find a better model.
2023-05-10 21:10:10.040: [iter 14 : loss : 1.1357 = 0.6851 + 0.4505 + 0.0001, time: 20.927593]
2023-05-10 21:10:10.338: epoch 14:	0.01305892  	0.03082806  	0.02705083  
2023-05-10 21:10:10.338: Find a better model.
2023-05-10 21:10:31.372: [iter 15 : loss : 1.1289 = 0.6815 + 0.4473 + 0.0001, time: 21.029695]
2023-05-10 21:10:31.662: epoch 15:	0.01571666  	0.03900741  	0.03474334  
2023-05-10 21:10:31.663: Find a better model.
2023-05-10 21:10:52.625: [iter 16 : loss : 1.1289 = 0.6757 + 0.4531 + 0.0001, time: 20.958927]
2023-05-10 21:10:52.916: epoch 16:	0.01881118  	0.04704932  	0.04168241  
2023-05-10 21:10:52.916: Find a better model.
2023-05-10 21:11:13.757: [iter 17 : loss : 1.1153 = 0.6664 + 0.4488 + 0.0002, time: 20.837357]
2023-05-10 21:11:14.051: epoch 17:	0.02152816  	0.05447435  	0.04801317  
2023-05-10 21:11:14.052: Find a better model.
2023-05-10 21:11:35.007: [iter 18 : loss : 1.1075 = 0.6508 + 0.4565 + 0.0002, time: 20.951925]
2023-05-10 21:11:35.298: epoch 18:	0.02388982  	0.06177993  	0.05375665  
2023-05-10 21:11:35.298: Find a better model.
2023-05-10 21:11:56.361: [iter 19 : loss : 1.0785 = 0.6269 + 0.4513 + 0.0003, time: 21.058616]
2023-05-10 21:11:56.652: epoch 19:	0.02594794  	0.06842382  	0.05836609  
2023-05-10 21:11:56.652: Find a better model.
2023-05-10 21:12:17.577: [iter 20 : loss : 1.0548 = 0.5933 + 0.4610 + 0.0005, time: 20.921080]
2023-05-10 21:12:17.873: epoch 20:	0.02722130  	0.07154231  	0.06127243  
2023-05-10 21:12:17.873: Find a better model.
2023-05-10 21:12:38.937: [iter 21 : loss : 1.0068 = 0.5500 + 0.4561 + 0.0007, time: 21.059624]
2023-05-10 21:12:39.229: epoch 21:	0.02819113  	0.07490247  	0.06353049  
2023-05-10 21:12:39.229: Find a better model.
2023-05-10 21:13:00.017: [iter 22 : loss : 0.9685 = 0.5002 + 0.4674 + 0.0009, time: 20.784553]
2023-05-10 21:13:00.308: epoch 22:	0.02857610  	0.07650000  	0.06427547  
2023-05-10 21:13:00.308: Find a better model.
2023-05-10 21:13:21.310: [iter 23 : loss : 0.9126 = 0.4493 + 0.4622 + 0.0012, time: 20.997820]
2023-05-10 21:13:21.600: epoch 23:	0.02886482  	0.07755280  	0.06483980  
2023-05-10 21:13:21.600: Find a better model.
2023-05-10 21:13:42.413: [iter 24 : loss : 0.8761 = 0.4013 + 0.4733 + 0.0015, time: 20.807959]
2023-05-10 21:13:42.700: epoch 24:	0.02909431  	0.07830323  	0.06489575  
2023-05-10 21:13:42.700: Find a better model.
2023-05-10 21:14:03.684: [iter 25 : loss : 0.8268 = 0.3583 + 0.4667 + 0.0018, time: 20.979829]
2023-05-10 21:14:03.971: epoch 25:	0.02928681  	0.07923490  	0.06519538  
2023-05-10 21:14:03.971: Find a better model.
2023-05-10 21:14:24.574: [iter 26 : loss : 0.8006 = 0.3220 + 0.4764 + 0.0021, time: 20.599676]
2023-05-10 21:14:24.855: epoch 26:	0.02914615  	0.07927396  	0.06519629  
2023-05-10 21:14:24.855: Find a better model.
2023-05-10 21:14:45.514: [iter 27 : loss : 0.7618 = 0.2910 + 0.4684 + 0.0024, time: 20.654947]
2023-05-10 21:14:45.798: epoch 27:	0.02945710  	0.08040117  	0.06568515  
2023-05-10 21:14:45.798: Find a better model.
2023-05-10 21:15:06.562: [iter 28 : loss : 0.7449 = 0.2654 + 0.4769 + 0.0026, time: 20.760614]
2023-05-10 21:15:06.844: epoch 28:	0.02947931  	0.08085617  	0.06593852  
2023-05-10 21:15:06.845: Find a better model.
2023-05-10 21:15:27.512: [iter 29 : loss : 0.7149 = 0.2438 + 0.4682 + 0.0029, time: 20.662920]
2023-05-10 21:15:27.796: epoch 29:	0.02951632  	0.08097341  	0.06601376  
2023-05-10 21:15:27.796: Find a better model.
2023-05-10 21:15:48.322: [iter 30 : loss : 0.7039 = 0.2244 + 0.4763 + 0.0032, time: 20.521431]
2023-05-10 21:15:48.610: epoch 30:	0.02953111  	0.08100313  	0.06636479  
2023-05-10 21:15:48.610: Find a better model.
2023-05-10 21:16:09.436: [iter 31 : loss : 0.6788 = 0.2082 + 0.4672 + 0.0034, time: 20.822396]
2023-05-10 21:16:09.719: epoch 31:	0.02963477  	0.08106330  	0.06647565  
2023-05-10 21:16:09.719: Find a better model.
2023-05-10 21:16:30.335: [iter 32 : loss : 0.6730 = 0.1944 + 0.4750 + 0.0036, time: 20.612077]
2023-05-10 21:16:30.618: epoch 32:	0.02965697  	0.08084857  	0.06663746  
2023-05-10 21:16:51.310: [iter 33 : loss : 0.6527 = 0.1829 + 0.4659 + 0.0039, time: 20.688826]
2023-05-10 21:16:51.593: epoch 33:	0.02977542  	0.08098398  	0.06684346  
2023-05-10 21:17:12.259: [iter 34 : loss : 0.6495 = 0.1719 + 0.4735 + 0.0041, time: 20.661911]
2023-05-10 21:17:12.539: epoch 34:	0.02983465  	0.08076111  	0.06694534  
2023-05-10 21:17:33.229: [iter 35 : loss : 0.6305 = 0.1618 + 0.4644 + 0.0043, time: 20.685817]
2023-05-10 21:17:33.505: epoch 35:	0.02986426  	0.08112532  	0.06708122  
2023-05-10 21:17:33.505: Find a better model.
2023-05-10 21:17:54.097: [iter 36 : loss : 0.6301 = 0.1535 + 0.4721 + 0.0045, time: 20.589145]
2023-05-10 21:17:54.373: epoch 36:	0.02998272  	0.08124487  	0.06721078  
2023-05-10 21:17:54.373: Find a better model.
2023-05-10 21:18:15.213: [iter 37 : loss : 0.6137 = 0.1458 + 0.4632 + 0.0047, time: 20.836326]
2023-05-10 21:18:15.487: epoch 37:	0.03004194  	0.08157887  	0.06748419  
2023-05-10 21:18:15.487: Find a better model.
2023-05-10 21:18:36.071: [iter 38 : loss : 0.6147 = 0.1389 + 0.4709 + 0.0049, time: 20.581210]
2023-05-10 21:18:36.348: epoch 38:	0.02986426  	0.08133566  	0.06722555  
2023-05-10 21:18:56.976: [iter 39 : loss : 0.5999 = 0.1329 + 0.4619 + 0.0051, time: 20.624041]
2023-05-10 21:18:57.249: epoch 39:	0.02988647  	0.08121809  	0.06728256  
2023-05-10 21:19:18.045: [iter 40 : loss : 0.6021 = 0.1272 + 0.4697 + 0.0052, time: 20.792479]
2023-05-10 21:19:18.319: epoch 40:	0.02990868  	0.08121989  	0.06728523  
2023-05-10 21:19:38.996: [iter 41 : loss : 0.5874 = 0.1210 + 0.4610 + 0.0054, time: 20.673865]
2023-05-10 21:19:39.268: epoch 41:	0.02990128  	0.08110519  	0.06719150  
2023-05-10 21:19:59.852: [iter 42 : loss : 0.5908 = 0.1162 + 0.4690 + 0.0056, time: 20.578166]
2023-05-10 21:20:00.127: epoch 42:	0.02977542  	0.08093525  	0.06723249  
2023-05-10 21:20:20.979: [iter 43 : loss : 0.5778 = 0.1120 + 0.4601 + 0.0057, time: 20.848263]
2023-05-10 21:20:21.250: epoch 43:	0.02993829  	0.08137876  	0.06758359  
2023-05-10 21:20:41.854: [iter 44 : loss : 0.5823 = 0.1085 + 0.4679 + 0.0059, time: 20.600110]
2023-05-10 21:20:42.124: epoch 44:	0.02987907  	0.08086767  	0.06734996  
2023-05-10 21:21:02.946: [iter 45 : loss : 0.5694 = 0.1042 + 0.4592 + 0.0061, time: 20.817542]
2023-05-10 21:21:03.219: epoch 45:	0.02979763  	0.08052528  	0.06725502  
2023-05-10 21:21:24.030: [iter 46 : loss : 0.5745 = 0.1009 + 0.4673 + 0.0062, time: 20.808421]
2023-05-10 21:21:24.303: epoch 46:	0.02981242  	0.08067120  	0.06732395  
2023-05-10 21:21:45.133: [iter 47 : loss : 0.5620 = 0.0972 + 0.4584 + 0.0064, time: 20.825383]
2023-05-10 21:21:45.404: epoch 47:	0.02976801  	0.08052120  	0.06710386  
2023-05-10 21:22:06.148: [iter 48 : loss : 0.5673 = 0.0942 + 0.4666 + 0.0065, time: 20.740677]
2023-05-10 21:22:06.423: epoch 48:	0.02959033  	0.08002427  	0.06714109  
2023-05-10 21:22:27.345: [iter 49 : loss : 0.5557 = 0.0913 + 0.4578 + 0.0067, time: 20.917044]
2023-05-10 21:22:27.616: epoch 49:	0.02967177  	0.08025816  	0.06726750  
2023-05-10 21:22:48.220: [iter 50 : loss : 0.5616 = 0.0890 + 0.4658 + 0.0068, time: 20.601107]
2023-05-10 21:22:48.493: epoch 50:	0.02947189  	0.07950882  	0.06698085  
2023-05-10 21:23:09.318: [iter 51 : loss : 0.5497 = 0.0857 + 0.4571 + 0.0069, time: 20.821373]
2023-05-10 21:23:09.590: epoch 51:	0.02944968  	0.07960861  	0.06693894  
2023-05-10 21:23:30.364: [iter 52 : loss : 0.5557 = 0.0833 + 0.4653 + 0.0071, time: 20.769558]
2023-05-10 21:23:30.636: epoch 52:	0.02934604  	0.07924709  	0.06669012  
2023-05-10 21:23:51.460: [iter 53 : loss : 0.5462 = 0.0824 + 0.4566 + 0.0072, time: 20.820440]
2023-05-10 21:23:51.732: epoch 53:	0.02931642  	0.07906587  	0.06663404  
2023-05-10 21:24:15.115: [iter 54 : loss : 0.5515 = 0.0795 + 0.4647 + 0.0073, time: 23.377518]
2023-05-10 21:24:15.373: epoch 54:	0.02922018  	0.07877423  	0.06638578  
2023-05-10 21:24:35.994: [iter 55 : loss : 0.5415 = 0.0779 + 0.4561 + 0.0075, time: 20.616958]
2023-05-10 21:24:36.248: epoch 55:	0.02922759  	0.07867517  	0.06630116  
2023-05-10 21:24:56.592: [iter 56 : loss : 0.5473 = 0.0753 + 0.4644 + 0.0076, time: 20.340871]
2023-05-10 21:24:56.889: epoch 56:	0.02926460  	0.07873933  	0.06631335  
2023-05-10 21:25:17.944: [iter 57 : loss : 0.5366 = 0.0732 + 0.4557 + 0.0077, time: 21.048909]
2023-05-10 21:25:18.240: epoch 57:	0.02913135  	0.07838807  	0.06611032  
2023-05-10 21:25:39.571: [iter 58 : loss : 0.5439 = 0.0720 + 0.4641 + 0.0078, time: 21.326422]
2023-05-10 21:25:39.859: epoch 58:	0.02915355  	0.07827424  	0.06593962  
2023-05-10 21:26:29.550: my pid: 6372
2023-05-10 21:26:29.550: model: model.general_recommender.SGL
2023-05-10 21:26:29.550: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-10 21:26:29.550: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-10 21:26:32.792: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-10 21:26:52.865: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.073442]
2023-05-10 21:26:53.129: epoch 1:	0.00139177  	0.00252695  	0.00218609  
2023-05-10 21:26:53.129: Find a better model.
2023-05-10 21:27:13.468: [iter 2 : loss : 1.1357 = 0.6930 + 0.4426 + 0.0000, time: 20.334466]
2023-05-10 21:27:13.745: epoch 2:	0.00161386  	0.00280524  	0.00241016  
2023-05-10 21:27:13.745: Find a better model.
2023-05-10 21:27:33.968: [iter 3 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 20.219582]
2023-05-10 21:27:34.276: epoch 3:	0.00239858  	0.00444133  	0.00362510  
2023-05-10 21:27:34.276: Find a better model.
2023-05-10 21:27:54.517: [iter 4 : loss : 1.1366 = 0.6928 + 0.4437 + 0.0000, time: 20.236766]
2023-05-10 21:27:54.807: epoch 4:	0.00247261  	0.00455619  	0.00369353  
2023-05-10 21:27:54.807: Find a better model.
2023-05-10 21:28:15.193: [iter 5 : loss : 1.1356 = 0.6927 + 0.4429 + 0.0000, time: 20.383560]
2023-05-10 21:28:15.468: epoch 5:	0.00304264  	0.00596069  	0.00500153  
2023-05-10 21:28:15.468: Find a better model.
2023-05-10 21:28:35.418: [iter 6 : loss : 1.1373 = 0.6926 + 0.4447 + 0.0000, time: 19.947320]
2023-05-10 21:28:35.705: epoch 6:	0.00340539  	0.00612930  	0.00514046  
2023-05-10 21:28:35.705: Find a better model.
2023-05-10 21:28:56.008: [iter 7 : loss : 1.1360 = 0.6923 + 0.4436 + 0.0000, time: 20.299940]
2023-05-10 21:28:56.313: epoch 7:	0.00424193  	0.00908457  	0.00756063  
2023-05-10 21:28:56.313: Find a better model.
2023-05-10 21:29:16.424: [iter 8 : loss : 1.1379 = 0.6921 + 0.4457 + 0.0000, time: 20.106588]
2023-05-10 21:29:16.705: epoch 8:	0.00450843  	0.00858320  	0.00770114  
2023-05-10 21:29:36.953: [iter 9 : loss : 1.1362 = 0.6917 + 0.4444 + 0.0000, time: 20.243734]
2023-05-10 21:29:37.235: epoch 9:	0.00586317  	0.01249005  	0.01070334  
2023-05-10 21:29:37.235: Find a better model.
2023-05-10 21:29:57.222: [iter 10 : loss : 1.1382 = 0.6912 + 0.4470 + 0.0000, time: 19.982492]
2023-05-10 21:29:57.509: epoch 10:	0.00586318  	0.01182113  	0.01063611  
2023-05-10 21:30:17.919: [iter 11 : loss : 1.1356 = 0.6903 + 0.4453 + 0.0000, time: 20.407288]
2023-05-10 21:30:18.210: epoch 11:	0.00729195  	0.01571322  	0.01380545  
2023-05-10 21:30:18.211: Find a better model.
2023-05-10 21:30:38.374: [iter 12 : loss : 1.1376 = 0.6891 + 0.4485 + 0.0000, time: 20.159431]
2023-05-10 21:30:38.650: epoch 12:	0.00854304  	0.01818279  	0.01608597  
2023-05-10 21:30:38.650: Find a better model.
2023-05-10 21:30:58.707: [iter 13 : loss : 1.1337 = 0.6874 + 0.4463 + 0.0000, time: 20.052945]
2023-05-10 21:30:58.983: epoch 13:	0.01015691  	0.02363574  	0.02087268  
2023-05-10 21:30:58.983: Find a better model.
2023-05-10 21:31:18.947: [iter 14 : loss : 1.1356 = 0.6850 + 0.4505 + 0.0001, time: 19.961817]
2023-05-10 21:31:19.221: epoch 14:	0.01271097  	0.02999482  	0.02596341  
2023-05-10 21:31:19.221: Find a better model.
2023-05-10 21:31:38.933: [iter 15 : loss : 1.1288 = 0.6815 + 0.4473 + 0.0001, time: 19.707467]
2023-05-10 21:31:39.202: epoch 15:	0.01527248  	0.03806533  	0.03308835  
2023-05-10 21:31:39.202: Find a better model.
2023-05-10 21:31:58.945: [iter 16 : loss : 1.1288 = 0.6755 + 0.4531 + 0.0001, time: 19.738380]
2023-05-10 21:31:59.214: epoch 16:	0.01821894  	0.04464441  	0.03979772  
2023-05-10 21:31:59.214: Find a better model.
2023-05-10 21:32:18.892: [iter 17 : loss : 1.1151 = 0.6662 + 0.4488 + 0.0002, time: 19.674661]
2023-05-10 21:32:19.161: epoch 17:	0.02106920  	0.05335566  	0.04739445  
2023-05-10 21:32:19.161: Find a better model.
2023-05-10 21:32:38.772: [iter 18 : loss : 1.1072 = 0.6505 + 0.4565 + 0.0002, time: 19.606450]
2023-05-10 21:32:39.040: epoch 18:	0.02375657  	0.06037286  	0.05310659  
2023-05-10 21:32:39.040: Find a better model.
2023-05-10 21:32:58.709: [iter 19 : loss : 1.0781 = 0.6264 + 0.4513 + 0.0003, time: 19.665113]
2023-05-10 21:32:58.978: epoch 19:	0.02608861  	0.06716132  	0.05819296  
2023-05-10 21:32:58.978: Find a better model.
2023-05-10 21:33:18.690: [iter 20 : loss : 1.0543 = 0.5928 + 0.4610 + 0.0005, time: 19.708673]
2023-05-10 21:33:18.960: epoch 20:	0.02725834  	0.07090390  	0.06092141  
2023-05-10 21:33:18.960: Find a better model.
2023-05-10 21:33:38.692: [iter 21 : loss : 1.0060 = 0.5493 + 0.4560 + 0.0007, time: 19.727733]
2023-05-10 21:33:38.947: epoch 21:	0.02831700  	0.07436490  	0.06312490  
2023-05-10 21:33:38.947: Find a better model.
2023-05-10 21:33:58.699: [iter 22 : loss : 0.9678 = 0.4996 + 0.4673 + 0.0009, time: 19.749168]
2023-05-10 21:33:58.964: epoch 22:	0.02868713  	0.07568021  	0.06394478  
2023-05-10 21:33:58.964: Find a better model.
2023-05-10 21:34:18.846: [iter 23 : loss : 0.9120 = 0.4487 + 0.4621 + 0.0012, time: 19.877738]
2023-05-10 21:34:19.109: epoch 23:	0.02913873  	0.07655944  	0.06478669  
2023-05-10 21:34:19.109: Find a better model.
2023-05-10 21:34:38.535: [iter 24 : loss : 0.8755 = 0.4008 + 0.4732 + 0.0015, time: 19.422698]
2023-05-10 21:34:38.804: epoch 24:	0.02924239  	0.07809474  	0.06535109  
2023-05-10 21:34:38.804: Find a better model.
2023-05-10 21:34:58.803: [iter 25 : loss : 0.8262 = 0.3578 + 0.4666 + 0.0018, time: 19.996079]
2023-05-10 21:34:59.071: epoch 25:	0.02935343  	0.07821807  	0.06537604  
2023-05-10 21:34:59.072: Find a better model.
2023-05-10 21:35:19.085: [iter 26 : loss : 0.8002 = 0.3218 + 0.4763 + 0.0021, time: 20.010221]
2023-05-10 21:35:19.353: epoch 26:	0.02933863  	0.07911667  	0.06554782  
2023-05-10 21:35:19.353: Find a better model.
2023-05-10 21:35:39.419: [iter 27 : loss : 0.7614 = 0.2908 + 0.4682 + 0.0024, time: 20.062242]
2023-05-10 21:35:39.687: epoch 27:	0.02956814  	0.07990071  	0.06592954  
2023-05-10 21:35:39.687: Find a better model.
2023-05-10 21:35:59.864: [iter 28 : loss : 0.7444 = 0.2650 + 0.4768 + 0.0027, time: 20.174491]
2023-05-10 21:36:00.135: epoch 28:	0.02971620  	0.08070002  	0.06619515  
2023-05-10 21:36:00.135: Find a better model.
2023-05-10 21:36:20.408: [iter 29 : loss : 0.7144 = 0.2434 + 0.4681 + 0.0029, time: 20.268164]
2023-05-10 21:36:20.674: epoch 29:	0.02977542  	0.08071730  	0.06627586  
2023-05-10 21:36:20.674: Find a better model.
2023-05-10 21:36:40.646: [iter 30 : loss : 0.7036 = 0.2243 + 0.4762 + 0.0032, time: 19.966173]
2023-05-10 21:36:40.910: epoch 30:	0.02970139  	0.08058029  	0.06606968  
2023-05-10 21:37:00.972: [iter 31 : loss : 0.6786 = 0.2081 + 0.4671 + 0.0034, time: 20.057996]
2023-05-10 21:37:01.237: epoch 31:	0.02978283  	0.08118483  	0.06629696  
2023-05-10 21:37:01.237: Find a better model.
2023-05-10 21:37:21.459: [iter 32 : loss : 0.6728 = 0.1943 + 0.4748 + 0.0036, time: 20.218725]
2023-05-10 21:37:21.725: epoch 32:	0.02984945  	0.08112649  	0.06641509  
2023-05-10 21:37:41.786: [iter 33 : loss : 0.6522 = 0.1826 + 0.4658 + 0.0039, time: 20.057395]
2023-05-10 21:37:42.052: epoch 33:	0.02999011  	0.08122959  	0.06653240  
2023-05-10 21:37:42.052: Find a better model.
2023-05-10 21:38:02.192: [iter 34 : loss : 0.6492 = 0.1718 + 0.4734 + 0.0041, time: 20.136798]
2023-05-10 21:38:02.460: epoch 34:	0.02999012  	0.08133809  	0.06662424  
2023-05-10 21:38:02.460: Find a better model.
2023-05-10 21:38:22.954: [iter 35 : loss : 0.6301 = 0.1616 + 0.4643 + 0.0043, time: 20.491165]
2023-05-10 21:38:23.221: epoch 35:	0.03017519  	0.08180790  	0.06669902  
2023-05-10 21:38:23.221: Find a better model.
2023-05-10 21:38:43.440: [iter 36 : loss : 0.6298 = 0.1533 + 0.4720 + 0.0045, time: 20.214863]
2023-05-10 21:38:43.706: epoch 36:	0.03029363  	0.08202413  	0.06687750  
2023-05-10 21:38:43.706: Find a better model.
2023-05-10 21:39:04.144: [iter 37 : loss : 0.6134 = 0.1457 + 0.4631 + 0.0047, time: 20.433546]
2023-05-10 21:39:04.410: epoch 37:	0.03021221  	0.08231203  	0.06688748  
2023-05-10 21:39:04.411: Find a better model.
2023-05-10 21:39:24.597: [iter 38 : loss : 0.6146 = 0.1388 + 0.4709 + 0.0049, time: 20.182446]
2023-05-10 21:39:24.865: epoch 38:	0.03017518  	0.08224382  	0.06689698  
2023-05-10 21:39:45.306: [iter 39 : loss : 0.5999 = 0.1329 + 0.4619 + 0.0051, time: 20.436651]
2023-05-10 21:39:45.572: epoch 39:	0.03010855  	0.08213138  	0.06695303  
2023-05-10 21:40:05.780: [iter 40 : loss : 0.6019 = 0.1269 + 0.4697 + 0.0052, time: 20.203914]
2023-05-10 21:40:06.048: epoch 40:	0.03006415  	0.08230422  	0.06701405  
2023-05-10 21:40:26.519: [iter 41 : loss : 0.5872 = 0.1209 + 0.4609 + 0.0054, time: 20.467221]
2023-05-10 21:40:26.784: epoch 41:	0.02999011  	0.08204259  	0.06694911  
2023-05-10 21:40:46.965: [iter 42 : loss : 0.5908 = 0.1163 + 0.4689 + 0.0056, time: 20.177984]
2023-05-10 21:40:47.233: epoch 42:	0.02993829  	0.08198769  	0.06695689  
2023-05-10 21:41:07.718: [iter 43 : loss : 0.5775 = 0.1117 + 0.4601 + 0.0057, time: 20.482443]
2023-05-10 21:41:07.986: epoch 43:	0.02987165  	0.08139551  	0.06662539  
2023-05-10 21:41:28.351: [iter 44 : loss : 0.5822 = 0.1085 + 0.4678 + 0.0059, time: 20.361846]
2023-05-10 21:41:28.621: epoch 44:	0.02996790  	0.08123847  	0.06662580  
2023-05-10 21:41:49.057: [iter 45 : loss : 0.5693 = 0.1041 + 0.4592 + 0.0061, time: 20.432125]
2023-05-10 21:41:49.322: epoch 45:	0.02987165  	0.08075589  	0.06642741  
2023-05-10 21:42:09.734: [iter 46 : loss : 0.5743 = 0.1007 + 0.4673 + 0.0062, time: 20.409564]
2023-05-10 21:42:10.002: epoch 46:	0.02984204  	0.08038966  	0.06642241  
2023-05-10 21:42:30.513: [iter 47 : loss : 0.5617 = 0.0970 + 0.4584 + 0.0064, time: 20.507466]
2023-05-10 21:42:30.781: epoch 47:	0.02981983  	0.08070023  	0.06652203  
2023-05-10 21:42:50.881: [iter 48 : loss : 0.5669 = 0.0939 + 0.4665 + 0.0065, time: 20.096783]
2023-05-10 21:42:51.147: epoch 48:	0.02978281  	0.08078431  	0.06655260  
2023-05-10 21:43:11.450: [iter 49 : loss : 0.5557 = 0.0912 + 0.4578 + 0.0067, time: 20.299860]
2023-05-10 21:43:11.717: epoch 49:	0.02976801  	0.08024182  	0.06618533  
2023-05-10 21:43:31.947: [iter 50 : loss : 0.5615 = 0.0889 + 0.4657 + 0.0068, time: 20.226300]
2023-05-10 21:43:32.213: epoch 50:	0.02960514  	0.07982890  	0.06597831  
2023-05-10 21:43:52.437: [iter 51 : loss : 0.5499 = 0.0859 + 0.4571 + 0.0069, time: 20.220003]
2023-05-10 21:43:52.703: epoch 51:	0.02954591  	0.07934732  	0.06583968  
2023-05-10 21:44:12.898: [iter 52 : loss : 0.5557 = 0.0834 + 0.4652 + 0.0071, time: 20.190279]
2023-05-10 21:44:13.166: epoch 52:	0.02948668  	0.07930624  	0.06581021  
2023-05-10 21:44:33.613: [iter 53 : loss : 0.5461 = 0.0823 + 0.4566 + 0.0072, time: 20.444604]
2023-05-10 21:44:33.881: epoch 53:	0.02940525  	0.07898463  	0.06570540  
2023-05-10 21:44:54.079: [iter 54 : loss : 0.5515 = 0.0795 + 0.4647 + 0.0073, time: 20.194413]
2023-05-10 21:44:54.350: epoch 54:	0.02939785  	0.07912086  	0.06574278  
2023-05-10 21:45:14.627: [iter 55 : loss : 0.5414 = 0.0778 + 0.4561 + 0.0075, time: 20.273154]
2023-05-10 21:45:14.896: epoch 55:	0.02939045  	0.07882659  	0.06556439  
2023-05-10 21:45:35.065: [iter 56 : loss : 0.5472 = 0.0753 + 0.4643 + 0.0076, time: 20.165712]
2023-05-10 21:45:35.334: epoch 56:	0.02939785  	0.07871532  	0.06554740  
2023-05-10 21:45:55.808: [iter 57 : loss : 0.5366 = 0.0731 + 0.4557 + 0.0077, time: 20.471488]
2023-05-10 21:45:56.075: epoch 57:	0.02935343  	0.07833314  	0.06533211  
2023-05-10 21:46:16.442: [iter 58 : loss : 0.5440 = 0.0720 + 0.4641 + 0.0078, time: 20.363879]
2023-05-10 21:46:16.711: epoch 58:	0.02935342  	0.07824105  	0.06520913  
2023-05-10 21:46:37.151: [iter 59 : loss : 0.5335 = 0.0703 + 0.4553 + 0.0080, time: 20.436240]
2023-05-10 21:46:37.418: epoch 59:	0.02932381  	0.07809003  	0.06511836  
2023-05-10 21:46:57.643: [iter 60 : loss : 0.5404 = 0.0689 + 0.4634 + 0.0081, time: 20.220706]
2023-05-10 21:46:57.911: epoch 60:	0.02912393  	0.07742272  	0.06480405  
2023-05-10 21:47:18.383: [iter 61 : loss : 0.5302 = 0.0671 + 0.4549 + 0.0082, time: 20.468406]
2023-05-10 21:47:18.654: epoch 61:	0.02913133  	0.07747462  	0.06480490  
2023-05-10 21:47:38.996: [iter 62 : loss : 0.5374 = 0.0659 + 0.4632 + 0.0083, time: 20.338994]
2023-05-10 21:47:39.263: epoch 62:	0.02897586  	0.07650658  	0.06445947  
2023-05-10 21:47:39.263: Early stopping is trigger at epoch: 62
2023-05-10 21:47:39.263: best_result@epoch 37:

2023-05-10 21:47:39.263: 		0.0302      	0.0823      	0.0669      
2023-05-11 09:24:09.221: my pid: 9752
2023-05-11 09:24:09.221: model: model.general_recommender.SGL
2023-05-11 09:24:09.221: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 09:24:09.221: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 09:24:12.525: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 09:24:33.345: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.820284]
2023-05-11 09:24:33.627: epoch 1:	0.00146580  	0.00261388  	0.00216236  
2023-05-11 09:24:33.627: Find a better model.
2023-05-11 09:24:54.390: [iter 2 : loss : 1.1357 = 0.6930 + 0.4426 + 0.0000, time: 20.760802]
2023-05-11 09:24:54.690: epoch 2:	0.00172490  	0.00288000  	0.00244900  
2023-05-11 09:24:54.690: Find a better model.
2023-05-11 09:25:15.680: [iter 3 : loss : 1.1349 = 0.6929 + 0.4420 + 0.0000, time: 20.985067]
2023-05-11 09:25:15.966: epoch 3:	0.00197661  	0.00398761  	0.00314182  
2023-05-11 09:25:15.966: Find a better model.
2023-05-11 09:25:36.576: [iter 4 : loss : 1.1365 = 0.6928 + 0.4436 + 0.0000, time: 20.605932]
2023-05-11 09:25:36.859: epoch 4:	0.00222091  	0.00319100  	0.00291264  
2023-05-11 09:25:57.700: [iter 5 : loss : 1.1356 = 0.6927 + 0.4429 + 0.0000, time: 20.836410]
2023-05-11 09:25:57.981: epoch 5:	0.00305745  	0.00600990  	0.00483644  
2023-05-11 09:25:57.981: Find a better model.
2023-05-11 09:26:18.530: [iter 6 : loss : 1.1373 = 0.6926 + 0.4447 + 0.0000, time: 20.545647]
2023-05-11 09:26:18.815: epoch 6:	0.00323512  	0.00593293  	0.00479606  
2023-05-11 09:26:39.665: [iter 7 : loss : 1.1360 = 0.6923 + 0.4437 + 0.0000, time: 20.846723]
2023-05-11 09:26:39.968: epoch 7:	0.00435297  	0.00922914  	0.00735265  
2023-05-11 09:26:39.968: Find a better model.
2023-05-11 09:27:00.526: [iter 8 : loss : 1.1378 = 0.6921 + 0.4457 + 0.0000, time: 20.553881]
2023-05-11 09:27:00.827: epoch 8:	0.00441960  	0.00907967  	0.00800574  
2023-05-11 09:27:21.472: [iter 9 : loss : 1.1361 = 0.6917 + 0.4444 + 0.0000, time: 20.640868]
2023-05-11 09:27:21.769: epoch 9:	0.00555965  	0.01259134  	0.01067245  
2023-05-11 09:27:21.769: Find a better model.
2023-05-11 09:27:42.326: [iter 10 : loss : 1.1382 = 0.6912 + 0.4469 + 0.0000, time: 20.551871]
2023-05-11 09:27:42.629: epoch 10:	0.00596682  	0.01316220  	0.01131458  
2023-05-11 09:27:42.629: Find a better model.
2023-05-11 09:28:03.249: [iter 11 : loss : 1.1356 = 0.6903 + 0.4452 + 0.0000, time: 20.616384]
2023-05-11 09:28:03.543: epoch 11:	0.00701063  	0.01635455  	0.01364999  
2023-05-11 09:28:03.543: Find a better model.
2023-05-11 09:28:24.117: [iter 12 : loss : 1.1376 = 0.6890 + 0.4486 + 0.0000, time: 20.571631]
2023-05-11 09:28:24.392: epoch 12:	0.00781015  	0.01806384  	0.01533786  
2023-05-11 09:28:24.392: Find a better model.
2023-05-11 09:28:45.041: [iter 13 : loss : 1.1337 = 0.6874 + 0.4462 + 0.0000, time: 20.645576]
2023-05-11 09:28:45.323: epoch 13:	0.01010509  	0.02537424  	0.02117660  
2023-05-11 09:28:45.323: Find a better model.
2023-05-11 09:29:05.693: [iter 14 : loss : 1.1356 = 0.6849 + 0.4506 + 0.0001, time: 20.365519]
2023-05-11 09:29:05.963: epoch 14:	0.01225938  	0.02967099  	0.02588885  
2023-05-11 09:29:05.963: Find a better model.
2023-05-11 09:29:26.616: [iter 15 : loss : 1.1288 = 0.6814 + 0.4473 + 0.0001, time: 20.648706]
2023-05-11 09:29:26.888: epoch 15:	0.01499116  	0.03781652  	0.03375829  
2023-05-11 09:29:26.889: Find a better model.
2023-05-11 09:29:47.065: [iter 16 : loss : 1.1285 = 0.6753 + 0.4531 + 0.0001, time: 20.172160]
2023-05-11 09:29:47.342: epoch 16:	0.01796724  	0.04505965  	0.04041943  
2023-05-11 09:29:47.342: Find a better model.
2023-05-11 09:30:07.790: [iter 17 : loss : 1.1147 = 0.6657 + 0.4488 + 0.0002, time: 20.443790]
2023-05-11 09:30:08.061: epoch 17:	0.02128390  	0.05511370  	0.04817370  
2023-05-11 09:30:08.061: Find a better model.
2023-05-11 09:30:28.470: [iter 18 : loss : 1.1065 = 0.6496 + 0.4566 + 0.0002, time: 20.405687]
2023-05-11 09:30:28.742: epoch 18:	0.02394167  	0.06192600  	0.05430717  
2023-05-11 09:30:28.743: Find a better model.
2023-05-11 09:30:49.012: [iter 19 : loss : 1.0769 = 0.6250 + 0.4515 + 0.0003, time: 20.265684]
2023-05-11 09:30:49.299: epoch 19:	0.02597017  	0.06749344  	0.05863262  
2023-05-11 09:30:49.299: Find a better model.
2023-05-11 09:31:09.617: [iter 20 : loss : 1.0524 = 0.5906 + 0.4614 + 0.0005, time: 20.314996]
2023-05-11 09:31:09.886: epoch 20:	0.02714727  	0.07071675  	0.06072948  
2023-05-11 09:31:09.886: Find a better model.
2023-05-11 09:31:30.194: [iter 21 : loss : 1.0035 = 0.5464 + 0.4563 + 0.0007, time: 20.303615]
2023-05-11 09:31:30.480: epoch 21:	0.02830216  	0.07440417  	0.06307866  
2023-05-11 09:31:30.480: Find a better model.
2023-05-11 09:31:51.039: [iter 22 : loss : 0.9648 = 0.4961 + 0.4678 + 0.0010, time: 20.555156]
2023-05-11 09:31:51.327: epoch 22:	0.02865013  	0.07663891  	0.06394970  
2023-05-11 09:31:51.327: Find a better model.
2023-05-11 09:32:11.984: [iter 23 : loss : 0.9088 = 0.4451 + 0.4625 + 0.0012, time: 20.652920]
2023-05-11 09:32:12.265: epoch 23:	0.02918314  	0.07741173  	0.06460984  
2023-05-11 09:32:12.265: Find a better model.
2023-05-11 09:32:32.817: [iter 24 : loss : 0.8726 = 0.3974 + 0.4736 + 0.0015, time: 20.547539]
2023-05-11 09:32:33.108: epoch 24:	0.02910912  	0.07783552  	0.06449389  
2023-05-11 09:32:33.108: Find a better model.
2023-05-11 09:32:53.711: [iter 25 : loss : 0.8235 = 0.3548 + 0.4669 + 0.0018, time: 20.600057]
2023-05-11 09:32:53.996: epoch 25:	0.02954592  	0.07955012  	0.06500823  
2023-05-11 09:32:53.996: Find a better model.
2023-05-11 09:33:14.390: [iter 26 : loss : 0.7979 = 0.3193 + 0.4765 + 0.0021, time: 20.390793]
2023-05-11 09:33:14.668: epoch 26:	0.02953853  	0.08018089  	0.06524921  
2023-05-11 09:33:14.668: Find a better model.
2023-05-11 09:33:35.333: [iter 27 : loss : 0.7594 = 0.2885 + 0.4685 + 0.0024, time: 20.660449]
2023-05-11 09:33:35.613: epoch 27:	0.02975322  	0.08082681  	0.06562091  
2023-05-11 09:33:35.613: Find a better model.
2023-05-11 09:33:56.176: [iter 28 : loss : 0.7430 = 0.2633 + 0.4770 + 0.0027, time: 20.559257]
2023-05-11 09:33:56.455: epoch 28:	0.02976802  	0.08077986  	0.06574196  
2023-05-11 09:34:17.124: [iter 29 : loss : 0.7131 = 0.2419 + 0.4683 + 0.0029, time: 20.664278]
2023-05-11 09:34:17.400: epoch 29:	0.02974581  	0.08093007  	0.06597846  
2023-05-11 09:34:17.400: Find a better model.
2023-05-11 09:34:37.970: [iter 30 : loss : 0.7022 = 0.2229 + 0.4761 + 0.0032, time: 20.567198]
2023-05-11 09:34:38.251: epoch 30:	0.02985686  	0.08194896  	0.06630030  
2023-05-11 09:34:38.251: Find a better model.
2023-05-11 09:34:59.071: [iter 31 : loss : 0.6777 = 0.2070 + 0.4673 + 0.0034, time: 20.816466]
2023-05-11 09:34:59.348: epoch 31:	0.02982725  	0.08152192  	0.06630173  
2023-05-11 09:35:19.918: [iter 32 : loss : 0.6718 = 0.1933 + 0.4748 + 0.0037, time: 20.565305]
2023-05-11 09:35:20.194: epoch 32:	0.02985687  	0.08182817  	0.06634760  
2023-05-11 09:35:40.864: [iter 33 : loss : 0.6517 = 0.1820 + 0.4658 + 0.0039, time: 20.665847]
2023-05-11 09:35:41.145: epoch 33:	0.02993829  	0.08179219  	0.06647294  
2023-05-11 09:36:01.937: [iter 34 : loss : 0.6485 = 0.1710 + 0.4734 + 0.0041, time: 20.789448]
2023-05-11 09:36:02.216: epoch 34:	0.02995309  	0.08173753  	0.06644367  
2023-05-11 09:36:22.871: [iter 35 : loss : 0.6296 = 0.1610 + 0.4644 + 0.0043, time: 20.651911]
2023-05-11 09:36:23.149: epoch 35:	0.02996049  	0.08165877  	0.06640028  
2023-05-11 09:36:43.752: [iter 36 : loss : 0.6297 = 0.1530 + 0.4722 + 0.0045, time: 20.600269]
2023-05-11 09:36:44.032: epoch 36:	0.02995308  	0.08166838  	0.06636008  
2023-05-11 09:37:05.015: [iter 37 : loss : 0.6133 = 0.1453 + 0.4633 + 0.0047, time: 20.978881]
2023-05-11 09:37:05.294: epoch 37:	0.02995308  	0.08185515  	0.06663428  
2023-05-11 09:37:25.964: [iter 38 : loss : 0.6142 = 0.1383 + 0.4711 + 0.0049, time: 20.665857]
2023-05-11 09:37:26.242: epoch 38:	0.02986424  	0.08127172  	0.06646275  
2023-05-11 09:37:47.208: [iter 39 : loss : 0.5996 = 0.1325 + 0.4621 + 0.0051, time: 20.962930]
2023-05-11 09:37:47.486: epoch 39:	0.02989385  	0.08171265  	0.06658240  
2023-05-11 09:38:08.480: [iter 40 : loss : 0.6012 = 0.1264 + 0.4696 + 0.0052, time: 20.990768]
2023-05-11 09:38:08.758: epoch 40:	0.02981981  	0.08123902  	0.06646436  
2023-05-11 09:38:29.627: [iter 41 : loss : 0.5873 = 0.1209 + 0.4610 + 0.0054, time: 20.865171]
2023-05-11 09:38:29.904: epoch 41:	0.02972358  	0.08090975  	0.06620976  
2023-05-11 09:38:50.868: [iter 42 : loss : 0.5903 = 0.1159 + 0.4688 + 0.0056, time: 20.960943]
2023-05-11 09:38:51.146: epoch 42:	0.02960513  	0.08075554  	0.06609050  
2023-05-11 09:39:12.220: [iter 43 : loss : 0.5772 = 0.1114 + 0.4601 + 0.0057, time: 21.069500]
2023-05-11 09:39:12.495: epoch 43:	0.02962734  	0.08089563  	0.06613943  
2023-05-11 09:39:33.453: [iter 44 : loss : 0.5821 = 0.1083 + 0.4679 + 0.0059, time: 20.954021]
2023-05-11 09:39:33.729: epoch 44:	0.02974579  	0.08123177  	0.06635177  
2023-05-11 09:39:54.758: [iter 45 : loss : 0.5691 = 0.1039 + 0.4591 + 0.0061, time: 21.024901]
2023-05-11 09:39:55.033: epoch 45:	0.02971617  	0.08121035  	0.06629831  
2023-05-11 09:40:16.081: [iter 46 : loss : 0.5740 = 0.1005 + 0.4673 + 0.0062, time: 21.044570]
2023-05-11 09:40:16.359: epoch 46:	0.02968656  	0.08124998  	0.06621634  
2023-05-11 09:40:37.389: [iter 47 : loss : 0.5617 = 0.0969 + 0.4584 + 0.0064, time: 21.027631]
2023-05-11 09:40:37.665: epoch 47:	0.02969397  	0.08135256  	0.06616477  
2023-05-11 09:40:58.434: [iter 48 : loss : 0.5672 = 0.0943 + 0.4664 + 0.0065, time: 20.763534]
2023-05-11 09:40:58.711: epoch 48:	0.02973097  	0.08161177  	0.06628325  
2023-05-11 09:41:19.772: [iter 49 : loss : 0.5556 = 0.0912 + 0.4578 + 0.0067, time: 21.056572]
2023-05-11 09:41:20.053: epoch 49:	0.02967176  	0.08095141  	0.06603222  
2023-05-11 09:41:40.823: [iter 50 : loss : 0.5612 = 0.0887 + 0.4656 + 0.0068, time: 20.767498]
2023-05-11 09:41:41.101: epoch 50:	0.02958292  	0.08058764  	0.06590430  
2023-05-11 09:42:02.161: [iter 51 : loss : 0.5500 = 0.0859 + 0.4571 + 0.0069, time: 21.056543]
2023-05-11 09:42:02.440: epoch 51:	0.02948667  	0.07993229  	0.06576342  
2023-05-11 09:42:23.432: [iter 52 : loss : 0.5553 = 0.0831 + 0.4652 + 0.0071, time: 20.989730]
2023-05-11 09:42:23.713: epoch 52:	0.02939043  	0.07960741  	0.06552475  
2023-05-11 09:42:44.926: [iter 53 : loss : 0.5458 = 0.0820 + 0.4565 + 0.0072, time: 21.210038]
2023-05-11 09:42:45.205: epoch 53:	0.02926457  	0.07935213  	0.06539781  
2023-05-11 09:43:06.377: [iter 54 : loss : 0.5520 = 0.0799 + 0.4648 + 0.0073, time: 21.168170]
2023-05-11 09:43:06.656: epoch 54:	0.02929418  	0.07937118  	0.06529784  
2023-05-11 09:43:27.933: [iter 55 : loss : 0.5415 = 0.0779 + 0.4562 + 0.0075, time: 21.272146]
2023-05-11 09:43:28.212: epoch 55:	0.02919054  	0.07915615  	0.06506152  
2023-05-11 09:43:28.212: Early stopping is trigger at epoch: 55
2023-05-11 09:43:28.212: best_result@epoch 30:

2023-05-11 09:43:28.212: 		0.0299      	0.0819      	0.0663      
2023-05-11 10:07:22.486: my pid: 15408
2023-05-11 10:07:22.486: model: model.general_recommender.SGL
2023-05-11 10:07:22.486: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 10:07:22.486: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 10:07:26.053: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 10:07:47.022: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.968918]
2023-05-11 10:07:47.305: epoch 1:	0.00157684  	0.00317119  	0.00251242  
2023-05-11 10:07:47.305: Find a better model.
2023-05-11 10:08:08.285: [iter 2 : loss : 1.1357 = 0.6930 + 0.4426 + 0.0000, time: 20.976939]
2023-05-11 10:08:08.587: epoch 2:	0.00183595  	0.00269718  	0.00265057  
2023-05-11 10:08:29.767: [iter 3 : loss : 1.1350 = 0.6929 + 0.4420 + 0.0000, time: 21.176130]
2023-05-11 10:08:30.068: epoch 3:	0.00233195  	0.00468913  	0.00376564  
2023-05-11 10:08:30.068: Find a better model.
2023-05-11 10:08:51.260: [iter 4 : loss : 1.1365 = 0.6928 + 0.4437 + 0.0000, time: 21.187608]
2023-05-11 10:08:51.558: epoch 4:	0.00228754  	0.00398219  	0.00328285  
2023-05-11 10:09:12.590: [iter 5 : loss : 1.1356 = 0.6927 + 0.4429 + 0.0000, time: 21.026631]
2023-05-11 10:09:12.893: epoch 5:	0.00330175  	0.00673489  	0.00531461  
2023-05-11 10:09:12.893: Find a better model.
2023-05-11 10:09:33.637: [iter 6 : loss : 1.1373 = 0.6926 + 0.4447 + 0.0000, time: 20.741556]
2023-05-11 10:09:33.937: epoch 6:	0.00336838  	0.00601590  	0.00507474  
2023-05-11 10:09:54.776: [iter 7 : loss : 1.1360 = 0.6923 + 0.4437 + 0.0000, time: 20.835303]
2023-05-11 10:09:55.072: epoch 7:	0.00417530  	0.00876472  	0.00709480  
2023-05-11 10:09:55.072: Find a better model.
2023-05-11 10:10:16.025: [iter 8 : loss : 1.1379 = 0.6921 + 0.4457 + 0.0000, time: 20.948030]
2023-05-11 10:10:16.323: epoch 8:	0.00442700  	0.00813931  	0.00710144  
2023-05-11 10:10:37.365: [iter 9 : loss : 1.1362 = 0.6917 + 0.4444 + 0.0000, time: 21.038638]
2023-05-11 10:10:37.656: epoch 9:	0.00538939  	0.01252442  	0.01073107  
2023-05-11 10:10:37.656: Find a better model.
2023-05-11 10:10:58.399: [iter 10 : loss : 1.1382 = 0.6912 + 0.4470 + 0.0000, time: 20.739243]
2023-05-11 10:10:58.694: epoch 10:	0.00593720  	0.01265656  	0.01137852  
2023-05-11 10:10:58.694: Find a better model.
2023-05-11 10:11:19.533: [iter 11 : loss : 1.1356 = 0.6903 + 0.4453 + 0.0000, time: 20.836408]
2023-05-11 10:11:19.826: epoch 11:	0.00696622  	0.01577961  	0.01362165  
2023-05-11 10:11:19.826: Find a better model.
2023-05-11 10:11:40.590: [iter 12 : loss : 1.1376 = 0.6890 + 0.4486 + 0.0000, time: 20.759702]
2023-05-11 10:11:40.881: epoch 12:	0.00819511  	0.01818858  	0.01642727  
2023-05-11 10:11:40.881: Find a better model.
2023-05-11 10:12:01.563: [iter 13 : loss : 1.1336 = 0.6873 + 0.4463 + 0.0000, time: 20.677830]
2023-05-11 10:12:01.849: epoch 13:	0.01074916  	0.02583944  	0.02259440  
2023-05-11 10:12:01.849: Find a better model.
2023-05-11 10:12:22.427: [iter 14 : loss : 1.1356 = 0.6849 + 0.4507 + 0.0001, time: 20.572805]
2023-05-11 10:12:22.713: epoch 14:	0.01267396  	0.03008242  	0.02718151  
2023-05-11 10:12:22.714: Find a better model.
2023-05-11 10:12:43.574: [iter 15 : loss : 1.1287 = 0.6813 + 0.4474 + 0.0001, time: 20.855589]
2023-05-11 10:12:43.861: epoch 15:	0.01550199  	0.03827178  	0.03396346  
2023-05-11 10:12:43.861: Find a better model.
2023-05-11 10:13:04.384: [iter 16 : loss : 1.1286 = 0.6753 + 0.4532 + 0.0001, time: 20.519340]
2023-05-11 10:13:04.673: epoch 16:	0.01834480  	0.04578878  	0.04067163  
2023-05-11 10:13:04.673: Find a better model.
2023-05-11 10:13:25.476: [iter 17 : loss : 1.1147 = 0.6657 + 0.4489 + 0.0002, time: 20.798563]
2023-05-11 10:13:25.769: epoch 17:	0.02135051  	0.05461779  	0.04800648  
2023-05-11 10:13:25.769: Find a better model.
2023-05-11 10:13:47.213: [iter 18 : loss : 1.1065 = 0.6496 + 0.4566 + 0.0002, time: 21.439616]
2023-05-11 10:13:47.514: epoch 18:	0.02381579  	0.06205977  	0.05379194  
2023-05-11 10:13:47.514: Find a better model.
2023-05-11 10:14:08.779: [iter 19 : loss : 1.0770 = 0.6251 + 0.4516 + 0.0003, time: 21.261674]
2023-05-11 10:14:09.088: epoch 19:	0.02599976  	0.06759015  	0.05835174  
2023-05-11 10:14:09.088: Find a better model.
2023-05-11 10:14:30.365: [iter 20 : loss : 1.0527 = 0.5909 + 0.4614 + 0.0005, time: 21.273555]
2023-05-11 10:14:30.671: epoch 20:	0.02713246  	0.07202580  	0.06086669  
2023-05-11 10:14:30.671: Find a better model.
2023-05-11 10:14:51.762: [iter 21 : loss : 1.0040 = 0.5466 + 0.4566 + 0.0007, time: 21.086355]
2023-05-11 10:14:52.062: epoch 21:	0.02813931  	0.07470486  	0.06294930  
2023-05-11 10:14:52.062: Find a better model.
2023-05-11 10:15:12.959: [iter 22 : loss : 0.9653 = 0.4964 + 0.4679 + 0.0010, time: 20.892351]
2023-05-11 10:15:13.244: epoch 22:	0.02822075  	0.07582103  	0.06346074  
2023-05-11 10:15:13.244: Find a better model.
2023-05-11 10:15:34.135: [iter 23 : loss : 0.9091 = 0.4450 + 0.4629 + 0.0012, time: 20.885946]
2023-05-11 10:15:34.418: epoch 23:	0.02890924  	0.07781882  	0.06460492  
2023-05-11 10:15:34.418: Find a better model.
2023-05-11 10:15:55.140: [iter 24 : loss : 0.8729 = 0.3975 + 0.4740 + 0.0015, time: 20.717885]
2023-05-11 10:15:55.423: epoch 24:	0.02891665  	0.07867496  	0.06485946  
2023-05-11 10:15:55.424: Find a better model.
2023-05-11 10:16:16.431: [iter 25 : loss : 0.8241 = 0.3552 + 0.4672 + 0.0018, time: 21.003719]
2023-05-11 10:16:16.709: epoch 25:	0.02931642  	0.07990906  	0.06537485  
2023-05-11 10:16:16.709: Find a better model.
2023-05-11 10:16:37.503: [iter 26 : loss : 0.7982 = 0.3194 + 0.4767 + 0.0021, time: 20.787822]
2023-05-11 10:16:37.783: epoch 26:	0.02930902  	0.08034737  	0.06563677  
2023-05-11 10:16:37.783: Find a better model.
2023-05-11 10:16:58.658: [iter 27 : loss : 0.7602 = 0.2890 + 0.4688 + 0.0024, time: 20.870507]
2023-05-11 10:16:58.942: epoch 27:	0.02959034  	0.08071285  	0.06578861  
2023-05-11 10:16:58.942: Find a better model.
2023-05-11 10:17:19.694: [iter 28 : loss : 0.7434 = 0.2635 + 0.4773 + 0.0027, time: 20.748425]
2023-05-11 10:17:19.977: epoch 28:	0.02952371  	0.08063719  	0.06592241  
2023-05-11 10:17:40.871: [iter 29 : loss : 0.7134 = 0.2420 + 0.4684 + 0.0029, time: 20.890045]
2023-05-11 10:17:41.149: epoch 29:	0.02968659  	0.08137454  	0.06633629  
2023-05-11 10:17:41.149: Find a better model.
2023-05-11 10:18:02.060: [iter 30 : loss : 0.7027 = 0.2232 + 0.4763 + 0.0032, time: 20.906816]
2023-05-11 10:18:02.343: epoch 30:	0.02972361  	0.08160502  	0.06628224  
2023-05-11 10:18:02.343: Find a better model.
2023-05-11 10:18:23.408: [iter 31 : loss : 0.6782 = 0.2073 + 0.4674 + 0.0034, time: 21.059566]
2023-05-11 10:18:23.689: epoch 31:	0.02981985  	0.08169872  	0.06659317  
2023-05-11 10:18:23.689: Find a better model.
2023-05-11 10:18:44.475: [iter 32 : loss : 0.6723 = 0.1937 + 0.4749 + 0.0037, time: 20.782019]
2023-05-11 10:18:44.754: epoch 32:	0.02976804  	0.08171564  	0.06659639  
2023-05-11 10:18:44.754: Find a better model.
2023-05-11 10:19:05.827: [iter 33 : loss : 0.6522 = 0.1823 + 0.4660 + 0.0039, time: 21.067841]
2023-05-11 10:19:06.103: epoch 33:	0.02999754  	0.08227493  	0.06682398  
2023-05-11 10:19:06.103: Find a better model.
2023-05-11 10:19:27.007: [iter 34 : loss : 0.6491 = 0.1714 + 0.4736 + 0.0041, time: 20.900402]
2023-05-11 10:19:27.288: epoch 34:	0.02994571  	0.08220191  	0.06681586  
2023-05-11 10:19:48.208: [iter 35 : loss : 0.6301 = 0.1613 + 0.4645 + 0.0043, time: 20.915871]
2023-05-11 10:19:48.490: epoch 35:	0.02999014  	0.08207981  	0.06688923  
2023-05-11 10:20:09.435: [iter 36 : loss : 0.6300 = 0.1533 + 0.4722 + 0.0045, time: 20.941478]
2023-05-11 10:20:09.712: epoch 36:	0.02999754  	0.08216318  	0.06701338  
2023-05-11 10:20:30.803: [iter 37 : loss : 0.6137 = 0.1457 + 0.4633 + 0.0047, time: 21.087955]
2023-05-11 10:20:31.088: epoch 37:	0.03001235  	0.08212893  	0.06706695  
2023-05-11 10:20:52.054: [iter 38 : loss : 0.6146 = 0.1388 + 0.4709 + 0.0049, time: 20.961677]
2023-05-11 10:20:52.352: epoch 38:	0.02991611  	0.08177891  	0.06691568  
2023-05-11 10:21:13.776: [iter 39 : loss : 0.6001 = 0.1329 + 0.4621 + 0.0051, time: 21.420414]
2023-05-11 10:21:14.072: epoch 39:	0.03001235  	0.08188146  	0.06699194  
2023-05-11 10:21:35.485: [iter 40 : loss : 0.6017 = 0.1268 + 0.4697 + 0.0052, time: 21.408343]
2023-05-11 10:21:35.807: epoch 40:	0.02983468  	0.08127207  	0.06677078  
2023-05-11 10:21:57.848: [iter 41 : loss : 0.5876 = 0.1212 + 0.4610 + 0.0054, time: 22.037244]
2023-05-11 10:21:58.147: epoch 41:	0.02990869  	0.08184718  	0.06698635  
2023-05-11 10:22:20.043: [iter 42 : loss : 0.5907 = 0.1162 + 0.4689 + 0.0056, time: 21.892194]
2023-05-11 10:22:20.340: epoch 42:	0.02974582  	0.08103605  	0.06662345  
2023-05-11 10:22:41.748: [iter 43 : loss : 0.5778 = 0.1119 + 0.4601 + 0.0057, time: 21.403405]
2023-05-11 10:22:42.033: epoch 43:	0.02974582  	0.08111232  	0.06664050  
2023-05-11 10:23:03.002: [iter 44 : loss : 0.5821 = 0.1082 + 0.4680 + 0.0059, time: 20.964884]
2023-05-11 10:23:03.301: epoch 44:	0.02959036  	0.08105332  	0.06667366  
2023-05-11 10:23:24.516: [iter 45 : loss : 0.5693 = 0.1040 + 0.4593 + 0.0061, time: 21.211063]
2023-05-11 10:23:24.805: epoch 45:	0.02970142  	0.08109816  	0.06670259  
2023-05-11 10:23:45.986: [iter 46 : loss : 0.5737 = 0.1004 + 0.4671 + 0.0062, time: 21.174292]
2023-05-11 10:23:46.275: epoch 46:	0.02971621  	0.08093920  	0.06665172  
2023-05-11 10:24:09.928: [iter 47 : loss : 0.5623 = 0.0975 + 0.4584 + 0.0064, time: 23.649496]
2023-05-11 10:24:10.226: epoch 47:	0.02973102  	0.08106211  	0.06645365  
2023-05-11 10:24:30.699: [iter 48 : loss : 0.5672 = 0.0943 + 0.4665 + 0.0065, time: 20.468471]
2023-05-11 10:24:30.963: epoch 48:	0.02978284  	0.08127397  	0.06636286  
2023-05-11 10:24:51.663: [iter 49 : loss : 0.5561 = 0.0917 + 0.4578 + 0.0067, time: 20.696390]
2023-05-11 10:24:51.924: epoch 49:	0.02973842  	0.08087102  	0.06616218  
2023-05-11 10:25:12.136: [iter 50 : loss : 0.5616 = 0.0890 + 0.4657 + 0.0068, time: 20.207408]
2023-05-11 10:25:12.395: epoch 50:	0.02963477  	0.08049823  	0.06605121  
2023-05-11 10:25:32.861: [iter 51 : loss : 0.5501 = 0.0860 + 0.4572 + 0.0069, time: 20.461521]
2023-05-11 10:25:33.143: epoch 51:	0.02964958  	0.08053143  	0.06597956  
2023-05-11 10:25:53.503: [iter 52 : loss : 0.5559 = 0.0835 + 0.4653 + 0.0071, time: 20.356628]
2023-05-11 10:25:53.770: epoch 52:	0.02969399  	0.08043609  	0.06593092  
2023-05-11 10:26:14.219: [iter 53 : loss : 0.5461 = 0.0822 + 0.4567 + 0.0072, time: 20.444607]
2023-05-11 10:26:14.482: epoch 53:	0.02964216  	0.08009582  	0.06578324  
2023-05-11 10:26:34.707: [iter 54 : loss : 0.5519 = 0.0798 + 0.4648 + 0.0073, time: 20.221239]
2023-05-11 10:26:34.968: epoch 54:	0.02952371  	0.07968096  	0.06566153  
2023-05-11 10:26:55.629: [iter 55 : loss : 0.5414 = 0.0777 + 0.4562 + 0.0075, time: 20.656174]
2023-05-11 10:26:55.890: epoch 55:	0.02958294  	0.08002567  	0.06568564  
2023-05-11 10:27:16.090: [iter 56 : loss : 0.5472 = 0.0755 + 0.4642 + 0.0076, time: 20.196428]
2023-05-11 10:27:16.356: epoch 56:	0.02945708  	0.07962973  	0.06556355  
2023-05-11 10:27:36.672: [iter 57 : loss : 0.5365 = 0.0732 + 0.4556 + 0.0077, time: 20.312023]
2023-05-11 10:27:36.932: epoch 57:	0.02949410  	0.07958856  	0.06553927  
2023-05-11 10:27:57.455: [iter 58 : loss : 0.5435 = 0.0718 + 0.4639 + 0.0078, time: 20.518459]
2023-05-11 10:27:57.717: epoch 58:	0.02937565  	0.07908121  	0.06533267  
2023-05-11 10:27:57.717: Early stopping is trigger at epoch: 58
2023-05-11 10:27:57.717: best_result@epoch 33:

2023-05-11 10:27:57.717: 		0.0300      	0.0823      	0.0668      
2023-05-11 10:31:25.038: my pid: 3616
2023-05-11 10:31:25.039: model: model.general_recommender.SGL
2023-05-11 10:31:25.039: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 10:31:25.039: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 10:31:28.415: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 10:31:49.323: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.907193]
2023-05-11 10:31:49.589: epoch 1:	0.00119929  	0.00272656  	0.00201535  
2023-05-11 10:31:49.589: Find a better model.
2023-05-11 10:32:10.646: [iter 2 : loss : 1.1364 = 0.6930 + 0.4434 + 0.0000, time: 21.053353]
2023-05-11 10:32:10.928: epoch 2:	0.00156204  	0.00267985  	0.00230864  
2023-05-11 10:32:32.148: [iter 3 : loss : 1.1349 = 0.6929 + 0.4420 + 0.0000, time: 21.216390]
2023-05-11 10:32:32.431: epoch 3:	0.00200622  	0.00359869  	0.00288610  
2023-05-11 10:32:32.431: Find a better model.
2023-05-11 10:32:53.574: [iter 4 : loss : 1.1373 = 0.6929 + 0.4444 + 0.0000, time: 21.138371]
2023-05-11 10:32:53.861: epoch 4:	0.00213947  	0.00381901  	0.00326377  
2023-05-11 10:32:53.861: Find a better model.
2023-05-11 10:33:15.117: [iter 5 : loss : 1.1355 = 0.6927 + 0.4428 + 0.0000, time: 21.252625]
2023-05-11 10:33:15.409: epoch 5:	0.00290198  	0.00575392  	0.00504113  
2023-05-11 10:33:15.409: Find a better model.
2023-05-11 10:33:36.566: [iter 6 : loss : 1.1380 = 0.6926 + 0.4454 + 0.0000, time: 21.151435]
2023-05-11 10:33:36.873: epoch 6:	0.00276873  	0.00557745  	0.00438853  
2023-05-11 10:33:58.123: [iter 7 : loss : 1.1359 = 0.6924 + 0.4435 + 0.0000, time: 21.246764]
2023-05-11 10:33:58.409: epoch 7:	0.00387918  	0.00835500  	0.00687591  
2023-05-11 10:33:58.409: Find a better model.
2023-05-11 10:34:19.337: [iter 8 : loss : 1.1386 = 0.6922 + 0.4464 + 0.0000, time: 20.924262]
2023-05-11 10:34:19.622: epoch 8:	0.00378294  	0.00796951  	0.00651584  
2023-05-11 10:34:40.885: [iter 9 : loss : 1.1361 = 0.6918 + 0.4442 + 0.0000, time: 21.258705]
2023-05-11 10:34:41.186: epoch 9:	0.00465650  	0.01100015  	0.00902548  
2023-05-11 10:34:41.186: Find a better model.
2023-05-11 10:35:02.326: [iter 10 : loss : 1.1391 = 0.6914 + 0.4476 + 0.0000, time: 21.134765]
2023-05-11 10:35:02.633: epoch 10:	0.00476754  	0.01050624  	0.00897903  
2023-05-11 10:35:23.684: [iter 11 : loss : 1.1358 = 0.6907 + 0.4450 + 0.0000, time: 21.041696]
2023-05-11 10:35:23.981: epoch 11:	0.00610748  	0.01431867  	0.01202960  
2023-05-11 10:35:23.981: Find a better model.
2023-05-11 10:35:44.911: [iter 12 : loss : 1.1388 = 0.6897 + 0.4491 + 0.0000, time: 20.926511]
2023-05-11 10:35:45.206: epoch 12:	0.00649983  	0.01471708  	0.01251726  
2023-05-11 10:35:45.206: Find a better model.
2023-05-11 10:36:06.460: [iter 13 : loss : 1.1342 = 0.6883 + 0.4459 + 0.0000, time: 21.250155]
2023-05-11 10:36:06.759: epoch 13:	0.00836537  	0.02036261  	0.01708779  
2023-05-11 10:36:06.759: Find a better model.
2023-05-11 10:36:27.916: [iter 14 : loss : 1.1373 = 0.6864 + 0.4509 + 0.0001, time: 21.154289]
2023-05-11 10:36:28.214: epoch 14:	0.01022354  	0.02389601  	0.02119693  
2023-05-11 10:36:28.214: Find a better model.
2023-05-11 10:36:49.422: [iter 15 : loss : 1.1306 = 0.6837 + 0.4468 + 0.0001, time: 21.204107]
2023-05-11 10:36:49.717: epoch 15:	0.01284424  	0.03114571  	0.02745901  
2023-05-11 10:36:49.717: Find a better model.
2023-05-11 10:37:11.072: [iter 16 : loss : 1.1327 = 0.6795 + 0.4531 + 0.0001, time: 21.350631]
2023-05-11 10:37:11.363: epoch 16:	0.01538352  	0.03689681  	0.03278761  
2023-05-11 10:37:11.363: Find a better model.
2023-05-11 10:37:32.404: [iter 17 : loss : 1.1211 = 0.6729 + 0.4481 + 0.0001, time: 21.038640]
2023-05-11 10:37:32.694: epoch 17:	0.01858169  	0.04492083  	0.04042037  
2023-05-11 10:37:32.694: Find a better model.
2023-05-11 10:37:53.832: [iter 18 : loss : 1.1184 = 0.6618 + 0.4563 + 0.0002, time: 21.133275]
2023-05-11 10:37:54.120: epoch 18:	0.02121725  	0.05246047  	0.04633526  
2023-05-11 10:37:54.120: Find a better model.
2023-05-11 10:38:15.231: [iter 19 : loss : 1.0949 = 0.6445 + 0.4501 + 0.0003, time: 21.107903]
2023-05-11 10:38:15.521: epoch 19:	0.02389722  	0.05988170  	0.05249582  
2023-05-11 10:38:15.521: Find a better model.
2023-05-11 10:38:36.615: [iter 20 : loss : 1.0802 = 0.6190 + 0.4608 + 0.0004, time: 21.089469]
2023-05-11 10:38:36.905: epoch 20:	0.02582207  	0.06574912  	0.05652898  
2023-05-11 10:38:36.906: Find a better model.
2023-05-11 10:38:57.988: [iter 21 : loss : 1.0378 = 0.5834 + 0.4539 + 0.0005, time: 21.079524]
2023-05-11 10:38:58.276: epoch 21:	0.02714726  	0.07055607  	0.05966123  
2023-05-11 10:38:58.276: Find a better model.
2023-05-11 10:39:19.410: [iter 22 : loss : 1.0064 = 0.5390 + 0.4666 + 0.0008, time: 21.130360]
2023-05-11 10:39:19.700: epoch 22:	0.02773951  	0.07274557  	0.06089612  
2023-05-11 10:39:19.700: Find a better model.
2023-05-11 10:39:40.810: [iter 23 : loss : 0.9503 = 0.4895 + 0.4597 + 0.0010, time: 21.106460]
2023-05-11 10:39:41.100: epoch 23:	0.02830956  	0.07503465  	0.06237588  
2023-05-11 10:39:41.101: Find a better model.
2023-05-11 10:40:02.204: [iter 24 : loss : 0.9143 = 0.4398 + 0.4732 + 0.0013, time: 21.099580]
2023-05-11 10:40:02.493: epoch 24:	0.02849464  	0.07667914  	0.06297506  
2023-05-11 10:40:02.493: Find a better model.
2023-05-11 10:40:23.544: [iter 25 : loss : 0.8597 = 0.3931 + 0.4650 + 0.0016, time: 21.047608]
2023-05-11 10:40:23.835: epoch 25:	0.02898324  	0.07804100  	0.06383637  
2023-05-11 10:40:23.835: Find a better model.
2023-05-11 10:40:44.811: [iter 26 : loss : 0.8320 = 0.3527 + 0.4775 + 0.0019, time: 20.973374]
2023-05-11 10:40:45.102: epoch 26:	0.02899066  	0.07874881  	0.06388899  
2023-05-11 10:40:45.103: Find a better model.
2023-05-11 10:41:06.317: [iter 27 : loss : 0.7874 = 0.3174 + 0.4679 + 0.0022, time: 21.211098]
2023-05-11 10:41:06.607: epoch 27:	0.02913872  	0.07905452  	0.06412952  
2023-05-11 10:41:06.607: Find a better model.
2023-05-11 10:41:27.582: [iter 28 : loss : 0.7691 = 0.2880 + 0.4786 + 0.0024, time: 20.970408]
2023-05-11 10:41:27.872: epoch 28:	0.02920535  	0.07930937  	0.06418794  
2023-05-11 10:41:27.872: Find a better model.
2023-05-11 10:41:50.712: [iter 29 : loss : 0.7338 = 0.2629 + 0.4682 + 0.0027, time: 22.835059]
2023-05-11 10:41:51.008: epoch 29:	0.02939043  	0.07972117  	0.06436082  
2023-05-11 10:41:51.008: Find a better model.
2023-05-11 10:42:11.754: [iter 30 : loss : 0.7227 = 0.2412 + 0.4785 + 0.0030, time: 20.743257]
2023-05-11 10:42:12.065: epoch 30:	0.02941263  	0.07971974  	0.06456687  
2023-05-11 10:42:32.730: [iter 31 : loss : 0.6937 = 0.2229 + 0.4676 + 0.0032, time: 20.660899]
2023-05-11 10:42:33.019: epoch 31:	0.02960513  	0.08049713  	0.06498682  
2023-05-11 10:42:33.019: Find a better model.
2023-05-11 10:42:53.560: [iter 32 : loss : 0.6880 = 0.2072 + 0.4774 + 0.0034, time: 20.518436]
2023-05-11 10:42:53.843: epoch 32:	0.02969396  	0.08092182  	0.06521328  
2023-05-11 10:42:53.843: Find a better model.
2023-05-11 10:43:14.727: [iter 33 : loss : 0.6641 = 0.1942 + 0.4663 + 0.0037, time: 20.878507]
2023-05-11 10:43:15.010: epoch 33:	0.02982722  	0.08100326  	0.06551757  
2023-05-11 10:43:15.010: Find a better model.
2023-05-11 10:43:35.965: [iter 34 : loss : 0.6617 = 0.1817 + 0.4761 + 0.0039, time: 20.951059]
2023-05-11 10:43:36.241: epoch 34:	0.02991606  	0.08103491  	0.06564543  
2023-05-11 10:43:36.241: Find a better model.
2023-05-11 10:43:56.884: [iter 35 : loss : 0.6397 = 0.1708 + 0.4648 + 0.0041, time: 20.639147]
2023-05-11 10:43:57.171: epoch 35:	0.02995309  	0.08108088  	0.06601374  
2023-05-11 10:43:57.171: Find a better model.
2023-05-11 10:44:17.754: [iter 36 : loss : 0.6404 = 0.1616 + 0.4745 + 0.0043, time: 20.579072]
2023-05-11 10:44:18.037: epoch 36:	0.03007155  	0.08146109  	0.06627323  
2023-05-11 10:44:18.037: Find a better model.
2023-05-11 10:44:38.848: [iter 37 : loss : 0.6213 = 0.1533 + 0.4635 + 0.0045, time: 20.805736]
2023-05-11 10:44:39.146: epoch 37:	0.03004193  	0.08115239  	0.06627116  
2023-05-11 10:44:59.694: [iter 38 : loss : 0.6235 = 0.1452 + 0.4736 + 0.0047, time: 20.543812]
2023-05-11 10:44:59.959: epoch 38:	0.03010856  	0.08120164  	0.06638111  
2023-05-11 10:45:20.837: [iter 39 : loss : 0.6059 = 0.1387 + 0.4623 + 0.0049, time: 20.873847]
2023-05-11 10:45:21.148: epoch 39:	0.03007894  	0.08074511  	0.06642120  
2023-05-11 10:45:41.868: [iter 40 : loss : 0.6098 = 0.1324 + 0.4723 + 0.0051, time: 20.716714]
2023-05-11 10:45:42.156: epoch 40:	0.03012336  	0.08081126  	0.06631260  
2023-05-11 10:46:03.235: [iter 41 : loss : 0.5925 = 0.1261 + 0.4612 + 0.0052, time: 21.074093]
2023-05-11 10:46:03.514: epoch 41:	0.03016037  	0.08099885  	0.06643048  
2023-05-11 10:46:24.649: [iter 42 : loss : 0.5975 = 0.1207 + 0.4714 + 0.0054, time: 21.130998]
2023-05-11 10:46:24.929: epoch 42:	0.03008633  	0.08053305  	0.06622062  
2023-05-11 10:46:46.041: [iter 43 : loss : 0.5820 = 0.1162 + 0.4603 + 0.0056, time: 21.109359]
2023-05-11 10:46:46.323: epoch 43:	0.03013816  	0.08051998  	0.06626583  
2023-05-11 10:47:07.471: [iter 44 : loss : 0.5883 = 0.1123 + 0.4702 + 0.0058, time: 21.143230]
2023-05-11 10:47:07.754: epoch 44:	0.03001231  	0.08014918  	0.06592353  
2023-05-11 10:47:29.001: [iter 45 : loss : 0.5729 = 0.1077 + 0.4593 + 0.0059, time: 21.242923]
2023-05-11 10:47:29.281: epoch 45:	0.02995308  	0.08007439  	0.06595041  
2023-05-11 10:47:50.270: [iter 46 : loss : 0.5796 = 0.1042 + 0.4694 + 0.0061, time: 20.984761]
2023-05-11 10:47:50.548: epoch 46:	0.02990126  	0.07982557  	0.06556481  
2023-05-11 10:48:11.588: [iter 47 : loss : 0.5652 = 0.1004 + 0.4585 + 0.0062, time: 21.035611]
2023-05-11 10:48:11.872: epoch 47:	0.02981242  	0.07941616  	0.06545857  
2023-05-11 10:48:33.026: [iter 48 : loss : 0.5725 = 0.0973 + 0.4688 + 0.0064, time: 21.149483]
2023-05-11 10:48:33.305: epoch 48:	0.02977541  	0.07936442  	0.06544749  
2023-05-11 10:48:54.395: [iter 49 : loss : 0.5587 = 0.0944 + 0.4578 + 0.0065, time: 21.086501]
2023-05-11 10:48:54.685: epoch 49:	0.02976800  	0.07939760  	0.06525245  
2023-05-11 10:49:15.787: [iter 50 : loss : 0.5663 = 0.0915 + 0.4682 + 0.0067, time: 21.096644]
2023-05-11 10:49:16.066: epoch 50:	0.02961993  	0.07872622  	0.06491905  
2023-05-11 10:49:37.384: [iter 51 : loss : 0.5525 = 0.0886 + 0.4571 + 0.0068, time: 21.314597]
2023-05-11 10:49:37.670: epoch 51:	0.02958292  	0.07860512  	0.06485527  
2023-05-11 10:50:00.967: [iter 52 : loss : 0.5604 = 0.0858 + 0.4677 + 0.0069, time: 23.293436]
2023-05-11 10:50:01.269: epoch 52:	0.02956811  	0.07793722  	0.06471103  
2023-05-11 10:50:23.425: [iter 53 : loss : 0.5481 = 0.0844 + 0.4566 + 0.0071, time: 22.152643]
2023-05-11 10:50:23.697: epoch 53:	0.02947186  	0.07766701  	0.06457182  
2023-05-11 10:51:07.196: my pid: 10976
2023-05-11 10:51:07.196: model: model.general_recommender.SGL
2023-05-11 10:51:07.196: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 10:51:07.197: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 10:51:10.598: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 10:51:31.561: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 20.962298]
2023-05-11 10:51:31.824: epoch 1:	0.00131034  	0.00281668  	0.00214696  
2023-05-11 10:51:31.824: Find a better model.
2023-05-11 10:51:53.186: [iter 2 : loss : 1.1366 = 0.6930 + 0.4436 + 0.0000, time: 21.358097]
2023-05-11 10:51:53.491: epoch 2:	0.00178413  	0.00326235  	0.00259270  
2023-05-11 10:51:53.491: Find a better model.
2023-05-11 10:52:15.152: [iter 3 : loss : 1.1348 = 0.6929 + 0.4419 + 0.0000, time: 21.657091]
2023-05-11 10:52:15.466: epoch 3:	0.00191738  	0.00374674  	0.00276106  
2023-05-11 10:52:15.466: Find a better model.
2023-05-11 10:52:37.166: [iter 4 : loss : 1.1374 = 0.6929 + 0.4446 + 0.0000, time: 21.696176]
2023-05-11 10:52:37.480: epoch 4:	0.00230234  	0.00459010  	0.00343032  
2023-05-11 10:52:37.480: Find a better model.
2023-05-11 10:52:58.933: [iter 5 : loss : 1.1355 = 0.6927 + 0.4428 + 0.0000, time: 21.449924]
2023-05-11 10:52:59.221: epoch 5:	0.00265028  	0.00538200  	0.00421679  
2023-05-11 10:52:59.221: Find a better model.
2023-05-11 10:53:20.747: [iter 6 : loss : 1.1383 = 0.6926 + 0.4456 + 0.0000, time: 21.521915]
2023-05-11 10:53:21.039: epoch 6:	0.00277613  	0.00529133  	0.00427809  
2023-05-11 10:53:42.514: [iter 7 : loss : 1.1359 = 0.6924 + 0.4434 + 0.0000, time: 21.470740]
2023-05-11 10:53:42.796: epoch 7:	0.00342020  	0.00688914  	0.00551429  
2023-05-11 10:53:42.796: Find a better model.
2023-05-11 10:54:04.366: [iter 8 : loss : 1.1388 = 0.6922 + 0.4466 + 0.0000, time: 21.566907]
2023-05-11 10:54:04.666: epoch 8:	0.00357566  	0.00695669  	0.00577472  
2023-05-11 10:54:04.666: Find a better model.
2023-05-11 10:54:26.255: [iter 9 : loss : 1.1361 = 0.6919 + 0.4442 + 0.0000, time: 21.584052]
2023-05-11 10:54:26.575: epoch 9:	0.00454545  	0.00991594  	0.00840615  
2023-05-11 10:54:26.576: Find a better model.
2023-05-11 10:54:48.104: [iter 10 : loss : 1.1392 = 0.6915 + 0.4477 + 0.0000, time: 21.524285]
2023-05-11 10:54:48.392: epoch 10:	0.00454545  	0.00954846  	0.00827511  
2023-05-11 10:55:09.857: [iter 11 : loss : 1.1357 = 0.6908 + 0.4449 + 0.0000, time: 21.461264]
2023-05-11 10:55:10.139: epoch 11:	0.00568551  	0.01294405  	0.01076816  
2023-05-11 10:55:10.139: Find a better model.
2023-05-11 10:55:31.687: [iter 12 : loss : 1.1390 = 0.6898 + 0.4492 + 0.0000, time: 21.544447]
2023-05-11 10:55:31.982: epoch 12:	0.00614449  	0.01242108  	0.01135116  
2023-05-11 10:55:53.646: [iter 13 : loss : 1.1343 = 0.6885 + 0.4458 + 0.0000, time: 21.661033]
2023-05-11 10:55:53.944: epoch 13:	0.00758066  	0.01728582  	0.01494458  
2023-05-11 10:55:53.944: Find a better model.
2023-05-11 10:56:15.480: [iter 14 : loss : 1.1377 = 0.6868 + 0.4509 + 0.0001, time: 21.530606]
2023-05-11 10:56:15.773: epoch 14:	0.00915009  	0.02056284  	0.01817890  
2023-05-11 10:56:15.773: Find a better model.
2023-05-11 10:56:37.262: [iter 15 : loss : 1.1311 = 0.6843 + 0.4466 + 0.0001, time: 21.484231]
2023-05-11 10:56:37.558: epoch 15:	0.01183740  	0.02909131  	0.02471368  
2023-05-11 10:56:37.558: Find a better model.
2023-05-11 10:56:59.051: [iter 16 : loss : 1.1339 = 0.6805 + 0.4532 + 0.0001, time: 21.490082]
2023-05-11 10:56:59.343: epoch 16:	0.01453957  	0.03649199  	0.03110146  
2023-05-11 10:56:59.343: Find a better model.
2023-05-11 10:57:20.822: [iter 17 : loss : 1.1227 = 0.6747 + 0.4478 + 0.0001, time: 21.476115]
2023-05-11 10:57:21.111: epoch 17:	0.01775994  	0.04530476  	0.03915456  
2023-05-11 10:57:21.111: Find a better model.
2023-05-11 10:57:42.482: [iter 18 : loss : 1.1213 = 0.6650 + 0.4561 + 0.0002, time: 21.366603]
2023-05-11 10:57:42.777: epoch 18:	0.02064722  	0.05194688  	0.04587851  
2023-05-11 10:57:42.777: Find a better model.
2023-05-11 10:58:04.386: [iter 19 : loss : 1.0994 = 0.6495 + 0.4496 + 0.0002, time: 21.605686]
2023-05-11 10:58:04.672: epoch 19:	0.02346785  	0.06021471  	0.05246024  
2023-05-11 10:58:04.673: Find a better model.
2023-05-11 10:58:26.077: [iter 20 : loss : 1.0869 = 0.6262 + 0.4604 + 0.0003, time: 21.400410]
2023-05-11 10:58:26.362: epoch 20:	0.02513358  	0.06499159  	0.05639014  
2023-05-11 10:58:26.362: Find a better model.
2023-05-11 10:58:47.771: [iter 21 : loss : 1.0470 = 0.5931 + 0.4535 + 0.0005, time: 21.406358]
2023-05-11 10:58:48.059: epoch 21:	0.02697700  	0.07090311  	0.06046901  
2023-05-11 10:58:48.059: Find a better model.
2023-05-11 10:59:09.632: [iter 22 : loss : 1.0180 = 0.5511 + 0.4662 + 0.0007, time: 21.569887]
2023-05-11 10:59:09.929: epoch 22:	0.02785057  	0.07442552  	0.06239645  
2023-05-11 10:59:09.929: Find a better model.
2023-05-11 10:59:31.344: [iter 23 : loss : 0.9626 = 0.5027 + 0.4589 + 0.0009, time: 21.410883]
2023-05-11 10:59:31.630: epoch 23:	0.02854646  	0.07678664  	0.06383795  
2023-05-11 10:59:31.630: Find a better model.
2023-05-11 10:59:52.983: [iter 24 : loss : 0.9272 = 0.4530 + 0.4730 + 0.0012, time: 21.349780]
2023-05-11 10:59:53.270: epoch 24:	0.02882781  	0.07825790  	0.06445367  
2023-05-11 10:59:53.270: Find a better model.
2023-05-11 11:00:14.728: [iter 25 : loss : 0.8713 = 0.4054 + 0.4644 + 0.0015, time: 21.453310]
2023-05-11 11:00:15.015: epoch 25:	0.02930903  	0.07965000  	0.06497362  
2023-05-11 11:00:15.015: Find a better model.
2023-05-11 11:00:38.051: [iter 26 : loss : 0.8432 = 0.3637 + 0.4777 + 0.0018, time: 23.031133]
2023-05-11 11:00:38.343: epoch 26:	0.02925721  	0.08010183  	0.06508066  
2023-05-11 11:00:38.343: Find a better model.
2023-05-11 11:00:59.398: [iter 27 : loss : 0.7967 = 0.3269 + 0.4678 + 0.0021, time: 21.050900]
2023-05-11 11:00:59.672: epoch 27:	0.02951633  	0.08079580  	0.06539375  
2023-05-11 11:00:59.672: Find a better model.
2023-05-11 11:01:20.982: [iter 28 : loss : 0.7780 = 0.2962 + 0.4795 + 0.0024, time: 21.305356]
2023-05-11 11:01:21.245: epoch 28:	0.02953854  	0.08117213  	0.06557885  
2023-05-11 11:01:21.245: Find a better model.
2023-05-11 11:01:42.363: [iter 29 : loss : 0.7409 = 0.2699 + 0.4684 + 0.0026, time: 21.114683]
2023-05-11 11:01:42.640: epoch 29:	0.02967920  	0.08121437  	0.06584007  
2023-05-11 11:01:42.640: Find a better model.
2023-05-11 11:02:03.754: [iter 30 : loss : 0.7295 = 0.2475 + 0.4791 + 0.0029, time: 21.110868]
2023-05-11 11:02:04.044: epoch 30:	0.02964959  	0.08133280  	0.06587952  
2023-05-11 11:02:04.044: Find a better model.
2023-05-11 11:02:25.305: [iter 31 : loss : 0.6993 = 0.2283 + 0.4678 + 0.0031, time: 21.256713]
2023-05-11 11:02:25.604: epoch 31:	0.02978285  	0.08194665  	0.06620064  
2023-05-11 11:02:25.604: Find a better model.
2023-05-11 11:02:46.594: [iter 32 : loss : 0.6937 = 0.2122 + 0.4781 + 0.0034, time: 20.985236]
2023-05-11 11:02:46.870: epoch 32:	0.02992350  	0.08209836  	0.06631191  
2023-05-11 11:02:46.870: Find a better model.
2023-05-11 11:03:08.115: [iter 33 : loss : 0.6687 = 0.1986 + 0.4664 + 0.0036, time: 21.241421]
2023-05-11 11:03:08.396: epoch 33:	0.03013078  	0.08254096  	0.06666114  
2023-05-11 11:03:08.396: Find a better model.
2023-05-11 11:03:29.583: [iter 34 : loss : 0.6664 = 0.1858 + 0.4768 + 0.0038, time: 21.182911]
2023-05-11 11:03:29.850: epoch 34:	0.03018260  	0.08237021  	0.06674724  
2023-05-11 11:03:51.120: [iter 35 : loss : 0.6429 = 0.1739 + 0.4649 + 0.0041, time: 21.265838]
2023-05-11 11:03:51.384: epoch 35:	0.03024923  	0.08268750  	0.06678601  
2023-05-11 11:03:51.384: Find a better model.
2023-05-11 11:04:12.545: [iter 36 : loss : 0.6444 = 0.1648 + 0.4754 + 0.0043, time: 21.156523]
2023-05-11 11:04:12.824: epoch 36:	0.03027886  	0.08254128  	0.06672510  
2023-05-11 11:04:34.463: [iter 37 : loss : 0.6243 = 0.1561 + 0.4638 + 0.0045, time: 21.635314]
2023-05-11 11:04:34.723: epoch 37:	0.03033808  	0.08307265  	0.06697740  
2023-05-11 11:04:34.723: Find a better model.
2023-05-11 11:04:55.911: [iter 38 : loss : 0.6268 = 0.1479 + 0.4742 + 0.0047, time: 21.184072]
2023-05-11 11:04:56.194: epoch 38:	0.03035289  	0.08278961  	0.06686290  
2023-05-11 11:05:17.822: [iter 39 : loss : 0.6087 = 0.1415 + 0.4624 + 0.0048, time: 21.625146]
2023-05-11 11:05:18.100: epoch 39:	0.03028626  	0.08243600  	0.06677470  
2023-05-11 11:05:39.716: [iter 40 : loss : 0.6121 = 0.1341 + 0.4730 + 0.0050, time: 21.613685]
2023-05-11 11:05:39.994: epoch 40:	0.03028625  	0.08246464  	0.06686623  
2023-05-11 11:06:01.459: [iter 41 : loss : 0.5946 = 0.1281 + 0.4613 + 0.0052, time: 21.459192]
2023-05-11 11:06:01.738: epoch 41:	0.03029366  	0.08243404  	0.06677115  
2023-05-11 11:06:23.295: [iter 42 : loss : 0.6002 = 0.1228 + 0.4720 + 0.0054, time: 21.553872]
2023-05-11 11:06:23.575: epoch 42:	0.03016041  	0.08194368  	0.06651778  
2023-05-11 11:06:45.248: [iter 43 : loss : 0.5836 = 0.1176 + 0.4604 + 0.0055, time: 21.668007]
2023-05-11 11:06:45.530: epoch 43:	0.03013819  	0.08171702  	0.06666596  
2023-05-11 11:07:07.069: [iter 44 : loss : 0.5907 = 0.1140 + 0.4711 + 0.0057, time: 21.534933]
2023-05-11 11:07:07.346: epoch 44:	0.03003455  	0.08134076  	0.06641573  
2023-05-11 11:07:29.179: [iter 45 : loss : 0.5747 = 0.1095 + 0.4594 + 0.0059, time: 21.828946]
2023-05-11 11:07:29.455: epoch 45:	0.03008636  	0.08187218  	0.06658329  
2023-05-11 11:07:51.078: [iter 46 : loss : 0.5822 = 0.1060 + 0.4702 + 0.0060, time: 21.619669]
2023-05-11 11:07:51.356: epoch 46:	0.03001974  	0.08178239  	0.06639943  
2023-05-11 11:08:13.010: [iter 47 : loss : 0.5665 = 0.1017 + 0.4587 + 0.0062, time: 21.649014]
2023-05-11 11:08:13.287: epoch 47:	0.03015299  	0.08199157  	0.06639234  
2023-05-11 11:08:34.864: [iter 48 : loss : 0.5741 = 0.0984 + 0.4693 + 0.0063, time: 21.572814]
2023-05-11 11:08:35.151: epoch 48:	0.02985686  	0.08136352  	0.06625465  
2023-05-11 11:08:56.994: [iter 49 : loss : 0.5597 = 0.0953 + 0.4579 + 0.0065, time: 21.837926]
2023-05-11 11:08:57.272: epoch 49:	0.02978283  	0.08113635  	0.06611537  
2023-05-11 11:09:18.841: [iter 50 : loss : 0.5680 = 0.0926 + 0.4687 + 0.0066, time: 21.564414]
2023-05-11 11:09:19.118: epoch 50:	0.02967179  	0.08052570  	0.06591408  
2023-05-11 11:09:40.927: [iter 51 : loss : 0.5535 = 0.0894 + 0.4573 + 0.0068, time: 21.804091]
2023-05-11 11:09:41.203: epoch 51:	0.02960516  	0.08039316  	0.06581189  
2023-05-11 11:10:02.804: [iter 52 : loss : 0.5616 = 0.0866 + 0.4682 + 0.0069, time: 21.595716]
2023-05-11 11:10:03.078: epoch 52:	0.02956074  	0.08009081  	0.06565627  
2023-05-11 11:10:24.745: [iter 53 : loss : 0.5486 = 0.0850 + 0.4566 + 0.0070, time: 21.663753]
2023-05-11 11:10:25.024: epoch 53:	0.02950892  	0.07995822  	0.06563387  
2023-05-11 11:10:46.591: [iter 54 : loss : 0.5572 = 0.0825 + 0.4675 + 0.0072, time: 21.564167]
2023-05-11 11:10:46.871: epoch 54:	0.02936825  	0.07974257  	0.06554662  
2023-05-11 11:11:08.732: [iter 55 : loss : 0.5438 = 0.0804 + 0.4561 + 0.0073, time: 21.855867]
2023-05-11 11:11:09.015: epoch 55:	0.02922018  	0.07952497  	0.06538534  
2023-05-11 11:11:30.767: [iter 56 : loss : 0.5527 = 0.0782 + 0.4671 + 0.0074, time: 21.748236]
2023-05-11 11:11:31.043: epoch 56:	0.02904250  	0.07922967  	0.06522036  
2023-05-11 11:11:52.743: [iter 57 : loss : 0.5388 = 0.0756 + 0.4556 + 0.0076, time: 21.697487]
2023-05-11 11:11:53.018: epoch 57:	0.02899068  	0.07905615  	0.06503200  
2023-05-11 11:12:14.741: [iter 58 : loss : 0.5491 = 0.0745 + 0.4669 + 0.0077, time: 21.718357]
2023-05-11 11:12:15.021: epoch 58:	0.02895366  	0.07866215  	0.06486675  
2023-05-11 11:12:36.702: [iter 59 : loss : 0.5356 = 0.0727 + 0.4552 + 0.0078, time: 21.677718]
2023-05-11 11:12:36.980: epoch 59:	0.02897587  	0.07863042  	0.06492714  
2023-05-11 11:13:00.147: [iter 60 : loss : 0.5453 = 0.0711 + 0.4663 + 0.0079, time: 23.164074]
2023-05-11 11:13:00.423: epoch 60:	0.02888703  	0.07826205  	0.06464525  
2023-05-11 11:13:21.930: [iter 61 : loss : 0.5322 = 0.0693 + 0.4548 + 0.0080, time: 21.503320]
2023-05-11 11:13:22.206: epoch 61:	0.02886482  	0.07795754  	0.06450928  
2023-05-11 11:13:43.532: [iter 62 : loss : 0.5424 = 0.0683 + 0.4660 + 0.0082, time: 21.322953]
2023-05-11 11:13:43.803: epoch 62:	0.02865013  	0.07737583  	0.06415965  
2023-05-11 11:13:43.803: Early stopping is trigger at epoch: 62
2023-05-11 11:13:43.803: best_result@epoch 37:

2023-05-11 11:13:43.803: 		0.0303      	0.0831      	0.0670      
2023-05-11 11:27:58.483: my pid: 4492
2023-05-11 11:27:58.483: model: model.general_recommender.SGL
2023-05-11 11:27:58.483: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 11:27:58.483: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 11:28:01.849: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 11:28:22.961: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.111964]
2023-05-11 11:28:23.257: epoch 1:	0.00123630  	0.00278000  	0.00219343  
2023-05-11 11:28:23.257: Find a better model.
2023-05-11 11:28:44.800: [iter 2 : loss : 1.1368 = 0.6930 + 0.4437 + 0.0000, time: 21.539814]
2023-05-11 11:28:45.097: epoch 2:	0.00148060  	0.00297200  	0.00233228  
2023-05-11 11:28:45.097: Find a better model.
2023-05-11 11:29:06.589: [iter 3 : loss : 1.1348 = 0.6929 + 0.4418 + 0.0000, time: 21.488623]
2023-05-11 11:29:06.883: epoch 3:	0.00192479  	0.00412033  	0.00315410  
2023-05-11 11:29:06.883: Find a better model.
2023-05-11 11:29:28.389: [iter 4 : loss : 1.1375 = 0.6929 + 0.4447 + 0.0000, time: 21.503266]
2023-05-11 11:29:28.687: epoch 4:	0.00206544  	0.00368385  	0.00310080  
2023-05-11 11:29:50.326: [iter 5 : loss : 1.1354 = 0.6927 + 0.4426 + 0.0000, time: 21.635676]
2023-05-11 11:29:50.636: epoch 5:	0.00243560  	0.00455944  	0.00384366  
2023-05-11 11:29:50.636: Find a better model.
2023-05-11 11:30:12.208: [iter 6 : loss : 1.1383 = 0.6926 + 0.4457 + 0.0000, time: 21.568960]
2023-05-11 11:30:12.506: epoch 6:	0.00259106  	0.00557017  	0.00422860  
2023-05-11 11:30:12.506: Find a better model.
2023-05-11 11:30:33.945: [iter 7 : loss : 1.1358 = 0.6924 + 0.4434 + 0.0000, time: 21.435718]
2023-05-11 11:30:34.243: epoch 7:	0.00326473  	0.00626115  	0.00523493  
2023-05-11 11:30:34.244: Find a better model.
2023-05-11 11:30:55.582: [iter 8 : loss : 1.1389 = 0.6922 + 0.4466 + 0.0000, time: 21.334512]
2023-05-11 11:30:55.873: epoch 8:	0.00356826  	0.00687205  	0.00579120  
2023-05-11 11:30:55.873: Find a better model.
2023-05-11 11:31:17.300: [iter 9 : loss : 1.1360 = 0.6919 + 0.4440 + 0.0000, time: 21.423488]
2023-05-11 11:31:17.617: epoch 9:	0.00416050  	0.00856858  	0.00723665  
2023-05-11 11:31:17.617: Find a better model.
2023-05-11 11:31:38.945: [iter 10 : loss : 1.1393 = 0.6915 + 0.4478 + 0.0000, time: 21.324960]
2023-05-11 11:31:39.233: epoch 10:	0.00404945  	0.00769408  	0.00670716  
2023-05-11 11:32:00.502: [iter 11 : loss : 1.1356 = 0.6908 + 0.4448 + 0.0000, time: 21.264664]
2023-05-11 11:32:00.820: epoch 11:	0.00512288  	0.01178818  	0.00938201  
2023-05-11 11:32:00.820: Find a better model.
2023-05-11 11:32:22.133: [iter 12 : loss : 1.1391 = 0.6899 + 0.4492 + 0.0000, time: 21.308223]
2023-05-11 11:32:22.435: epoch 12:	0.00538939  	0.01153282  	0.00960729  
2023-05-11 11:32:44.055: [iter 13 : loss : 1.1342 = 0.6886 + 0.4456 + 0.0000, time: 21.615329]
2023-05-11 11:32:44.354: epoch 13:	0.00763248  	0.01770664  	0.01478854  
2023-05-11 11:32:44.354: Find a better model.
2023-05-11 11:33:05.889: [iter 14 : loss : 1.1379 = 0.6870 + 0.4509 + 0.0000, time: 21.530597]
2023-05-11 11:33:06.188: epoch 14:	0.00868370  	0.01989966  	0.01723862  
2023-05-11 11:33:06.188: Find a better model.
2023-05-11 11:33:27.657: [iter 15 : loss : 1.1313 = 0.6847 + 0.4465 + 0.0001, time: 21.465364]
2023-05-11 11:33:27.953: epoch 15:	0.01131179  	0.02746181  	0.02408404  
2023-05-11 11:33:27.953: Find a better model.
2023-05-11 11:33:49.287: [iter 16 : loss : 1.1343 = 0.6812 + 0.4531 + 0.0001, time: 21.329695]
2023-05-11 11:33:49.581: epoch 16:	0.01357715  	0.03310804  	0.02953671  
2023-05-11 11:33:49.581: Find a better model.
2023-05-11 11:34:11.042: [iter 17 : loss : 1.1234 = 0.6757 + 0.4476 + 0.0001, time: 21.457327]
2023-05-11 11:34:11.337: epoch 17:	0.01692339  	0.04195878  	0.03693644  
2023-05-11 11:34:11.337: Find a better model.
2023-05-11 11:34:32.675: [iter 18 : loss : 1.1230 = 0.6667 + 0.4561 + 0.0002, time: 21.333129]
2023-05-11 11:34:32.966: epoch 18:	0.02003273  	0.05002867  	0.04357336  
2023-05-11 11:34:32.967: Find a better model.
2023-05-11 11:34:54.425: [iter 19 : loss : 1.1019 = 0.6523 + 0.4494 + 0.0002, time: 21.455214]
2023-05-11 11:34:54.720: epoch 19:	0.02270530  	0.05757273  	0.05007685  
2023-05-11 11:34:54.720: Find a better model.
2023-05-11 11:35:16.038: [iter 20 : loss : 1.0910 = 0.6304 + 0.4603 + 0.0003, time: 21.312863]
2023-05-11 11:35:16.333: epoch 20:	0.02487446  	0.06421890  	0.05538084  
2023-05-11 11:35:16.333: Find a better model.
2023-05-11 11:35:37.796: [iter 21 : loss : 1.0523 = 0.5988 + 0.4530 + 0.0005, time: 21.459207]
2023-05-11 11:35:38.088: epoch 21:	0.02661422  	0.06950094  	0.05903632  
2023-05-11 11:35:38.088: Find a better model.
2023-05-11 11:35:59.467: [iter 22 : loss : 1.0248 = 0.5580 + 0.4661 + 0.0007, time: 21.375145]
2023-05-11 11:35:59.757: epoch 22:	0.02702881  	0.07116635  	0.06057886  
2023-05-11 11:35:59.757: Find a better model.
2023-05-11 11:36:21.014: [iter 23 : loss : 0.9698 = 0.5101 + 0.4588 + 0.0009, time: 21.253301]
2023-05-11 11:36:21.304: epoch 23:	0.02824293  	0.07463651  	0.06259508  
2023-05-11 11:36:21.304: Find a better model.
2023-05-11 11:36:42.636: [iter 24 : loss : 0.9344 = 0.4604 + 0.4729 + 0.0012, time: 21.328984]
2023-05-11 11:36:42.926: epoch 24:	0.02833918  	0.07593945  	0.06348565  
2023-05-11 11:36:42.926: Find a better model.
2023-05-11 11:37:04.355: [iter 25 : loss : 0.8782 = 0.4122 + 0.4645 + 0.0015, time: 21.423761]
2023-05-11 11:37:04.640: epoch 25:	0.02886482  	0.07763353  	0.06432188  
2023-05-11 11:37:04.641: Find a better model.
2023-05-11 11:37:25.811: [iter 26 : loss : 0.8495 = 0.3697 + 0.4780 + 0.0017, time: 21.167161]
2023-05-11 11:37:26.099: epoch 26:	0.02909432  	0.07906289  	0.06486377  
2023-05-11 11:37:26.099: Find a better model.
2023-05-11 11:37:47.361: [iter 27 : loss : 0.8020 = 0.3323 + 0.4676 + 0.0020, time: 21.257499]
2023-05-11 11:37:47.647: epoch 27:	0.02920537  	0.07960084  	0.06521560  
2023-05-11 11:37:47.648: Find a better model.
2023-05-11 11:38:08.988: [iter 28 : loss : 0.7826 = 0.3008 + 0.4796 + 0.0023, time: 21.335593]
2023-05-11 11:38:09.275: epoch 28:	0.02927941  	0.08015274  	0.06538846  
2023-05-11 11:38:09.275: Find a better model.
2023-05-11 11:38:30.742: [iter 29 : loss : 0.7452 = 0.2743 + 0.4684 + 0.0026, time: 21.462545]
2023-05-11 11:38:31.028: epoch 29:	0.02934605  	0.08083577  	0.06586964  
2023-05-11 11:38:31.028: Find a better model.
2023-05-11 11:38:52.379: [iter 30 : loss : 0.7333 = 0.2509 + 0.4796 + 0.0029, time: 21.347566]
2023-05-11 11:38:52.668: epoch 30:	0.02939788  	0.08133278  	0.06594594  
2023-05-11 11:38:52.668: Find a better model.
2023-05-11 11:39:14.117: [iter 31 : loss : 0.7027 = 0.2317 + 0.4679 + 0.0031, time: 21.446218]
2023-05-11 11:39:14.401: epoch 31:	0.02956073  	0.08152361  	0.06617836  
2023-05-11 11:39:14.401: Find a better model.
2023-05-11 11:39:35.755: [iter 32 : loss : 0.6969 = 0.2150 + 0.4786 + 0.0033, time: 21.350300]
2023-05-11 11:39:36.042: epoch 32:	0.02947931  	0.08106314  	0.06632668  
2023-05-11 11:39:57.337: [iter 33 : loss : 0.6712 = 0.2011 + 0.4665 + 0.0036, time: 21.290025]
2023-05-11 11:39:57.622: epoch 33:	0.02958295  	0.08122212  	0.06622754  
2023-05-11 11:40:18.951: [iter 34 : loss : 0.6690 = 0.1880 + 0.4772 + 0.0038, time: 21.324656]
2023-05-11 11:40:19.237: epoch 34:	0.02964217  	0.08101956  	0.06636859  
2023-05-11 11:40:40.683: [iter 35 : loss : 0.6453 = 0.1762 + 0.4651 + 0.0040, time: 21.442349]
2023-05-11 11:40:40.978: epoch 35:	0.02968660  	0.08136827  	0.06642027  
2023-05-11 11:41:02.534: [iter 36 : loss : 0.6464 = 0.1664 + 0.4758 + 0.0042, time: 21.551325]
2023-05-11 11:41:02.816: epoch 36:	0.02968660  	0.08171503  	0.06658489  
2023-05-11 11:41:02.816: Find a better model.
2023-05-11 11:41:24.109: [iter 37 : loss : 0.6256 = 0.1574 + 0.4637 + 0.0044, time: 21.288763]
2023-05-11 11:41:24.393: epoch 37:	0.02976802  	0.08170158  	0.06679667  
2023-05-11 11:41:45.770: [iter 38 : loss : 0.6284 = 0.1494 + 0.4744 + 0.0046, time: 21.370508]
2023-05-11 11:41:46.057: epoch 38:	0.02969400  	0.08151061  	0.06663567  
2023-05-11 11:42:07.672: [iter 39 : loss : 0.6101 = 0.1428 + 0.4625 + 0.0048, time: 21.611868]
2023-05-11 11:42:07.959: epoch 39:	0.02973841  	0.08158279  	0.06686158  
2023-05-11 11:42:29.309: [iter 40 : loss : 0.6141 = 0.1359 + 0.4732 + 0.0050, time: 21.346605]
2023-05-11 11:42:29.592: epoch 40:	0.02970139  	0.08152264  	0.06678438  
2023-05-11 11:42:51.059: [iter 41 : loss : 0.5959 = 0.1294 + 0.4613 + 0.0052, time: 21.462190]
2023-05-11 11:42:51.343: epoch 41:	0.02970139  	0.08143776  	0.06660489  
2023-05-11 11:43:12.879: [iter 42 : loss : 0.6012 = 0.1236 + 0.4722 + 0.0053, time: 21.532966]
2023-05-11 11:43:13.161: epoch 42:	0.02954593  	0.08081029  	0.06646016  
2023-05-11 11:43:34.660: [iter 43 : loss : 0.5848 = 0.1189 + 0.4604 + 0.0055, time: 21.494075]
2023-05-11 11:43:34.944: epoch 43:	0.02952372  	0.08047338  	0.06649510  
2023-05-11 11:43:56.270: [iter 44 : loss : 0.5920 = 0.1150 + 0.4714 + 0.0057, time: 21.320637]
2023-05-11 11:43:56.555: epoch 44:	0.02939047  	0.08012902  	0.06635857  
2023-05-11 11:44:18.028: [iter 45 : loss : 0.5753 = 0.1101 + 0.4594 + 0.0058, time: 21.467565]
2023-05-11 11:44:18.315: epoch 45:	0.02944228  	0.07969104  	0.06640564  
2023-05-11 11:44:39.648: [iter 46 : loss : 0.5829 = 0.1064 + 0.4705 + 0.0060, time: 21.329729]
2023-05-11 11:44:39.931: epoch 46:	0.02944968  	0.07984760  	0.06646354  
2023-05-11 11:45:01.208: [iter 47 : loss : 0.5673 = 0.1025 + 0.4586 + 0.0062, time: 21.274952]
2023-05-11 11:45:01.492: epoch 47:	0.02949409  	0.07982779  	0.06626660  
2023-05-11 11:45:22.842: [iter 48 : loss : 0.5751 = 0.0992 + 0.4696 + 0.0063, time: 21.346878]
2023-05-11 11:45:23.126: epoch 48:	0.02936084  	0.07917745  	0.06599519  
2023-05-11 11:45:44.590: [iter 49 : loss : 0.5605 = 0.0962 + 0.4578 + 0.0065, time: 21.458728]
2023-05-11 11:45:44.875: epoch 49:	0.02941266  	0.07905492  	0.06596034  
2023-05-11 11:46:06.217: [iter 50 : loss : 0.5686 = 0.0931 + 0.4689 + 0.0066, time: 21.338603]
2023-05-11 11:46:06.504: epoch 50:	0.02928680  	0.07901425  	0.06580629  
2023-05-11 11:46:27.974: [iter 51 : loss : 0.5539 = 0.0900 + 0.4571 + 0.0067, time: 21.466210]
2023-05-11 11:46:28.259: epoch 51:	0.02924239  	0.07861208  	0.06558529  
2023-05-11 11:46:49.623: [iter 52 : loss : 0.5627 = 0.0872 + 0.4686 + 0.0069, time: 21.358521]
2023-05-11 11:46:49.907: epoch 52:	0.02927200  	0.07877591  	0.06562269  
2023-05-11 11:47:11.176: [iter 53 : loss : 0.5496 = 0.0860 + 0.4565 + 0.0070, time: 21.266341]
2023-05-11 11:47:11.463: epoch 53:	0.02919797  	0.07846817  	0.06557005  
2023-05-11 11:47:32.850: [iter 54 : loss : 0.5581 = 0.0832 + 0.4678 + 0.0072, time: 21.382703]
2023-05-11 11:47:33.133: epoch 54:	0.02902030  	0.07765549  	0.06544641  
2023-05-11 11:47:54.542: [iter 55 : loss : 0.5443 = 0.0809 + 0.4561 + 0.0073, time: 21.405386]
2023-05-11 11:47:54.826: epoch 55:	0.02895367  	0.07761847  	0.06522023  
2023-05-11 11:48:15.994: [iter 56 : loss : 0.5531 = 0.0784 + 0.4673 + 0.0074, time: 21.162641]
2023-05-11 11:48:16.278: epoch 56:	0.02886484  	0.07732517  	0.06515631  
2023-05-11 11:48:37.711: [iter 57 : loss : 0.5396 = 0.0765 + 0.4556 + 0.0075, time: 21.430614]
2023-05-11 11:48:37.996: epoch 57:	0.02876859  	0.07714685  	0.06494977  
2023-05-11 11:48:59.195: [iter 58 : loss : 0.5495 = 0.0749 + 0.4670 + 0.0077, time: 21.195078]
2023-05-11 11:48:59.485: epoch 58:	0.02868715  	0.07694551  	0.06473500  
2023-05-11 11:49:20.908: [iter 59 : loss : 0.5356 = 0.0727 + 0.4551 + 0.0078, time: 21.418349]
2023-05-11 11:49:21.193: epoch 59:	0.02864273  	0.07680853  	0.06454534  
2023-05-11 11:49:42.588: [iter 60 : loss : 0.5458 = 0.0714 + 0.4665 + 0.0079, time: 21.391420]
2023-05-11 11:49:42.871: epoch 60:	0.02859092  	0.07662736  	0.06437034  
2023-05-11 11:50:04.288: [iter 61 : loss : 0.5326 = 0.0699 + 0.4547 + 0.0080, time: 21.413378]
2023-05-11 11:50:04.570: epoch 61:	0.02850948  	0.07635900  	0.06425646  
2023-05-11 11:50:04.570: Early stopping is trigger at epoch: 61
2023-05-11 11:50:04.570: best_result@epoch 36:

2023-05-11 11:50:04.570: 		0.0297      	0.0817      	0.0666      
2023-05-11 14:34:32.930: my pid: 15260
2023-05-11 14:34:32.930: model: model.general_recommender.SGL
2023-05-11 14:34:32.930: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 14:34:32.930: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 14:34:36.291: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 14:34:57.358: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.067090]
2023-05-11 14:34:57.662: epoch 1:	0.00122890  	0.00224795  	0.00190011  
2023-05-11 14:34:57.662: Find a better model.
2023-05-11 14:35:19.195: [iter 2 : loss : 1.1368 = 0.6930 + 0.4437 + 0.0000, time: 21.530519]
2023-05-11 14:35:19.480: epoch 2:	0.00148801  	0.00271515  	0.00223006  
2023-05-11 14:35:19.480: Find a better model.
2023-05-11 14:35:40.774: [iter 3 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 21.290138]
2023-05-11 14:35:41.063: epoch 3:	0.00194700  	0.00393428  	0.00307112  
2023-05-11 14:35:41.063: Find a better model.
2023-05-11 14:36:02.441: [iter 4 : loss : 1.1375 = 0.6929 + 0.4447 + 0.0000, time: 21.373872]
2023-05-11 14:36:02.741: epoch 4:	0.00205064  	0.00355055  	0.00284854  
2023-05-11 14:36:24.289: [iter 5 : loss : 1.1354 = 0.6927 + 0.4426 + 0.0000, time: 21.544157]
2023-05-11 14:36:24.575: epoch 5:	0.00254664  	0.00509710  	0.00374438  
2023-05-11 14:36:24.575: Find a better model.
2023-05-11 14:36:45.924: [iter 6 : loss : 1.1383 = 0.6926 + 0.4457 + 0.0000, time: 21.346422]
2023-05-11 14:36:46.234: epoch 6:	0.00271691  	0.00503411  	0.00402081  
2023-05-11 14:37:07.677: [iter 7 : loss : 1.1358 = 0.6924 + 0.4433 + 0.0000, time: 21.438897]
2023-05-11 14:37:07.961: epoch 7:	0.00344981  	0.00775779  	0.00583266  
2023-05-11 14:37:07.962: Find a better model.
2023-05-11 14:37:29.332: [iter 8 : loss : 1.1389 = 0.6922 + 0.4467 + 0.0000, time: 21.366424]
2023-05-11 14:37:29.615: epoch 8:	0.00339058  	0.00710012  	0.00564071  
2023-05-11 14:37:51.071: [iter 9 : loss : 1.1360 = 0.6919 + 0.4440 + 0.0000, time: 21.453506]
2023-05-11 14:37:51.357: epoch 9:	0.00430856  	0.01032887  	0.00806936  
2023-05-11 14:37:51.357: Find a better model.
2023-05-11 14:38:12.697: [iter 10 : loss : 1.1393 = 0.6915 + 0.4477 + 0.0000, time: 21.337014]
2023-05-11 14:38:12.973: epoch 10:	0.00444921  	0.00900682  	0.00762940  
2023-05-11 14:38:34.277: [iter 11 : loss : 1.1357 = 0.6909 + 0.4447 + 0.0000, time: 21.300815]
2023-05-11 14:38:34.574: epoch 11:	0.00529315  	0.01173584  	0.00987136  
2023-05-11 14:38:34.574: Find a better model.
2023-05-11 14:38:55.904: [iter 12 : loss : 1.1391 = 0.6899 + 0.4491 + 0.0000, time: 21.325525]
2023-05-11 14:38:56.195: epoch 12:	0.00567070  	0.01206873  	0.01052565  
2023-05-11 14:38:56.195: Find a better model.
2023-05-11 14:39:17.673: [iter 13 : loss : 1.1343 = 0.6887 + 0.4456 + 0.0000, time: 21.473565]
2023-05-11 14:39:17.963: epoch 13:	0.00755845  	0.01767485  	0.01493264  
2023-05-11 14:39:17.963: Find a better model.
2023-05-11 14:39:39.249: [iter 14 : loss : 1.1380 = 0.6870 + 0.4509 + 0.0000, time: 21.282824]
2023-05-11 14:39:39.541: epoch 14:	0.00869110  	0.02085866  	0.01779733  
2023-05-11 14:39:39.541: Find a better model.
2023-05-11 14:40:01.007: [iter 15 : loss : 1.1313 = 0.6848 + 0.4464 + 0.0001, time: 21.462751]
2023-05-11 14:40:01.294: epoch 15:	0.01140803  	0.02820393  	0.02489860  
2023-05-11 14:40:01.294: Find a better model.
2023-05-11 14:40:22.677: [iter 16 : loss : 1.1344 = 0.6813 + 0.4530 + 0.0001, time: 21.378605]
2023-05-11 14:40:22.962: epoch 16:	0.01396213  	0.03426994  	0.03044443  
2023-05-11 14:40:22.963: Find a better model.
2023-05-11 14:40:44.390: [iter 17 : loss : 1.1236 = 0.6760 + 0.4474 + 0.0001, time: 21.423427]
2023-05-11 14:40:44.685: epoch 17:	0.01722692  	0.04359982  	0.03883041  
2023-05-11 14:40:44.685: Find a better model.
2023-05-11 14:41:06.065: [iter 18 : loss : 1.1232 = 0.6671 + 0.4559 + 0.0002, time: 21.376451]
2023-05-11 14:41:06.356: epoch 18:	0.01997350  	0.05081271  	0.04505835  
2023-05-11 14:41:06.356: Find a better model.
2023-05-11 14:41:27.787: [iter 19 : loss : 1.1024 = 0.6530 + 0.4492 + 0.0002, time: 21.427302]
2023-05-11 14:41:28.075: epoch 19:	0.02289779  	0.05914078  	0.05131255  
2023-05-11 14:41:28.075: Find a better model.
2023-05-11 14:41:49.417: [iter 20 : loss : 1.0917 = 0.6313 + 0.4600 + 0.0003, time: 21.338606]
2023-05-11 14:41:49.709: epoch 20:	0.02499292  	0.06504998  	0.05636512  
2023-05-11 14:41:49.709: Find a better model.
2023-05-11 14:42:11.031: [iter 21 : loss : 1.0533 = 0.6001 + 0.4527 + 0.0005, time: 21.319538]
2023-05-11 14:42:11.322: epoch 21:	0.02668085  	0.07014354  	0.05991140  
2023-05-11 14:42:11.322: Find a better model.
2023-05-11 14:42:32.642: [iter 22 : loss : 1.0262 = 0.5599 + 0.4657 + 0.0007, time: 21.315775]
2023-05-11 14:42:32.927: epoch 22:	0.02775433  	0.07395829  	0.06225073  
2023-05-11 14:42:32.927: Find a better model.
2023-05-11 14:42:54.217: [iter 23 : loss : 0.9712 = 0.5122 + 0.4581 + 0.0009, time: 21.286980]
2023-05-11 14:42:54.502: epoch 23:	0.02843541  	0.07586326  	0.06353765  
2023-05-11 14:42:54.502: Find a better model.
2023-05-11 14:43:15.632: [iter 24 : loss : 0.9362 = 0.4626 + 0.4724 + 0.0012, time: 21.126359]
2023-05-11 14:43:15.913: epoch 24:	0.02899066  	0.07769149  	0.06464511  
2023-05-11 14:43:15.913: Find a better model.
2023-05-11 14:43:37.156: [iter 25 : loss : 0.8796 = 0.4143 + 0.4638 + 0.0014, time: 21.237952]
2023-05-11 14:43:37.434: epoch 25:	0.02915355  	0.07880457  	0.06503591  
2023-05-11 14:43:37.434: Find a better model.
2023-05-11 14:43:58.584: [iter 26 : loss : 0.8505 = 0.3714 + 0.4774 + 0.0017, time: 21.144741]
2023-05-11 14:43:58.866: epoch 26:	0.02928680  	0.07945266  	0.06528870  
2023-05-11 14:43:58.866: Find a better model.
2023-05-11 14:44:20.102: [iter 27 : loss : 0.8029 = 0.3337 + 0.4672 + 0.0020, time: 21.232089]
2023-05-11 14:44:20.386: epoch 27:	0.02939786  	0.07955616  	0.06548667  
2023-05-11 14:44:20.386: Find a better model.
2023-05-11 14:44:41.605: [iter 28 : loss : 0.7837 = 0.3022 + 0.4792 + 0.0023, time: 21.216235]
2023-05-11 14:44:41.884: epoch 28:	0.02971621  	0.08108436  	0.06601517  
2023-05-11 14:44:41.884: Find a better model.
2023-05-11 14:45:03.126: [iter 29 : loss : 0.7461 = 0.2754 + 0.4681 + 0.0026, time: 21.238316]
2023-05-11 14:45:03.403: epoch 29:	0.02987168  	0.08176076  	0.06644040  
2023-05-11 14:45:03.403: Find a better model.
2023-05-11 14:45:24.784: [iter 30 : loss : 0.7340 = 0.2518 + 0.4793 + 0.0028, time: 21.376768]
2023-05-11 14:45:25.064: epoch 30:	0.03004935  	0.08215649  	0.06661537  
2023-05-11 14:45:25.064: Find a better model.
2023-05-11 14:45:46.338: [iter 31 : loss : 0.7028 = 0.2322 + 0.4676 + 0.0031, time: 21.268903]
2023-05-11 14:45:46.616: epoch 31:	0.03011596  	0.08217591  	0.06695811  
2023-05-11 14:45:46.616: Find a better model.
2023-05-11 14:46:07.970: [iter 32 : loss : 0.6972 = 0.2156 + 0.4783 + 0.0033, time: 21.350549]
2023-05-11 14:46:08.251: epoch 32:	0.03018260  	0.08273078  	0.06716015  
2023-05-11 14:46:08.251: Find a better model.
2023-05-11 14:46:29.687: [iter 33 : loss : 0.6713 = 0.2015 + 0.4663 + 0.0036, time: 21.432277]
2023-05-11 14:46:29.967: epoch 33:	0.03018260  	0.08288147  	0.06719499  
2023-05-11 14:46:29.967: Find a better model.
2023-05-11 14:46:51.322: [iter 34 : loss : 0.6693 = 0.1886 + 0.4768 + 0.0038, time: 21.352543]
2023-05-11 14:46:51.603: epoch 34:	0.03021961  	0.08292380  	0.06737705  
2023-05-11 14:46:51.603: Find a better model.
2023-05-11 14:47:13.072: [iter 35 : loss : 0.6454 = 0.1763 + 0.4650 + 0.0040, time: 21.465202]
2023-05-11 14:47:13.356: epoch 35:	0.03026403  	0.08308816  	0.06755925  
2023-05-11 14:47:13.356: Find a better model.
2023-05-11 14:47:34.938: [iter 36 : loss : 0.6463 = 0.1665 + 0.4756 + 0.0042, time: 21.578794]
2023-05-11 14:47:35.217: epoch 36:	0.03015299  	0.08257042  	0.06764656  
2023-05-11 14:47:56.662: [iter 37 : loss : 0.6259 = 0.1579 + 0.4636 + 0.0044, time: 21.440248]
2023-05-11 14:47:56.941: epoch 37:	0.03045651  	0.08336999  	0.06803115  
2023-05-11 14:47:56.941: Find a better model.
2023-05-11 14:48:18.523: [iter 38 : loss : 0.6287 = 0.1498 + 0.4743 + 0.0046, time: 21.577784]
2023-05-11 14:48:18.801: epoch 38:	0.03039728  	0.08307141  	0.06797534  
2023-05-11 14:48:40.235: [iter 39 : loss : 0.6101 = 0.1430 + 0.4624 + 0.0048, time: 21.430306]
2023-05-11 14:48:40.513: epoch 39:	0.03040469  	0.08312724  	0.06813627  
2023-05-11 14:49:01.860: [iter 40 : loss : 0.6143 = 0.1362 + 0.4731 + 0.0050, time: 21.341588]
2023-05-11 14:49:02.139: epoch 40:	0.03058236  	0.08346537  	0.06853845  
2023-05-11 14:49:02.139: Find a better model.
2023-05-11 14:49:23.862: [iter 41 : loss : 0.5959 = 0.1295 + 0.4612 + 0.0052, time: 21.719496]
2023-05-11 14:49:24.142: epoch 41:	0.03056755  	0.08325186  	0.06854185  
2023-05-11 14:49:45.525: [iter 42 : loss : 0.6014 = 0.1241 + 0.4720 + 0.0053, time: 21.378521]
2023-05-11 14:49:45.804: epoch 42:	0.03047131  	0.08320848  	0.06847627  
2023-05-11 14:50:07.435: [iter 43 : loss : 0.5846 = 0.1189 + 0.4602 + 0.0055, time: 21.625760]
2023-05-11 14:50:07.714: epoch 43:	0.03047130  	0.08328443  	0.06864326  
2023-05-11 14:50:29.262: [iter 44 : loss : 0.5914 = 0.1147 + 0.4711 + 0.0057, time: 21.544008]
2023-05-11 14:50:29.539: epoch 44:	0.03048612  	0.08351710  	0.06853347  
2023-05-11 14:50:29.539: Find a better model.
2023-05-11 14:50:50.984: [iter 45 : loss : 0.5752 = 0.1101 + 0.4593 + 0.0058, time: 21.441281]
2023-05-11 14:50:51.263: epoch 45:	0.03047872  	0.08354507  	0.06859957  
2023-05-11 14:50:51.263: Find a better model.
2023-05-11 14:51:12.843: [iter 46 : loss : 0.5824 = 0.1062 + 0.4702 + 0.0060, time: 21.575027]
2023-05-11 14:51:13.121: epoch 46:	0.03040468  	0.08297907  	0.06833575  
2023-05-11 14:51:34.767: [iter 47 : loss : 0.5670 = 0.1024 + 0.4584 + 0.0062, time: 21.642600]
2023-05-11 14:51:35.044: epoch 47:	0.03039728  	0.08324088  	0.06826816  
2023-05-11 14:51:56.477: [iter 48 : loss : 0.5750 = 0.0991 + 0.4696 + 0.0063, time: 21.429295]
2023-05-11 14:51:56.754: epoch 48:	0.03036026  	0.08304225  	0.06807817  
2023-05-11 14:52:18.379: [iter 49 : loss : 0.5604 = 0.0962 + 0.4577 + 0.0065, time: 21.622156]
2023-05-11 14:52:18.657: epoch 49:	0.03026403  	0.08257031  	0.06774214  
2023-05-11 14:52:42.105: [iter 50 : loss : 0.5687 = 0.0933 + 0.4688 + 0.0066, time: 23.443449]
2023-05-11 14:52:42.370: epoch 50:	0.03009376  	0.08256902  	0.06758826  
2023-05-11 14:53:03.609: [iter 51 : loss : 0.5537 = 0.0899 + 0.4570 + 0.0067, time: 21.235502]
2023-05-11 14:53:03.877: epoch 51:	0.02999752  	0.08222770  	0.06754261  
2023-05-11 14:53:25.274: [iter 52 : loss : 0.5622 = 0.0870 + 0.4684 + 0.0069, time: 21.394294]
2023-05-11 14:53:25.544: epoch 52:	0.02998271  	0.08187324  	0.06734052  
2023-05-11 14:53:46.753: [iter 53 : loss : 0.5493 = 0.0858 + 0.4565 + 0.0070, time: 21.204996]
2023-05-11 14:53:47.017: epoch 53:	0.03001233  	0.08175661  	0.06726742  
2023-05-11 14:54:08.188: [iter 54 : loss : 0.5578 = 0.0830 + 0.4676 + 0.0072, time: 21.168735]
2023-05-11 14:54:08.467: epoch 54:	0.02991608  	0.08104149  	0.06692658  
2023-05-11 14:54:29.715: [iter 55 : loss : 0.5443 = 0.0811 + 0.4558 + 0.0073, time: 21.244320]
2023-05-11 14:54:29.979: epoch 55:	0.02987167  	0.08109848  	0.06695230  
2023-05-11 14:54:51.390: [iter 56 : loss : 0.5530 = 0.0784 + 0.4672 + 0.0074, time: 21.407381]
2023-05-11 14:54:51.667: epoch 56:	0.02978283  	0.08061074  	0.06658781  
2023-05-11 14:55:12.705: [iter 57 : loss : 0.5390 = 0.0760 + 0.4554 + 0.0075, time: 21.033845]
2023-05-11 14:55:12.969: epoch 57:	0.02980503  	0.08039480  	0.06664126  
2023-05-11 14:55:34.393: [iter 58 : loss : 0.5492 = 0.0747 + 0.4668 + 0.0077, time: 21.419884]
2023-05-11 14:55:34.675: epoch 58:	0.02972359  	0.08043741  	0.06640212  
2023-05-11 14:55:55.882: [iter 59 : loss : 0.5356 = 0.0728 + 0.4550 + 0.0078, time: 21.202849]
2023-05-11 14:55:56.161: epoch 59:	0.02971619  	0.08058095  	0.06646202  
2023-05-11 14:56:17.159: [iter 60 : loss : 0.5458 = 0.0714 + 0.4665 + 0.0079, time: 20.993875]
2023-05-11 14:56:17.419: epoch 60:	0.02960514  	0.07990710  	0.06611027  
2023-05-11 14:56:38.708: [iter 61 : loss : 0.5323 = 0.0696 + 0.4547 + 0.0080, time: 21.283688]
2023-05-11 14:56:38.976: epoch 61:	0.02956813  	0.07996196  	0.06601506  
2023-05-11 14:57:00.173: [iter 62 : loss : 0.5427 = 0.0685 + 0.4661 + 0.0081, time: 21.193029]
2023-05-11 14:57:00.434: epoch 62:	0.02959033  	0.08001027  	0.06596649  
2023-05-11 14:57:21.500: [iter 63 : loss : 0.5295 = 0.0670 + 0.4543 + 0.0082, time: 21.062247]
2023-05-11 14:57:21.785: epoch 63:	0.02952371  	0.07962014  	0.06582244  
2023-05-11 14:57:42.735: [iter 64 : loss : 0.5401 = 0.0659 + 0.4658 + 0.0084, time: 20.943444]
2023-05-11 14:57:43.013: epoch 64:	0.02942747  	0.07924023  	0.06565806  
2023-05-11 14:58:04.452: [iter 65 : loss : 0.5266 = 0.0643 + 0.4539 + 0.0085, time: 21.434330]
2023-05-11 14:58:04.729: epoch 65:	0.02927200  	0.07858133  	0.06536315  
2023-05-11 14:58:25.873: [iter 66 : loss : 0.5370 = 0.0630 + 0.4654 + 0.0086, time: 21.139380]
2023-05-11 14:58:26.148: epoch 66:	0.02924239  	0.07849006  	0.06531618  
2023-05-11 14:58:47.417: [iter 67 : loss : 0.5239 = 0.0616 + 0.4536 + 0.0087, time: 21.265122]
2023-05-11 14:58:47.697: epoch 67:	0.02928680  	0.07849180  	0.06523095  
2023-05-11 14:59:09.088: [iter 68 : loss : 0.5348 = 0.0608 + 0.4652 + 0.0088, time: 21.388519]
2023-05-11 14:59:09.365: epoch 68:	0.02910173  	0.07775610  	0.06484874  
2023-05-11 14:59:30.600: [iter 69 : loss : 0.5216 = 0.0593 + 0.4534 + 0.0089, time: 21.230994]
2023-05-11 14:59:30.877: epoch 69:	0.02915354  	0.07785025  	0.06480853  
2023-05-11 14:59:52.084: [iter 70 : loss : 0.5322 = 0.0583 + 0.4649 + 0.0090, time: 21.202597]
2023-05-11 14:59:52.359: epoch 70:	0.02907211  	0.07740404  	0.06460587  
2023-05-11 14:59:52.360: Early stopping is trigger at epoch: 70
2023-05-11 14:59:52.360: best_result@epoch 45:

2023-05-11 14:59:52.360: 		0.0305      	0.0835      	0.0686      
2023-05-11 15:01:43.511: my pid: 7312
2023-05-11 15:01:43.511: model: model.general_recommender.SGL
2023-05-11 15:01:43.511: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 15:01:43.511: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 15:01:46.835: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 15:02:07.975: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.140039]
2023-05-11 15:02:08.241: epoch 1:	0.00123630  	0.00246183  	0.00202487  
2023-05-11 15:02:08.242: Find a better model.
2023-05-11 15:02:29.617: [iter 2 : loss : 1.1366 = 0.6930 + 0.4436 + 0.0000, time: 21.371110]
2023-05-11 15:02:29.900: epoch 2:	0.00156204  	0.00303791  	0.00245906  
2023-05-11 15:02:29.900: Find a better model.
2023-05-11 15:02:51.173: [iter 3 : loss : 1.1348 = 0.6929 + 0.4419 + 0.0000, time: 21.269003]
2023-05-11 15:02:51.457: epoch 3:	0.00195440  	0.00333171  	0.00290741  
2023-05-11 15:02:51.457: Find a better model.
2023-05-11 15:03:12.824: [iter 4 : loss : 1.1374 = 0.6929 + 0.4445 + 0.0000, time: 21.362167]
2023-05-11 15:03:13.137: epoch 4:	0.00210246  	0.00347822  	0.00293291  
2023-05-11 15:03:13.138: Find a better model.
2023-05-11 15:03:34.585: [iter 5 : loss : 1.1354 = 0.6927 + 0.4427 + 0.0000, time: 21.444164]
2023-05-11 15:03:34.868: epoch 5:	0.00272431  	0.00565557  	0.00456662  
2023-05-11 15:03:34.868: Find a better model.
2023-05-11 15:03:56.007: [iter 6 : loss : 1.1381 = 0.6926 + 0.4455 + 0.0000, time: 21.135546]
2023-05-11 15:03:56.304: epoch 6:	0.00288718  	0.00514853  	0.00411260  
2023-05-11 15:04:17.711: [iter 7 : loss : 1.1359 = 0.6924 + 0.4434 + 0.0000, time: 21.403954]
2023-05-11 15:04:17.997: epoch 7:	0.00356826  	0.00778208  	0.00635676  
2023-05-11 15:04:17.997: Find a better model.
2023-05-11 15:04:39.000: [iter 8 : loss : 1.1388 = 0.6922 + 0.4465 + 0.0000, time: 20.999426]
2023-05-11 15:04:39.295: epoch 8:	0.00362748  	0.00714265  	0.00586805  
2023-05-11 15:05:00.559: [iter 9 : loss : 1.1360 = 0.6919 + 0.4441 + 0.0000, time: 21.260701]
2023-05-11 15:05:00.836: epoch 9:	0.00462688  	0.01051000  	0.00829096  
2023-05-11 15:05:00.836: Find a better model.
2023-05-11 15:05:21.960: [iter 10 : loss : 1.1391 = 0.6915 + 0.4476 + 0.0000, time: 21.119900]
2023-05-11 15:05:22.256: epoch 10:	0.00451584  	0.00987984  	0.00793695  
2023-05-11 15:05:43.289: [iter 11 : loss : 1.1357 = 0.6908 + 0.4449 + 0.0000, time: 21.029782]
2023-05-11 15:05:43.588: epoch 11:	0.00550043  	0.01284537  	0.01053557  
2023-05-11 15:05:43.589: Find a better model.
2023-05-11 15:06:04.750: [iter 12 : loss : 1.1389 = 0.6898 + 0.4491 + 0.0000, time: 21.153935]
2023-05-11 15:06:05.045: epoch 12:	0.00592240  	0.01374076  	0.01108721  
2023-05-11 15:06:05.045: Find a better model.
2023-05-11 15:06:25.941: [iter 13 : loss : 1.1343 = 0.6885 + 0.4458 + 0.0000, time: 20.893282]
2023-05-11 15:06:26.259: epoch 13:	0.00775833  	0.01911230  	0.01538508  
2023-05-11 15:06:26.259: Find a better model.
2023-05-11 15:06:47.142: [iter 14 : loss : 1.1377 = 0.6868 + 0.4509 + 0.0001, time: 20.879075]
2023-05-11 15:06:47.419: epoch 14:	0.00923151  	0.02145888  	0.01877855  
2023-05-11 15:06:47.419: Find a better model.
2023-05-11 15:07:08.299: [iter 15 : loss : 1.1311 = 0.6844 + 0.4466 + 0.0001, time: 20.877340]
2023-05-11 15:07:08.573: epoch 15:	0.01202988  	0.03021891  	0.02547781  
2023-05-11 15:07:08.573: Find a better model.
2023-05-11 15:07:29.566: [iter 16 : loss : 1.1337 = 0.6806 + 0.4530 + 0.0001, time: 20.987996]
2023-05-11 15:07:29.855: epoch 16:	0.01444333  	0.03595234  	0.03117372  
2023-05-11 15:07:29.855: Find a better model.
2023-05-11 15:07:50.856: [iter 17 : loss : 1.1227 = 0.6748 + 0.4477 + 0.0001, time: 20.997528]
2023-05-11 15:07:51.125: epoch 17:	0.01733797  	0.04465366  	0.03849459  
2023-05-11 15:07:51.125: Find a better model.
2023-05-11 15:08:12.090: [iter 18 : loss : 1.1213 = 0.6651 + 0.4560 + 0.0002, time: 20.960004]
2023-05-11 15:08:12.381: epoch 18:	0.02039550  	0.05199263  	0.04580605  
2023-05-11 15:08:12.381: Find a better model.
2023-05-11 15:08:33.597: [iter 19 : loss : 1.0996 = 0.6496 + 0.4498 + 0.0002, time: 21.213028]
2023-05-11 15:08:33.885: epoch 19:	0.02338640  	0.06014263  	0.05212615  
2023-05-11 15:08:33.885: Find a better model.
2023-05-11 15:08:54.880: [iter 20 : loss : 1.0870 = 0.6263 + 0.4604 + 0.0003, time: 20.990741]
2023-05-11 15:08:55.163: epoch 20:	0.02524462  	0.06610077  	0.05683401  
2023-05-11 15:08:55.163: Find a better model.
2023-05-11 15:09:16.272: [iter 21 : loss : 1.0469 = 0.5930 + 0.4534 + 0.0005, time: 21.104395]
2023-05-11 15:09:16.553: epoch 21:	0.02692514  	0.07062931  	0.06037671  
2023-05-11 15:09:16.553: Find a better model.
2023-05-11 15:09:37.672: [iter 22 : loss : 1.0177 = 0.5507 + 0.4663 + 0.0007, time: 21.115333]
2023-05-11 15:09:37.958: epoch 22:	0.02753962  	0.07284974  	0.06186355  
2023-05-11 15:09:37.958: Find a better model.
2023-05-11 15:09:59.010: [iter 23 : loss : 0.9621 = 0.5020 + 0.4591 + 0.0009, time: 21.047559]
2023-05-11 15:09:59.289: epoch 23:	0.02841321  	0.07553859  	0.06334596  
2023-05-11 15:09:59.289: Find a better model.
2023-05-11 15:10:20.453: [iter 24 : loss : 0.9264 = 0.4520 + 0.4731 + 0.0012, time: 21.160177]
2023-05-11 15:10:20.738: epoch 24:	0.02859828  	0.07674579  	0.06411482  
2023-05-11 15:10:20.738: Find a better model.
2023-05-11 15:10:41.998: [iter 25 : loss : 0.8710 = 0.4046 + 0.4649 + 0.0015, time: 21.254950]
2023-05-11 15:10:42.277: epoch 25:	0.02913132  	0.07824713  	0.06496592  
2023-05-11 15:10:42.277: Find a better model.
2023-05-11 15:11:03.442: [iter 26 : loss : 0.8421 = 0.3627 + 0.4776 + 0.0018, time: 21.161244]
2023-05-11 15:11:03.719: epoch 26:	0.02922756  	0.07925287  	0.06542219  
2023-05-11 15:11:03.719: Find a better model.
2023-05-11 15:11:24.770: [iter 27 : loss : 0.7957 = 0.3259 + 0.4678 + 0.0021, time: 21.046582]
2023-05-11 15:11:25.048: epoch 27:	0.02927938  	0.07906947  	0.06540487  
2023-05-11 15:11:46.220: [iter 28 : loss : 0.7769 = 0.2953 + 0.4792 + 0.0024, time: 21.167347]
2023-05-11 15:11:46.502: epoch 28:	0.02951629  	0.07989410  	0.06551050  
2023-05-11 15:11:46.502: Find a better model.
2023-05-11 15:12:07.594: [iter 29 : loss : 0.7403 = 0.2693 + 0.4683 + 0.0026, time: 21.088455]
2023-05-11 15:12:07.871: epoch 29:	0.02964214  	0.08025678  	0.06594701  
2023-05-11 15:12:07.871: Find a better model.
2023-05-11 15:12:29.006: [iter 30 : loss : 0.7287 = 0.2467 + 0.4791 + 0.0029, time: 21.130284]
2023-05-11 15:12:29.284: epoch 30:	0.02964214  	0.08063569  	0.06601535  
2023-05-11 15:12:29.284: Find a better model.
2023-05-11 15:12:50.542: [iter 31 : loss : 0.6987 = 0.2278 + 0.4678 + 0.0031, time: 21.254869]
2023-05-11 15:12:50.822: epoch 31:	0.02990866  	0.08170146  	0.06633869  
2023-05-11 15:12:50.822: Find a better model.
2023-05-11 15:13:11.997: [iter 32 : loss : 0.6931 = 0.2119 + 0.4779 + 0.0034, time: 21.170336]
2023-05-11 15:13:12.273: epoch 32:	0.02993087  	0.08154081  	0.06625554  
2023-05-11 15:13:33.561: [iter 33 : loss : 0.6682 = 0.1982 + 0.4664 + 0.0036, time: 21.284777]
2023-05-11 15:13:33.842: epoch 33:	0.03007893  	0.08193022  	0.06641799  
2023-05-11 15:13:33.842: Find a better model.
2023-05-11 15:13:55.187: [iter 34 : loss : 0.6659 = 0.1854 + 0.4766 + 0.0038, time: 21.341819]
2023-05-11 15:13:55.467: epoch 34:	0.02994567  	0.08096775  	0.06616975  
2023-05-11 15:14:16.725: [iter 35 : loss : 0.6429 = 0.1739 + 0.4650 + 0.0041, time: 21.252466]
2023-05-11 15:14:17.000: epoch 35:	0.02999751  	0.08149478  	0.06642713  
2023-05-11 15:14:38.372: [iter 36 : loss : 0.6439 = 0.1644 + 0.4753 + 0.0043, time: 21.368524]
2023-05-11 15:14:38.650: epoch 36:	0.03007154  	0.08152957  	0.06652488  
2023-05-11 15:15:00.125: [iter 37 : loss : 0.6241 = 0.1560 + 0.4637 + 0.0045, time: 21.471263]
2023-05-11 15:15:00.400: epoch 37:	0.03010114  	0.08158182  	0.06646796  
2023-05-11 15:15:21.559: [iter 38 : loss : 0.6264 = 0.1477 + 0.4741 + 0.0047, time: 21.155204]
2023-05-11 15:15:21.838: epoch 38:	0.03006413  	0.08186731  	0.06653666  
2023-05-11 15:15:43.114: [iter 39 : loss : 0.6081 = 0.1410 + 0.4623 + 0.0048, time: 21.272394]
2023-05-11 15:15:43.399: epoch 39:	0.03019738  	0.08227366  	0.06688943  
2023-05-11 15:15:43.399: Find a better model.
2023-05-11 15:16:04.738: [iter 40 : loss : 0.6119 = 0.1343 + 0.4726 + 0.0050, time: 21.335729]
2023-05-11 15:16:05.015: epoch 40:	0.03025660  	0.08213078  	0.06691840  
2023-05-11 15:16:26.282: [iter 41 : loss : 0.5940 = 0.1277 + 0.4611 + 0.0052, time: 21.262037]
2023-05-11 15:16:26.560: epoch 41:	0.03045648  	0.08240220  	0.06715364  
2023-05-11 15:16:26.560: Find a better model.
2023-05-11 15:16:47.741: [iter 42 : loss : 0.5996 = 0.1224 + 0.4719 + 0.0054, time: 21.177122]
2023-05-11 15:16:48.018: epoch 42:	0.03038246  	0.08232709  	0.06700180  
2023-05-11 15:17:09.268: [iter 43 : loss : 0.5832 = 0.1175 + 0.4602 + 0.0055, time: 21.247916]
2023-05-11 15:17:09.548: epoch 43:	0.03030102  	0.08199779  	0.06678171  
2023-05-11 15:17:30.721: [iter 44 : loss : 0.5906 = 0.1139 + 0.4710 + 0.0057, time: 21.170172]
2023-05-11 15:17:30.996: epoch 44:	0.03006412  	0.08147189  	0.06659327  
2023-05-11 15:17:52.260: [iter 45 : loss : 0.5743 = 0.1091 + 0.4593 + 0.0059, time: 21.260022]
2023-05-11 15:17:52.540: epoch 45:	0.03009373  	0.08159927  	0.06657556  
2023-05-11 15:18:13.892: [iter 46 : loss : 0.5816 = 0.1054 + 0.4701 + 0.0060, time: 21.347777]
2023-05-11 15:18:14.170: epoch 46:	0.02993827  	0.08113052  	0.06636336  
2023-05-11 15:18:35.411: [iter 47 : loss : 0.5658 = 0.1013 + 0.4584 + 0.0062, time: 21.236929]
2023-05-11 15:18:35.687: epoch 47:	0.03003451  	0.08104914  	0.06644001  
2023-05-11 15:18:59.099: [iter 48 : loss : 0.5740 = 0.0983 + 0.4693 + 0.0063, time: 23.408343]
2023-05-11 15:18:59.380: epoch 48:	0.02985684  	0.08077155  	0.06620326  
2023-05-11 15:19:20.700: [iter 49 : loss : 0.5593 = 0.0951 + 0.4578 + 0.0065, time: 21.316751]
2023-05-11 15:19:20.965: epoch 49:	0.02984944  	0.08085192  	0.06613334  
2023-05-11 15:20:12.364: my pid: 376
2023-05-11 15:20:12.365: model: model.general_recommender.SGL
2023-05-11 15:20:12.365: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 15:20:12.365: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 15:20:15.672: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 15:20:36.793: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.120005]
2023-05-11 15:20:37.056: epoch 1:	0.00111045  	0.00235054  	0.00182102  
2023-05-11 15:20:37.056: Find a better model.
2023-05-11 15:20:58.634: [iter 2 : loss : 1.1367 = 0.6930 + 0.4437 + 0.0000, time: 21.575292]
2023-05-11 15:20:58.936: epoch 2:	0.00139177  	0.00288951  	0.00224723  
2023-05-11 15:20:58.936: Find a better model.
2023-05-11 15:21:20.615: [iter 3 : loss : 1.1347 = 0.6929 + 0.4417 + 0.0000, time: 21.675289]
2023-05-11 15:21:20.909: epoch 3:	0.00194700  	0.00344983  	0.00291330  
2023-05-11 15:21:20.909: Find a better model.
2023-05-11 15:21:42.260: [iter 4 : loss : 1.1374 = 0.6929 + 0.4446 + 0.0000, time: 21.347159]
2023-05-11 15:21:42.564: epoch 4:	0.00221350  	0.00400825  	0.00348825  
2023-05-11 15:21:42.564: Find a better model.
2023-05-11 15:22:04.160: [iter 5 : loss : 1.1353 = 0.6927 + 0.4425 + 0.0000, time: 21.592578]
2023-05-11 15:22:04.468: epoch 5:	0.00251703  	0.00516344  	0.00396493  
2023-05-11 15:22:04.468: Find a better model.
2023-05-11 15:22:26.054: [iter 6 : loss : 1.1382 = 0.6926 + 0.4455 + 0.0000, time: 21.582587]
2023-05-11 15:22:26.343: epoch 6:	0.00269470  	0.00404397  	0.00375858  
2023-05-11 15:22:47.938: [iter 7 : loss : 1.1357 = 0.6924 + 0.4433 + 0.0000, time: 21.591997]
2023-05-11 15:22:48.224: epoch 7:	0.00329435  	0.00637778  	0.00525537  
2023-05-11 15:22:48.224: Find a better model.
2023-05-11 15:23:09.828: [iter 8 : loss : 1.1388 = 0.6923 + 0.4465 + 0.0000, time: 21.599496]
2023-05-11 15:23:10.115: epoch 8:	0.00339799  	0.00636915  	0.00530029  
2023-05-11 15:23:31.777: [iter 9 : loss : 1.1359 = 0.6920 + 0.4439 + 0.0000, time: 21.657072]
2023-05-11 15:23:32.059: epoch 9:	0.00444181  	0.00973403  	0.00802942  
2023-05-11 15:23:32.060: Find a better model.
2023-05-11 15:23:53.599: [iter 10 : loss : 1.1392 = 0.6916 + 0.4476 + 0.0000, time: 21.536236]
2023-05-11 15:23:53.912: epoch 10:	0.00447142  	0.00956219  	0.00787029  
2023-05-11 15:24:15.534: [iter 11 : loss : 1.1357 = 0.6910 + 0.4446 + 0.0000, time: 21.617173]
2023-05-11 15:24:15.815: epoch 11:	0.00553745  	0.01292695  	0.01017034  
2023-05-11 15:24:15.815: Find a better model.
2023-05-11 15:24:37.340: [iter 12 : loss : 1.1391 = 0.6902 + 0.4488 + 0.0000, time: 21.521667]
2023-05-11 15:24:37.638: epoch 12:	0.00595201  	0.01311858  	0.01105722  
2023-05-11 15:24:37.638: Find a better model.
2023-05-11 15:24:59.486: [iter 13 : loss : 1.1344 = 0.6889 + 0.4455 + 0.0000, time: 21.843352]
2023-05-11 15:24:59.788: epoch 13:	0.00745481  	0.01811257  	0.01510934  
2023-05-11 15:24:59.788: Find a better model.
2023-05-11 15:25:21.556: [iter 14 : loss : 1.1378 = 0.6872 + 0.4506 + 0.0000, time: 21.764463]
2023-05-11 15:25:21.855: epoch 14:	0.00900943  	0.02039105  	0.01760234  
2023-05-11 15:25:21.855: Find a better model.
2023-05-11 15:25:43.492: [iter 15 : loss : 1.1313 = 0.6849 + 0.4463 + 0.0001, time: 21.631422]
2023-05-11 15:25:43.788: epoch 15:	0.01167453  	0.02856706  	0.02486051  
2023-05-11 15:25:43.788: Find a better model.
2023-05-11 15:26:05.493: [iter 16 : loss : 1.1343 = 0.6814 + 0.4528 + 0.0001, time: 21.702826]
2023-05-11 15:26:05.790: epoch 16:	0.01415459  	0.03429731  	0.03004136  
2023-05-11 15:26:05.790: Find a better model.
2023-05-11 15:26:27.445: [iter 17 : loss : 1.1236 = 0.6761 + 0.4473 + 0.0001, time: 21.651699]
2023-05-11 15:26:27.738: epoch 17:	0.01711585  	0.04242368  	0.03759537  
2023-05-11 15:26:27.738: Find a better model.
2023-05-11 15:26:49.358: [iter 18 : loss : 1.1231 = 0.6672 + 0.4557 + 0.0002, time: 21.616851]
2023-05-11 15:26:49.652: epoch 18:	0.02006974  	0.05039851  	0.04386834  
2023-05-11 15:26:49.653: Find a better model.
2023-05-11 15:27:11.297: [iter 19 : loss : 1.1024 = 0.6531 + 0.4491 + 0.0002, time: 21.640644]
2023-05-11 15:27:11.588: epoch 19:	0.02303847  	0.05962879  	0.05110055  
2023-05-11 15:27:11.588: Find a better model.
2023-05-11 15:27:33.091: [iter 20 : loss : 1.0918 = 0.6317 + 0.4598 + 0.0003, time: 21.499290]
2023-05-11 15:27:33.382: epoch 20:	0.02482263  	0.06488749  	0.05581541  
2023-05-11 15:27:33.382: Find a better model.
2023-05-11 15:27:54.805: [iter 21 : loss : 1.0535 = 0.6005 + 0.4525 + 0.0005, time: 21.420303]
2023-05-11 15:27:55.096: epoch 21:	0.02671788  	0.06966069  	0.05966091  
2023-05-11 15:27:55.097: Find a better model.
2023-05-11 15:28:16.696: [iter 22 : loss : 1.0264 = 0.5603 + 0.4655 + 0.0007, time: 21.594995]
2023-05-11 15:28:16.985: epoch 22:	0.02760628  	0.07357664  	0.06170958  
2023-05-11 15:28:16.985: Find a better model.
2023-05-11 15:28:38.449: [iter 23 : loss : 0.9714 = 0.5126 + 0.4579 + 0.0009, time: 21.460325]
2023-05-11 15:28:38.736: epoch 23:	0.02839102  	0.07605261  	0.06339695  
2023-05-11 15:28:38.736: Find a better model.
2023-05-11 15:29:00.075: [iter 24 : loss : 0.9364 = 0.4630 + 0.4722 + 0.0012, time: 21.336592]
2023-05-11 15:29:00.363: epoch 24:	0.02857613  	0.07753451  	0.06406105  
2023-05-11 15:29:00.363: Find a better model.
2023-05-11 15:29:21.835: [iter 25 : loss : 0.8802 = 0.4149 + 0.4638 + 0.0014, time: 21.467255]
2023-05-11 15:29:22.127: epoch 25:	0.02896108  	0.07863765  	0.06462973  
2023-05-11 15:29:22.128: Find a better model.
2023-05-11 15:29:43.665: [iter 26 : loss : 0.8507 = 0.3718 + 0.4772 + 0.0017, time: 21.532566]
2023-05-11 15:29:43.957: epoch 26:	0.02880563  	0.07888108  	0.06480027  
2023-05-11 15:29:43.957: Find a better model.
2023-05-11 15:30:05.395: [iter 27 : loss : 0.8032 = 0.3339 + 0.4672 + 0.0020, time: 21.434960]
2023-05-11 15:30:05.687: epoch 27:	0.02911657  	0.07975049  	0.06510457  
2023-05-11 15:30:05.687: Find a better model.
2023-05-11 15:30:27.077: [iter 28 : loss : 0.7837 = 0.3023 + 0.4791 + 0.0023, time: 21.384799]
2023-05-11 15:30:27.367: epoch 28:	0.02912399  	0.07979667  	0.06536752  
2023-05-11 15:30:27.367: Find a better model.
2023-05-11 15:30:48.792: [iter 29 : loss : 0.7460 = 0.2756 + 0.4679 + 0.0026, time: 21.421167]
2023-05-11 15:30:49.078: epoch 29:	0.02929427  	0.08054388  	0.06568766  
2023-05-11 15:30:49.078: Find a better model.
2023-05-11 15:31:10.459: [iter 30 : loss : 0.7336 = 0.2518 + 0.4790 + 0.0028, time: 21.377684]
2023-05-11 15:31:10.746: epoch 30:	0.02953118  	0.08132794  	0.06603435  
2023-05-11 15:31:10.746: Find a better model.
2023-05-11 15:31:32.194: [iter 31 : loss : 0.7029 = 0.2323 + 0.4675 + 0.0031, time: 21.444521]
2023-05-11 15:31:32.474: epoch 31:	0.02960520  	0.08122462  	0.06628001  
2023-05-11 15:31:53.841: [iter 32 : loss : 0.6968 = 0.2155 + 0.4780 + 0.0033, time: 21.364412]
2023-05-11 15:31:54.122: epoch 32:	0.02960519  	0.08104286  	0.06636817  
2023-05-11 15:32:15.551: [iter 33 : loss : 0.6713 = 0.2015 + 0.4663 + 0.0036, time: 21.424874]
2023-05-11 15:32:15.833: epoch 33:	0.02962739  	0.08109326  	0.06649579  
2023-05-11 15:32:37.394: [iter 34 : loss : 0.6691 = 0.1887 + 0.4767 + 0.0038, time: 21.556446]
2023-05-11 15:32:37.677: epoch 34:	0.02961999  	0.08086862  	0.06652831  
2023-05-11 15:32:59.160: [iter 35 : loss : 0.6451 = 0.1763 + 0.4648 + 0.0040, time: 21.479157]
2023-05-11 15:32:59.441: epoch 35:	0.02982728  	0.08130898  	0.06677426  
2023-05-11 15:33:20.831: [iter 36 : loss : 0.6461 = 0.1666 + 0.4753 + 0.0042, time: 21.385670]
2023-05-11 15:33:21.111: epoch 36:	0.02979768  	0.08140230  	0.06692444  
2023-05-11 15:33:21.111: Find a better model.
2023-05-11 15:33:42.689: [iter 37 : loss : 0.6258 = 0.1579 + 0.4634 + 0.0044, time: 21.574988]
2023-05-11 15:33:42.968: epoch 37:	0.02990131  	0.08181843  	0.06702620  
2023-05-11 15:33:42.968: Find a better model.
2023-05-11 15:34:04.547: [iter 38 : loss : 0.6280 = 0.1494 + 0.4740 + 0.0046, time: 21.575586]
2023-05-11 15:34:04.825: epoch 38:	0.02990872  	0.08175705  	0.06696729  
2023-05-11 15:34:26.286: [iter 39 : loss : 0.6099 = 0.1428 + 0.4622 + 0.0048, time: 21.456342]
2023-05-11 15:34:26.565: epoch 39:	0.03004937  	0.08207139  	0.06711802  
2023-05-11 15:34:26.565: Find a better model.
2023-05-11 15:34:48.160: [iter 40 : loss : 0.6133 = 0.1356 + 0.4727 + 0.0050, time: 21.592013]
2023-05-11 15:34:48.439: epoch 40:	0.02991612  	0.08141185  	0.06701446  
2023-05-11 15:35:10.114: [iter 41 : loss : 0.5957 = 0.1295 + 0.4610 + 0.0052, time: 21.672102]
2023-05-11 15:35:10.392: epoch 41:	0.02981247  	0.08114336  	0.06696169  
2023-05-11 15:35:31.904: [iter 42 : loss : 0.6010 = 0.1238 + 0.4718 + 0.0053, time: 21.507681]
2023-05-11 15:35:32.186: epoch 42:	0.02976064  	0.08149103  	0.06703051  
2023-05-11 15:35:53.853: [iter 43 : loss : 0.5845 = 0.1188 + 0.4601 + 0.0055, time: 21.664569]
2023-05-11 15:35:54.133: epoch 43:	0.02979026  	0.08154445  	0.06714513  
2023-05-11 15:36:15.769: [iter 44 : loss : 0.5917 = 0.1151 + 0.4709 + 0.0057, time: 21.632432]
2023-05-11 15:36:16.050: epoch 44:	0.02984208  	0.08174718  	0.06707872  
2023-05-11 15:36:40.658: [iter 45 : loss : 0.5753 = 0.1101 + 0.4593 + 0.0058, time: 24.604504]
2023-05-11 15:36:40.946: epoch 45:	0.02976064  	0.08120356  	0.06681202  
2023-05-11 15:37:02.763: [iter 46 : loss : 0.5823 = 0.1063 + 0.4701 + 0.0060, time: 21.812239]
2023-05-11 15:37:03.137: epoch 46:	0.02971622  	0.08121881  	0.06676031  
2023-05-11 15:37:25.809: my pid: 8412
2023-05-11 15:37:25.809: model: model.general_recommender.SGL
2023-05-11 15:37:25.809: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 15:37:25.809: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 15:37:29.365: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 15:37:51.338: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.972257]
2023-05-11 15:37:51.639: epoch 1:	0.00125851  	0.00292050  	0.00220912  
2023-05-11 15:37:51.639: Find a better model.
2023-05-11 15:38:13.298: [iter 2 : loss : 1.1365 = 0.6930 + 0.4435 + 0.0000, time: 21.654798]
2023-05-11 15:38:13.603: epoch 2:	0.00159165  	0.00312031  	0.00232389  
2023-05-11 15:38:13.603: Find a better model.
2023-05-11 15:38:35.245: [iter 3 : loss : 1.1346 = 0.6929 + 0.4417 + 0.0000, time: 21.636943]
2023-05-11 15:38:35.531: epoch 3:	0.00202103  	0.00402108  	0.00300604  
2023-05-11 15:38:35.531: Find a better model.
2023-05-11 15:38:57.084: [iter 4 : loss : 1.1372 = 0.6929 + 0.4444 + 0.0000, time: 21.549271]
2023-05-11 15:38:57.367: epoch 4:	0.00188777  	0.00356144  	0.00274460  
2023-05-11 15:39:19.056: [iter 5 : loss : 1.1352 = 0.6927 + 0.4425 + 0.0000, time: 21.685380]
2023-05-11 15:39:19.342: epoch 5:	0.00255404  	0.00496978  	0.00396066  
2023-05-11 15:39:19.342: Find a better model.
2023-05-11 15:39:40.901: [iter 6 : loss : 1.1379 = 0.6926 + 0.4453 + 0.0000, time: 21.555363]
2023-05-11 15:39:41.183: epoch 6:	0.00259846  	0.00492153  	0.00391271  
2023-05-11 15:40:02.612: [iter 7 : loss : 1.1356 = 0.6925 + 0.4432 + 0.0000, time: 21.424626]
2023-05-11 15:40:02.893: epoch 7:	0.00324993  	0.00661370  	0.00558246  
2023-05-11 15:40:02.893: Find a better model.
2023-05-11 15:40:24.466: [iter 8 : loss : 1.1386 = 0.6923 + 0.4463 + 0.0000, time: 21.570253]
2023-05-11 15:40:24.760: epoch 8:	0.00353864  	0.00742232  	0.00580091  
2023-05-11 15:40:24.760: Find a better model.
2023-05-11 15:40:46.446: [iter 9 : loss : 1.1358 = 0.6920 + 0.4438 + 0.0000, time: 21.682143]
2023-05-11 15:40:46.749: epoch 9:	0.00442700  	0.01113238  	0.00827474  
2023-05-11 15:40:46.749: Find a better model.
2023-05-11 15:41:08.038: [iter 10 : loss : 1.1389 = 0.6916 + 0.4473 + 0.0000, time: 21.285179]
2023-05-11 15:41:08.319: epoch 10:	0.00447883  	0.01015237  	0.00788758  
2023-05-11 15:41:29.787: [iter 11 : loss : 1.1355 = 0.6910 + 0.4445 + 0.0000, time: 21.462987]
2023-05-11 15:41:30.064: epoch 11:	0.00535978  	0.01328759  	0.01060806  
2023-05-11 15:41:30.064: Find a better model.
2023-05-11 15:41:51.479: [iter 12 : loss : 1.1388 = 0.6901 + 0.4486 + 0.0000, time: 21.410944]
2023-05-11 15:41:51.772: epoch 12:	0.00586318  	0.01267477  	0.01092061  
2023-05-11 15:42:13.370: [iter 13 : loss : 1.1342 = 0.6889 + 0.4453 + 0.0000, time: 21.593361]
2023-05-11 15:42:13.660: epoch 13:	0.00753624  	0.01766826  	0.01558867  
2023-05-11 15:42:13.660: Find a better model.
2023-05-11 15:42:35.210: [iter 14 : loss : 1.1375 = 0.6872 + 0.4503 + 0.0000, time: 21.546975]
2023-05-11 15:42:35.498: epoch 14:	0.00853564  	0.01981271  	0.01703348  
2023-05-11 15:42:35.498: Find a better model.
2023-05-11 15:42:57.164: [iter 15 : loss : 1.1313 = 0.6850 + 0.4463 + 0.0001, time: 21.661640]
2023-05-11 15:42:57.449: epoch 15:	0.01134881  	0.02831588  	0.02390914  
2023-05-11 15:42:57.449: Find a better model.
2023-05-11 15:43:18.997: [iter 16 : loss : 1.1340 = 0.6815 + 0.4524 + 0.0001, time: 21.545095]
2023-05-11 15:43:19.282: epoch 16:	0.01382146  	0.03461270  	0.02998901  
2023-05-11 15:43:19.282: Find a better model.
2023-05-11 15:43:40.954: [iter 17 : loss : 1.1235 = 0.6761 + 0.4473 + 0.0001, time: 21.667630]
2023-05-11 15:43:41.241: epoch 17:	0.01725654  	0.04401373  	0.03886158  
2023-05-11 15:43:41.241: Find a better model.
2023-05-11 15:44:02.772: [iter 18 : loss : 1.1226 = 0.6672 + 0.4553 + 0.0002, time: 21.528038]
2023-05-11 15:44:03.060: epoch 18:	0.02032886  	0.05211619  	0.04542517  
2023-05-11 15:44:03.060: Find a better model.
2023-05-11 15:44:24.738: [iter 19 : loss : 1.1023 = 0.6529 + 0.4491 + 0.0002, time: 21.672538]
2023-05-11 15:44:25.024: epoch 19:	0.02311989  	0.05969677  	0.05223464  
2023-05-11 15:44:25.024: Find a better model.
2023-05-11 15:44:46.418: [iter 20 : loss : 1.0907 = 0.6311 + 0.4593 + 0.0003, time: 21.390619]
2023-05-11 15:44:46.704: epoch 20:	0.02521499  	0.06534766  	0.05664619  
2023-05-11 15:44:46.704: Find a better model.
2023-05-11 15:45:08.298: [iter 21 : loss : 1.0527 = 0.5995 + 0.4527 + 0.0005, time: 21.588860]
2023-05-11 15:45:08.582: epoch 21:	0.02671785  	0.06931222  	0.06019066  
2023-05-11 15:45:08.583: Find a better model.
2023-05-11 15:45:29.990: [iter 22 : loss : 1.0245 = 0.5585 + 0.4653 + 0.0007, time: 21.402432]
2023-05-11 15:45:30.274: epoch 22:	0.02739155  	0.07148737  	0.06176204  
2023-05-11 15:45:30.274: Find a better model.
2023-05-11 15:45:51.885: [iter 23 : loss : 0.9698 = 0.5106 + 0.4583 + 0.0009, time: 21.607754]
2023-05-11 15:45:52.167: epoch 23:	0.02842062  	0.07427353  	0.06335475  
2023-05-11 15:45:52.167: Find a better model.
2023-05-11 15:46:13.563: [iter 24 : loss : 0.9339 = 0.4606 + 0.4720 + 0.0012, time: 21.390599]
2023-05-11 15:46:13.843: epoch 24:	0.02840581  	0.07569714  	0.06398086  
2023-05-11 15:46:13.843: Find a better model.
2023-05-11 15:46:35.476: [iter 25 : loss : 0.8780 = 0.4124 + 0.4642 + 0.0015, time: 21.628794]
2023-05-11 15:46:35.755: epoch 25:	0.02907951  	0.07735517  	0.06483124  
2023-05-11 15:46:35.755: Find a better model.
2023-05-11 15:46:57.147: [iter 26 : loss : 0.8481 = 0.3694 + 0.4770 + 0.0017, time: 21.388385]
2023-05-11 15:46:57.426: epoch 26:	0.02915356  	0.07905992  	0.06537403  
2023-05-11 15:46:57.426: Find a better model.
2023-05-11 15:47:19.060: [iter 27 : loss : 0.8015 = 0.3320 + 0.4675 + 0.0020, time: 21.630679]
2023-05-11 15:47:19.339: epoch 27:	0.02930901  	0.07940982  	0.06560843  
2023-05-11 15:47:19.339: Find a better model.
2023-05-11 15:47:40.908: [iter 28 : loss : 0.7820 = 0.3007 + 0.4790 + 0.0023, time: 21.565427]
2023-05-11 15:47:41.187: epoch 28:	0.02936824  	0.07999243  	0.06576245  
2023-05-11 15:47:41.187: Find a better model.
2023-05-11 15:48:02.840: [iter 29 : loss : 0.7444 = 0.2736 + 0.4681 + 0.0026, time: 21.649676]
2023-05-11 15:48:03.117: epoch 29:	0.02951630  	0.08068195  	0.06605963  
2023-05-11 15:48:03.117: Find a better model.
2023-05-11 15:48:24.698: [iter 30 : loss : 0.7321 = 0.2506 + 0.4786 + 0.0029, time: 21.577860]
2023-05-11 15:48:24.976: epoch 30:	0.02969399  	0.08136764  	0.06637609  
2023-05-11 15:48:24.976: Find a better model.
2023-05-11 15:48:46.623: [iter 31 : loss : 0.7019 = 0.2313 + 0.4675 + 0.0031, time: 21.642625]
2023-05-11 15:48:46.905: epoch 31:	0.02985685  	0.08163474  	0.06670567  
2023-05-11 15:48:46.905: Find a better model.
2023-05-11 15:49:08.477: [iter 32 : loss : 0.6956 = 0.2145 + 0.4777 + 0.0034, time: 21.567868]
2023-05-11 15:49:08.754: epoch 32:	0.02996791  	0.08208685  	0.06709760  
2023-05-11 15:49:08.754: Find a better model.
2023-05-11 15:49:30.409: [iter 33 : loss : 0.6704 = 0.2007 + 0.4661 + 0.0036, time: 21.650990]
2023-05-11 15:49:30.684: epoch 33:	0.02996790  	0.08225749  	0.06728796  
2023-05-11 15:49:30.684: Find a better model.
2023-05-11 15:49:52.257: [iter 34 : loss : 0.6677 = 0.1875 + 0.4763 + 0.0038, time: 21.568884]
2023-05-11 15:49:52.538: epoch 34:	0.03005676  	0.08256653  	0.06737502  
2023-05-11 15:49:52.538: Find a better model.
2023-05-11 15:50:14.200: [iter 35 : loss : 0.6447 = 0.1758 + 0.4649 + 0.0040, time: 21.658090]
2023-05-11 15:50:14.477: epoch 35:	0.03022702  	0.08263508  	0.06765451  
2023-05-11 15:50:14.477: Find a better model.
2023-05-11 15:50:36.058: [iter 36 : loss : 0.6452 = 0.1661 + 0.4749 + 0.0042, time: 21.577035]
2023-05-11 15:50:36.336: epoch 36:	0.03019000  	0.08260476  	0.06762127  
2023-05-11 15:50:58.001: [iter 37 : loss : 0.6254 = 0.1574 + 0.4635 + 0.0044, time: 21.661547]
2023-05-11 15:50:58.278: epoch 37:	0.03018259  	0.08276164  	0.06760444  
2023-05-11 15:50:58.278: Find a better model.
2023-05-11 15:51:19.841: [iter 38 : loss : 0.6274 = 0.1493 + 0.4735 + 0.0046, time: 21.558937]
2023-05-11 15:51:20.118: epoch 38:	0.03021960  	0.08283812  	0.06776504  
2023-05-11 15:51:20.118: Find a better model.
2023-05-11 15:51:41.791: [iter 39 : loss : 0.6094 = 0.1424 + 0.4622 + 0.0048, time: 21.669567]
2023-05-11 15:51:42.069: epoch 39:	0.03013076  	0.08267068  	0.06782287  
2023-05-11 15:52:03.626: [iter 40 : loss : 0.6128 = 0.1356 + 0.4721 + 0.0050, time: 21.553930]
2023-05-11 15:52:03.902: epoch 40:	0.03006414  	0.08257402  	0.06777302  
2023-05-11 15:52:25.559: [iter 41 : loss : 0.5955 = 0.1293 + 0.4611 + 0.0052, time: 21.652634]
2023-05-11 15:52:25.836: epoch 41:	0.03009376  	0.08237279  	0.06773211  
2023-05-11 15:52:47.414: [iter 42 : loss : 0.6006 = 0.1239 + 0.4714 + 0.0054, time: 21.573864]
2023-05-11 15:52:47.691: epoch 42:	0.02999752  	0.08235676  	0.06772354  
2023-05-11 15:53:09.353: [iter 43 : loss : 0.5842 = 0.1188 + 0.4599 + 0.0055, time: 21.658611]
2023-05-11 15:53:09.629: epoch 43:	0.03001233  	0.08211001  	0.06764305  
2023-05-11 15:53:31.193: [iter 44 : loss : 0.5907 = 0.1147 + 0.4703 + 0.0057, time: 21.559892]
2023-05-11 15:53:31.476: epoch 44:	0.03004934  	0.08228487  	0.06763707  
2023-05-11 15:53:53.323: [iter 45 : loss : 0.5750 = 0.1101 + 0.4590 + 0.0059, time: 21.844911]
2023-05-11 15:53:53.600: epoch 45:	0.02999752  	0.08207534  	0.06757823  
2023-05-11 15:54:15.177: [iter 46 : loss : 0.5815 = 0.1061 + 0.4693 + 0.0060, time: 21.572909]
2023-05-11 15:54:15.453: epoch 46:	0.02983465  	0.08154553  	0.06724069  
2023-05-11 15:54:37.150: [iter 47 : loss : 0.5667 = 0.1023 + 0.4582 + 0.0062, time: 21.693522]
2023-05-11 15:54:37.427: epoch 47:	0.02972359  	0.08130357  	0.06685594  
2023-05-11 15:54:59.159: [iter 48 : loss : 0.5745 = 0.0993 + 0.4689 + 0.0063, time: 21.728300]
2023-05-11 15:54:59.434: epoch 48:	0.02970138  	0.08115969  	0.06679401  
2023-05-11 15:55:21.103: [iter 49 : loss : 0.5600 = 0.0959 + 0.4575 + 0.0065, time: 21.663567]
2023-05-11 15:55:21.379: epoch 49:	0.02959033  	0.08107800  	0.06660857  
2023-05-11 15:55:42.775: [iter 50 : loss : 0.5678 = 0.0933 + 0.4679 + 0.0066, time: 21.392585]
2023-05-11 15:55:43.050: epoch 50:	0.02959773  	0.08068937  	0.06645796  
2023-05-11 15:56:04.711: [iter 51 : loss : 0.5539 = 0.0903 + 0.4568 + 0.0068, time: 21.657108]
2023-05-11 15:56:04.988: epoch 51:	0.02964215  	0.08081344  	0.06655990  
2023-05-11 15:56:26.539: [iter 52 : loss : 0.5613 = 0.0871 + 0.4673 + 0.0069, time: 21.547975]
2023-05-11 15:56:26.815: epoch 52:	0.02947929  	0.08039933  	0.06638733  
2023-05-11 15:56:48.294: [iter 53 : loss : 0.5492 = 0.0860 + 0.4562 + 0.0070, time: 21.475450]
2023-05-11 15:56:48.570: epoch 53:	0.02943485  	0.08038975  	0.06633388  
2023-05-11 15:57:10.297: [iter 54 : loss : 0.5572 = 0.0832 + 0.4668 + 0.0072, time: 21.721945]
2023-05-11 15:57:10.570: epoch 54:	0.02928679  	0.07978687  	0.06594261  
2023-05-11 15:57:32.253: [iter 55 : loss : 0.5436 = 0.0807 + 0.4557 + 0.0073, time: 21.677047]
2023-05-11 15:57:32.531: epoch 55:	0.02927938  	0.07977962  	0.06590614  
2023-05-11 15:57:54.090: [iter 56 : loss : 0.5519 = 0.0782 + 0.4663 + 0.0074, time: 21.555952]
2023-05-11 15:57:54.365: epoch 56:	0.02916094  	0.07929246  	0.06574891  
2023-05-11 15:58:16.050: [iter 57 : loss : 0.5390 = 0.0763 + 0.4552 + 0.0075, time: 21.681519]
2023-05-11 15:58:16.327: epoch 57:	0.02913133  	0.07936123  	0.06578866  
2023-05-11 15:58:37.895: [iter 58 : loss : 0.5483 = 0.0746 + 0.4660 + 0.0077, time: 21.563930]
2023-05-11 15:58:38.171: epoch 58:	0.02916094  	0.07930815  	0.06551202  
2023-05-11 15:58:59.816: [iter 59 : loss : 0.5352 = 0.0726 + 0.4548 + 0.0078, time: 21.641621]
2023-05-11 15:59:00.096: epoch 59:	0.02902768  	0.07904512  	0.06531200  
2023-05-11 15:59:21.695: [iter 60 : loss : 0.5449 = 0.0713 + 0.4657 + 0.0079, time: 21.594807]
2023-05-11 15:59:21.972: epoch 60:	0.02888702  	0.07824162  	0.06506512  
2023-05-11 15:59:43.591: [iter 61 : loss : 0.5321 = 0.0696 + 0.4545 + 0.0080, time: 21.615753]
2023-05-11 15:59:43.866: epoch 61:	0.02886481  	0.07819071  	0.06496689  
2023-05-11 16:00:05.441: [iter 62 : loss : 0.5418 = 0.0684 + 0.4653 + 0.0081, time: 21.571878]
2023-05-11 16:00:05.718: epoch 62:	0.02872415  	0.07771129  	0.06478804  
2023-05-11 16:00:27.409: [iter 63 : loss : 0.5294 = 0.0671 + 0.4540 + 0.0083, time: 21.686533]
2023-05-11 16:00:27.685: epoch 63:	0.02871675  	0.07756257  	0.06463309  
2023-05-11 16:00:27.685: Early stopping is trigger at epoch: 63
2023-05-11 16:00:27.685: best_result@epoch 38:

2023-05-11 16:00:27.685: 		0.0302      	0.0828      	0.0678      
2023-05-11 16:07:31.339: my pid: 11120
2023-05-11 16:07:31.340: model: model.general_recommender.SGL
2023-05-11 16:07:31.340: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 16:07:31.340: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 16:07:34.907: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 16:07:56.874: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.966950]
2023-05-11 16:07:57.168: epoch 1:	0.00095499  	0.00238204  	0.00171495  
2023-05-11 16:07:57.168: Find a better model.
2023-05-11 16:08:19.711: [iter 2 : loss : 1.1364 = 0.6930 + 0.4433 + 0.0000, time: 22.539037]
2023-05-11 16:08:20.018: epoch 2:	0.00130293  	0.00247725  	0.00213764  
2023-05-11 16:08:20.018: Find a better model.
2023-05-11 16:08:42.851: [iter 3 : loss : 1.1345 = 0.6929 + 0.4415 + 0.0000, time: 22.829613]
2023-05-11 16:08:43.154: epoch 3:	0.00168049  	0.00390929  	0.00308177  
2023-05-11 16:08:43.154: Find a better model.
2023-05-11 16:09:05.712: [iter 4 : loss : 1.1370 = 0.6929 + 0.4441 + 0.0000, time: 22.555129]
2023-05-11 16:09:06.018: epoch 4:	0.00181374  	0.00373582  	0.00301812  
2023-05-11 16:09:28.639: [iter 5 : loss : 1.1350 = 0.6928 + 0.4423 + 0.0000, time: 22.617681]
2023-05-11 16:09:28.943: epoch 5:	0.00213207  	0.00473802  	0.00356923  
2023-05-11 16:09:28.943: Find a better model.
2023-05-11 16:09:51.493: [iter 6 : loss : 1.1375 = 0.6926 + 0.4449 + 0.0000, time: 22.546716]
2023-05-11 16:09:51.794: epoch 6:	0.00230234  	0.00445473  	0.00355272  
2023-05-11 16:10:14.601: [iter 7 : loss : 1.1354 = 0.6925 + 0.4429 + 0.0000, time: 22.804089]
2023-05-11 16:10:14.906: epoch 7:	0.00299822  	0.00638054  	0.00503743  
2023-05-11 16:10:14.906: Find a better model.
2023-05-11 16:10:37.447: [iter 8 : loss : 1.1381 = 0.6923 + 0.4458 + 0.0000, time: 22.537624]
2023-05-11 16:10:37.754: epoch 8:	0.00319070  	0.00662575  	0.00519836  
2023-05-11 16:10:37.754: Find a better model.
2023-05-11 16:11:00.193: [iter 9 : loss : 1.1356 = 0.6920 + 0.4435 + 0.0000, time: 22.436110]
2023-05-11 16:11:00.491: epoch 9:	0.00412348  	0.00971563  	0.00743607  
2023-05-11 16:11:00.491: Find a better model.
2023-05-11 16:11:22.883: [iter 10 : loss : 1.1384 = 0.6917 + 0.4467 + 0.0000, time: 22.387663]
2023-05-11 16:11:23.202: epoch 10:	0.00442700  	0.01012240  	0.00784219  
2023-05-11 16:11:23.202: Find a better model.
2023-05-11 16:11:45.561: [iter 11 : loss : 1.1354 = 0.6912 + 0.4442 + 0.0000, time: 22.355241]
2023-05-11 16:11:45.866: epoch 11:	0.00589278  	0.01356597  	0.01097730  
2023-05-11 16:11:45.866: Find a better model.
2023-05-11 16:12:08.249: [iter 12 : loss : 1.1384 = 0.6904 + 0.4479 + 0.0000, time: 22.377129]
2023-05-11 16:12:08.549: epoch 12:	0.00631475  	0.01496671  	0.01229091  
2023-05-11 16:12:08.549: Find a better model.
2023-05-11 16:12:30.972: [iter 13 : loss : 1.1342 = 0.6892 + 0.4450 + 0.0000, time: 22.419148]
2023-05-11 16:12:31.266: epoch 13:	0.00786937  	0.01941594  	0.01643008  
2023-05-11 16:12:31.266: Find a better model.
2023-05-11 16:12:53.618: [iter 14 : loss : 1.1371 = 0.6875 + 0.4496 + 0.0000, time: 22.347234]
2023-05-11 16:12:53.906: epoch 14:	0.00892059  	0.02190926  	0.01913102  
2023-05-11 16:12:53.907: Find a better model.
2023-05-11 16:13:16.166: [iter 15 : loss : 1.1312 = 0.6852 + 0.4460 + 0.0001, time: 22.255451]
2023-05-11 16:13:16.455: epoch 15:	0.01143764  	0.02969639  	0.02559504  
2023-05-11 16:13:16.455: Find a better model.
2023-05-11 16:13:38.797: [iter 16 : loss : 1.1335 = 0.6817 + 0.4517 + 0.0001, time: 22.337252]
2023-05-11 16:13:39.094: epoch 16:	0.01350312  	0.03389250  	0.03032402  
2023-05-11 16:13:39.094: Find a better model.
2023-05-11 16:14:01.363: [iter 17 : loss : 1.1236 = 0.6764 + 0.4471 + 0.0001, time: 22.264600]
2023-05-11 16:14:01.650: epoch 17:	0.01678273  	0.04263547  	0.03757069  
2023-05-11 16:14:01.651: Find a better model.
2023-05-11 16:14:23.824: [iter 18 : loss : 1.1220 = 0.6674 + 0.4544 + 0.0002, time: 22.168983]
2023-05-11 16:14:24.114: epoch 18:	0.01981062  	0.04995922  	0.04418156  
2023-05-11 16:14:24.114: Find a better model.
2023-05-11 16:14:46.319: [iter 19 : loss : 1.1021 = 0.6531 + 0.4488 + 0.0002, time: 22.199787]
2023-05-11 16:14:46.607: epoch 19:	0.02258685  	0.05819843  	0.05055618  
2023-05-11 16:14:46.607: Find a better model.
2023-05-11 16:15:08.789: [iter 20 : loss : 1.0898 = 0.6310 + 0.4584 + 0.0003, time: 22.177064]
2023-05-11 16:15:09.071: epoch 20:	0.02448208  	0.06400394  	0.05544102  
2023-05-11 16:15:09.071: Find a better model.
2023-05-11 16:15:31.319: [iter 21 : loss : 1.0519 = 0.5992 + 0.4522 + 0.0005, time: 22.242760]
2023-05-11 16:15:31.600: epoch 21:	0.02617741  	0.06874499  	0.05920790  
2023-05-11 16:15:31.601: Find a better model.
2023-05-11 16:15:53.571: [iter 22 : loss : 1.0228 = 0.5581 + 0.4641 + 0.0007, time: 21.967225]
2023-05-11 16:15:53.852: epoch 22:	0.02711764  	0.07168986  	0.06089562  
2023-05-11 16:15:53.852: Find a better model.
2023-05-11 16:16:16.260: [iter 23 : loss : 0.9687 = 0.5099 + 0.4579 + 0.0009, time: 22.404245]
2023-05-11 16:16:16.542: epoch 23:	0.02776913  	0.07388137  	0.06228618  
2023-05-11 16:16:16.542: Find a better model.
2023-05-11 16:16:38.760: [iter 24 : loss : 0.9319 = 0.4598 + 0.4709 + 0.0012, time: 22.215840]
2023-05-11 16:16:39.045: epoch 24:	0.02813930  	0.07526506  	0.06312968  
2023-05-11 16:16:39.045: Find a better model.
2023-05-11 16:17:01.321: [iter 25 : loss : 0.8762 = 0.4111 + 0.4637 + 0.0015, time: 22.271463]
2023-05-11 16:17:01.603: epoch 25:	0.02864272  	0.07729954  	0.06422500  
2023-05-11 16:17:01.603: Find a better model.
2023-05-11 16:17:23.929: [iter 26 : loss : 0.8460 = 0.3685 + 0.4758 + 0.0018, time: 22.322422]
2023-05-11 16:17:24.222: epoch 26:	0.02896846  	0.07848538  	0.06468632  
2023-05-11 16:17:24.222: Find a better model.
2023-05-11 16:17:46.826: [iter 27 : loss : 0.8002 = 0.3310 + 0.4671 + 0.0020, time: 22.598552]
2023-05-11 16:17:47.105: epoch 27:	0.02913875  	0.07935114  	0.06506309  
2023-05-11 16:17:47.105: Find a better model.
2023-05-11 16:18:09.322: [iter 28 : loss : 0.7797 = 0.2998 + 0.4776 + 0.0023, time: 22.212675]
2023-05-11 16:18:09.604: epoch 28:	0.02919798  	0.07996221  	0.06540640  
2023-05-11 16:18:09.604: Find a better model.
2023-05-11 16:18:32.216: [iter 29 : loss : 0.7434 = 0.2731 + 0.4677 + 0.0026, time: 22.608581]
2023-05-11 16:18:32.497: epoch 29:	0.02939787  	0.08054177  	0.06572371  
2023-05-11 16:18:32.497: Find a better model.
2023-05-11 16:18:54.901: [iter 30 : loss : 0.7304 = 0.2499 + 0.4776 + 0.0029, time: 22.398261]
2023-05-11 16:18:55.178: epoch 30:	0.02939047  	0.08026344  	0.06591291  
2023-05-11 16:19:17.598: [iter 31 : loss : 0.7005 = 0.2304 + 0.4670 + 0.0031, time: 22.417612]
2023-05-11 16:19:17.877: epoch 31:	0.02951632  	0.08120316  	0.06644621  
2023-05-11 16:19:17.877: Find a better model.
2023-05-11 16:19:40.102: [iter 32 : loss : 0.6938 = 0.2139 + 0.4765 + 0.0034, time: 22.219399]
2023-05-11 16:19:40.382: epoch 32:	0.02956815  	0.08147740  	0.06669810  
2023-05-11 16:19:40.382: Find a better model.
2023-05-11 16:20:03.015: [iter 33 : loss : 0.6696 = 0.2002 + 0.4659 + 0.0036, time: 22.629562]
2023-05-11 16:20:03.295: epoch 33:	0.02967920  	0.08188707  	0.06709515  
2023-05-11 16:20:03.295: Find a better model.
2023-05-11 16:20:25.652: [iter 34 : loss : 0.6664 = 0.1873 + 0.4752 + 0.0038, time: 22.352352]
2023-05-11 16:20:25.931: epoch 34:	0.02973841  	0.08192959  	0.06700806  
2023-05-11 16:20:25.931: Find a better model.
2023-05-11 16:20:48.617: [iter 35 : loss : 0.6440 = 0.1755 + 0.4645 + 0.0040, time: 22.681731]
2023-05-11 16:20:48.896: epoch 35:	0.02964217  	0.08162208  	0.06705648  
2023-05-11 16:21:11.473: [iter 36 : loss : 0.6437 = 0.1659 + 0.4736 + 0.0042, time: 22.573837]
2023-05-11 16:21:11.751: epoch 36:	0.02975322  	0.08190832  	0.06721456  
2023-05-11 16:21:34.175: [iter 37 : loss : 0.6249 = 0.1573 + 0.4632 + 0.0044, time: 22.419990]
2023-05-11 16:21:34.456: epoch 37:	0.02974583  	0.08205184  	0.06739006  
2023-05-11 16:21:34.456: Find a better model.
2023-05-11 16:21:56.880: [iter 38 : loss : 0.6262 = 0.1491 + 0.4724 + 0.0046, time: 22.419384]
2023-05-11 16:21:57.157: epoch 38:	0.02970880  	0.08192918  	0.06728712  
2023-05-11 16:22:19.745: [iter 39 : loss : 0.6090 = 0.1425 + 0.4617 + 0.0048, time: 22.583789]
2023-05-11 16:22:20.022: epoch 39:	0.02970139  	0.08185808  	0.06732444  
2023-05-11 16:22:42.248: [iter 40 : loss : 0.6116 = 0.1354 + 0.4712 + 0.0050, time: 22.222827]
2023-05-11 16:22:42.528: epoch 40:	0.02973100  	0.08174662  	0.06728137  
2023-05-11 16:23:05.158: [iter 41 : loss : 0.5949 = 0.1289 + 0.4607 + 0.0052, time: 22.625663]
2023-05-11 16:23:05.438: epoch 41:	0.02985685  	0.08185935  	0.06741370  
2023-05-11 16:23:27.834: [iter 42 : loss : 0.5992 = 0.1237 + 0.4702 + 0.0054, time: 22.391996]
2023-05-11 16:23:28.114: epoch 42:	0.02965696  	0.08124956  	0.06726645  
2023-05-11 16:23:50.507: [iter 43 : loss : 0.5837 = 0.1185 + 0.4597 + 0.0055, time: 22.388486]
2023-05-11 16:23:50.790: epoch 43:	0.02964215  	0.08113212  	0.06714590  
2023-05-11 16:24:13.206: [iter 44 : loss : 0.5897 = 0.1148 + 0.4692 + 0.0057, time: 22.412013]
2023-05-11 16:24:13.490: epoch 44:	0.02957552  	0.08100396  	0.06701459  
2023-05-11 16:24:36.104: [iter 45 : loss : 0.5744 = 0.1097 + 0.4588 + 0.0059, time: 22.609976]
2023-05-11 16:24:36.384: epoch 45:	0.02964216  	0.08081461  	0.06704294  
2023-05-11 16:24:58.759: [iter 46 : loss : 0.5807 = 0.1062 + 0.4685 + 0.0060, time: 22.371328]
2023-05-11 16:24:59.042: epoch 46:	0.02956071  	0.08041706  	0.06700369  
2023-05-11 16:25:24.070: [iter 47 : loss : 0.5664 = 0.1023 + 0.4580 + 0.0062, time: 25.023769]
2023-05-11 16:25:24.337: epoch 47:	0.02951629  	0.08023105  	0.06686499  
2023-05-11 16:25:46.549: [iter 48 : loss : 0.5734 = 0.0993 + 0.4678 + 0.0063, time: 22.208534]
2023-05-11 16:25:46.825: epoch 48:	0.02948668  	0.08017052  	0.06680469  
2023-05-11 16:26:09.079: [iter 49 : loss : 0.5595 = 0.0958 + 0.4572 + 0.0065, time: 22.249886]
2023-05-11 16:26:09.342: epoch 49:	0.02946447  	0.08016703  	0.06674981  
2023-05-11 16:26:31.544: [iter 50 : loss : 0.5670 = 0.0935 + 0.4669 + 0.0066, time: 22.196233]
2023-05-11 16:26:31.822: epoch 50:	0.02935342  	0.07971890  	0.06648402  
2023-05-11 16:26:54.251: [iter 51 : loss : 0.5535 = 0.0901 + 0.4567 + 0.0068, time: 22.424059]
2023-05-11 16:26:54.516: epoch 51:	0.02936822  	0.07979042  	0.06652667  
2023-05-11 16:27:16.521: [iter 52 : loss : 0.5607 = 0.0874 + 0.4664 + 0.0069, time: 21.999570]
2023-05-11 16:27:16.795: epoch 52:	0.02930160  	0.07947184  	0.06631536  
2023-05-11 16:27:39.252: [iter 53 : loss : 0.5490 = 0.0858 + 0.4561 + 0.0070, time: 22.454563]
2023-05-11 16:27:39.517: epoch 53:	0.02928680  	0.07903302  	0.06621081  
2023-05-11 16:28:01.535: [iter 54 : loss : 0.5562 = 0.0834 + 0.4656 + 0.0072, time: 22.014409]
2023-05-11 16:28:01.810: epoch 54:	0.02922017  	0.07907649  	0.06618610  
2023-05-11 16:28:23.865: [iter 55 : loss : 0.5434 = 0.0806 + 0.4555 + 0.0073, time: 22.051315]
2023-05-11 16:28:24.135: epoch 55:	0.02922016  	0.07862975  	0.06601426  
2023-05-11 16:28:46.286: [iter 56 : loss : 0.5508 = 0.0780 + 0.4654 + 0.0074, time: 22.147696]
2023-05-11 16:28:46.550: epoch 56:	0.02916833  	0.07882830  	0.06587344  
2023-05-11 16:29:08.841: [iter 57 : loss : 0.5385 = 0.0760 + 0.4550 + 0.0076, time: 22.287166]
2023-05-11 16:29:09.113: epoch 57:	0.02904989  	0.07823090  	0.06557003  
2023-05-11 16:29:31.103: [iter 58 : loss : 0.5475 = 0.0748 + 0.4650 + 0.0077, time: 21.985763]
2023-05-11 16:29:31.368: epoch 58:	0.02899066  	0.07781336  	0.06536467  
2023-05-11 16:29:53.591: [iter 59 : loss : 0.5354 = 0.0729 + 0.4546 + 0.0078, time: 22.219615]
2023-05-11 16:29:53.859: epoch 59:	0.02898325  	0.07748477  	0.06523102  
2023-05-11 16:30:16.098: [iter 60 : loss : 0.5438 = 0.0714 + 0.4645 + 0.0079, time: 22.234435]
2023-05-11 16:30:16.360: epoch 60:	0.02899064  	0.07750114  	0.06507099  
2023-05-11 16:30:38.594: [iter 61 : loss : 0.5323 = 0.0700 + 0.4542 + 0.0080, time: 22.229221]
2023-05-11 16:30:38.861: epoch 61:	0.02892402  	0.07721054  	0.06495357  
2023-05-11 16:31:01.071: [iter 62 : loss : 0.5413 = 0.0687 + 0.4644 + 0.0082, time: 22.206625]
2023-05-11 16:31:01.337: epoch 62:	0.02886479  	0.07703748  	0.06492934  
2023-05-11 16:31:01.337: Early stopping is trigger at epoch: 62
2023-05-11 16:31:01.337: best_result@epoch 37:

2023-05-11 16:31:01.337: 		0.0297      	0.0821      	0.0674      
2023-05-11 16:32:29.377: my pid: 10952
2023-05-11 16:32:29.378: model: model.general_recommender.SGL
2023-05-11 16:32:29.378: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 16:32:29.378: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 16:32:32.721: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 16:32:54.801: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.079197]
2023-05-11 16:32:55.081: epoch 1:	0.00103642  	0.00236408  	0.00172585  
2023-05-11 16:32:55.081: Find a better model.
2023-05-11 16:33:17.237: [iter 2 : loss : 1.1362 = 0.6930 + 0.4431 + 0.0000, time: 22.151819]
2023-05-11 16:33:17.525: epoch 2:	0.00130293  	0.00277614  	0.00213845  
2023-05-11 16:33:17.525: Find a better model.
2023-05-11 16:33:39.973: [iter 3 : loss : 1.1343 = 0.6929 + 0.4414 + 0.0000, time: 22.444129]
2023-05-11 16:33:40.276: epoch 3:	0.00162867  	0.00358584  	0.00269094  
2023-05-11 16:33:40.277: Find a better model.
2023-05-11 16:34:02.855: [iter 4 : loss : 1.1366 = 0.6929 + 0.4437 + 0.0000, time: 22.575171]
2023-05-11 16:34:03.140: epoch 4:	0.00185816  	0.00395508  	0.00306895  
2023-05-11 16:34:03.140: Find a better model.
2023-05-11 16:34:25.563: [iter 5 : loss : 1.1348 = 0.6928 + 0.4421 + 0.0000, time: 22.418731]
2023-05-11 16:34:25.869: epoch 5:	0.00232455  	0.00524387  	0.00404855  
2023-05-11 16:34:25.869: Find a better model.
2023-05-11 16:34:48.201: [iter 6 : loss : 1.1372 = 0.6927 + 0.4445 + 0.0000, time: 22.329336]
2023-05-11 16:34:48.484: epoch 6:	0.00224312  	0.00425585  	0.00374776  
2023-05-11 16:35:10.926: [iter 7 : loss : 1.1352 = 0.6925 + 0.4427 + 0.0000, time: 22.438212]
2023-05-11 16:35:11.227: epoch 7:	0.00329435  	0.00802826  	0.00606187  
2023-05-11 16:35:11.227: Find a better model.
2023-05-11 16:35:33.386: [iter 8 : loss : 1.1375 = 0.6923 + 0.4452 + 0.0000, time: 22.154174]
2023-05-11 16:35:33.668: epoch 8:	0.00344981  	0.00724081  	0.00578888  
2023-05-11 16:35:56.133: [iter 9 : loss : 1.1354 = 0.6920 + 0.4433 + 0.0000, time: 22.461255]
2023-05-11 16:35:56.412: epoch 9:	0.00427154  	0.01000286  	0.00793023  
2023-05-11 16:35:56.412: Find a better model.
2023-05-11 16:36:18.759: [iter 10 : loss : 1.1379 = 0.6917 + 0.4462 + 0.0000, time: 22.342682]
2023-05-11 16:36:19.041: epoch 10:	0.00423452  	0.00902421  	0.00754196  
2023-05-11 16:36:41.300: [iter 11 : loss : 1.1352 = 0.6912 + 0.4439 + 0.0000, time: 22.254416]
2023-05-11 16:36:41.592: epoch 11:	0.00559667  	0.01259225  	0.01032069  
2023-05-11 16:36:41.593: Find a better model.
2023-05-11 16:37:03.973: [iter 12 : loss : 1.1378 = 0.6906 + 0.4472 + 0.0000, time: 22.376420]
2023-05-11 16:37:04.265: epoch 12:	0.00576693  	0.01386904  	0.01136361  
2023-05-11 16:37:04.265: Find a better model.
2023-05-11 16:37:26.874: [iter 13 : loss : 1.1343 = 0.6895 + 0.4448 + 0.0000, time: 22.605295]
2023-05-11 16:37:27.173: epoch 13:	0.00727714  	0.01801773  	0.01462791  
2023-05-11 16:37:27.173: Find a better model.
2023-05-11 16:37:49.551: [iter 14 : loss : 1.1365 = 0.6878 + 0.4487 + 0.0000, time: 22.373601]
2023-05-11 16:37:49.840: epoch 14:	0.00818030  	0.01977598  	0.01655227  
2023-05-11 16:37:49.840: Find a better model.
2023-05-11 16:38:12.284: [iter 15 : loss : 1.1312 = 0.6854 + 0.4457 + 0.0001, time: 22.440071]
2023-05-11 16:38:12.570: epoch 15:	0.01093424  	0.02689578  	0.02294891  
2023-05-11 16:38:12.570: Find a better model.
2023-05-11 16:38:34.898: [iter 16 : loss : 1.1329 = 0.6820 + 0.4509 + 0.0001, time: 22.322307]
2023-05-11 16:38:37.726: epoch 16:	0.01312556  	0.03252403  	0.02805119  
2023-05-11 16:38:37.726: Find a better model.
2023-05-11 16:38:59.835: [iter 17 : loss : 1.1236 = 0.6767 + 0.4467 + 0.0001, time: 22.105116]
2023-05-11 16:39:00.111: epoch 17:	0.01636817  	0.04152356  	0.03621549  
2023-05-11 16:39:00.111: Find a better model.
2023-05-11 16:39:21.945: [iter 18 : loss : 1.1214 = 0.6678 + 0.4535 + 0.0002, time: 21.829426]
2023-05-11 16:39:22.237: epoch 18:	0.01901849  	0.04845843  	0.04220549  
2023-05-11 16:39:22.237: Find a better model.
2023-05-11 16:39:44.241: [iter 19 : loss : 1.1023 = 0.6535 + 0.4485 + 0.0002, time: 21.998246]
2023-05-11 16:39:44.512: epoch 19:	0.02190576  	0.05688900  	0.04887405  
2023-05-11 16:39:44.513: Find a better model.
2023-05-11 16:40:06.520: [iter 20 : loss : 1.0892 = 0.6315 + 0.4573 + 0.0003, time: 22.004479]
2023-05-11 16:40:06.811: epoch 20:	0.02419335  	0.06271649  	0.05372635  
2023-05-11 16:40:06.811: Find a better model.
2023-05-11 16:40:28.813: [iter 21 : loss : 1.0519 = 0.5995 + 0.4519 + 0.0005, time: 21.998300]
2023-05-11 16:40:29.081: epoch 21:	0.02608860  	0.06769215  	0.05764252  
2023-05-11 16:40:29.081: Find a better model.
2023-05-11 16:40:50.900: [iter 22 : loss : 1.0216 = 0.5582 + 0.4628 + 0.0007, time: 21.815579]
2023-05-11 16:40:51.183: epoch 22:	0.02709546  	0.07116959  	0.05982855  
2023-05-11 16:40:51.183: Find a better model.
2023-05-11 16:41:13.233: [iter 23 : loss : 0.9681 = 0.5098 + 0.4573 + 0.0009, time: 22.046201]
2023-05-11 16:41:13.500: epoch 23:	0.02795424  	0.07396272  	0.06168069  
2023-05-11 16:41:13.500: Find a better model.
2023-05-11 16:41:35.504: [iter 24 : loss : 0.9302 = 0.4597 + 0.4693 + 0.0012, time: 22.000144]
2023-05-11 16:41:35.789: epoch 24:	0.02833922  	0.07549102  	0.06217772  
2023-05-11 16:41:35.789: Find a better model.
2023-05-11 16:41:57.837: [iter 25 : loss : 0.8753 = 0.4108 + 0.4630 + 0.0015, time: 22.043639]
2023-05-11 16:41:58.118: epoch 25:	0.02877600  	0.07702310  	0.06322812  
2023-05-11 16:41:58.118: Find a better model.
2023-05-11 16:42:20.059: [iter 26 : loss : 0.8441 = 0.3680 + 0.4743 + 0.0018, time: 21.937430]
2023-05-11 16:42:20.358: epoch 26:	0.02900550  	0.07800315  	0.06387869  
2023-05-11 16:42:20.358: Find a better model.
2023-05-11 16:42:42.167: [iter 27 : loss : 0.7988 = 0.3303 + 0.4664 + 0.0020, time: 21.805853]
2023-05-11 16:42:42.426: epoch 27:	0.02921280  	0.07904285  	0.06423128  
2023-05-11 16:42:42.427: Find a better model.
2023-05-11 16:43:04.443: [iter 28 : loss : 0.7772 = 0.2987 + 0.4762 + 0.0023, time: 22.013344]
2023-05-11 16:43:04.730: epoch 28:	0.02923501  	0.07921141  	0.06425350  
2023-05-11 16:43:04.730: Find a better model.
2023-05-11 16:43:27.134: [iter 29 : loss : 0.7421 = 0.2724 + 0.4671 + 0.0026, time: 22.400036]
2023-05-11 16:43:27.413: epoch 29:	0.02933865  	0.07972186  	0.06444162  
2023-05-11 16:43:27.413: Find a better model.
2023-05-11 16:43:52.233: [iter 30 : loss : 0.7282 = 0.2492 + 0.4761 + 0.0029, time: 24.816864]
2023-05-11 16:43:52.507: epoch 30:	0.02939048  	0.08035551  	0.06449668  
2023-05-11 16:43:52.507: Find a better model.
2023-05-11 16:44:14.584: [iter 31 : loss : 0.6993 = 0.2296 + 0.4666 + 0.0031, time: 22.073941]
2023-05-11 16:44:14.847: epoch 31:	0.02944972  	0.08071475  	0.06486794  
2023-05-11 16:44:14.847: Find a better model.
2023-05-11 16:44:36.821: [iter 32 : loss : 0.6918 = 0.2134 + 0.4751 + 0.0034, time: 21.969836]
2023-05-11 16:44:37.084: epoch 32:	0.02946452  	0.08072191  	0.06502886  
2023-05-11 16:44:37.085: Find a better model.
2023-05-11 16:44:59.134: [iter 33 : loss : 0.6683 = 0.1993 + 0.4654 + 0.0036, time: 22.044728]
2023-05-11 16:44:59.434: epoch 33:	0.02954595  	0.08137158  	0.06540237  
2023-05-11 16:44:59.434: Find a better model.
2023-05-11 16:45:21.432: [iter 34 : loss : 0.6644 = 0.1868 + 0.4738 + 0.0038, time: 21.994932]
2023-05-11 16:45:21.694: epoch 34:	0.02939789  	0.08086667  	0.06530990  
2023-05-11 16:45:43.944: [iter 35 : loss : 0.6428 = 0.1747 + 0.4640 + 0.0040, time: 22.246169]
2023-05-11 16:45:44.224: epoch 35:	0.02953115  	0.08110203  	0.06549861  
2023-05-11 16:46:06.150: [iter 36 : loss : 0.6417 = 0.1650 + 0.4724 + 0.0042, time: 21.923211]
2023-05-11 16:46:06.408: epoch 36:	0.02957557  	0.08108263  	0.06550188  
2023-05-11 16:46:28.515: [iter 37 : loss : 0.6237 = 0.1566 + 0.4627 + 0.0045, time: 22.103744]
2023-05-11 16:46:28.793: epoch 37:	0.02973104  	0.08119548  	0.06587409  
2023-05-11 16:46:50.937: [iter 38 : loss : 0.6240 = 0.1483 + 0.4710 + 0.0046, time: 22.140181]
2023-05-11 16:46:51.198: epoch 38:	0.02970142  	0.08130104  	0.06589352  
2023-05-11 16:47:13.320: [iter 39 : loss : 0.6081 = 0.1419 + 0.4614 + 0.0048, time: 22.118966]
2023-05-11 16:47:13.588: epoch 39:	0.02980507  	0.08189458  	0.06618419  
2023-05-11 16:47:13.589: Find a better model.
2023-05-11 16:47:35.935: [iter 40 : loss : 0.6098 = 0.1348 + 0.4700 + 0.0050, time: 22.342465]
2023-05-11 16:47:36.213: epoch 40:	0.02981248  	0.08186714  	0.06613284  
2023-05-11 16:47:58.673: [iter 41 : loss : 0.5941 = 0.1287 + 0.4602 + 0.0052, time: 22.451798]
2023-05-11 16:47:58.947: epoch 41:	0.02979767  	0.08169806  	0.06608309  
2023-05-11 16:48:21.352: [iter 42 : loss : 0.5977 = 0.1233 + 0.4690 + 0.0054, time: 22.401212]
2023-05-11 16:48:21.627: epoch 42:	0.02976806  	0.08162565  	0.06604899  
2023-05-11 16:48:44.248: [iter 43 : loss : 0.5832 = 0.1183 + 0.4594 + 0.0055, time: 22.616441]
2023-05-11 16:48:44.524: epoch 43:	0.02981988  	0.08201214  	0.06625055  
2023-05-11 16:48:44.524: Find a better model.
2023-05-11 16:49:07.126: [iter 44 : loss : 0.5881 = 0.1144 + 0.4680 + 0.0057, time: 22.599507]
2023-05-11 16:49:07.404: epoch 44:	0.02977545  	0.08150854  	0.06611989  
2023-05-11 16:49:30.059: [iter 45 : loss : 0.5742 = 0.1099 + 0.4584 + 0.0059, time: 22.651303]
2023-05-11 16:49:30.336: epoch 45:	0.02979025  	0.08151161  	0.06612668  
2023-05-11 16:49:52.887: [iter 46 : loss : 0.5794 = 0.1061 + 0.4672 + 0.0060, time: 22.547587]
2023-05-11 16:49:53.169: epoch 46:	0.02970140  	0.08083242  	0.06573330  
2023-05-11 16:50:16.004: [iter 47 : loss : 0.5661 = 0.1022 + 0.4577 + 0.0062, time: 22.830725]
2023-05-11 16:50:16.283: epoch 47:	0.02967179  	0.08085331  	0.06585047  
2023-05-11 16:50:38.656: [iter 48 : loss : 0.5715 = 0.0988 + 0.4664 + 0.0063, time: 22.369193]
2023-05-11 16:50:38.933: epoch 48:	0.02955334  	0.08050676  	0.06566422  
2023-05-11 16:51:01.602: [iter 49 : loss : 0.5590 = 0.0956 + 0.4570 + 0.0065, time: 22.665259]
2023-05-11 16:51:01.880: epoch 49:	0.02960517  	0.08040517  	0.06553346  
2023-05-11 16:51:24.438: [iter 50 : loss : 0.5654 = 0.0930 + 0.4658 + 0.0066, time: 22.553544]
2023-05-11 16:51:24.717: epoch 50:	0.02956075  	0.08031981  	0.06557108  
2023-05-11 16:51:47.367: [iter 51 : loss : 0.5526 = 0.0897 + 0.4562 + 0.0068, time: 22.646285]
2023-05-11 16:51:47.643: epoch 51:	0.02959036  	0.08046561  	0.06570436  
2023-05-11 16:52:10.244: [iter 52 : loss : 0.5593 = 0.0872 + 0.4652 + 0.0069, time: 22.598038]
2023-05-11 16:52:10.518: epoch 52:	0.02941269  	0.07976483  	0.06547305  
2023-05-11 16:52:33.368: [iter 53 : loss : 0.5483 = 0.0855 + 0.4557 + 0.0070, time: 22.845594]
2023-05-11 16:52:33.645: epoch 53:	0.02933866  	0.07940637  	0.06528834  
2023-05-11 16:52:56.052: [iter 54 : loss : 0.5552 = 0.0834 + 0.4647 + 0.0072, time: 22.403518]
2023-05-11 16:52:56.332: epoch 54:	0.02927202  	0.07924230  	0.06524551  
2023-05-11 16:53:18.959: [iter 55 : loss : 0.5431 = 0.0805 + 0.4552 + 0.0073, time: 22.624318]
2023-05-11 16:53:19.236: epoch 55:	0.02924241  	0.07915998  	0.06517167  
2023-05-11 16:53:41.818: [iter 56 : loss : 0.5500 = 0.0783 + 0.4643 + 0.0074, time: 22.577433]
2023-05-11 16:53:42.094: epoch 56:	0.02919058  	0.07886343  	0.06505937  
2023-05-11 16:54:04.556: [iter 57 : loss : 0.5384 = 0.0761 + 0.4548 + 0.0076, time: 22.459213]
2023-05-11 16:54:04.835: epoch 57:	0.02914617  	0.07844996  	0.06480160  
2023-05-11 16:54:27.190: [iter 58 : loss : 0.5464 = 0.0747 + 0.4640 + 0.0077, time: 22.351859]
2023-05-11 16:54:27.468: epoch 58:	0.02907215  	0.07817437  	0.06471563  
2023-05-11 16:54:50.136: [iter 59 : loss : 0.5352 = 0.0730 + 0.4544 + 0.0078, time: 22.663285]
2023-05-11 16:54:50.410: epoch 59:	0.02902773  	0.07781315  	0.06452349  
2023-05-11 16:55:12.792: [iter 60 : loss : 0.5428 = 0.0714 + 0.4634 + 0.0079, time: 22.378108]
2023-05-11 16:55:13.071: epoch 60:	0.02880562  	0.07687069  	0.06408032  
2023-05-11 16:55:35.704: [iter 61 : loss : 0.5313 = 0.0693 + 0.4539 + 0.0081, time: 22.627208]
2023-05-11 16:55:35.983: epoch 61:	0.02880563  	0.07662568  	0.06396180  
2023-05-11 16:55:58.375: [iter 62 : loss : 0.5396 = 0.0683 + 0.4631 + 0.0082, time: 22.389181]
2023-05-11 16:55:58.651: epoch 62:	0.02881303  	0.07692226  	0.06405904  
2023-05-11 16:56:21.059: [iter 63 : loss : 0.5290 = 0.0670 + 0.4537 + 0.0083, time: 22.403043]
2023-05-11 16:56:21.335: epoch 63:	0.02879823  	0.07678826  	0.06393658  
2023-05-11 16:56:43.749: [iter 64 : loss : 0.5366 = 0.0656 + 0.4626 + 0.0084, time: 22.409914]
2023-05-11 16:56:44.024: epoch 64:	0.02873899  	0.07649353  	0.06381700  
2023-05-11 16:57:06.667: [iter 65 : loss : 0.5258 = 0.0639 + 0.4534 + 0.0085, time: 22.638395]
2023-05-11 16:57:06.945: epoch 65:	0.02867236  	0.07644680  	0.06375021  
2023-05-11 16:57:29.354: [iter 66 : loss : 0.5340 = 0.0629 + 0.4625 + 0.0086, time: 22.405089]
2023-05-11 16:57:29.629: epoch 66:	0.02865016  	0.07661135  	0.06367227  
2023-05-11 16:57:52.341: [iter 67 : loss : 0.5235 = 0.0618 + 0.4530 + 0.0087, time: 22.708022]
2023-05-11 16:57:52.625: epoch 67:	0.02850208  	0.07629583  	0.06348967  
2023-05-11 16:58:15.158: [iter 68 : loss : 0.5318 = 0.0609 + 0.4621 + 0.0088, time: 22.530722]
2023-05-11 16:58:15.434: epoch 68:	0.02833180  	0.07579008  	0.06317414  
2023-05-11 16:58:15.435: Early stopping is trigger at epoch: 68
2023-05-11 16:58:15.435: best_result@epoch 43:

2023-05-11 16:58:15.435: 		0.0298      	0.0820      	0.0663      
2023-05-11 17:20:28.274: my pid: 11980
2023-05-11 17:20:28.274: model: model.general_recommender.SGL
2023-05-11 17:20:28.275: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 17:20:28.275: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 17:20:31.622: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 17:20:52.961: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.339253]
2023-05-11 17:20:53.251: epoch 1:	0.00133254  	0.00248341  	0.00194699  
2023-05-11 17:20:53.251: Find a better model.
2023-05-11 17:21:15.016: [iter 2 : loss : 1.1368 = 0.6930 + 0.4438 + 0.0000, time: 21.760545]
2023-05-11 17:21:15.337: epoch 2:	0.00170270  	0.00307734  	0.00259010  
2023-05-11 17:21:15.337: Find a better model.
2023-05-11 17:21:37.292: [iter 3 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 21.952319]
2023-05-11 17:21:37.579: epoch 3:	0.00162126  	0.00320417  	0.00272743  
2023-05-11 17:21:37.579: Find a better model.
2023-05-11 17:21:59.140: [iter 4 : loss : 1.1376 = 0.6929 + 0.4447 + 0.0000, time: 21.556204]
2023-05-11 17:21:59.444: epoch 4:	0.00203583  	0.00355325  	0.00304081  
2023-05-11 17:21:59.445: Find a better model.
2023-05-11 17:22:21.115: [iter 5 : loss : 1.1353 = 0.6927 + 0.4425 + 0.0000, time: 21.667516]
2023-05-11 17:22:21.413: epoch 5:	0.00233936  	0.00449151  	0.00364208  
2023-05-11 17:22:21.413: Find a better model.
2023-05-11 17:22:43.214: [iter 6 : loss : 1.1383 = 0.6926 + 0.4457 + 0.0000, time: 21.796969]
2023-05-11 17:22:43.505: epoch 6:	0.00245040  	0.00427791  	0.00346155  
2023-05-11 17:23:05.273: [iter 7 : loss : 1.1356 = 0.6924 + 0.4432 + 0.0000, time: 21.764482]
2023-05-11 17:23:05.562: epoch 7:	0.00310927  	0.00636085  	0.00490857  
2023-05-11 17:23:05.562: Find a better model.
2023-05-11 17:23:27.318: [iter 8 : loss : 1.1389 = 0.6923 + 0.4467 + 0.0000, time: 21.751458]
2023-05-11 17:23:27.613: epoch 8:	0.00320551  	0.00603228  	0.00498978  
2023-05-11 17:23:49.233: [iter 9 : loss : 1.1359 = 0.6919 + 0.4439 + 0.0000, time: 21.614637]
2023-05-11 17:23:49.517: epoch 9:	0.00371632  	0.00747396  	0.00619099  
2023-05-11 17:23:49.517: Find a better model.
2023-05-11 17:24:11.153: [iter 10 : loss : 1.1394 = 0.6915 + 0.4478 + 0.0000, time: 21.632343]
2023-05-11 17:24:11.446: epoch 10:	0.00387918  	0.00794357  	0.00648589  
2023-05-11 17:24:11.446: Find a better model.
2023-05-11 17:24:33.027: [iter 11 : loss : 1.1355 = 0.6909 + 0.4446 + 0.0000, time: 21.577021]
2023-05-11 17:24:33.306: epoch 11:	0.00485638  	0.01122538  	0.00893554  
2023-05-11 17:24:33.306: Find a better model.
2023-05-11 17:24:56.731: [iter 12 : loss : 1.1391 = 0.6899 + 0.4491 + 0.0000, time: 23.420586]
2023-05-11 17:24:57.008: epoch 12:	0.00525613  	0.01162918  	0.00994626  
2023-05-11 17:24:57.008: Find a better model.
2023-05-11 17:25:18.442: [iter 13 : loss : 1.1341 = 0.6888 + 0.4453 + 0.0000, time: 21.430529]
2023-05-11 17:25:18.716: epoch 13:	0.00686257  	0.01655476  	0.01378630  
2023-05-11 17:25:18.716: Find a better model.
2023-05-11 17:25:40.030: [iter 14 : loss : 1.1382 = 0.6872 + 0.4509 + 0.0000, time: 21.310719]
2023-05-11 17:25:40.324: epoch 14:	0.00809886  	0.01894554  	0.01621195  
2023-05-11 17:25:40.324: Find a better model.
2023-05-11 17:26:01.822: [iter 15 : loss : 1.1314 = 0.6851 + 0.4462 + 0.0001, time: 21.493169]
2023-05-11 17:26:02.096: epoch 15:	0.01057149  	0.02614577  	0.02248592  
2023-05-11 17:26:02.096: Find a better model.
2023-05-11 17:26:23.456: [iter 16 : loss : 1.1350 = 0.6819 + 0.4530 + 0.0001, time: 21.355985]
2023-05-11 17:26:23.726: epoch 16:	0.01308854  	0.03263497  	0.02816181  
2023-05-11 17:26:23.726: Find a better model.
2023-05-11 17:26:45.166: [iter 17 : loss : 1.1243 = 0.6769 + 0.4473 + 0.0001, time: 21.434298]
2023-05-11 17:26:45.444: epoch 17:	0.01639777  	0.04150411  	0.03628888  
2023-05-11 17:26:45.444: Find a better model.
2023-05-11 17:27:06.859: [iter 18 : loss : 1.1246 = 0.6686 + 0.4558 + 0.0001, time: 21.410411]
2023-05-11 17:27:07.134: epoch 18:	0.01908514  	0.04886712  	0.04315101  
2023-05-11 17:27:07.135: Find a better model.
2023-05-11 17:27:28.364: [iter 19 : loss : 1.1045 = 0.6555 + 0.4489 + 0.0002, time: 21.225996]
2023-05-11 17:27:28.636: epoch 19:	0.02232035  	0.05842435  	0.05075991  
2023-05-11 17:27:28.637: Find a better model.
2023-05-11 17:27:50.069: [iter 20 : loss : 1.0955 = 0.6353 + 0.4600 + 0.0003, time: 21.427521]
2023-05-11 17:27:50.353: epoch 20:	0.02428961  	0.06344946  	0.05510025  
2023-05-11 17:27:50.354: Find a better model.
2023-05-11 17:28:11.742: [iter 21 : loss : 1.0584 = 0.6057 + 0.4522 + 0.0004, time: 21.384240]
2023-05-11 17:28:12.009: epoch 21:	0.02612562  	0.06847959  	0.05898827  
2023-05-11 17:28:12.009: Find a better model.
2023-05-11 17:28:33.239: [iter 22 : loss : 1.0329 = 0.5670 + 0.4653 + 0.0006, time: 21.225187]
2023-05-11 17:28:33.507: epoch 22:	0.02686595  	0.07157069  	0.06109028  
2023-05-11 17:28:33.507: Find a better model.
2023-05-11 17:28:54.924: [iter 23 : loss : 0.9789 = 0.5205 + 0.4575 + 0.0009, time: 21.412329]
2023-05-11 17:28:55.185: epoch 23:	0.02758407  	0.07327303  	0.06270680  
2023-05-11 17:28:55.186: Find a better model.
2023-05-11 17:29:16.613: [iter 24 : loss : 0.9445 = 0.4712 + 0.4722 + 0.0011, time: 21.424327]
2023-05-11 17:29:16.877: epoch 24:	0.02806527  	0.07443765  	0.06354146  
2023-05-11 17:29:16.878: Find a better model.
2023-05-11 17:29:38.138: [iter 25 : loss : 0.8873 = 0.4224 + 0.4635 + 0.0014, time: 21.256877]
2023-05-11 17:29:38.413: epoch 25:	0.02840582  	0.07512394  	0.06405369  
2023-05-11 17:29:38.413: Find a better model.
2023-05-11 17:29:59.803: [iter 26 : loss : 0.8582 = 0.3789 + 0.4775 + 0.0017, time: 21.385726]
2023-05-11 17:30:00.069: epoch 26:	0.02862051  	0.07635542  	0.06455759  
2023-05-11 17:30:00.069: Find a better model.
2023-05-11 17:30:21.547: [iter 27 : loss : 0.8095 = 0.3404 + 0.4671 + 0.0020, time: 21.474184]
2023-05-11 17:30:21.814: epoch 27:	0.02870196  	0.07672661  	0.06458204  
2023-05-11 17:30:21.814: Find a better model.
2023-05-11 17:30:43.139: [iter 28 : loss : 0.7893 = 0.3075 + 0.4796 + 0.0023, time: 21.321869]
2023-05-11 17:30:43.412: epoch 28:	0.02883521  	0.07756580  	0.06509593  
2023-05-11 17:30:43.412: Find a better model.
2023-05-11 17:31:05.120: [iter 29 : loss : 0.7508 = 0.2803 + 0.4680 + 0.0025, time: 21.703156]
2023-05-11 17:31:05.394: epoch 29:	0.02902029  	0.07832743  	0.06546358  
2023-05-11 17:31:05.394: Find a better model.
2023-05-11 17:31:26.933: [iter 30 : loss : 0.7385 = 0.2562 + 0.4795 + 0.0028, time: 21.535328]
2023-05-11 17:31:27.195: epoch 30:	0.02891665  	0.07775239  	0.06535506  
2023-05-11 17:31:48.859: [iter 31 : loss : 0.7067 = 0.2360 + 0.4676 + 0.0031, time: 21.659782]
2023-05-11 17:31:49.132: epoch 31:	0.02902029  	0.07837877  	0.06567866  
2023-05-11 17:31:49.132: Find a better model.
2023-05-11 17:32:10.946: [iter 32 : loss : 0.7010 = 0.2191 + 0.4786 + 0.0033, time: 21.809172]
2023-05-11 17:32:11.206: epoch 32:	0.02902029  	0.07858311  	0.06560020  
2023-05-11 17:32:11.207: Find a better model.
2023-05-11 17:32:32.865: [iter 33 : loss : 0.6744 = 0.2045 + 0.4664 + 0.0035, time: 21.654681]
2023-05-11 17:32:33.127: epoch 33:	0.02929422  	0.07903028  	0.06609459  
2023-05-11 17:32:33.127: Find a better model.
2023-05-11 17:32:54.728: [iter 34 : loss : 0.6719 = 0.1910 + 0.4771 + 0.0038, time: 21.597710]
2023-05-11 17:32:54.990: epoch 34:	0.02941267  	0.07957377  	0.06625726  
2023-05-11 17:32:54.990: Find a better model.
2023-05-11 17:33:16.652: [iter 35 : loss : 0.6479 = 0.1790 + 0.4649 + 0.0040, time: 21.658526]
2023-05-11 17:33:16.916: epoch 35:	0.02938305  	0.07965862  	0.06644715  
2023-05-11 17:33:16.916: Find a better model.
2023-05-11 17:33:38.732: [iter 36 : loss : 0.6489 = 0.1690 + 0.4758 + 0.0042, time: 21.812108]
2023-05-11 17:33:38.997: epoch 36:	0.02954593  	0.07995092  	0.06668002  
2023-05-11 17:33:38.997: Find a better model.
2023-05-11 17:34:00.448: [iter 37 : loss : 0.6281 = 0.1600 + 0.4637 + 0.0044, time: 21.448243]
2023-05-11 17:34:00.715: epoch 37:	0.02958294  	0.07967827  	0.06676445  
2023-05-11 17:34:22.497: [iter 38 : loss : 0.6309 = 0.1517 + 0.4746 + 0.0046, time: 21.778409]
2023-05-11 17:34:22.758: epoch 38:	0.02944968  	0.07951365  	0.06661434  
2023-05-11 17:34:44.425: [iter 39 : loss : 0.6117 = 0.1445 + 0.4623 + 0.0048, time: 21.664796]
2023-05-11 17:34:44.687: epoch 39:	0.02961255  	0.07998775  	0.06683134  
2023-05-11 17:34:44.688: Find a better model.
2023-05-11 17:35:06.287: [iter 40 : loss : 0.6154 = 0.1373 + 0.4731 + 0.0050, time: 21.596255]
2023-05-11 17:35:06.548: epoch 40:	0.02953853  	0.07967443  	0.06668062  
2023-05-11 17:35:28.060: [iter 41 : loss : 0.5973 = 0.1310 + 0.4612 + 0.0051, time: 21.507410]
2023-05-11 17:35:28.343: epoch 41:	0.02952372  	0.07928487  	0.06686503  
2023-05-11 17:35:50.036: [iter 42 : loss : 0.6027 = 0.1252 + 0.4722 + 0.0053, time: 21.688428]
2023-05-11 17:35:50.317: epoch 42:	0.02956814  	0.07975501  	0.06697051  
2023-05-11 17:36:12.015: [iter 43 : loss : 0.5857 = 0.1199 + 0.4602 + 0.0055, time: 21.694860]
2023-05-11 17:36:12.296: epoch 43:	0.02942007  	0.07895420  	0.06675915  
2023-05-11 17:36:33.862: [iter 44 : loss : 0.5932 = 0.1161 + 0.4714 + 0.0057, time: 21.560854]
2023-05-11 17:36:34.126: epoch 44:	0.02943488  	0.07893860  	0.06671577  
2023-05-11 17:36:55.806: [iter 45 : loss : 0.5765 = 0.1115 + 0.4592 + 0.0058, time: 21.675468]
2023-05-11 17:36:56.066: epoch 45:	0.02951632  	0.07905911  	0.06669556  
2023-05-11 17:37:17.461: [iter 46 : loss : 0.5840 = 0.1075 + 0.4706 + 0.0060, time: 21.390184]
2023-05-11 17:37:17.720: epoch 46:	0.02948670  	0.07924425  	0.06679541  
2023-05-11 17:37:39.179: [iter 47 : loss : 0.5680 = 0.1035 + 0.4584 + 0.0061, time: 21.455422]
2023-05-11 17:37:39.448: epoch 47:	0.02942748  	0.07893628  	0.06663230  
2023-05-11 17:38:01.041: [iter 48 : loss : 0.5762 = 0.1003 + 0.4697 + 0.0063, time: 21.588907]
2023-05-11 17:38:01.320: epoch 48:	0.02939786  	0.07860301  	0.06650476  
2023-05-11 17:38:22.772: [iter 49 : loss : 0.5610 = 0.0969 + 0.4577 + 0.0064, time: 21.439442]
2023-05-11 17:38:23.034: epoch 49:	0.02926460  	0.07786642  	0.06625769  
2023-05-11 17:38:44.633: [iter 50 : loss : 0.5695 = 0.0940 + 0.4689 + 0.0066, time: 21.593771]
2023-05-11 17:38:44.891: epoch 50:	0.02906472  	0.07756015  	0.06602912  
2023-05-11 17:39:06.329: [iter 51 : loss : 0.5546 = 0.0909 + 0.4570 + 0.0067, time: 21.433826]
2023-05-11 17:39:06.589: epoch 51:	0.02931643  	0.07783624  	0.06614687  
2023-05-11 17:39:28.008: [iter 52 : loss : 0.5632 = 0.0880 + 0.4684 + 0.0069, time: 21.415741]
2023-05-11 17:39:28.289: epoch 52:	0.02919058  	0.07768659  	0.06605973  
2023-05-11 17:39:49.888: [iter 53 : loss : 0.5494 = 0.0861 + 0.4563 + 0.0070, time: 21.594907]
2023-05-11 17:39:50.149: epoch 53:	0.02938305  	0.07821542  	0.06634398  
2023-05-11 17:40:11.770: [iter 54 : loss : 0.5588 = 0.0838 + 0.4678 + 0.0071, time: 21.617788]
2023-05-11 17:40:12.031: epoch 54:	0.02919057  	0.07748307  	0.06589749  
2023-05-11 17:40:33.353: [iter 55 : loss : 0.5448 = 0.0817 + 0.4558 + 0.0073, time: 21.317788]
2023-05-11 17:40:33.615: epoch 55:	0.02916836  	0.07757005  	0.06591113  
2023-05-11 17:40:55.131: [iter 56 : loss : 0.5538 = 0.0789 + 0.4675 + 0.0074, time: 21.511413]
2023-05-11 17:40:55.404: epoch 56:	0.02907952  	0.07704954  	0.06571921  
2023-05-11 17:41:16.929: [iter 57 : loss : 0.5396 = 0.0767 + 0.4554 + 0.0075, time: 21.521026]
2023-05-11 17:41:17.188: epoch 57:	0.02915355  	0.07692381  	0.06582231  
2023-05-11 17:41:38.545: [iter 58 : loss : 0.5498 = 0.0751 + 0.4670 + 0.0076, time: 21.351558]
2023-05-11 17:41:38.805: epoch 58:	0.02906472  	0.07643986  	0.06565646  
2023-05-11 17:42:00.259: [iter 59 : loss : 0.5359 = 0.0732 + 0.4549 + 0.0078, time: 21.450215]
2023-05-11 17:42:00.519: epoch 59:	0.02914616  	0.07674737  	0.06558628  
2023-05-11 17:42:21.927: [iter 60 : loss : 0.5462 = 0.0718 + 0.4665 + 0.0079, time: 21.402421]
2023-05-11 17:42:22.183: epoch 60:	0.02903511  	0.07626077  	0.06542129  
2023-05-11 17:42:43.276: [iter 61 : loss : 0.5325 = 0.0701 + 0.4545 + 0.0080, time: 21.088471]
2023-05-11 17:42:43.535: epoch 61:	0.02912395  	0.07626257  	0.06539304  
2023-05-11 17:43:04.913: [iter 62 : loss : 0.5433 = 0.0690 + 0.4662 + 0.0081, time: 21.374955]
2023-05-11 17:43:05.173: epoch 62:	0.02897588  	0.07599212  	0.06512762  
2023-05-11 17:43:26.466: [iter 63 : loss : 0.5299 = 0.0675 + 0.4542 + 0.0082, time: 21.289818]
2023-05-11 17:43:26.731: epoch 63:	0.02893147  	0.07596355  	0.06522683  
2023-05-11 17:43:47.917: [iter 64 : loss : 0.5404 = 0.0662 + 0.4658 + 0.0083, time: 21.182641]
2023-05-11 17:43:48.180: epoch 64:	0.02892406  	0.07574076  	0.06505757  
2023-05-11 17:43:48.180: Early stopping is trigger at epoch: 64
2023-05-11 17:43:48.180: best_result@epoch 39:

2023-05-11 17:43:48.180: 		0.0296      	0.0800      	0.0668      
2023-05-11 18:15:20.135: my pid: 2720
2023-05-11 18:15:20.135: model: model.general_recommender.SGL
2023-05-11 18:15:20.135: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 18:15:20.135: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 18:15:23.531: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 18:15:44.610: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.079408]
2023-05-11 18:15:44.875: epoch 1:	0.00136956  	0.00252590  	0.00208436  
2023-05-11 18:15:44.875: Find a better model.
2023-05-11 18:16:06.410: [iter 2 : loss : 1.1369 = 0.6930 + 0.4438 + 0.0000, time: 21.530931]
2023-05-11 18:16:06.700: epoch 2:	0.00163607  	0.00300964  	0.00244508  
2023-05-11 18:16:06.700: Find a better model.
2023-05-11 18:16:28.388: [iter 3 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 21.684664]
2023-05-11 18:16:28.676: epoch 3:	0.00195440  	0.00377507  	0.00319510  
2023-05-11 18:16:28.676: Find a better model.
2023-05-11 18:16:50.183: [iter 4 : loss : 1.1376 = 0.6929 + 0.4448 + 0.0000, time: 21.502801]
2023-05-11 18:16:50.476: epoch 4:	0.00205064  	0.00369308  	0.00295184  
2023-05-11 18:17:12.179: [iter 5 : loss : 1.1353 = 0.6927 + 0.4426 + 0.0000, time: 21.698907]
2023-05-11 18:17:12.473: epoch 5:	0.00236897  	0.00491449  	0.00366627  
2023-05-11 18:17:12.473: Find a better model.
2023-05-11 18:17:34.010: [iter 6 : loss : 1.1384 = 0.6926 + 0.4457 + 0.0000, time: 21.534039]
2023-05-11 18:17:34.321: epoch 6:	0.00248741  	0.00474819  	0.00368611  
2023-05-11 18:17:55.984: [iter 7 : loss : 1.1357 = 0.6924 + 0.4432 + 0.0000, time: 21.659399]
2023-05-11 18:17:56.293: epoch 7:	0.00319810  	0.00691013  	0.00544794  
2023-05-11 18:17:56.293: Find a better model.
2023-05-11 18:18:17.964: [iter 8 : loss : 1.1390 = 0.6923 + 0.4467 + 0.0000, time: 21.666986]
2023-05-11 18:18:18.269: epoch 8:	0.00327954  	0.00659804  	0.00523193  
2023-05-11 18:18:39.951: [iter 9 : loss : 1.1358 = 0.6920 + 0.4439 + 0.0000, time: 21.675768]
2023-05-11 18:18:40.239: epoch 9:	0.00412348  	0.00961274  	0.00724906  
2023-05-11 18:18:40.239: Find a better model.
2023-05-11 18:19:01.752: [iter 10 : loss : 1.1395 = 0.6916 + 0.4478 + 0.0000, time: 21.505048]
2023-05-11 18:19:02.041: epoch 10:	0.00411608  	0.00892032  	0.00722147  
2023-05-11 18:19:23.738: [iter 11 : loss : 1.1357 = 0.6911 + 0.4445 + 0.0000, time: 21.694026]
2023-05-11 18:19:24.018: epoch 11:	0.00550783  	0.01408806  	0.01092324  
2023-05-11 18:19:24.018: Find a better model.
2023-05-11 18:19:45.551: [iter 12 : loss : 1.1394 = 0.6903 + 0.4490 + 0.0000, time: 21.528996]
2023-05-11 18:19:45.840: epoch 12:	0.00565589  	0.01269016  	0.01041627  
2023-05-11 18:20:07.524: [iter 13 : loss : 1.1344 = 0.6890 + 0.4454 + 0.0000, time: 21.679985]
2023-05-11 18:20:07.809: epoch 13:	0.00692180  	0.01652876  	0.01399652  
2023-05-11 18:20:07.809: Find a better model.
2023-05-11 18:20:29.304: [iter 14 : loss : 1.1383 = 0.6874 + 0.4508 + 0.0000, time: 21.489097]
2023-05-11 18:20:29.587: epoch 14:	0.00832096  	0.01992206  	0.01688132  
2023-05-11 18:20:29.587: Find a better model.
2023-05-11 18:20:51.125: [iter 15 : loss : 1.1316 = 0.6853 + 0.4462 + 0.0001, time: 21.535938]
2023-05-11 18:20:51.417: epoch 15:	0.01069734  	0.02642494  	0.02313343  
2023-05-11 18:20:51.417: Find a better model.
2023-05-11 18:21:12.895: [iter 16 : loss : 1.1350 = 0.6821 + 0.4529 + 0.0001, time: 21.473073]
2023-05-11 18:21:13.173: epoch 16:	0.01308114  	0.03185384  	0.02833790  
2023-05-11 18:21:13.173: Find a better model.
2023-05-11 18:21:34.888: [iter 17 : loss : 1.1245 = 0.6772 + 0.4472 + 0.0001, time: 21.710421]
2023-05-11 18:21:35.160: epoch 17:	0.01626450  	0.04097568  	0.03643906  
2023-05-11 18:21:35.160: Find a better model.
2023-05-11 18:21:56.686: [iter 18 : loss : 1.1251 = 0.6691 + 0.4558 + 0.0001, time: 21.523070]
2023-05-11 18:21:56.966: epoch 18:	0.01910732  	0.04767489  	0.04280315  
2023-05-11 18:21:56.966: Find a better model.
2023-05-11 18:22:18.472: [iter 19 : loss : 1.1052 = 0.6562 + 0.4488 + 0.0002, time: 21.502655]
2023-05-11 18:22:18.760: epoch 19:	0.02220187  	0.05654892  	0.05000938  
2023-05-11 18:22:18.760: Find a better model.
2023-05-11 18:22:40.101: [iter 20 : loss : 1.0963 = 0.6362 + 0.4598 + 0.0003, time: 21.336723]
2023-05-11 18:22:40.389: epoch 20:	0.02456351  	0.06208341  	0.05473785  
2023-05-11 18:22:40.389: Find a better model.
2023-05-11 18:23:02.053: [iter 21 : loss : 1.0597 = 0.6070 + 0.4523 + 0.0004, time: 21.660622]
2023-05-11 18:23:02.346: epoch 21:	0.02653277  	0.06887331  	0.05895264  
2023-05-11 18:23:02.346: Find a better model.
2023-05-11 18:23:23.851: [iter 22 : loss : 1.0345 = 0.5684 + 0.4655 + 0.0006, time: 21.501167]
2023-05-11 18:23:24.132: epoch 22:	0.02719166  	0.07139591  	0.06078650  
2023-05-11 18:23:24.132: Find a better model.
2023-05-11 18:23:45.622: [iter 23 : loss : 0.9803 = 0.5218 + 0.4576 + 0.0009, time: 21.486731]
2023-05-11 18:23:45.901: epoch 23:	0.02809486  	0.07438138  	0.06232781  
2023-05-11 18:23:45.901: Find a better model.
2023-05-11 18:24:07.259: [iter 24 : loss : 0.9460 = 0.4725 + 0.4724 + 0.0011, time: 21.354540]
2023-05-11 18:24:07.536: epoch 24:	0.02868712  	0.07689077  	0.06338885  
2023-05-11 18:24:07.536: Find a better model.
2023-05-11 18:24:29.031: [iter 25 : loss : 0.8891 = 0.4240 + 0.4637 + 0.0014, time: 21.490144]
2023-05-11 18:24:29.326: epoch 25:	0.02939783  	0.07877235  	0.06457271  
2023-05-11 18:24:29.326: Find a better model.
2023-05-11 18:24:50.635: [iter 26 : loss : 0.8595 = 0.3803 + 0.4776 + 0.0017, time: 21.305328]
2023-05-11 18:24:50.933: epoch 26:	0.02942746  	0.07942916  	0.06491898  
2023-05-11 18:24:50.933: Find a better model.
2023-05-11 18:25:12.257: [iter 27 : loss : 0.8108 = 0.3416 + 0.4672 + 0.0020, time: 21.320536]
2023-05-11 18:25:12.534: epoch 27:	0.02944966  	0.07923638  	0.06511540  
2023-05-11 18:25:33.843: [iter 28 : loss : 0.7912 = 0.3092 + 0.4797 + 0.0023, time: 21.304661]
2023-05-11 18:25:34.115: epoch 28:	0.02973100  	0.08067754  	0.06569313  
2023-05-11 18:25:34.115: Find a better model.
2023-05-11 18:25:55.427: [iter 29 : loss : 0.7522 = 0.2816 + 0.4681 + 0.0025, time: 21.308326]
2023-05-11 18:25:55.697: epoch 29:	0.02987167  	0.08107942  	0.06587918  
2023-05-11 18:25:55.697: Find a better model.
2023-05-11 18:26:17.010: [iter 30 : loss : 0.7397 = 0.2573 + 0.4795 + 0.0028, time: 21.308994]
2023-05-11 18:26:17.297: epoch 30:	0.02993089  	0.08120453  	0.06605147  
2023-05-11 18:26:17.297: Find a better model.
2023-05-11 18:26:38.574: [iter 31 : loss : 0.7076 = 0.2370 + 0.4675 + 0.0030, time: 21.272594]
2023-05-11 18:26:38.844: epoch 31:	0.03001972  	0.08146966  	0.06622161  
2023-05-11 18:26:38.845: Find a better model.
2023-05-11 18:27:00.189: [iter 32 : loss : 0.7018 = 0.2200 + 0.4785 + 0.0033, time: 21.340159]
2023-05-11 18:27:00.468: epoch 32:	0.03018259  	0.08216662  	0.06661383  
2023-05-11 18:27:00.468: Find a better model.
2023-05-11 18:27:21.976: [iter 33 : loss : 0.6752 = 0.2053 + 0.4664 + 0.0035, time: 21.503586]
2023-05-11 18:27:22.244: epoch 33:	0.03030103  	0.08201855  	0.06661779  
2023-05-11 18:27:43.755: [iter 34 : loss : 0.6730 = 0.1921 + 0.4772 + 0.0037, time: 21.507955]
2023-05-11 18:27:44.024: epoch 34:	0.03033064  	0.08235882  	0.06687281  
2023-05-11 18:27:44.024: Find a better model.
2023-05-11 18:28:05.560: [iter 35 : loss : 0.6486 = 0.1798 + 0.4648 + 0.0040, time: 21.530806]
2023-05-11 18:28:05.833: epoch 35:	0.03031584  	0.08230355  	0.06695367  
2023-05-11 18:28:27.357: [iter 36 : loss : 0.6497 = 0.1698 + 0.4758 + 0.0042, time: 21.520234]
2023-05-11 18:28:27.627: epoch 36:	0.03036768  	0.08269283  	0.06717297  
2023-05-11 18:28:27.627: Find a better model.
2023-05-11 18:28:49.348: [iter 37 : loss : 0.6288 = 0.1606 + 0.4638 + 0.0044, time: 21.715570]
2023-05-11 18:28:49.617: epoch 37:	0.03041209  	0.08266732  	0.06729665  
2023-05-11 18:29:11.182: [iter 38 : loss : 0.6312 = 0.1520 + 0.4746 + 0.0046, time: 21.561101]
2023-05-11 18:29:11.470: epoch 38:	0.03033066  	0.08213644  	0.06713335  
2023-05-11 18:29:33.139: [iter 39 : loss : 0.6118 = 0.1448 + 0.4623 + 0.0048, time: 21.666925]
2023-05-11 18:29:33.444: epoch 39:	0.03043430  	0.08219568  	0.06724521  
2023-05-11 18:29:54.928: [iter 40 : loss : 0.6159 = 0.1377 + 0.4732 + 0.0050, time: 21.470674]
2023-05-11 18:29:55.198: epoch 40:	0.03038989  	0.08189336  	0.06717145  
2023-05-11 18:30:16.919: [iter 41 : loss : 0.5976 = 0.1313 + 0.4612 + 0.0051, time: 21.716850]
2023-05-11 18:30:17.192: epoch 41:	0.03047132  	0.08224508  	0.06722238  
2023-05-11 18:30:38.932: [iter 42 : loss : 0.6034 = 0.1258 + 0.4723 + 0.0053, time: 21.736924]
2023-05-11 18:30:39.202: epoch 42:	0.03033805  	0.08188395  	0.06712376  
2023-05-11 18:31:00.895: [iter 43 : loss : 0.5861 = 0.1204 + 0.4602 + 0.0055, time: 21.689323]
2023-05-11 18:31:01.167: epoch 43:	0.03041209  	0.08207276  	0.06731299  
2023-05-11 18:31:22.915: [iter 44 : loss : 0.5934 = 0.1164 + 0.4714 + 0.0056, time: 21.742904]
2023-05-11 18:31:23.201: epoch 44:	0.03026403  	0.08192238  	0.06723637  
2023-05-11 18:31:45.271: [iter 45 : loss : 0.5767 = 0.1117 + 0.4592 + 0.0058, time: 22.064296]
2023-05-11 18:31:45.553: epoch 45:	0.03027143  	0.08199110  	0.06729505  
2023-05-11 18:32:07.296: [iter 46 : loss : 0.5842 = 0.1078 + 0.4705 + 0.0060, time: 21.738112]
2023-05-11 18:32:07.583: epoch 46:	0.03030104  	0.08182302  	0.06729189  
2023-05-11 18:32:29.274: [iter 47 : loss : 0.5681 = 0.1037 + 0.4583 + 0.0061, time: 21.687111]
2023-05-11 18:32:29.562: epoch 47:	0.03030845  	0.08195423  	0.06735869  
2023-05-11 18:32:51.466: [iter 48 : loss : 0.5762 = 0.1003 + 0.4696 + 0.0063, time: 21.900255]
2023-05-11 18:32:51.756: epoch 48:	0.03016779  	0.08143084  	0.06695698  
2023-05-11 18:33:13.438: [iter 49 : loss : 0.5610 = 0.0970 + 0.4576 + 0.0064, time: 21.678653]
2023-05-11 18:33:13.706: epoch 49:	0.03014558  	0.08161870  	0.06699929  
2023-05-11 18:33:35.290: [iter 50 : loss : 0.5696 = 0.0942 + 0.4689 + 0.0066, time: 21.580129]
2023-05-11 18:33:35.570: epoch 50:	0.03001232  	0.08150747  	0.06665681  
2023-05-11 18:33:57.441: [iter 51 : loss : 0.5544 = 0.0908 + 0.4569 + 0.0067, time: 21.867537]
2023-05-11 18:33:57.714: epoch 51:	0.03004934  	0.08145779  	0.06657358  
2023-05-11 18:34:19.294: [iter 52 : loss : 0.5628 = 0.0877 + 0.4683 + 0.0068, time: 21.575795]
2023-05-11 18:34:19.574: epoch 52:	0.02994570  	0.08104692  	0.06649016  
2023-05-11 18:34:42.089: [iter 53 : loss : 0.5500 = 0.0867 + 0.4564 + 0.0070, time: 22.510813]
2023-05-11 18:34:42.382: epoch 53:	0.02996791  	0.08098887  	0.06653195  
2023-05-11 18:35:04.053: [iter 54 : loss : 0.5590 = 0.0840 + 0.4679 + 0.0071, time: 21.667732]
2023-05-11 18:35:04.342: epoch 54:	0.02989388  	0.08097523  	0.06654997  
2023-05-11 18:35:26.329: [iter 55 : loss : 0.5445 = 0.0815 + 0.4558 + 0.0073, time: 21.984631]
2023-05-11 18:35:26.618: epoch 55:	0.02981983  	0.08061540  	0.06637480  
2023-05-11 18:35:48.627: [iter 56 : loss : 0.5539 = 0.0791 + 0.4673 + 0.0074, time: 22.004797]
2023-05-11 18:35:48.906: epoch 56:	0.02963475  	0.07988346  	0.06589031  
2023-05-11 18:36:11.518: [iter 57 : loss : 0.5394 = 0.0766 + 0.4553 + 0.0075, time: 22.608594]
2023-05-11 18:36:11.791: epoch 57:	0.02959774  	0.07946406  	0.06576336  
2023-05-11 18:36:33.584: [iter 58 : loss : 0.5501 = 0.0753 + 0.4672 + 0.0076, time: 21.790025]
2023-05-11 18:36:33.856: epoch 58:	0.02945706  	0.07884676  	0.06552394  
2023-05-11 18:36:55.585: [iter 59 : loss : 0.5360 = 0.0733 + 0.4549 + 0.0078, time: 21.725621]
2023-05-11 18:36:55.856: epoch 59:	0.02946448  	0.07872479  	0.06543822  
2023-05-11 18:37:17.597: [iter 60 : loss : 0.5463 = 0.0716 + 0.4668 + 0.0079, time: 21.738654]
2023-05-11 18:37:17.866: epoch 60:	0.02930159  	0.07816906  	0.06516430  
2023-05-11 18:37:39.554: [iter 61 : loss : 0.5325 = 0.0699 + 0.4546 + 0.0080, time: 21.683612]
2023-05-11 18:37:39.823: epoch 61:	0.02934602  	0.07806194  	0.06508460  
2023-05-11 18:37:39.823: Early stopping is trigger at epoch: 61
2023-05-11 18:37:39.823: best_result@epoch 36:

2023-05-11 18:37:39.823: 		0.0304      	0.0827      	0.0672      
2023-05-11 18:47:22.615: my pid: 8044
2023-05-11 18:47:22.616: model: model.general_recommender.SGL
2023-05-11 18:47:22.616: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 18:47:22.616: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 18:47:26.031: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 18:47:46.961: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.928509]
2023-05-11 18:47:47.224: epoch 1:	0.00119189  	0.00254829  	0.00196468  
2023-05-11 18:47:47.224: Find a better model.
2023-05-11 18:48:08.799: [iter 2 : loss : 1.1369 = 0.6930 + 0.4438 + 0.0000, time: 21.572553]
2023-05-11 18:48:09.103: epoch 2:	0.00152502  	0.00269385  	0.00228419  
2023-05-11 18:48:09.103: Find a better model.
2023-05-11 18:48:30.712: [iter 3 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 21.606191]
2023-05-11 18:48:31.019: epoch 3:	0.00200622  	0.00363531  	0.00307044  
2023-05-11 18:48:31.019: Find a better model.
2023-05-11 18:48:52.761: [iter 4 : loss : 1.1376 = 0.6929 + 0.4448 + 0.0000, time: 21.737193]
2023-05-11 18:48:53.073: epoch 4:	0.00204323  	0.00345249  	0.00286424  
2023-05-11 18:49:14.904: [iter 5 : loss : 1.1353 = 0.6927 + 0.4425 + 0.0000, time: 21.827677]
2023-05-11 18:49:15.200: epoch 5:	0.00250222  	0.00513670  	0.00388877  
2023-05-11 18:49:15.200: Find a better model.
2023-05-11 18:49:36.946: [iter 6 : loss : 1.1383 = 0.6926 + 0.4457 + 0.0000, time: 21.741712]
2023-05-11 18:49:37.233: epoch 6:	0.00245040  	0.00447898  	0.00358592  
2023-05-11 18:49:59.065: [iter 7 : loss : 1.1357 = 0.6925 + 0.4432 + 0.0000, time: 21.827767]
2023-05-11 18:49:59.346: epoch 7:	0.00317590  	0.00677182  	0.00545281  
2023-05-11 18:49:59.346: Find a better model.
2023-05-11 18:50:21.141: [iter 8 : loss : 1.1390 = 0.6923 + 0.4467 + 0.0000, time: 21.791347]
2023-05-11 18:50:21.444: epoch 8:	0.00324252  	0.00629181  	0.00537130  
2023-05-11 18:50:43.090: [iter 9 : loss : 1.1359 = 0.6920 + 0.4439 + 0.0000, time: 21.643030]
2023-05-11 18:50:43.377: epoch 9:	0.00414569  	0.00953825  	0.00749737  
2023-05-11 18:50:43.377: Find a better model.
2023-05-11 18:51:05.135: [iter 10 : loss : 1.1394 = 0.6916 + 0.4478 + 0.0000, time: 21.753769]
2023-05-11 18:51:05.417: epoch 10:	0.00401984  	0.00885064  	0.00693768  
2023-05-11 18:51:27.277: [iter 11 : loss : 1.1356 = 0.6910 + 0.4446 + 0.0000, time: 21.856445]
2023-05-11 18:51:27.564: epoch 11:	0.00513769  	0.01164021  	0.00911309  
2023-05-11 18:51:27.564: Find a better model.
2023-05-11 18:51:50.693: [iter 12 : loss : 1.1393 = 0.6901 + 0.4491 + 0.0000, time: 23.125256]
2023-05-11 18:51:50.970: epoch 12:	0.00541900  	0.01185225  	0.00992812  
2023-05-11 18:51:50.970: Find a better model.
2023-05-11 18:52:12.800: [iter 13 : loss : 1.1343 = 0.6889 + 0.4454 + 0.0000, time: 21.825665]
2023-05-11 18:52:13.096: epoch 13:	0.00693660  	0.01674273  	0.01405560  
2023-05-11 18:52:13.096: Find a better model.
2023-05-11 18:52:34.681: [iter 14 : loss : 1.1383 = 0.6874 + 0.4509 + 0.0000, time: 21.582922]
2023-05-11 18:52:34.952: epoch 14:	0.00809146  	0.01870379  	0.01607374  
2023-05-11 18:52:34.952: Find a better model.
2023-05-11 18:52:56.592: [iter 15 : loss : 1.1315 = 0.6853 + 0.4462 + 0.0001, time: 21.635376]
2023-05-11 18:52:56.860: epoch 15:	0.01047525  	0.02626724  	0.02234391  
2023-05-11 18:52:56.861: Find a better model.
2023-05-11 18:53:18.439: [iter 16 : loss : 1.1352 = 0.6821 + 0.4530 + 0.0001, time: 21.573672]
2023-05-11 18:53:18.707: epoch 16:	0.01297749  	0.03250381  	0.02820112  
2023-05-11 18:53:18.707: Find a better model.
2023-05-11 18:53:40.193: [iter 17 : loss : 1.1246 = 0.6773 + 0.4472 + 0.0001, time: 21.482357]
2023-05-11 18:53:40.473: epoch 17:	0.01658285  	0.04190079  	0.03603389  
2023-05-11 18:53:40.473: Find a better model.
2023-05-11 18:54:01.837: [iter 18 : loss : 1.1252 = 0.6693 + 0.4557 + 0.0001, time: 21.358521]
2023-05-11 18:54:02.132: epoch 18:	0.01947010  	0.04908651  	0.04284831  
2023-05-11 18:54:02.132: Find a better model.
2023-05-11 18:54:23.615: [iter 19 : loss : 1.1055 = 0.6565 + 0.4488 + 0.0002, time: 21.479979]
2023-05-11 18:54:23.883: epoch 19:	0.02238697  	0.05613706  	0.04917277  
2023-05-11 18:54:23.884: Find a better model.
2023-05-11 18:54:45.443: [iter 20 : loss : 1.0968 = 0.6366 + 0.4598 + 0.0003, time: 21.555345]
2023-05-11 18:54:45.713: epoch 20:	0.02423779  	0.06137686  	0.05331489  
2023-05-11 18:54:45.713: Find a better model.
2023-05-11 18:55:07.037: [iter 21 : loss : 1.0603 = 0.6077 + 0.4522 + 0.0004, time: 21.319722]
2023-05-11 18:55:07.312: epoch 21:	0.02622925  	0.06732251  	0.05768177  
2023-05-11 18:55:07.312: Find a better model.
2023-05-11 18:55:28.618: [iter 22 : loss : 1.0353 = 0.5692 + 0.4654 + 0.0006, time: 21.302871]
2023-05-11 18:55:28.883: epoch 22:	0.02705843  	0.07011928  	0.05955472  
2023-05-11 18:55:28.883: Find a better model.
2023-05-11 18:55:50.160: [iter 23 : loss : 0.9814 = 0.5231 + 0.4575 + 0.0008, time: 21.273600]
2023-05-11 18:55:50.425: epoch 23:	0.02794681  	0.07275343  	0.06157410  
2023-05-11 18:55:50.425: Find a better model.
2023-05-11 18:56:11.838: [iter 24 : loss : 0.9473 = 0.4739 + 0.4723 + 0.0011, time: 21.410125]
2023-05-11 18:56:12.138: epoch 24:	0.02811709  	0.07403901  	0.06239486  
2023-05-11 18:56:12.138: Find a better model.
2023-05-11 18:56:33.341: [iter 25 : loss : 0.8901 = 0.4252 + 0.4635 + 0.0014, time: 21.200120]
2023-05-11 18:56:33.602: epoch 25:	0.02867234  	0.07569443  	0.06376469  
2023-05-11 18:56:33.602: Find a better model.
2023-05-11 18:56:54.971: [iter 26 : loss : 0.8603 = 0.3813 + 0.4773 + 0.0017, time: 21.365638]
2023-05-11 18:56:55.242: epoch 26:	0.02887222  	0.07642441  	0.06387264  
2023-05-11 18:56:55.242: Find a better model.
2023-05-11 18:57:16.928: [iter 27 : loss : 0.8115 = 0.3426 + 0.4669 + 0.0020, time: 21.681321]
2023-05-11 18:57:17.201: epoch 27:	0.02895366  	0.07731816  	0.06424903  
2023-05-11 18:57:17.201: Find a better model.
2023-05-11 18:57:38.757: [iter 28 : loss : 0.7916 = 0.3098 + 0.4796 + 0.0022, time: 21.552019]
2023-05-11 18:57:39.038: epoch 28:	0.02910173  	0.07760923  	0.06429993  
2023-05-11 18:57:39.038: Find a better model.
2023-05-11 18:58:00.534: [iter 29 : loss : 0.7523 = 0.2817 + 0.4680 + 0.0025, time: 21.492442]
2023-05-11 18:58:00.795: epoch 29:	0.02938306  	0.07891370  	0.06493609  
2023-05-11 18:58:00.795: Find a better model.
2023-05-11 18:58:22.404: [iter 30 : loss : 0.7398 = 0.2575 + 0.4795 + 0.0028, time: 21.604441]
2023-05-11 18:58:22.687: epoch 30:	0.02942748  	0.07901629  	0.06507055  
2023-05-11 18:58:22.687: Find a better model.
2023-05-11 18:58:44.315: [iter 31 : loss : 0.7079 = 0.2373 + 0.4676 + 0.0030, time: 21.623172]
2023-05-11 18:58:44.573: epoch 31:	0.02950151  	0.07920860  	0.06523289  
2023-05-11 18:58:44.573: Find a better model.
2023-05-11 18:59:08.132: [iter 32 : loss : 0.7017 = 0.2198 + 0.4786 + 0.0033, time: 23.554740]
2023-05-11 18:59:08.393: epoch 32:	0.02968659  	0.07968516  	0.06556338  
2023-05-11 18:59:08.394: Find a better model.
2023-05-11 18:59:30.090: [iter 33 : loss : 0.6753 = 0.2053 + 0.4664 + 0.0035, time: 21.692854]
2023-05-11 18:59:30.352: epoch 33:	0.02979024  	0.08012749  	0.06584541  
2023-05-11 18:59:30.353: Find a better model.
2023-05-11 18:59:52.140: [iter 34 : loss : 0.6729 = 0.1919 + 0.4773 + 0.0037, time: 21.783625]
2023-05-11 18:59:52.396: epoch 34:	0.02981984  	0.08010687  	0.06589972  
2023-05-11 19:00:14.245: [iter 35 : loss : 0.6489 = 0.1799 + 0.4650 + 0.0040, time: 21.842799]
2023-05-11 19:00:14.502: epoch 35:	0.02987907  	0.08039914  	0.06607100  
2023-05-11 19:00:14.502: Find a better model.
2023-05-11 19:00:36.132: [iter 36 : loss : 0.6495 = 0.1694 + 0.4759 + 0.0042, time: 21.625698]
2023-05-11 19:00:36.394: epoch 36:	0.02990867  	0.08056104  	0.06637079  
2023-05-11 19:00:36.394: Find a better model.
2023-05-11 19:00:58.054: [iter 37 : loss : 0.6286 = 0.1605 + 0.4637 + 0.0044, time: 21.656469]
2023-05-11 19:00:58.320: epoch 37:	0.02991607  	0.08067813  	0.06640163  
2023-05-11 19:00:58.320: Find a better model.
2023-05-11 19:01:20.068: [iter 38 : loss : 0.6313 = 0.1521 + 0.4747 + 0.0046, time: 21.745216]
2023-05-11 19:01:20.332: epoch 38:	0.02980502  	0.08018703  	0.06642620  
2023-05-11 19:01:42.050: [iter 39 : loss : 0.6121 = 0.1449 + 0.4624 + 0.0048, time: 21.713368]
2023-05-11 19:01:42.315: epoch 39:	0.02986424  	0.08055561  	0.06648726  
2023-05-11 19:02:03.891: [iter 40 : loss : 0.6163 = 0.1379 + 0.4735 + 0.0049, time: 21.573040]
2023-05-11 19:02:04.168: epoch 40:	0.02990125  	0.08050101  	0.06635238  
2023-05-11 19:02:25.864: [iter 41 : loss : 0.5980 = 0.1316 + 0.4613 + 0.0051, time: 21.692406]
2023-05-11 19:02:26.143: epoch 41:	0.02993087  	0.08065525  	0.06648379  
2023-05-11 19:02:47.840: [iter 42 : loss : 0.6034 = 0.1258 + 0.4723 + 0.0053, time: 21.692694]
2023-05-11 19:02:48.121: epoch 42:	0.02978281  	0.07999638  	0.06630699  
2023-05-11 19:03:09.814: [iter 43 : loss : 0.5865 = 0.1206 + 0.4604 + 0.0055, time: 21.688335]
2023-05-11 19:03:10.077: epoch 43:	0.02993087  	0.08042616  	0.06647035  
2023-05-11 19:03:31.857: [iter 44 : loss : 0.5940 = 0.1168 + 0.4715 + 0.0056, time: 21.776662]
2023-05-11 19:03:32.138: epoch 44:	0.02981243  	0.07984188  	0.06623165  
2023-05-11 19:03:53.947: [iter 45 : loss : 0.5767 = 0.1116 + 0.4593 + 0.0058, time: 21.804589]
2023-05-11 19:03:54.220: epoch 45:	0.02977541  	0.07961299  	0.06615499  
2023-05-11 19:04:15.814: [iter 46 : loss : 0.5842 = 0.1077 + 0.4705 + 0.0060, time: 21.589194]
2023-05-11 19:04:16.073: epoch 46:	0.02977541  	0.07973794  	0.06616757  
2023-05-11 19:04:37.771: [iter 47 : loss : 0.5683 = 0.1037 + 0.4585 + 0.0061, time: 21.693754]
2023-05-11 19:04:38.032: epoch 47:	0.02963475  	0.07926196  	0.06596076  
2023-05-11 19:04:59.616: [iter 48 : loss : 0.5767 = 0.1006 + 0.4698 + 0.0063, time: 21.580231]
2023-05-11 19:04:59.873: epoch 48:	0.02951629  	0.07876406  	0.06584699  
2023-05-11 19:05:21.339: [iter 49 : loss : 0.5612 = 0.0971 + 0.4577 + 0.0064, time: 21.460870]
2023-05-11 19:05:21.603: epoch 49:	0.02945707  	0.07877329  	0.06576640  
2023-05-11 19:05:43.216: [iter 50 : loss : 0.5699 = 0.0943 + 0.4690 + 0.0066, time: 21.608561]
2023-05-11 19:05:43.474: epoch 50:	0.02930160  	0.07856280  	0.06556723  
2023-05-11 19:06:05.172: [iter 51 : loss : 0.5548 = 0.0911 + 0.4570 + 0.0067, time: 21.694396]
2023-05-11 19:06:05.431: epoch 51:	0.02913132  	0.07796922  	0.06538431  
2023-05-11 19:06:26.780: [iter 52 : loss : 0.5635 = 0.0882 + 0.4685 + 0.0068, time: 21.344857]
2023-05-11 19:06:27.036: epoch 52:	0.02914613  	0.07807029  	0.06545302  
2023-05-11 19:06:48.688: [iter 53 : loss : 0.5501 = 0.0867 + 0.4565 + 0.0070, time: 21.647734]
2023-05-11 19:06:48.946: epoch 53:	0.02913872  	0.07787391  	0.06525451  
2023-05-11 19:07:10.606: [iter 54 : loss : 0.5595 = 0.0843 + 0.4681 + 0.0071, time: 21.656321]
2023-05-11 19:07:10.865: epoch 54:	0.02913872  	0.07801639  	0.06517677  
2023-05-11 19:07:32.311: [iter 55 : loss : 0.5448 = 0.0816 + 0.4559 + 0.0072, time: 21.441189]
2023-05-11 19:07:32.568: epoch 55:	0.02910172  	0.07812434  	0.06519407  
2023-05-11 19:07:53.944: [iter 56 : loss : 0.5540 = 0.0792 + 0.4675 + 0.0074, time: 21.372361]
2023-05-11 19:07:54.219: epoch 56:	0.02903509  	0.07799994  	0.06514850  
2023-05-11 19:08:15.902: [iter 57 : loss : 0.5399 = 0.0770 + 0.4554 + 0.0075, time: 21.677689]
2023-05-11 19:08:16.179: epoch 57:	0.02915354  	0.07814223  	0.06509960  
2023-05-11 19:08:37.535: [iter 58 : loss : 0.5505 = 0.0756 + 0.4672 + 0.0076, time: 21.350825]
2023-05-11 19:08:37.798: epoch 58:	0.02900547  	0.07762450  	0.06504814  
2023-05-11 19:08:59.298: [iter 59 : loss : 0.5365 = 0.0736 + 0.4551 + 0.0077, time: 21.496900]
2023-05-11 19:08:59.558: epoch 59:	0.02901288  	0.07737102  	0.06481959  
2023-05-11 19:09:21.133: [iter 60 : loss : 0.5467 = 0.0722 + 0.4667 + 0.0079, time: 21.570848]
2023-05-11 19:09:21.391: epoch 60:	0.02891664  	0.07722247  	0.06468304  
2023-05-11 19:09:42.902: [iter 61 : loss : 0.5331 = 0.0705 + 0.4546 + 0.0080, time: 21.506700]
2023-05-11 19:09:43.176: epoch 61:	0.02887963  	0.07728413  	0.06466500  
2023-05-11 19:10:04.526: [iter 62 : loss : 0.5438 = 0.0693 + 0.4664 + 0.0081, time: 21.346173]
2023-05-11 19:10:04.791: epoch 62:	0.02883521  	0.07742553  	0.06464695  
2023-05-11 19:10:04.791: Early stopping is trigger at epoch: 62
2023-05-11 19:10:04.791: best_result@epoch 37:

2023-05-11 19:10:04.791: 		0.0299      	0.0807      	0.0664      
2023-05-11 19:13:27.884: my pid: 9520
2023-05-11 19:13:27.884: model: model.general_recommender.SGL
2023-05-11 19:13:27.884: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 19:13:27.884: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 19:13:31.287: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 19:13:52.673: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.383398]
2023-05-11 19:13:52.938: epoch 1:	0.00135475  	0.00271324  	0.00206866  
2023-05-11 19:13:52.938: Find a better model.
2023-05-11 19:14:14.444: [iter 2 : loss : 1.1368 = 0.6930 + 0.4438 + 0.0000, time: 21.501669]
2023-05-11 19:14:14.746: epoch 2:	0.00154723  	0.00320653  	0.00249339  
2023-05-11 19:14:14.746: Find a better model.
2023-05-11 19:14:36.577: [iter 3 : loss : 1.1346 = 0.6929 + 0.4417 + 0.0000, time: 21.828840]
2023-05-11 19:14:36.883: epoch 3:	0.00181374  	0.00321116  	0.00277119  
2023-05-11 19:14:36.883: Find a better model.
2023-05-11 19:14:58.440: [iter 4 : loss : 1.1375 = 0.6929 + 0.4447 + 0.0000, time: 21.552966]
2023-05-11 19:14:58.725: epoch 4:	0.00222091  	0.00379854  	0.00308324  
2023-05-11 19:14:58.725: Find a better model.
2023-05-11 19:15:20.222: [iter 5 : loss : 1.1352 = 0.6927 + 0.4425 + 0.0000, time: 21.492710]
2023-05-11 19:15:20.510: epoch 5:	0.00259106  	0.00499497  	0.00393364  
2023-05-11 19:15:20.510: Find a better model.
2023-05-11 19:15:42.045: [iter 6 : loss : 1.1383 = 0.6926 + 0.4456 + 0.0000, time: 21.530959]
2023-05-11 19:15:42.336: epoch 6:	0.00260586  	0.00466126  	0.00393644  
2023-05-11 19:16:03.746: [iter 7 : loss : 1.1356 = 0.6925 + 0.4432 + 0.0000, time: 21.407074]
2023-05-11 19:16:04.026: epoch 7:	0.00325733  	0.00690581  	0.00554526  
2023-05-11 19:16:04.026: Find a better model.
2023-05-11 19:16:25.624: [iter 8 : loss : 1.1388 = 0.6923 + 0.4465 + 0.0000, time: 21.594270]
2023-05-11 19:16:25.923: epoch 8:	0.00325733  	0.00641003  	0.00533369  
2023-05-11 19:16:47.561: [iter 9 : loss : 1.1358 = 0.6920 + 0.4438 + 0.0000, time: 21.634404]
2023-05-11 19:16:47.860: epoch 9:	0.00416790  	0.00862039  	0.00702121  
2023-05-11 19:16:47.860: Find a better model.
2023-05-11 19:17:09.412: [iter 10 : loss : 1.1392 = 0.6916 + 0.4476 + 0.0000, time: 21.549488]
2023-05-11 19:17:09.694: epoch 10:	0.00430115  	0.00925555  	0.00738733  
2023-05-11 19:17:09.694: Find a better model.
2023-05-11 19:17:30.961: [iter 11 : loss : 1.1356 = 0.6911 + 0.4444 + 0.0000, time: 21.263082]
2023-05-11 19:17:31.240: epoch 11:	0.00544861  	0.01298922  	0.01017188  
2023-05-11 19:17:31.240: Find a better model.
2023-05-11 19:17:54.150: [iter 12 : loss : 1.1392 = 0.6903 + 0.4488 + 0.0000, time: 22.906996]
2023-05-11 19:17:54.425: epoch 12:	0.00537458  	0.01177221  	0.00979413  
2023-05-11 19:18:16.090: [iter 13 : loss : 1.1344 = 0.6891 + 0.4453 + 0.0000, time: 21.662585]
2023-05-11 19:18:16.379: epoch 13:	0.00683296  	0.01612902  	0.01328014  
2023-05-11 19:18:16.379: Find a better model.
2023-05-11 19:18:37.749: [iter 14 : loss : 1.1381 = 0.6875 + 0.4506 + 0.0000, time: 21.367272]
2023-05-11 19:18:38.027: epoch 14:	0.00823952  	0.01971586  	0.01652206  
2023-05-11 19:18:38.027: Find a better model.
2023-05-11 19:18:59.527: [iter 15 : loss : 1.1316 = 0.6853 + 0.4462 + 0.0001, time: 21.495586]
2023-05-11 19:18:59.797: epoch 15:	0.01073435  	0.02666998  	0.02297952  
2023-05-11 19:18:59.797: Find a better model.
2023-05-11 19:19:21.143: [iter 16 : loss : 1.1348 = 0.6821 + 0.4526 + 0.0001, time: 21.342117]
2023-05-11 19:19:21.414: epoch 16:	0.01277760  	0.03074179  	0.02712259  
2023-05-11 19:19:21.414: Find a better model.
2023-05-11 19:19:42.677: [iter 17 : loss : 1.1244 = 0.6772 + 0.4471 + 0.0001, time: 21.258874]
2023-05-11 19:19:42.953: epoch 17:	0.01595357  	0.03946023  	0.03500286  
2023-05-11 19:19:42.953: Find a better model.
2023-05-11 19:20:04.328: [iter 18 : loss : 1.1247 = 0.6690 + 0.4555 + 0.0001, time: 21.372015]
2023-05-11 19:20:04.603: epoch 18:	0.01906290  	0.04734524  	0.04154677  
2023-05-11 19:20:04.603: Find a better model.
2023-05-11 19:20:25.907: [iter 19 : loss : 1.1049 = 0.6559 + 0.4488 + 0.0002, time: 21.299205]
2023-05-11 19:20:26.202: epoch 19:	0.02240177  	0.05717672  	0.04936859  
2023-05-11 19:20:26.202: Find a better model.
2023-05-11 19:20:47.504: [iter 20 : loss : 1.0957 = 0.6357 + 0.4597 + 0.0003, time: 21.298279]
2023-05-11 19:20:47.773: epoch 20:	0.02439326  	0.06310198  	0.05432644  
2023-05-11 19:20:47.774: Find a better model.
2023-05-11 19:21:09.271: [iter 21 : loss : 1.0588 = 0.6061 + 0.4523 + 0.0004, time: 21.493100]
2023-05-11 19:21:09.545: epoch 21:	0.02648096  	0.06908692  	0.05854594  
2023-05-11 19:21:09.545: Find a better model.
2023-05-11 19:21:30.713: [iter 22 : loss : 1.0330 = 0.5671 + 0.4653 + 0.0006, time: 21.162365]
2023-05-11 19:21:30.977: epoch 22:	0.02720649  	0.07161987  	0.06049329  
2023-05-11 19:21:30.977: Find a better model.
2023-05-11 19:21:52.258: [iter 23 : loss : 0.9790 = 0.5204 + 0.4577 + 0.0009, time: 21.277114]
2023-05-11 19:21:52.521: epoch 23:	0.02789500  	0.07369919  	0.06185253  
2023-05-11 19:21:52.521: Find a better model.
2023-05-11 19:22:13.708: [iter 24 : loss : 0.9446 = 0.4713 + 0.4722 + 0.0011, time: 21.182850]
2023-05-11 19:22:13.972: epoch 24:	0.02830218  	0.07546322  	0.06275288  
2023-05-11 19:22:13.972: Find a better model.
2023-05-11 19:22:35.071: [iter 25 : loss : 0.8875 = 0.4224 + 0.4636 + 0.0014, time: 21.095626]
2023-05-11 19:22:35.340: epoch 25:	0.02865753  	0.07662577  	0.06323995  
2023-05-11 19:22:35.340: Find a better model.
2023-05-11 19:22:56.657: [iter 26 : loss : 0.8579 = 0.3789 + 0.4774 + 0.0017, time: 21.312424]
2023-05-11 19:22:56.920: epoch 26:	0.02861312  	0.07727843  	0.06352161  
2023-05-11 19:22:56.921: Find a better model.
2023-05-11 19:23:18.390: [iter 27 : loss : 0.8092 = 0.3402 + 0.4671 + 0.0020, time: 21.466060]
2023-05-11 19:23:18.653: epoch 27:	0.02879821  	0.07751513  	0.06353074  
2023-05-11 19:23:18.653: Find a better model.
2023-05-11 19:23:39.856: [iter 28 : loss : 0.7895 = 0.3080 + 0.4792 + 0.0023, time: 21.198722]
2023-05-11 19:23:40.140: epoch 28:	0.02887965  	0.07850577  	0.06388493  
2023-05-11 19:23:40.140: Find a better model.
2023-05-11 19:24:01.397: [iter 29 : loss : 0.7507 = 0.2802 + 0.4680 + 0.0025, time: 21.253547]
2023-05-11 19:24:01.660: epoch 29:	0.02900550  	0.07842344  	0.06425324  
2023-05-11 19:24:22.818: [iter 30 : loss : 0.7383 = 0.2563 + 0.4792 + 0.0028, time: 21.155096]
2023-05-11 19:24:23.081: epoch 30:	0.02896108  	0.07839032  	0.06429409  
2023-05-11 19:24:44.177: [iter 31 : loss : 0.7068 = 0.2361 + 0.4676 + 0.0031, time: 21.091388]
2023-05-11 19:24:44.440: epoch 31:	0.02908694  	0.07856397  	0.06453335  
2023-05-11 19:24:44.441: Find a better model.
2023-05-11 19:25:05.585: [iter 32 : loss : 0.7005 = 0.2190 + 0.4782 + 0.0033, time: 21.140677]
2023-05-11 19:25:05.851: epoch 32:	0.02919799  	0.07904328  	0.06486345  
2023-05-11 19:25:05.851: Find a better model.
2023-05-11 19:25:27.167: [iter 33 : loss : 0.6743 = 0.2044 + 0.4663 + 0.0035, time: 21.312554]
2023-05-11 19:25:27.427: epoch 33:	0.02939047  	0.07923242  	0.06522590  
2023-05-11 19:25:27.427: Find a better model.
2023-05-11 19:25:48.444: [iter 34 : loss : 0.6720 = 0.1913 + 0.4769 + 0.0038, time: 21.012653]
2023-05-11 19:25:48.706: epoch 34:	0.02928682  	0.07939594  	0.06544593  
2023-05-11 19:25:48.706: Find a better model.
2023-05-11 19:26:10.030: [iter 35 : loss : 0.6477 = 0.1791 + 0.4647 + 0.0040, time: 21.319169]
2023-05-11 19:26:10.297: epoch 35:	0.02943488  	0.07923896  	0.06560615  
2023-05-11 19:26:31.584: [iter 36 : loss : 0.6486 = 0.1690 + 0.4755 + 0.0042, time: 21.283417]
2023-05-11 19:26:31.850: epoch 36:	0.02942749  	0.07947706  	0.06574473  
2023-05-11 19:26:31.850: Find a better model.
2023-05-11 19:26:52.924: [iter 37 : loss : 0.6279 = 0.1600 + 0.4635 + 0.0044, time: 21.070687]
2023-05-11 19:26:53.218: epoch 37:	0.02939047  	0.07945784  	0.06583551  
2023-05-11 19:27:14.383: [iter 38 : loss : 0.6305 = 0.1517 + 0.4742 + 0.0046, time: 21.162231]
2023-05-11 19:27:14.665: epoch 38:	0.02931644  	0.07887889  	0.06576124  
2023-05-11 19:27:36.777: [iter 39 : loss : 0.6118 = 0.1448 + 0.4623 + 0.0048, time: 22.107751]
2023-05-11 19:27:37.039: epoch 39:	0.02930903  	0.07870376  	0.06574387  
2023-05-11 19:27:58.164: [iter 40 : loss : 0.6155 = 0.1375 + 0.4731 + 0.0050, time: 21.121793]
2023-05-11 19:27:58.425: epoch 40:	0.02939046  	0.07910701  	0.06585132  
2023-05-11 19:28:19.722: [iter 41 : loss : 0.5969 = 0.1308 + 0.4610 + 0.0051, time: 21.294076]
2023-05-11 19:28:19.983: epoch 41:	0.02941267  	0.07920759  	0.06589419  
2023-05-11 19:28:41.161: [iter 42 : loss : 0.6025 = 0.1252 + 0.4720 + 0.0053, time: 21.173686]
2023-05-11 19:28:41.421: epoch 42:	0.02938305  	0.07899362  	0.06584509  
2023-05-11 19:29:02.691: [iter 43 : loss : 0.5859 = 0.1203 + 0.4601 + 0.0055, time: 21.264872]
2023-05-11 19:29:02.955: epoch 43:	0.02947930  	0.07927660  	0.06599687  
2023-05-11 19:29:24.144: [iter 44 : loss : 0.5928 = 0.1162 + 0.4710 + 0.0057, time: 21.184112]
2023-05-11 19:29:24.405: epoch 44:	0.02959035  	0.07964712  	0.06596719  
2023-05-11 19:29:24.405: Find a better model.
2023-05-11 19:29:46.448: [iter 45 : loss : 0.5763 = 0.1114 + 0.4591 + 0.0058, time: 22.038119]
2023-05-11 19:29:46.734: epoch 45:	0.02958295  	0.07938688  	0.06594177  
2023-05-11 19:30:07.923: [iter 46 : loss : 0.5835 = 0.1074 + 0.4701 + 0.0060, time: 21.185971]
2023-05-11 19:30:08.208: epoch 46:	0.02954592  	0.07914487  	0.06590862  
2023-05-11 19:30:29.430: [iter 47 : loss : 0.5677 = 0.1034 + 0.4582 + 0.0061, time: 21.218626]
2023-05-11 19:30:29.709: epoch 47:	0.02956073  	0.07892504  	0.06598037  
2023-05-11 19:30:51.197: [iter 48 : loss : 0.5753 = 0.0997 + 0.4693 + 0.0063, time: 21.482575]
2023-05-11 19:30:51.461: epoch 48:	0.02947189  	0.07872085  	0.06568040  
2023-05-11 19:31:12.667: [iter 49 : loss : 0.5608 = 0.0969 + 0.4574 + 0.0064, time: 21.202368]
2023-05-11 19:31:12.928: epoch 49:	0.02944968  	0.07855672  	0.06544901  
2023-05-11 19:31:34.092: [iter 50 : loss : 0.5691 = 0.0939 + 0.4686 + 0.0066, time: 21.160333]
2023-05-11 19:31:34.359: epoch 50:	0.02946449  	0.07894278  	0.06551807  
2023-05-11 19:31:55.709: [iter 51 : loss : 0.5546 = 0.0910 + 0.4569 + 0.0067, time: 21.347412]
2023-05-11 19:31:55.994: epoch 51:	0.02936084  	0.07839002  	0.06538787  
2023-05-11 19:32:17.078: [iter 52 : loss : 0.5625 = 0.0877 + 0.4680 + 0.0069, time: 21.079705]
2023-05-11 19:32:17.354: epoch 52:	0.02927200  	0.07835951  	0.06524938  
2023-05-11 19:32:38.816: [iter 53 : loss : 0.5495 = 0.0863 + 0.4563 + 0.0070, time: 21.457733]
2023-05-11 19:32:39.080: epoch 53:	0.02924239  	0.07827207  	0.06520326  
2023-05-11 19:33:00.120: [iter 54 : loss : 0.5583 = 0.0838 + 0.4674 + 0.0071, time: 21.036564]
2023-05-11 19:33:00.385: epoch 54:	0.02922758  	0.07806213  	0.06512933  
2023-05-11 19:33:21.627: [iter 55 : loss : 0.5444 = 0.0815 + 0.4557 + 0.0073, time: 21.237309]
2023-05-11 19:33:21.907: epoch 55:	0.02927200  	0.07845188  	0.06528696  
2023-05-11 19:33:43.273: [iter 56 : loss : 0.5532 = 0.0788 + 0.4670 + 0.0074, time: 21.363000]
2023-05-11 19:33:43.554: epoch 56:	0.02905731  	0.07726640  	0.06455799  
2023-05-11 19:34:04.954: [iter 57 : loss : 0.5394 = 0.0767 + 0.4552 + 0.0075, time: 21.394203]
2023-05-11 19:34:05.257: epoch 57:	0.02904989  	0.07706091  	0.06446384  
2023-05-11 19:34:26.274: [iter 58 : loss : 0.5494 = 0.0751 + 0.4667 + 0.0076, time: 21.011068]
2023-05-11 19:34:26.553: epoch 58:	0.02885741  	0.07687964  	0.06420794  
2023-05-11 19:34:48.002: [iter 59 : loss : 0.5355 = 0.0729 + 0.4548 + 0.0078, time: 21.444141]
2023-05-11 19:34:48.307: epoch 59:	0.02883521  	0.07656647  	0.06410607  
2023-05-11 19:35:09.430: [iter 60 : loss : 0.5457 = 0.0716 + 0.4662 + 0.0079, time: 21.118762]
2023-05-11 19:35:09.696: epoch 60:	0.02867234  	0.07623367  	0.06384899  
2023-05-11 19:35:31.201: [iter 61 : loss : 0.5322 = 0.0698 + 0.4545 + 0.0080, time: 21.499185]
2023-05-11 19:35:31.481: epoch 61:	0.02862792  	0.07584944  	0.06364506  
2023-05-11 19:35:53.165: [iter 62 : loss : 0.5431 = 0.0690 + 0.4660 + 0.0081, time: 21.680499]
2023-05-11 19:35:53.443: epoch 62:	0.02862050  	0.07563335  	0.06362439  
2023-05-11 19:36:15.131: [iter 63 : loss : 0.5299 = 0.0675 + 0.4543 + 0.0082, time: 21.682495]
2023-05-11 19:36:15.412: epoch 63:	0.02859829  	0.07564094  	0.06362335  
2023-05-11 19:36:36.809: [iter 64 : loss : 0.5398 = 0.0659 + 0.4656 + 0.0083, time: 21.393498]
2023-05-11 19:36:37.090: epoch 64:	0.02856128  	0.07541082  	0.06336579  
2023-05-11 19:36:58.914: [iter 65 : loss : 0.5264 = 0.0642 + 0.4538 + 0.0084, time: 21.820009]
2023-05-11 19:36:59.194: epoch 65:	0.02854647  	0.07519238  	0.06327230  
2023-05-11 19:37:20.777: [iter 66 : loss : 0.5371 = 0.0632 + 0.4653 + 0.0086, time: 21.579930]
2023-05-11 19:37:21.058: epoch 66:	0.02839841  	0.07496209  	0.06304997  
2023-05-11 19:37:42.462: [iter 67 : loss : 0.5240 = 0.0619 + 0.4535 + 0.0087, time: 21.399461]
2023-05-11 19:37:42.744: epoch 67:	0.02845763  	0.07479342  	0.06306113  
2023-05-11 19:38:04.374: [iter 68 : loss : 0.5348 = 0.0610 + 0.4650 + 0.0088, time: 21.627709]
2023-05-11 19:38:04.658: epoch 68:	0.02833919  	0.07466053  	0.06297188  
2023-05-11 19:38:28.522: [iter 69 : loss : 0.5218 = 0.0597 + 0.4532 + 0.0089, time: 23.859156]
2023-05-11 19:38:28.790: epoch 69:	0.02829476  	0.07453058  	0.06284954  
2023-05-11 19:38:28.790: Early stopping is trigger at epoch: 69
2023-05-11 19:38:28.790: best_result@epoch 44:

2023-05-11 19:38:28.790: 		0.0296      	0.0796      	0.0660      
2023-05-11 19:39:17.001: my pid: 9748
2023-05-11 19:39:17.001: model: model.general_recommender.SGL
2023-05-11 19:39:17.001: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 19:39:17.001: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 19:39:20.413: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 19:39:41.437: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.023177]
2023-05-11 19:39:41.699: epoch 1:	0.00130293  	0.00261738  	0.00220958  
2023-05-11 19:39:41.699: Find a better model.
2023-05-11 19:40:03.090: [iter 2 : loss : 1.1367 = 0.6930 + 0.4437 + 0.0000, time: 21.387971]
2023-05-11 19:40:03.379: epoch 2:	0.00132514  	0.00260325  	0.00210066  
2023-05-11 19:40:24.862: [iter 3 : loss : 1.1348 = 0.6929 + 0.4418 + 0.0000, time: 21.479783]
2023-05-11 19:40:25.144: epoch 3:	0.00183595  	0.00305834  	0.00270736  
2023-05-11 19:40:25.144: Find a better model.
2023-05-11 19:40:46.301: [iter 4 : loss : 1.1376 = 0.6929 + 0.4447 + 0.0000, time: 21.153160]
2023-05-11 19:40:46.584: epoch 4:	0.00196180  	0.00379100  	0.00297201  
2023-05-11 19:40:46.584: Find a better model.
2023-05-11 19:41:08.055: [iter 5 : loss : 1.1354 = 0.6927 + 0.4427 + 0.0000, time: 21.466825]
2023-05-11 19:41:08.355: epoch 5:	0.00242079  	0.00461637  	0.00387264  
2023-05-11 19:41:08.355: Find a better model.
2023-05-11 19:41:29.668: [iter 6 : loss : 1.1384 = 0.6926 + 0.4457 + 0.0000, time: 21.309191]
2023-05-11 19:41:29.974: epoch 6:	0.00282795  	0.00515703  	0.00393104  
2023-05-11 19:41:29.974: Find a better model.
2023-05-11 19:41:51.261: [iter 7 : loss : 1.1359 = 0.6924 + 0.4434 + 0.0000, time: 21.283964]
2023-05-11 19:41:51.539: epoch 7:	0.00339799  	0.00639892  	0.00513128  
2023-05-11 19:41:51.539: Find a better model.
2023-05-11 19:42:12.878: [iter 8 : loss : 1.1389 = 0.6922 + 0.4466 + 0.0000, time: 21.335506]
2023-05-11 19:42:13.162: epoch 8:	0.00344981  	0.00622445  	0.00518820  
2023-05-11 19:42:34.420: [iter 9 : loss : 1.1359 = 0.6919 + 0.4440 + 0.0000, time: 21.254335]
2023-05-11 19:42:34.726: epoch 9:	0.00439739  	0.00972539  	0.00753617  
2023-05-11 19:42:34.726: Find a better model.
2023-05-11 19:42:55.879: [iter 10 : loss : 1.1394 = 0.6916 + 0.4478 + 0.0000, time: 21.149135]
2023-05-11 19:42:56.180: epoch 10:	0.00422712  	0.00900973  	0.00733693  
2023-05-11 19:43:17.396: [iter 11 : loss : 1.1358 = 0.6910 + 0.4447 + 0.0000, time: 21.211967]
2023-05-11 19:43:17.672: epoch 11:	0.00555965  	0.01239132  	0.00994290  
2023-05-11 19:43:17.672: Find a better model.
2023-05-11 19:43:40.230: [iter 12 : loss : 1.1393 = 0.6901 + 0.4491 + 0.0000, time: 22.553824]
2023-05-11 19:43:40.521: epoch 12:	0.00590019  	0.01344420  	0.01065484  
2023-05-11 19:43:40.521: Find a better model.
2023-05-11 19:44:01.772: [iter 13 : loss : 1.1344 = 0.6888 + 0.4455 + 0.0000, time: 21.247911]
2023-05-11 19:44:02.047: epoch 13:	0.00753625  	0.01829403  	0.01525835  
2023-05-11 19:44:02.047: Find a better model.
2023-05-11 19:44:23.064: [iter 14 : loss : 1.1381 = 0.6872 + 0.4509 + 0.0000, time: 21.012213]
2023-05-11 19:44:23.357: epoch 14:	0.00908346  	0.02153812  	0.01843152  
2023-05-11 19:44:23.358: Find a better model.
2023-05-11 19:44:44.364: [iter 15 : loss : 1.1314 = 0.6849 + 0.4464 + 0.0001, time: 21.002903]
2023-05-11 19:44:44.640: epoch 15:	0.01157830  	0.02872621  	0.02457956  
2023-05-11 19:44:44.640: Find a better model.
2023-05-11 19:45:05.798: [iter 16 : loss : 1.1346 = 0.6815 + 0.4530 + 0.0001, time: 21.154243]
2023-05-11 19:45:06.068: epoch 16:	0.01380665  	0.03298224  	0.02912438  
2023-05-11 19:45:06.068: Find a better model.
2023-05-11 19:45:27.110: [iter 17 : loss : 1.1239 = 0.6763 + 0.4475 + 0.0001, time: 21.036750]
2023-05-11 19:45:27.405: epoch 17:	0.01741200  	0.04388691  	0.03795164  
2023-05-11 19:45:27.405: Find a better model.
2023-05-11 19:45:48.357: [iter 18 : loss : 1.1237 = 0.6675 + 0.4560 + 0.0002, time: 20.948160]
2023-05-11 19:45:48.631: epoch 18:	0.02027704  	0.05090054  	0.04447572  
2023-05-11 19:45:48.631: Find a better model.
2023-05-11 19:46:09.730: [iter 19 : loss : 1.1031 = 0.6536 + 0.4492 + 0.0002, time: 21.094507]
2023-05-11 19:46:09.993: epoch 19:	0.02326795  	0.05922211  	0.05117046  
2023-05-11 19:46:09.993: Find a better model.
2023-05-11 19:46:30.718: [iter 20 : loss : 1.0927 = 0.6324 + 0.4601 + 0.0003, time: 20.719854]
2023-05-11 19:46:30.986: epoch 20:	0.02517059  	0.06443588  	0.05543188  
2023-05-11 19:46:30.986: Find a better model.
2023-05-11 19:46:51.886: [iter 21 : loss : 1.0547 = 0.6015 + 0.4528 + 0.0005, time: 20.896527]
2023-05-11 19:46:52.151: epoch 21:	0.02665124  	0.06932715  	0.05884232  
2023-05-11 19:46:52.151: Find a better model.
2023-05-11 19:47:13.108: [iter 22 : loss : 1.0280 = 0.5615 + 0.4659 + 0.0007, time: 20.952914]
2023-05-11 19:47:13.396: epoch 22:	0.02746559  	0.07222211  	0.06089265  
2023-05-11 19:47:13.397: Find a better model.
2023-05-11 19:47:34.266: [iter 23 : loss : 0.9732 = 0.5139 + 0.4584 + 0.0009, time: 20.863683]
2023-05-11 19:47:34.537: epoch 23:	0.02803565  	0.07430468  	0.06219459  
2023-05-11 19:47:34.537: Find a better model.
2023-05-11 19:47:55.360: [iter 24 : loss : 0.9380 = 0.4641 + 0.4728 + 0.0012, time: 20.819256]
2023-05-11 19:47:55.621: epoch 24:	0.02823554  	0.07546666  	0.06264152  
2023-05-11 19:47:55.621: Find a better model.
2023-05-11 19:48:16.683: [iter 25 : loss : 0.8817 = 0.4158 + 0.4644 + 0.0014, time: 21.057579]
2023-05-11 19:48:16.944: epoch 25:	0.02836881  	0.07618839  	0.06280302  
2023-05-11 19:48:16.944: Find a better model.
2023-05-11 19:48:37.892: [iter 26 : loss : 0.8521 = 0.3725 + 0.4778 + 0.0017, time: 20.944005]
2023-05-11 19:48:38.153: epoch 26:	0.02861313  	0.07700454  	0.06302106  
2023-05-11 19:48:38.153: Find a better model.
2023-05-11 19:48:59.258: [iter 27 : loss : 0.8045 = 0.3349 + 0.4676 + 0.0020, time: 21.101397]
2023-05-11 19:48:59.529: epoch 27:	0.02879820  	0.07737581  	0.06322733  
2023-05-11 19:48:59.529: Find a better model.
2023-05-11 19:49:20.668: [iter 28 : loss : 0.7850 = 0.3030 + 0.4796 + 0.0023, time: 21.134415]
2023-05-11 19:49:20.933: epoch 28:	0.02885744  	0.07802781  	0.06366083  
2023-05-11 19:49:20.933: Find a better model.
2023-05-11 19:49:42.036: [iter 29 : loss : 0.7472 = 0.2760 + 0.4686 + 0.0026, time: 21.098198]
2023-05-11 19:49:42.296: epoch 29:	0.02906472  	0.07860370  	0.06397852  
2023-05-11 19:49:42.296: Find a better model.
2023-05-11 19:50:03.144: [iter 30 : loss : 0.7350 = 0.2525 + 0.4796 + 0.0028, time: 20.844242]
2023-05-11 19:50:03.423: epoch 30:	0.02919058  	0.07932350  	0.06434854  
2023-05-11 19:50:03.423: Find a better model.
2023-05-11 19:50:24.399: [iter 31 : loss : 0.7040 = 0.2330 + 0.4678 + 0.0031, time: 20.972803]
2023-05-11 19:50:24.658: epoch 31:	0.02942748  	0.07959224  	0.06470052  
2023-05-11 19:50:24.658: Find a better model.
2023-05-11 19:50:45.514: [iter 32 : loss : 0.6981 = 0.2162 + 0.4786 + 0.0033, time: 20.850493]
2023-05-11 19:50:45.773: epoch 32:	0.02942009  	0.07961166  	0.06482869  
2023-05-11 19:50:45.774: Find a better model.
2023-05-11 19:51:06.841: [iter 33 : loss : 0.6724 = 0.2023 + 0.4665 + 0.0036, time: 21.063782]
2023-05-11 19:51:07.107: epoch 33:	0.02976063  	0.08031769  	0.06546240  
2023-05-11 19:51:07.107: Find a better model.
2023-05-11 19:51:28.072: [iter 34 : loss : 0.6703 = 0.1892 + 0.4773 + 0.0038, time: 20.960849]
2023-05-11 19:51:28.330: epoch 34:	0.02972361  	0.08039373  	0.06547770  
2023-05-11 19:51:28.330: Find a better model.
2023-05-11 19:51:49.207: [iter 35 : loss : 0.6461 = 0.1770 + 0.4651 + 0.0040, time: 20.874406]
2023-05-11 19:51:49.484: epoch 35:	0.02979024  	0.08056064  	0.06556306  
2023-05-11 19:51:49.484: Find a better model.
2023-05-11 19:52:10.274: [iter 36 : loss : 0.6470 = 0.1670 + 0.4758 + 0.0042, time: 20.786661]
2023-05-11 19:52:10.542: epoch 36:	0.02973841  	0.08045673  	0.06552482  
2023-05-11 19:52:31.576: [iter 37 : loss : 0.6268 = 0.1586 + 0.4638 + 0.0044, time: 21.030066]
2023-05-11 19:52:31.843: epoch 37:	0.02981245  	0.08005093  	0.06571068  
2023-05-11 19:52:52.662: [iter 38 : loss : 0.6294 = 0.1502 + 0.4745 + 0.0046, time: 20.813462]
2023-05-11 19:52:52.921: epoch 38:	0.02978283  	0.08019529  	0.06582732  
2023-05-11 19:53:13.940: [iter 39 : loss : 0.6107 = 0.1435 + 0.4624 + 0.0048, time: 21.015705]
2023-05-11 19:53:14.198: epoch 39:	0.02981985  	0.08020330  	0.06586785  
2023-05-11 19:53:35.211: [iter 40 : loss : 0.6144 = 0.1362 + 0.4732 + 0.0050, time: 21.009696]
2023-05-11 19:53:35.483: epoch 40:	0.02984206  	0.08002067  	0.06597417  
2023-05-11 19:53:56.389: [iter 41 : loss : 0.5963 = 0.1298 + 0.4613 + 0.0051, time: 20.901455]
2023-05-11 19:53:56.654: epoch 41:	0.02986427  	0.08028576  	0.06615384  
2023-05-11 19:54:17.764: [iter 42 : loss : 0.6021 = 0.1245 + 0.4723 + 0.0053, time: 21.105366]
2023-05-11 19:54:18.026: epoch 42:	0.02981985  	0.08011084  	0.06617410  
2023-05-11 19:54:39.113: [iter 43 : loss : 0.5853 = 0.1195 + 0.4603 + 0.0055, time: 21.083893]
2023-05-11 19:54:39.395: epoch 43:	0.02983466  	0.08027885  	0.06625932  
2023-05-11 19:55:00.193: [iter 44 : loss : 0.5922 = 0.1152 + 0.4714 + 0.0057, time: 20.793927]
2023-05-11 19:55:00.466: epoch 44:	0.02974582  	0.07973721  	0.06625012  
2023-05-11 19:55:21.501: [iter 45 : loss : 0.5758 = 0.1106 + 0.4594 + 0.0058, time: 21.030396]
2023-05-11 19:55:21.769: epoch 45:	0.02988648  	0.07994871  	0.06636044  
2023-05-11 19:55:42.782: [iter 46 : loss : 0.5831 = 0.1065 + 0.4706 + 0.0060, time: 21.008861]
2023-05-11 19:55:43.045: epoch 46:	0.02978284  	0.07966086  	0.06615948  
2023-05-11 19:56:04.094: [iter 47 : loss : 0.5675 = 0.1028 + 0.4586 + 0.0061, time: 21.045618]
2023-05-11 19:56:04.376: epoch 47:	0.02970140  	0.07939561  	0.06617685  
2023-05-11 19:56:25.168: [iter 48 : loss : 0.5757 = 0.0997 + 0.4697 + 0.0063, time: 20.787920]
2023-05-11 19:56:25.441: epoch 48:	0.02961256  	0.07886145  	0.06597819  
2023-05-11 19:56:46.517: [iter 49 : loss : 0.5604 = 0.0961 + 0.4578 + 0.0064, time: 21.072834]
2023-05-11 19:56:46.779: epoch 49:	0.02968659  	0.07887697  	0.06606136  
2023-05-11 19:57:07.714: [iter 50 : loss : 0.5687 = 0.0933 + 0.4689 + 0.0066, time: 20.930176]
2023-05-11 19:57:07.974: epoch 50:	0.02951631  	0.07830124  	0.06576422  
2023-05-11 19:57:28.895: [iter 51 : loss : 0.5543 = 0.0904 + 0.4571 + 0.0067, time: 20.917471]
2023-05-11 19:57:29.153: epoch 51:	0.02955332  	0.07863709  	0.06587297  
2023-05-11 19:57:50.161: [iter 52 : loss : 0.5629 = 0.0875 + 0.4685 + 0.0069, time: 21.003696]
2023-05-11 19:57:50.437: epoch 52:	0.02951630  	0.07859457  	0.06563857  
2023-05-11 19:58:11.279: [iter 53 : loss : 0.5496 = 0.0861 + 0.4565 + 0.0070, time: 20.837703]
2023-05-11 19:58:11.546: epoch 53:	0.02951630  	0.07837696  	0.06552407  
2023-05-11 19:58:32.704: [iter 54 : loss : 0.5583 = 0.0833 + 0.4678 + 0.0071, time: 21.154299]
2023-05-11 19:58:32.962: epoch 54:	0.02952370  	0.07806809  	0.06538055  
2023-05-11 19:58:54.070: [iter 55 : loss : 0.5443 = 0.0811 + 0.4559 + 0.0073, time: 21.102551]
2023-05-11 19:58:54.330: epoch 55:	0.02944967  	0.07791615  	0.06532411  
2023-05-11 19:59:15.121: [iter 56 : loss : 0.5534 = 0.0786 + 0.4673 + 0.0074, time: 20.786545]
2023-05-11 19:59:15.403: epoch 56:	0.02944227  	0.07805622  	0.06531415  
2023-05-11 19:59:36.275: [iter 57 : loss : 0.5395 = 0.0765 + 0.4555 + 0.0075, time: 20.869514]
2023-05-11 19:59:36.540: epoch 57:	0.02961254  	0.07812335  	0.06542152  
2023-05-11 19:59:57.694: [iter 58 : loss : 0.5499 = 0.0751 + 0.4671 + 0.0076, time: 21.149498]
2023-05-11 19:59:57.950: epoch 58:	0.02947929  	0.07777094  	0.06536645  
2023-05-11 20:00:18.817: [iter 59 : loss : 0.5356 = 0.0728 + 0.4551 + 0.0078, time: 20.863187]
2023-05-11 20:00:19.083: epoch 59:	0.02936083  	0.07761737  	0.06520189  
2023-05-11 20:00:40.056: [iter 60 : loss : 0.5457 = 0.0713 + 0.4666 + 0.0079, time: 20.968637]
2023-05-11 20:00:40.314: epoch 60:	0.02944227  	0.07774006  	0.06524956  
2023-05-11 20:00:40.314: Early stopping is trigger at epoch: 60
2023-05-11 20:00:40.314: best_result@epoch 35:

2023-05-11 20:00:40.314: 		0.0298      	0.0806      	0.0656      
2023-05-11 20:12:16.643: my pid: 14232
2023-05-11 20:12:16.643: model: model.general_recommender.SGL
2023-05-11 20:12:16.643: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 20:12:16.643: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 20:12:20.006: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 20:12:41.567: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.560102]
2023-05-11 20:12:41.833: epoch 1:	0.00117708  	0.00260327  	0.00207208  
2023-05-11 20:12:41.834: Find a better model.
2023-05-11 20:13:03.430: [iter 2 : loss : 1.1369 = 0.6930 + 0.4438 + 0.0000, time: 21.591727]
2023-05-11 20:13:03.712: epoch 2:	0.00148801  	0.00260133  	0.00217150  
2023-05-11 20:13:25.385: [iter 3 : loss : 1.1346 = 0.6929 + 0.4417 + 0.0000, time: 21.669116]
2023-05-11 20:13:25.668: epoch 3:	0.00163607  	0.00306666  	0.00263217  
2023-05-11 20:13:25.669: Find a better model.
2023-05-11 20:13:47.055: [iter 4 : loss : 1.1376 = 0.6929 + 0.4448 + 0.0000, time: 21.382475]
2023-05-11 20:13:47.360: epoch 4:	0.00202103  	0.00400121  	0.00314725  
2023-05-11 20:13:47.360: Find a better model.
2023-05-11 20:14:08.954: [iter 5 : loss : 1.1353 = 0.6928 + 0.4425 + 0.0000, time: 21.590547]
2023-05-11 20:14:09.234: epoch 5:	0.00225052  	0.00486809  	0.00404667  
2023-05-11 20:14:09.234: Find a better model.
2023-05-11 20:14:30.960: [iter 6 : loss : 1.1384 = 0.6926 + 0.4457 + 0.0000, time: 21.721610]
2023-05-11 20:14:31.242: epoch 6:	0.00261327  	0.00481313  	0.00380916  
2023-05-11 20:14:52.931: [iter 7 : loss : 1.1356 = 0.6925 + 0.4431 + 0.0000, time: 21.685508]
2023-05-11 20:14:53.211: epoch 7:	0.00310927  	0.00636454  	0.00491965  
2023-05-11 20:14:53.211: Find a better model.
2023-05-11 20:15:14.949: [iter 8 : loss : 1.1390 = 0.6923 + 0.4467 + 0.0000, time: 21.734427]
2023-05-11 20:15:15.229: epoch 8:	0.00318330  	0.00715893  	0.00551182  
2023-05-11 20:15:15.229: Find a better model.
2023-05-11 20:15:36.912: [iter 9 : loss : 1.1358 = 0.6920 + 0.4438 + 0.0000, time: 21.679755]
2023-05-11 20:15:37.186: epoch 9:	0.00402724  	0.00937626  	0.00746042  
2023-05-11 20:15:37.187: Find a better model.
2023-05-11 20:15:58.796: [iter 10 : loss : 1.1394 = 0.6916 + 0.4477 + 0.0000, time: 21.605473]
2023-05-11 20:15:59.073: epoch 10:	0.00419011  	0.00983505  	0.00775294  
2023-05-11 20:15:59.073: Find a better model.
2023-05-11 20:16:20.497: [iter 11 : loss : 1.1356 = 0.6911 + 0.4444 + 0.0000, time: 21.418323]
2023-05-11 20:16:20.778: epoch 11:	0.00541900  	0.01311080  	0.01052114  
2023-05-11 20:16:20.778: Find a better model.
2023-05-11 20:16:42.378: [iter 12 : loss : 1.1394 = 0.6903 + 0.4491 + 0.0000, time: 21.596990]
2023-05-11 20:16:42.654: epoch 12:	0.00534497  	0.01217398  	0.01001050  
2023-05-11 20:17:04.096: [iter 13 : loss : 1.1343 = 0.6891 + 0.4452 + 0.0000, time: 21.437284]
2023-05-11 20:17:04.394: epoch 13:	0.00691439  	0.01726247  	0.01409642  
2023-05-11 20:17:04.394: Find a better model.
2023-05-11 20:17:25.760: [iter 14 : loss : 1.1383 = 0.6875 + 0.4508 + 0.0000, time: 21.362506]
2023-05-11 20:17:26.028: epoch 14:	0.00785457  	0.01909050  	0.01630939  
2023-05-11 20:17:26.029: Find a better model.
2023-05-11 20:17:47.464: [iter 15 : loss : 1.1316 = 0.6855 + 0.4461 + 0.0001, time: 21.429940]
2023-05-11 20:17:47.732: epoch 15:	0.01030498  	0.02618220  	0.02238365  
2023-05-11 20:17:47.732: Find a better model.
2023-05-11 20:18:08.917: [iter 16 : loss : 1.1353 = 0.6824 + 0.4529 + 0.0001, time: 21.181327]
2023-05-11 20:18:09.187: epoch 16:	0.01268137  	0.03171357  	0.02676750  
2023-05-11 20:18:09.188: Find a better model.
2023-05-11 20:18:30.510: [iter 17 : loss : 1.1248 = 0.6777 + 0.4471 + 0.0001, time: 21.318752]
2023-05-11 20:18:30.780: epoch 17:	0.01596838  	0.03967795  	0.03429655  
2023-05-11 20:18:30.780: Find a better model.
2023-05-11 20:18:51.901: [iter 18 : loss : 1.1257 = 0.6699 + 0.4557 + 0.0001, time: 21.116763]
2023-05-11 20:18:52.176: epoch 18:	0.01850769  	0.04678855  	0.04055463  
2023-05-11 20:18:52.176: Find a better model.
2023-05-11 20:19:13.474: [iter 19 : loss : 1.1062 = 0.6573 + 0.4486 + 0.0002, time: 21.293977]
2023-05-11 20:19:13.748: epoch 19:	0.02157263  	0.05573643  	0.04762295  
2023-05-11 20:19:13.748: Find a better model.
2023-05-11 20:19:34.931: [iter 20 : loss : 1.0981 = 0.6381 + 0.4597 + 0.0003, time: 21.179536]
2023-05-11 20:19:35.196: epoch 20:	0.02363071  	0.06095791  	0.05212921  
2023-05-11 20:19:35.196: Find a better model.
2023-05-11 20:19:56.240: [iter 21 : loss : 1.0619 = 0.6096 + 0.4518 + 0.0004, time: 21.040608]
2023-05-11 20:19:56.518: epoch 21:	0.02603678  	0.06761124  	0.05666996  
2023-05-11 20:19:56.518: Find a better model.
2023-05-11 20:20:17.698: [iter 22 : loss : 1.0377 = 0.5718 + 0.4653 + 0.0006, time: 21.175828]
2023-05-11 20:20:17.966: epoch 22:	0.02673269  	0.07024161  	0.05877990  
2023-05-11 20:20:17.966: Find a better model.
2023-05-11 20:20:39.309: [iter 23 : loss : 0.9839 = 0.5258 + 0.4573 + 0.0008, time: 21.337829]
2023-05-11 20:20:39.594: epoch 23:	0.02783577  	0.07336453  	0.06062225  
2023-05-11 20:20:39.594: Find a better model.
2023-05-11 20:21:01.291: [iter 24 : loss : 0.9500 = 0.4767 + 0.4723 + 0.0011, time: 21.692952]
2023-05-11 20:21:01.580: epoch 24:	0.02822815  	0.07487606  	0.06154857  
2023-05-11 20:21:01.580: Find a better model.
2023-05-11 20:21:23.132: [iter 25 : loss : 0.8927 = 0.4280 + 0.4633 + 0.0014, time: 21.547569]
2023-05-11 20:21:23.434: epoch 25:	0.02865754  	0.07620957  	0.06248154  
2023-05-11 20:21:23.434: Find a better model.
2023-05-11 20:21:44.688: [iter 26 : loss : 0.8631 = 0.3839 + 0.4776 + 0.0017, time: 21.249991]
2023-05-11 20:21:44.950: epoch 26:	0.02866495  	0.07716423  	0.06289470  
2023-05-11 20:21:44.950: Find a better model.
2023-05-11 20:22:06.151: [iter 27 : loss : 0.8138 = 0.3449 + 0.4670 + 0.0019, time: 21.196895]
2023-05-11 20:22:06.430: epoch 27:	0.02891665  	0.07785421  	0.06345399  
2023-05-11 20:22:06.431: Find a better model.
2023-05-11 20:22:27.440: [iter 28 : loss : 0.7941 = 0.3121 + 0.4798 + 0.0022, time: 21.004953]
2023-05-11 20:22:27.703: epoch 28:	0.02886483  	0.07775611  	0.06349857  
2023-05-11 20:22:49.353: [iter 29 : loss : 0.7544 = 0.2840 + 0.4679 + 0.0025, time: 21.646369]
2023-05-11 20:22:49.651: epoch 29:	0.02915356  	0.07818889  	0.06394707  
2023-05-11 20:22:49.651: Find a better model.
2023-05-11 20:23:11.311: [iter 30 : loss : 0.7418 = 0.2594 + 0.4796 + 0.0028, time: 21.651018]
2023-05-11 20:23:11.589: epoch 30:	0.02908693  	0.07865285  	0.06410586  
2023-05-11 20:23:11.589: Find a better model.
2023-05-11 20:23:33.312: [iter 31 : loss : 0.7094 = 0.2390 + 0.4674 + 0.0030, time: 21.719268]
2023-05-11 20:23:33.590: epoch 31:	0.02927942  	0.07970924  	0.06450599  
2023-05-11 20:23:33.590: Find a better model.
2023-05-11 20:23:55.322: [iter 32 : loss : 0.7036 = 0.2217 + 0.4786 + 0.0033, time: 21.728121]
2023-05-11 20:23:55.596: epoch 32:	0.02939787  	0.07981139  	0.06484842  
2023-05-11 20:23:55.596: Find a better model.
2023-05-11 20:24:17.452: [iter 33 : loss : 0.6767 = 0.2069 + 0.4663 + 0.0035, time: 21.852671]
2023-05-11 20:24:17.729: epoch 33:	0.02962737  	0.08057351  	0.06512977  
2023-05-11 20:24:17.729: Find a better model.
2023-05-11 20:24:39.519: [iter 34 : loss : 0.6744 = 0.1933 + 0.4774 + 0.0037, time: 21.786209]
2023-05-11 20:24:39.787: epoch 34:	0.02956075  	0.08025393  	0.06511282  
2023-05-11 20:25:01.638: [iter 35 : loss : 0.6494 = 0.1808 + 0.4647 + 0.0039, time: 21.847839]
2023-05-11 20:25:01.925: epoch 35:	0.02968659  	0.08057525  	0.06524071  
2023-05-11 20:25:01.926: Find a better model.
2023-05-11 20:25:23.719: [iter 36 : loss : 0.6507 = 0.1707 + 0.4758 + 0.0042, time: 21.789277]
2023-05-11 20:25:23.987: epoch 36:	0.02983465  	0.08104140  	0.06543708  
2023-05-11 20:25:23.987: Find a better model.
2023-05-11 20:25:45.916: [iter 37 : loss : 0.6296 = 0.1617 + 0.4636 + 0.0044, time: 21.924272]
2023-05-11 20:25:46.188: epoch 37:	0.02987906  	0.08179881  	0.06569147  
2023-05-11 20:25:46.188: Find a better model.
2023-05-11 20:26:07.861: [iter 38 : loss : 0.6325 = 0.1532 + 0.4747 + 0.0046, time: 21.669135]
2023-05-11 20:26:08.125: epoch 38:	0.02984205  	0.08134459  	0.06562478  
2023-05-11 20:26:29.966: [iter 39 : loss : 0.6129 = 0.1460 + 0.4622 + 0.0048, time: 21.837301]
2023-05-11 20:26:30.265: epoch 39:	0.02993828  	0.08131096  	0.06570198  
2023-05-11 20:26:52.010: [iter 40 : loss : 0.6171 = 0.1388 + 0.4733 + 0.0049, time: 21.740346]
2023-05-11 20:26:52.273: epoch 40:	0.02996790  	0.08132526  	0.06577186  
2023-05-11 20:27:14.110: [iter 41 : loss : 0.5984 = 0.1322 + 0.4612 + 0.0051, time: 21.832921]
2023-05-11 20:27:14.375: epoch 41:	0.03001232  	0.08134784  	0.06579716  
2023-05-11 20:27:35.782: [iter 42 : loss : 0.6043 = 0.1265 + 0.4725 + 0.0053, time: 21.403644]
2023-05-11 20:27:36.047: epoch 42:	0.02990127  	0.08132813  	0.06572951  
2023-05-11 20:27:57.967: [iter 43 : loss : 0.5868 = 0.1212 + 0.4602 + 0.0055, time: 21.915081]
2023-05-11 20:27:58.238: epoch 43:	0.02996790  	0.08139705  	0.06582007  
2023-05-11 20:28:20.682: [iter 44 : loss : 0.5943 = 0.1173 + 0.4714 + 0.0056, time: 22.439656]
2023-05-11 20:28:20.950: epoch 44:	0.03000490  	0.08143520  	0.06572647  
2023-05-11 20:28:57.251: my pid: 3192
2023-05-11 20:28:57.251: model: model.general_recommender.SGL
2023-05-11 20:28:57.251: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 20:28:57.251: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 20:29:00.885: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 20:29:22.436: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.550386]
2023-05-11 20:29:22.710: epoch 1:	0.00112526  	0.00252860  	0.00201618  
2023-05-11 20:29:22.710: Find a better model.
2023-05-11 20:29:44.300: [iter 2 : loss : 1.1369 = 0.6930 + 0.4438 + 0.0000, time: 21.586046]
2023-05-11 20:29:44.600: epoch 2:	0.00150281  	0.00258858  	0.00217047  
2023-05-11 20:29:44.600: Find a better model.
2023-05-11 20:30:06.496: [iter 3 : loss : 1.1346 = 0.6929 + 0.4416 + 0.0000, time: 21.893518]
2023-05-11 20:30:06.785: epoch 3:	0.00168789  	0.00353206  	0.00266443  
2023-05-11 20:30:06.785: Find a better model.
2023-05-11 20:30:28.275: [iter 4 : loss : 1.1376 = 0.6929 + 0.4447 + 0.0000, time: 21.485500]
2023-05-11 20:30:28.594: epoch 4:	0.00187297  	0.00293843  	0.00262830  
2023-05-11 20:30:50.197: [iter 5 : loss : 1.1352 = 0.6927 + 0.4425 + 0.0000, time: 21.600047]
2023-05-11 20:30:50.482: epoch 5:	0.00217649  	0.00408904  	0.00324234  
2023-05-11 20:30:50.482: Find a better model.
2023-05-11 20:31:12.067: [iter 6 : loss : 1.1384 = 0.6926 + 0.4457 + 0.0000, time: 21.581226]
2023-05-11 20:31:12.353: epoch 6:	0.00250222  	0.00415677  	0.00368257  
2023-05-11 20:31:12.353: Find a better model.
2023-05-11 20:31:34.007: [iter 7 : loss : 1.1356 = 0.6925 + 0.4431 + 0.0000, time: 21.648978]
2023-05-11 20:31:34.291: epoch 7:	0.00322031  	0.00624738  	0.00516794  
2023-05-11 20:31:34.291: Find a better model.
2023-05-11 20:31:55.804: [iter 8 : loss : 1.1390 = 0.6923 + 0.4467 + 0.0000, time: 21.509474]
2023-05-11 20:31:56.088: epoch 8:	0.00333136  	0.00587655  	0.00504347  
2023-05-11 20:32:17.571: [iter 9 : loss : 1.1358 = 0.6920 + 0.4438 + 0.0000, time: 21.480785]
2023-05-11 20:32:17.850: epoch 9:	0.00418271  	0.00802130  	0.00669232  
2023-05-11 20:32:17.850: Find a better model.
2023-05-11 20:32:39.445: [iter 10 : loss : 1.1394 = 0.6916 + 0.4477 + 0.0000, time: 21.590425]
2023-05-11 20:32:39.738: epoch 10:	0.00432336  	0.00888514  	0.00731478  
2023-05-11 20:32:39.738: Find a better model.
2023-05-11 20:33:01.344: [iter 11 : loss : 1.1356 = 0.6911 + 0.4445 + 0.0000, time: 21.602691]
2023-05-11 20:33:01.641: epoch 11:	0.00481936  	0.01068054  	0.00895128  
2023-05-11 20:33:01.642: Find a better model.
2023-05-11 20:33:22.982: [iter 12 : loss : 1.1393 = 0.6902 + 0.4491 + 0.0000, time: 21.337363]
2023-05-11 20:33:23.256: epoch 12:	0.00506366  	0.01019620  	0.00916149  
2023-05-11 20:33:44.753: [iter 13 : loss : 1.1343 = 0.6890 + 0.4452 + 0.0000, time: 21.493497]
2023-05-11 20:33:45.028: epoch 13:	0.00667009  	0.01466501  	0.01289483  
2023-05-11 20:33:45.028: Find a better model.
2023-05-11 20:34:06.426: [iter 14 : loss : 1.1383 = 0.6875 + 0.4508 + 0.0000, time: 21.394459]
2023-05-11 20:34:06.710: epoch 14:	0.00783976  	0.01763201  	0.01541079  
2023-05-11 20:34:06.710: Find a better model.
2023-05-11 20:34:29.518: [iter 15 : loss : 1.1316 = 0.6855 + 0.4461 + 0.0001, time: 22.804049]
2023-05-11 20:34:29.792: epoch 15:	0.01029017  	0.02425046  	0.02129700  
2023-05-11 20:34:29.792: Find a better model.
2023-05-11 20:34:51.132: [iter 16 : loss : 1.1352 = 0.6823 + 0.4528 + 0.0001, time: 21.335436]
2023-05-11 20:34:51.407: epoch 16:	0.01234082  	0.02904050  	0.02621564  
2023-05-11 20:34:51.407: Find a better model.
2023-05-11 20:35:12.891: [iter 17 : loss : 1.1248 = 0.6776 + 0.4470 + 0.0001, time: 21.480302]
2023-05-11 20:35:13.158: epoch 17:	0.01573148  	0.03795310  	0.03435045  
2023-05-11 20:35:13.158: Find a better model.
2023-05-11 20:35:34.605: [iter 18 : loss : 1.1256 = 0.6698 + 0.4557 + 0.0001, time: 21.441524]
2023-05-11 20:35:34.875: epoch 18:	0.01858171  	0.04471150  	0.04045305  
2023-05-11 20:35:34.875: Find a better model.
2023-05-11 20:35:56.292: [iter 19 : loss : 1.1061 = 0.6572 + 0.4487 + 0.0002, time: 21.413464]
2023-05-11 20:35:56.560: epoch 19:	0.02173549  	0.05390495  	0.04741580  
2023-05-11 20:35:56.560: Find a better model.
2023-05-11 20:36:17.911: [iter 20 : loss : 1.0980 = 0.6379 + 0.4598 + 0.0003, time: 21.346338]
2023-05-11 20:36:18.180: epoch 20:	0.02374916  	0.05961097  	0.05211649  
2023-05-11 20:36:18.180: Find a better model.
2023-05-11 20:36:39.306: [iter 21 : loss : 1.0615 = 0.6091 + 0.4520 + 0.0004, time: 21.122124]
2023-05-11 20:36:39.576: epoch 21:	0.02584429  	0.06591062  	0.05635982  
2023-05-11 20:36:39.576: Find a better model.
2023-05-11 20:37:00.757: [iter 22 : loss : 1.0370 = 0.5710 + 0.4654 + 0.0006, time: 21.177962]
2023-05-11 20:37:01.020: epoch 22:	0.02639214  	0.06794285  	0.05803448  
2023-05-11 20:37:01.020: Find a better model.
2023-05-11 20:37:22.420: [iter 23 : loss : 0.9833 = 0.5249 + 0.4576 + 0.0008, time: 21.395710]
2023-05-11 20:37:22.700: epoch 23:	0.02728794  	0.07122877  	0.06018471  
2023-05-11 20:37:22.700: Find a better model.
2023-05-11 20:37:43.900: [iter 24 : loss : 0.9492 = 0.4757 + 0.4724 + 0.0011, time: 21.196685]
2023-05-11 20:37:44.159: epoch 24:	0.02741381  	0.07280557  	0.06123123  
2023-05-11 20:37:44.159: Find a better model.
2023-05-11 20:38:05.475: [iter 25 : loss : 0.8924 = 0.4273 + 0.4637 + 0.0014, time: 21.312555]
2023-05-11 20:38:05.749: epoch 25:	0.02811712  	0.07435961  	0.06209165  
2023-05-11 20:38:05.749: Find a better model.
2023-05-11 20:38:27.134: [iter 26 : loss : 0.8626 = 0.3833 + 0.4776 + 0.0017, time: 21.379440]
2023-05-11 20:38:27.394: epoch 26:	0.02826518  	0.07544360  	0.06272584  
2023-05-11 20:38:27.394: Find a better model.
2023-05-11 20:38:48.677: [iter 27 : loss : 0.8137 = 0.3445 + 0.4672 + 0.0020, time: 21.279277]
2023-05-11 20:38:48.937: epoch 27:	0.02852429  	0.07613605  	0.06305712  
2023-05-11 20:38:48.937: Find a better model.
2023-05-11 20:39:10.257: [iter 28 : loss : 0.7938 = 0.3117 + 0.4799 + 0.0022, time: 21.314986]
2023-05-11 20:39:10.519: epoch 28:	0.02869456  	0.07672098  	0.06333638  
2023-05-11 20:39:10.519: Find a better model.
2023-05-11 20:39:31.833: [iter 29 : loss : 0.7546 = 0.2839 + 0.4682 + 0.0025, time: 21.311205]
2023-05-11 20:39:32.093: epoch 29:	0.02890926  	0.07747497  	0.06377653  
2023-05-11 20:39:32.093: Find a better model.
2023-05-11 20:39:53.306: [iter 30 : loss : 0.7420 = 0.2595 + 0.4798 + 0.0028, time: 21.210350]
2023-05-11 20:39:53.568: epoch 30:	0.02924980  	0.07862546  	0.06427418  
2023-05-11 20:39:53.569: Find a better model.
2023-05-11 20:40:14.786: [iter 31 : loss : 0.7097 = 0.2390 + 0.4677 + 0.0030, time: 21.213061]
2023-05-11 20:40:15.051: epoch 31:	0.02933125  	0.07889012  	0.06427693  
2023-05-11 20:40:15.051: Find a better model.
2023-05-11 20:40:36.444: [iter 32 : loss : 0.7039 = 0.2219 + 0.4787 + 0.0033, time: 21.389491]
2023-05-11 20:40:36.719: epoch 32:	0.02930163  	0.07879446  	0.06437317  
2023-05-11 20:40:58.206: [iter 33 : loss : 0.6768 = 0.2069 + 0.4664 + 0.0035, time: 21.483701]
2023-05-11 20:40:58.465: epoch 33:	0.02930903  	0.07866778  	0.06435978  
2023-05-11 20:41:19.882: [iter 34 : loss : 0.6745 = 0.1935 + 0.4773 + 0.0037, time: 21.412698]
2023-05-11 20:41:20.147: epoch 34:	0.02945708  	0.07918324  	0.06465305  
2023-05-11 20:41:20.147: Find a better model.
2023-05-11 20:41:41.578: [iter 35 : loss : 0.6500 = 0.1811 + 0.4650 + 0.0040, time: 21.427900]
2023-05-11 20:41:41.841: epoch 35:	0.02948670  	0.07883608  	0.06468735  
2023-05-11 20:42:03.235: [iter 36 : loss : 0.6512 = 0.1711 + 0.4760 + 0.0042, time: 21.389840]
2023-05-11 20:42:03.495: epoch 36:	0.02952372  	0.07942014  	0.06489252  
2023-05-11 20:42:03.495: Find a better model.
2023-05-11 20:42:24.980: [iter 37 : loss : 0.6302 = 0.1621 + 0.4637 + 0.0044, time: 21.481231]
2023-05-11 20:42:25.244: epoch 37:	0.02942007  	0.07935818  	0.06488120  
2023-05-11 20:42:46.859: [iter 38 : loss : 0.6328 = 0.1536 + 0.4747 + 0.0046, time: 21.611115]
2023-05-11 20:42:47.121: epoch 38:	0.02947930  	0.07968386  	0.06495324  
2023-05-11 20:42:47.121: Find a better model.
2023-05-11 20:43:08.554: [iter 39 : loss : 0.6133 = 0.1462 + 0.4623 + 0.0048, time: 21.429254]
2023-05-11 20:43:08.819: epoch 39:	0.02951632  	0.07980991  	0.06500761  
2023-05-11 20:43:08.819: Find a better model.
2023-05-11 20:43:30.196: [iter 40 : loss : 0.6171 = 0.1389 + 0.4732 + 0.0049, time: 21.372567]
2023-05-11 20:43:30.456: epoch 40:	0.02950151  	0.07979001  	0.06510419  
2023-05-11 20:43:52.092: [iter 41 : loss : 0.5986 = 0.1325 + 0.4611 + 0.0051, time: 21.633155]
2023-05-11 20:43:52.350: epoch 41:	0.02956073  	0.07960533  	0.06529306  
2023-05-11 20:44:13.911: [iter 42 : loss : 0.6041 = 0.1265 + 0.4724 + 0.0053, time: 21.557184]
2023-05-11 20:44:14.194: epoch 42:	0.02966438  	0.08034647  	0.06549239  
2023-05-11 20:44:14.194: Find a better model.
2023-05-11 20:44:35.910: [iter 43 : loss : 0.5870 = 0.1213 + 0.4602 + 0.0055, time: 21.711782]
2023-05-11 20:44:36.189: epoch 43:	0.02958295  	0.07992741  	0.06541054  
2023-05-11 20:44:58.185: [iter 44 : loss : 0.5946 = 0.1176 + 0.4714 + 0.0056, time: 21.991175]
2023-05-11 20:44:58.448: epoch 44:	0.02950152  	0.07949537  	0.06515947  
2023-05-11 20:45:20.138: [iter 45 : loss : 0.5774 = 0.1124 + 0.4591 + 0.0058, time: 21.684900]
2023-05-11 20:45:20.416: epoch 45:	0.02935345  	0.07877426  	0.06490929  
2023-05-11 20:45:41.984: [iter 46 : loss : 0.5848 = 0.1083 + 0.4705 + 0.0060, time: 21.563883]
2023-05-11 20:45:42.264: epoch 46:	0.02930162  	0.07895785  	0.06488666  
2023-05-11 20:46:03.921: [iter 47 : loss : 0.5688 = 0.1044 + 0.4583 + 0.0061, time: 21.653828]
2023-05-11 20:46:04.185: epoch 47:	0.02923499  	0.07830722  	0.06475850  
2023-05-11 20:46:26.311: [iter 48 : loss : 0.5771 = 0.1010 + 0.4699 + 0.0063, time: 22.121191]
2023-05-11 20:46:26.574: epoch 48:	0.02927199  	0.07815319  	0.06481098  
2023-05-11 20:47:19.200: my pid: 15264
2023-05-11 20:47:19.200: model: model.general_recommender.SGL
2023-05-11 20:47:19.200: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 20:47:19.200: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 20:47:22.553: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 20:47:43.458: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.905679]
2023-05-11 20:47:43.757: epoch 1:	0.00116968  	0.00270139  	0.00208631  
2023-05-11 20:47:43.757: Find a better model.
2023-05-11 20:48:05.330: [iter 2 : loss : 1.1362 = 0.6930 + 0.4432 + 0.0000, time: 21.569904]
2023-05-11 20:48:05.721: epoch 2:	0.00168049  	0.00258303  	0.00233200  
2023-05-11 20:48:37.246: [iter 3 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 31.517483]
2023-05-11 20:48:37.643: epoch 3:	0.00205064  	0.00402240  	0.00309366  
2023-05-11 20:48:37.644: Find a better model.
2023-05-11 20:49:08.881: [iter 4 : loss : 1.1371 = 0.6928 + 0.4443 + 0.0000, time: 31.231441]
2023-05-11 20:49:09.276: epoch 4:	0.00221350  	0.00336870  	0.00296726  
2023-05-11 20:49:40.404: [iter 5 : loss : 1.1357 = 0.6927 + 0.4429 + 0.0000, time: 31.123207]
2023-05-11 20:49:40.703: epoch 5:	0.00282795  	0.00513562  	0.00456558  
2023-05-11 20:49:40.703: Find a better model.
2023-05-11 20:50:02.499: [iter 6 : loss : 1.1378 = 0.6926 + 0.4453 + 0.0000, time: 21.791972]
2023-05-11 20:50:02.796: epoch 6:	0.00299082  	0.00501110  	0.00442636  
2023-05-11 20:50:24.072: [iter 7 : loss : 1.1361 = 0.6924 + 0.4437 + 0.0000, time: 21.272201]
2023-05-11 20:50:24.418: epoch 7:	0.00373112  	0.00740259  	0.00634968  
2023-05-11 20:50:24.418: Find a better model.
2023-05-11 20:50:46.268: [iter 8 : loss : 1.1384 = 0.6922 + 0.4463 + 0.0000, time: 21.844383]
2023-05-11 20:50:46.564: epoch 8:	0.00380515  	0.00801298  	0.00655425  
2023-05-11 20:50:46.564: Find a better model.
2023-05-11 20:51:07.797: [iter 9 : loss : 1.1362 = 0.6918 + 0.4444 + 0.0000, time: 21.229951]
2023-05-11 20:51:08.079: epoch 9:	0.00523392  	0.01237472  	0.00999826  
2023-05-11 20:51:08.079: Find a better model.
2023-05-11 20:51:29.249: [iter 10 : loss : 1.1388 = 0.6913 + 0.4475 + 0.0000, time: 21.166009]
2023-05-11 20:51:29.555: epoch 10:	0.00524873  	0.01090497  	0.00953504  
2023-05-11 20:51:50.751: [iter 11 : loss : 1.1357 = 0.6905 + 0.4452 + 0.0000, time: 21.192873]
2023-05-11 20:51:51.063: epoch 11:	0.00639619  	0.01501535  	0.01243115  
2023-05-11 20:51:51.063: Find a better model.
2023-05-11 20:52:12.562: [iter 12 : loss : 1.1383 = 0.6893 + 0.4489 + 0.0000, time: 21.493575]
2023-05-11 20:52:12.850: epoch 12:	0.00729195  	0.01576496  	0.01340309  
2023-05-11 20:52:12.850: Find a better model.
2023-05-11 20:52:34.384: [iter 13 : loss : 1.1340 = 0.6879 + 0.4461 + 0.0000, time: 21.530724]
2023-05-11 20:52:34.681: epoch 13:	0.00945361  	0.02264226  	0.01898532  
2023-05-11 20:52:34.682: Find a better model.
2023-05-11 20:52:55.998: [iter 14 : loss : 1.1367 = 0.6858 + 0.4508 + 0.0001, time: 21.313035]
2023-05-11 20:52:56.277: epoch 14:	0.01131920  	0.02601596  	0.02279036  
2023-05-11 20:52:56.277: Find a better model.
2023-05-11 20:53:17.463: [iter 15 : loss : 1.1300 = 0.6829 + 0.4471 + 0.0001, time: 21.180773]
2023-05-11 20:53:17.745: epoch 15:	0.01398432  	0.03424045  	0.02933441  
2023-05-11 20:53:17.746: Find a better model.
2023-05-11 20:53:38.855: [iter 16 : loss : 1.1313 = 0.6781 + 0.4531 + 0.0001, time: 21.105196]
2023-05-11 20:53:39.140: epoch 16:	0.01639777  	0.04012550  	0.03497108  
2023-05-11 20:53:39.140: Find a better model.
2023-05-11 20:54:00.614: [iter 17 : loss : 1.1191 = 0.6706 + 0.4483 + 0.0001, time: 21.469203]
2023-05-11 20:54:00.898: epoch 17:	0.01933682  	0.04897738  	0.04224213  
2023-05-11 20:54:00.898: Find a better model.
2023-05-11 20:54:22.109: [iter 18 : loss : 1.1148 = 0.6582 + 0.4564 + 0.0002, time: 21.206280]
2023-05-11 20:54:22.387: epoch 18:	0.02169107  	0.05528131  	0.04796400  
2023-05-11 20:54:22.387: Find a better model.
2023-05-11 20:54:43.582: [iter 19 : loss : 1.0895 = 0.6387 + 0.4505 + 0.0003, time: 21.190967]
2023-05-11 20:54:43.857: epoch 19:	0.02430443  	0.06214564  	0.05399970  
2023-05-11 20:54:43.858: Find a better model.
2023-05-11 20:55:04.975: [iter 20 : loss : 1.0716 = 0.6103 + 0.4609 + 0.0004, time: 21.113942]
2023-05-11 20:55:05.291: epoch 20:	0.02582948  	0.06685218  	0.05745620  
2023-05-11 20:55:05.291: Find a better model.
2023-05-11 20:55:26.946: [iter 21 : loss : 1.0271 = 0.5715 + 0.4550 + 0.0006, time: 21.649933]
2023-05-11 20:55:27.228: epoch 21:	0.02696960  	0.07104357  	0.06050238  
2023-05-11 20:55:27.228: Find a better model.
2023-05-11 20:55:48.198: [iter 22 : loss : 0.9931 = 0.5249 + 0.4674 + 0.0008, time: 20.966307]
2023-05-11 20:55:48.536: epoch 22:	0.02774696  	0.07384763  	0.06205641  
2023-05-11 20:55:48.536: Find a better model.
2023-05-11 20:56:09.600: [iter 23 : loss : 0.9367 = 0.4745 + 0.4611 + 0.0011, time: 21.061065]
2023-05-11 20:56:09.876: epoch 23:	0.02837622  	0.07547221  	0.06337918  
2023-05-11 20:56:09.876: Find a better model.
2023-05-11 20:56:31.518: [iter 24 : loss : 0.9003 = 0.4252 + 0.4738 + 0.0014, time: 21.636489]
2023-05-11 20:56:31.822: epoch 24:	0.02850207  	0.07701690  	0.06396013  
2023-05-11 20:56:31.822: Find a better model.
2023-05-11 20:56:53.747: [iter 25 : loss : 0.8478 = 0.3799 + 0.4663 + 0.0017, time: 21.920809]
2023-05-11 20:56:54.039: epoch 25:	0.02876858  	0.07814534  	0.06470089  
2023-05-11 20:56:54.039: Find a better model.
2023-05-11 20:57:15.743: [iter 26 : loss : 0.8206 = 0.3411 + 0.4776 + 0.0019, time: 21.699155]
2023-05-11 20:57:16.038: epoch 26:	0.02899068  	0.07940980  	0.06511083  
2023-05-11 20:57:16.038: Find a better model.
2023-05-11 20:57:38.012: [iter 27 : loss : 0.7781 = 0.3074 + 0.4684 + 0.0022, time: 21.969960]
2023-05-11 20:57:38.326: epoch 27:	0.02912395  	0.07931936  	0.06508176  
2023-05-11 20:58:00.024: [iter 28 : loss : 0.7606 = 0.2795 + 0.4786 + 0.0025, time: 21.693601]
2023-05-11 20:58:00.311: epoch 28:	0.02915356  	0.07902683  	0.06508297  
2023-05-11 20:58:22.150: [iter 29 : loss : 0.7271 = 0.2556 + 0.4686 + 0.0028, time: 21.834749]
2023-05-11 20:58:22.438: epoch 29:	0.02929420  	0.07944632  	0.06538659  
2023-05-11 20:58:22.438: Find a better model.
2023-05-11 20:58:43.960: [iter 30 : loss : 0.7157 = 0.2348 + 0.4779 + 0.0030, time: 21.519625]
2023-05-11 20:58:44.342: epoch 30:	0.02935343  	0.07987364  	0.06558072  
2023-05-11 20:58:44.342: Find a better model.
2023-05-11 20:59:06.492: [iter 31 : loss : 0.6883 = 0.2174 + 0.4676 + 0.0033, time: 22.145080]
2023-05-11 20:59:06.782: epoch 31:	0.02945708  	0.08026667  	0.06584548  
2023-05-11 20:59:06.782: Find a better model.
2023-05-11 20:59:28.586: [iter 32 : loss : 0.6827 = 0.2024 + 0.4767 + 0.0035, time: 21.799519]
2023-05-11 20:59:28.873: epoch 32:	0.02942747  	0.08032735  	0.06584391  
2023-05-11 20:59:28.873: Find a better model.
2023-05-11 20:59:50.838: [iter 33 : loss : 0.6600 = 0.1899 + 0.4663 + 0.0037, time: 21.961513]
2023-05-11 20:59:51.124: epoch 33:	0.02942747  	0.07997777  	0.06564347  
2023-05-11 21:00:12.846: [iter 34 : loss : 0.6576 = 0.1783 + 0.4753 + 0.0040, time: 21.716831]
2023-05-11 21:00:13.146: epoch 34:	0.02957554  	0.08029583  	0.06564101  
2023-05-11 21:00:34.784: [iter 35 : loss : 0.6365 = 0.1676 + 0.4648 + 0.0042, time: 21.630857]
2023-05-11 21:00:35.112: epoch 35:	0.02959035  	0.07998077  	0.06542695  
2023-05-11 21:00:56.847: [iter 36 : loss : 0.6369 = 0.1586 + 0.4740 + 0.0044, time: 21.731261]
2023-05-11 21:00:57.194: epoch 36:	0.02934603  	0.07923896  	0.06504525  
2023-05-11 21:01:18.808: [iter 37 : loss : 0.6191 = 0.1509 + 0.4636 + 0.0046, time: 21.609691]
2023-05-11 21:01:19.116: epoch 37:	0.02940525  	0.07938043  	0.06513313  
2023-05-11 21:02:13.296: my pid: 8352
2023-05-11 21:02:13.296: model: model.general_recommender.SGL
2023-05-11 21:02:13.296: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 21:02:13.297: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 21:02:16.782: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 21:02:39.092: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.309336]
2023-05-11 21:02:39.401: epoch 1:	0.00080693  	0.00164983  	0.00134304  
2023-05-11 21:02:39.401: Find a better model.
2023-05-11 21:03:02.282: [iter 2 : loss : 1.1369 = 0.6930 + 0.4438 + 0.0000, time: 22.877040]
2023-05-11 21:03:02.615: epoch 2:	0.00135475  	0.00253070  	0.00206740  
2023-05-11 21:03:02.615: Find a better model.
2023-05-11 21:03:25.676: [iter 3 : loss : 1.1343 = 0.6930 + 0.4414 + 0.0000, time: 23.056767]
2023-05-11 21:03:25.985: epoch 3:	0.00147320  	0.00268783  	0.00220971  
2023-05-11 21:03:25.985: Find a better model.
2023-05-11 21:03:48.822: [iter 4 : loss : 1.1375 = 0.6929 + 0.4447 + 0.0000, time: 22.833093]
2023-05-11 21:03:49.123: epoch 4:	0.00162866  	0.00279800  	0.00224134  
2023-05-11 21:03:49.123: Find a better model.
2023-05-11 21:04:11.973: [iter 5 : loss : 1.1349 = 0.6928 + 0.4421 + 0.0000, time: 22.846387]
2023-05-11 21:04:12.278: epoch 5:	0.00216168  	0.00419046  	0.00340506  
2023-05-11 21:04:12.278: Find a better model.
2023-05-11 21:04:35.064: [iter 6 : loss : 1.1383 = 0.6927 + 0.4456 + 0.0000, time: 22.782902]
2023-05-11 21:04:35.375: epoch 6:	0.00233936  	0.00457651  	0.00346584  
2023-05-11 21:04:35.375: Find a better model.
2023-05-11 21:04:58.151: [iter 7 : loss : 1.1353 = 0.6925 + 0.4428 + 0.0000, time: 22.771884]
2023-05-11 21:04:58.466: epoch 7:	0.00267989  	0.00508104  	0.00392748  
2023-05-11 21:04:58.466: Find a better model.
2023-05-11 21:05:21.235: [iter 8 : loss : 1.1389 = 0.6923 + 0.4466 + 0.0000, time: 22.763757]
2023-05-11 21:05:21.567: epoch 8:	0.00278354  	0.00551744  	0.00410196  
2023-05-11 21:05:21.568: Find a better model.
2023-05-11 21:05:43.553: [iter 9 : loss : 1.1354 = 0.6921 + 0.4434 + 0.0000, time: 21.981709]
2023-05-11 21:05:43.861: epoch 9:	0.00349423  	0.00790093  	0.00576411  
2023-05-11 21:05:43.861: Find a better model.
2023-05-11 21:06:06.876: [iter 10 : loss : 1.1394 = 0.6917 + 0.4476 + 0.0000, time: 23.010571]
2023-05-11 21:06:07.179: epoch 10:	0.00342760  	0.00690950  	0.00550182  
2023-05-11 21:06:30.043: [iter 11 : loss : 1.1353 = 0.6913 + 0.4440 + 0.0000, time: 22.861326]
2023-05-11 21:06:30.357: epoch 11:	0.00433817  	0.00988414  	0.00791929  
2023-05-11 21:06:30.357: Find a better model.
2023-05-11 21:06:55.450: [iter 12 : loss : 1.1394 = 0.6905 + 0.4488 + 0.0000, time: 25.088877]
2023-05-11 21:06:55.768: epoch 12:	0.00456766  	0.01069564  	0.00859633  
2023-05-11 21:06:55.769: Find a better model.
2023-05-11 21:07:18.613: [iter 13 : loss : 1.1342 = 0.6894 + 0.4447 + 0.0000, time: 22.839909]
2023-05-11 21:07:18.924: epoch 13:	0.00610747  	0.01477486  	0.01201428  
2023-05-11 21:07:18.925: Find a better model.
2023-05-11 21:07:41.772: [iter 14 : loss : 1.1386 = 0.6880 + 0.4506 + 0.0000, time: 22.844591]
2023-05-11 21:07:42.057: epoch 14:	0.00681815  	0.01631206  	0.01350532  
2023-05-11 21:07:42.057: Find a better model.
2023-05-11 21:08:04.789: [iter 15 : loss : 1.1318 = 0.6862 + 0.4456 + 0.0001, time: 22.727423]
2023-05-11 21:08:05.067: epoch 15:	0.00879474  	0.02120715  	0.01833057  
2023-05-11 21:08:05.068: Find a better model.
2023-05-11 21:08:27.797: [iter 16 : loss : 1.1361 = 0.6835 + 0.4526 + 0.0001, time: 22.726454]
2023-05-11 21:08:28.085: epoch 16:	0.01047525  	0.02548458  	0.02188584  
2023-05-11 21:08:28.085: Find a better model.
2023-05-11 21:08:50.736: [iter 17 : loss : 1.1260 = 0.6794 + 0.4464 + 0.0001, time: 22.646549]
2023-05-11 21:08:51.030: epoch 17:	0.01368821  	0.03445359  	0.02956269  
2023-05-11 21:08:51.030: Find a better model.
2023-05-11 21:09:13.643: [iter 18 : loss : 1.1281 = 0.6727 + 0.4552 + 0.0001, time: 22.608053]
2023-05-11 21:09:13.928: epoch 18:	0.01650140  	0.04162387  	0.03565088  
2023-05-11 21:09:13.928: Find a better model.
2023-05-11 21:09:36.672: [iter 19 : loss : 1.1100 = 0.6619 + 0.4479 + 0.0002, time: 22.740053]
2023-05-11 21:09:36.956: epoch 19:	0.02009196  	0.05091671  	0.04332736  
2023-05-11 21:09:36.956: Find a better model.
2023-05-11 21:09:59.751: [iter 20 : loss : 1.1043 = 0.6450 + 0.4590 + 0.0003, time: 22.790422]
2023-05-11 21:10:00.032: epoch 20:	0.02251285  	0.05762904  	0.04880266  
2023-05-11 21:10:00.032: Find a better model.
2023-05-11 21:10:22.647: [iter 21 : loss : 1.0706 = 0.6194 + 0.4508 + 0.0004, time: 22.611242]
2023-05-11 21:10:22.952: epoch 21:	0.02496331  	0.06429248  	0.05388687  
2023-05-11 21:10:22.952: Find a better model.
2023-05-11 21:10:45.687: [iter 22 : loss : 1.0495 = 0.5845 + 0.4644 + 0.0006, time: 22.730425]
2023-05-11 21:10:45.965: epoch 22:	0.02622188  	0.06826964  	0.05683898  
2023-05-11 21:10:45.965: Find a better model.
2023-05-11 21:11:08.548: [iter 23 : loss : 0.9974 = 0.5407 + 0.4560 + 0.0008, time: 22.579858]
2023-05-11 21:11:08.832: epoch 23:	0.02738417  	0.07159306  	0.05914631  
2023-05-11 21:11:08.832: Find a better model.
2023-05-11 21:11:31.404: [iter 24 : loss : 0.9648 = 0.4926 + 0.4712 + 0.0010, time: 22.567789]
2023-05-11 21:11:31.693: epoch 24:	0.02770251  	0.07368547  	0.06043785  
2023-05-11 21:11:31.694: Find a better model.
2023-05-11 21:11:54.096: [iter 25 : loss : 0.9071 = 0.4437 + 0.4621 + 0.0013, time: 22.398842]
2023-05-11 21:11:54.372: epoch 25:	0.02820594  	0.07547238  	0.06165646  
2023-05-11 21:11:54.372: Find a better model.
2023-05-11 21:12:16.901: [iter 26 : loss : 0.8769 = 0.3987 + 0.4766 + 0.0016, time: 22.525392]
2023-05-11 21:12:17.208: epoch 26:	0.02836884  	0.07643326  	0.06240343  
2023-05-11 21:12:17.208: Find a better model.
2023-05-11 21:12:39.831: [iter 27 : loss : 0.8263 = 0.3582 + 0.4662 + 0.0019, time: 22.617296]
2023-05-11 21:12:40.101: epoch 27:	0.02852429  	0.07638124  	0.06264831  
2023-05-11 21:13:02.728: [iter 28 : loss : 0.8050 = 0.3237 + 0.4792 + 0.0021, time: 22.623451]
2023-05-11 21:13:03.000: epoch 28:	0.02864273  	0.07720739  	0.06288358  
2023-05-11 21:13:03.000: Find a better model.
2023-05-11 21:13:25.570: [iter 29 : loss : 0.7643 = 0.2944 + 0.4675 + 0.0024, time: 22.567148]
2023-05-11 21:13:25.864: epoch 29:	0.02882041  	0.07813520  	0.06342284  
2023-05-11 21:13:25.864: Find a better model.
2023-05-11 21:13:47.712: [iter 30 : loss : 0.7510 = 0.2688 + 0.4795 + 0.0027, time: 21.844759]
2023-05-11 21:13:47.979: epoch 30:	0.02893146  	0.07877894  	0.06363786  
2023-05-11 21:13:47.979: Find a better model.
2023-05-11 21:14:09.534: [iter 31 : loss : 0.7174 = 0.2472 + 0.4673 + 0.0030, time: 21.551907]
2023-05-11 21:14:09.797: epoch 31:	0.02894627  	0.07842419  	0.06358301  
2023-05-11 21:14:31.392: [iter 32 : loss : 0.7112 = 0.2294 + 0.4786 + 0.0032, time: 21.589984]
2023-05-11 21:14:31.669: epoch 32:	0.02912396  	0.07916471  	0.06404136  
2023-05-11 21:14:31.669: Find a better model.
2023-05-11 21:14:53.324: [iter 33 : loss : 0.6833 = 0.2138 + 0.4660 + 0.0034, time: 21.650542]
2023-05-11 21:14:53.605: epoch 33:	0.02919059  	0.07981825  	0.06439811  
2023-05-11 21:14:53.605: Find a better model.
2023-05-11 21:15:15.320: [iter 34 : loss : 0.6803 = 0.1996 + 0.4771 + 0.0037, time: 21.711442]
2023-05-11 21:15:15.602: epoch 34:	0.02921279  	0.07970119  	0.06448178  
2023-05-11 21:15:37.301: [iter 35 : loss : 0.6551 = 0.1866 + 0.4646 + 0.0039, time: 21.694398]
2023-05-11 21:15:37.586: epoch 35:	0.02929422  	0.07971029  	0.06470603  
2023-05-11 21:15:59.153: [iter 36 : loss : 0.6559 = 0.1760 + 0.4758 + 0.0041, time: 21.562829]
2023-05-11 21:15:59.416: epoch 36:	0.02926460  	0.07941305  	0.06463487  
2023-05-11 21:16:21.268: [iter 37 : loss : 0.6340 = 0.1664 + 0.4633 + 0.0043, time: 21.847927]
2023-05-11 21:16:21.558: epoch 37:	0.02955333  	0.08036951  	0.06507595  
2023-05-11 21:16:21.558: Find a better model.
2023-05-11 21:16:43.139: [iter 38 : loss : 0.6367 = 0.1577 + 0.4746 + 0.0045, time: 21.577209]
2023-05-11 21:16:43.401: epoch 38:	0.02949411  	0.08024607  	0.06511387  
2023-05-11 21:17:04.861: [iter 39 : loss : 0.6168 = 0.1501 + 0.4620 + 0.0047, time: 21.455210]
2023-05-11 21:17:05.118: epoch 39:	0.02945708  	0.07955834  	0.06508023  
2023-05-11 21:17:26.759: [iter 40 : loss : 0.6208 = 0.1427 + 0.4732 + 0.0049, time: 21.636847]
2023-05-11 21:17:27.021: epoch 40:	0.02937564  	0.07941386  	0.06494111  
2023-05-11 21:17:48.672: [iter 41 : loss : 0.6015 = 0.1357 + 0.4608 + 0.0051, time: 21.647952]
2023-05-11 21:17:48.934: epoch 41:	0.02950890  	0.07971144  	0.06504034  
2023-05-11 21:18:10.519: [iter 42 : loss : 0.6069 = 0.1296 + 0.4721 + 0.0052, time: 21.581949]
2023-05-11 21:18:10.788: epoch 42:	0.02955331  	0.07953726  	0.06475733  
2023-05-11 21:18:32.448: [iter 43 : loss : 0.5893 = 0.1240 + 0.4598 + 0.0054, time: 21.657211]
2023-05-11 21:18:32.716: epoch 43:	0.02961993  	0.07960917  	0.06485925  
2023-05-11 21:18:54.501: [iter 44 : loss : 0.5972 = 0.1202 + 0.4714 + 0.0056, time: 21.780592]
2023-05-11 21:18:54.769: epoch 44:	0.02952370  	0.07933763  	0.06465320  
2023-05-11 21:19:16.406: [iter 45 : loss : 0.5795 = 0.1149 + 0.4588 + 0.0057, time: 21.632920]
2023-05-11 21:19:16.680: epoch 45:	0.02942005  	0.07871163  	0.06448404  
2023-05-11 21:19:38.487: [iter 46 : loss : 0.5869 = 0.1106 + 0.4704 + 0.0059, time: 21.803922]
2023-05-11 21:19:38.754: epoch 46:	0.02944966  	0.07885126  	0.06437937  
2023-05-11 21:20:00.427: [iter 47 : loss : 0.5704 = 0.1065 + 0.4579 + 0.0061, time: 21.668605]
2023-05-11 21:20:00.699: epoch 47:	0.02931641  	0.07814004  	0.06428185  
2023-05-11 21:20:22.274: [iter 48 : loss : 0.5786 = 0.1029 + 0.4696 + 0.0062, time: 21.571808]
2023-05-11 21:20:22.556: epoch 48:	0.02916834  	0.07743444  	0.06403454  
2023-05-11 21:20:44.372: [iter 49 : loss : 0.5630 = 0.0996 + 0.4571 + 0.0064, time: 21.813411]
2023-05-11 21:20:44.649: epoch 49:	0.02908691  	0.07699736  	0.06393749  
2023-05-11 21:21:06.421: [iter 50 : loss : 0.5718 = 0.0966 + 0.4687 + 0.0065, time: 21.767323]
2023-05-11 21:21:06.695: epoch 50:	0.02907210  	0.07699738  	0.06373375  
2023-05-11 21:21:28.198: [iter 51 : loss : 0.5563 = 0.0931 + 0.4565 + 0.0067, time: 21.499085]
2023-05-11 21:21:28.458: epoch 51:	0.02901288  	0.07675523  	0.06362186  
2023-05-11 21:21:50.204: [iter 52 : loss : 0.5650 = 0.0901 + 0.4682 + 0.0068, time: 21.742397]
2023-05-11 21:21:50.468: epoch 52:	0.02901288  	0.07689238  	0.06364757  
2023-05-11 21:22:12.184: [iter 53 : loss : 0.5514 = 0.0885 + 0.4560 + 0.0069, time: 21.713291]
2023-05-11 21:22:12.445: epoch 53:	0.02910912  	0.07705960  	0.06368147  
2023-05-11 21:22:34.044: [iter 54 : loss : 0.5608 = 0.0861 + 0.4676 + 0.0071, time: 21.595420]
2023-05-11 21:22:34.307: epoch 54:	0.02900548  	0.07665605  	0.06334074  
2023-05-11 21:22:55.962: [iter 55 : loss : 0.5461 = 0.0836 + 0.4554 + 0.0072, time: 21.652108]
2023-05-11 21:22:56.223: epoch 55:	0.02889444  	0.07664248  	0.06330984  
2023-05-11 21:23:18.007: [iter 56 : loss : 0.5552 = 0.0808 + 0.4671 + 0.0073, time: 21.781149]
2023-05-11 21:23:18.266: epoch 56:	0.02888703  	0.07635185  	0.06309666  
2023-05-11 21:23:39.928: [iter 57 : loss : 0.5408 = 0.0785 + 0.4549 + 0.0075, time: 21.657768]
2023-05-11 21:23:40.191: epoch 57:	0.02882040  	0.07625455  	0.06306131  
2023-05-11 21:24:01.999: [iter 58 : loss : 0.5511 = 0.0767 + 0.4668 + 0.0076, time: 21.804125]
2023-05-11 21:24:02.261: epoch 58:	0.02881300  	0.07618273  	0.06298443  
2023-05-11 21:24:23.914: [iter 59 : loss : 0.5369 = 0.0747 + 0.4545 + 0.0077, time: 21.647557]
2023-05-11 21:24:24.174: epoch 59:	0.02878339  	0.07602008  	0.06293617  
2023-05-11 21:24:45.776: [iter 60 : loss : 0.5473 = 0.0732 + 0.4663 + 0.0078, time: 21.598867]
2023-05-11 21:24:46.036: epoch 60:	0.02876118  	0.07599256  	0.06276558  
2023-05-11 21:25:07.697: [iter 61 : loss : 0.5335 = 0.0715 + 0.4541 + 0.0080, time: 21.657974]
2023-05-11 21:25:07.955: epoch 61:	0.02869456  	0.07571448  	0.06259800  
2023-05-11 21:25:29.757: [iter 62 : loss : 0.5444 = 0.0704 + 0.4659 + 0.0081, time: 21.797663]
2023-05-11 21:25:30.023: epoch 62:	0.02863532  	0.07533301  	0.06247286  
2023-05-11 21:25:30.023: Early stopping is trigger at epoch: 62
2023-05-11 21:25:30.023: best_result@epoch 37:

2023-05-11 21:25:30.023: 		0.0296      	0.0804      	0.0651      
2023-05-11 21:28:47.967: my pid: 12200
2023-05-11 21:28:47.967: model: model.general_recommender.SGL
2023-05-11 21:28:47.967: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-11 21:28:47.967: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-11 21:28:51.349: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-11 21:29:15.230: [iter 1 : loss : 1.1341 = 0.6931 + 0.4410 + 0.0000, time: 23.879347]
2023-05-11 21:29:15.496: epoch 1:	0.00068848  	0.00177989  	0.00127614  
2023-05-11 21:29:15.496: Find a better model.
2023-05-11 21:29:39.876: [iter 2 : loss : 1.1350 = 0.6930 + 0.4419 + 0.0000, time: 24.377364]
2023-05-11 21:29:40.168: epoch 2:	0.00075511  	0.00140618  	0.00108245  
2023-05-11 21:30:04.949: [iter 3 : loss : 1.1333 = 0.6930 + 0.4404 + 0.0000, time: 24.776457]
2023-05-11 21:30:05.247: epoch 3:	0.00129553  	0.00328402  	0.00217041  
2023-05-11 21:30:05.247: Find a better model.
2023-05-11 21:30:30.347: [iter 4 : loss : 1.1350 = 0.6929 + 0.4420 + 0.0000, time: 25.097003]
2023-05-11 21:30:30.634: epoch 4:	0.00110305  	0.00229424  	0.00183215  
2023-05-11 21:30:54.281: [iter 5 : loss : 1.1337 = 0.6928 + 0.4408 + 0.0000, time: 23.642470]
2023-05-11 21:30:54.567: epoch 5:	0.00163607  	0.00327875  	0.00277102  
2023-05-11 21:31:18.317: [iter 6 : loss : 1.1352 = 0.6927 + 0.4425 + 0.0000, time: 23.746107]
2023-05-11 21:31:18.605: epoch 6:	0.00180634  	0.00355332  	0.00294859  
2023-05-11 21:31:18.605: Find a better model.
2023-05-11 21:31:42.265: [iter 7 : loss : 1.1339 = 0.6926 + 0.4412 + 0.0000, time: 23.656254]
2023-05-11 21:31:42.549: epoch 7:	0.00227273  	0.00499361  	0.00361809  
2023-05-11 21:31:42.549: Find a better model.
2023-05-11 21:32:05.992: [iter 8 : loss : 1.1355 = 0.6925 + 0.4430 + 0.0000, time: 23.439348]
2023-05-11 21:32:06.291: epoch 8:	0.00220610  	0.00414605  	0.00317435  
2023-05-11 21:32:30.414: [iter 9 : loss : 1.1340 = 0.6923 + 0.4417 + 0.0000, time: 24.120281]
2023-05-11 21:32:30.699: epoch 9:	0.00280574  	0.00642573  	0.00462161  
2023-05-11 21:32:30.699: Find a better model.
2023-05-11 21:32:54.109: [iter 10 : loss : 1.1357 = 0.6920 + 0.4436 + 0.0000, time: 23.406724]
2023-05-11 21:32:54.401: epoch 10:	0.00293159  	0.00649746  	0.00487852  
2023-05-11 21:32:54.401: Find a better model.
2023-05-11 21:33:18.193: [iter 11 : loss : 1.1339 = 0.6917 + 0.4422 + 0.0000, time: 23.788624]
2023-05-11 21:33:18.476: epoch 11:	0.00358306  	0.00854918  	0.00640154  
2023-05-11 21:33:18.476: Find a better model.
2023-05-11 21:33:42.048: [iter 12 : loss : 1.1356 = 0.6912 + 0.4444 + 0.0000, time: 23.567401]
2023-05-11 21:33:42.327: epoch 12:	0.00360527  	0.00804373  	0.00652786  
2023-05-11 21:34:05.980: [iter 13 : loss : 1.1334 = 0.6905 + 0.4428 + 0.0000, time: 23.649528]
2023-05-11 21:34:06.256: epoch 13:	0.00490079  	0.01256083  	0.00971687  
2023-05-11 21:34:06.256: Find a better model.
2023-05-11 21:34:29.709: [iter 14 : loss : 1.1348 = 0.6894 + 0.4454 + 0.0000, time: 23.447655]
2023-05-11 21:34:29.971: epoch 14:	0.00542640  	0.01386665  	0.01095393  
2023-05-11 21:34:29.971: Find a better model.
2023-05-11 21:34:55.697: [iter 15 : loss : 1.1312 = 0.6876 + 0.4436 + 0.0000, time: 25.721974]
2023-05-11 21:34:55.961: epoch 15:	0.00728453  	0.01943320  	0.01551899  
2023-05-11 21:34:55.961: Find a better model.
2023-05-11 21:35:19.394: [iter 16 : loss : 1.1319 = 0.6848 + 0.4471 + 0.0001, time: 23.429180]
2023-05-11 21:35:19.659: epoch 16:	0.00816549  	0.02205360  	0.01778062  
2023-05-11 21:35:19.660: Find a better model.
2023-05-11 21:35:43.167: [iter 17 : loss : 1.1255 = 0.6807 + 0.4447 + 0.0001, time: 23.504559]
2023-05-11 21:35:43.446: epoch 17:	0.01149687  	0.03112135  	0.02590087  
2023-05-11 21:35:43.446: Find a better model.
2023-05-11 21:36:06.619: [iter 18 : loss : 1.1230 = 0.6737 + 0.4492 + 0.0001, time: 23.168640]
2023-05-11 21:36:06.884: epoch 18:	0.01419163  	0.03827204  	0.03254557  
2023-05-11 21:36:06.884: Find a better model.
2023-05-11 21:36:30.302: [iter 19 : loss : 1.1085 = 0.6620 + 0.4463 + 0.0002, time: 23.413104]
2023-05-11 21:36:30.574: epoch 19:	0.01827078  	0.04863277  	0.04137254  
2023-05-11 21:36:30.574: Find a better model.
2023-05-11 21:36:53.978: [iter 20 : loss : 1.0958 = 0.6430 + 0.4526 + 0.0003, time: 23.399030]
2023-05-11 21:36:54.240: epoch 20:	0.02129129  	0.05602976  	0.04765868  
2023-05-11 21:36:54.240: Find a better model.
2023-05-11 21:37:17.884: [iter 21 : loss : 1.0637 = 0.6139 + 0.4494 + 0.0004, time: 23.640216]
2023-05-11 21:37:18.148: epoch 21:	0.02394166  	0.06199558  	0.05316050  
2023-05-11 21:37:18.148: Find a better model.
2023-05-11 21:37:41.531: [iter 22 : loss : 1.0326 = 0.5743 + 0.4577 + 0.0006, time: 23.379788]
2023-05-11 21:37:41.791: epoch 22:	0.02576287  	0.06750436  	0.05691319  
2023-05-11 21:37:41.791: Find a better model.
2023-05-11 21:38:05.289: [iter 23 : loss : 0.9820 = 0.5263 + 0.4549 + 0.0008, time: 23.492339]
2023-05-11 21:38:05.557: epoch 23:	0.02693257  	0.07051381  	0.05936522  
2023-05-11 21:38:05.557: Find a better model.
2023-05-11 21:38:28.958: [iter 24 : loss : 0.9402 = 0.4746 + 0.4645 + 0.0011, time: 23.395861]
2023-05-11 21:38:29.223: epoch 24:	0.02727312  	0.07274552  	0.06063698  
2023-05-11 21:38:29.223: Find a better model.
2023-05-11 21:38:52.612: [iter 25 : loss : 0.8867 = 0.4242 + 0.4611 + 0.0014, time: 23.385966]
2023-05-11 21:38:52.873: epoch 25:	0.02770991  	0.07393369  	0.06144563  
2023-05-11 21:38:52.873: Find a better model.
2023-05-11 21:39:16.310: [iter 26 : loss : 0.8509 = 0.3791 + 0.4701 + 0.0017, time: 23.432630]
2023-05-11 21:39:16.571: epoch 26:	0.02776916  	0.07446320  	0.06201670  
2023-05-11 21:39:16.571: Find a better model.
2023-05-11 21:39:40.233: [iter 27 : loss : 0.8068 = 0.3399 + 0.4650 + 0.0020, time: 23.657239]
2023-05-11 21:39:40.500: epoch 27:	0.02800607  	0.07557297  	0.06245672  
2023-05-11 21:39:40.500: Find a better model.
2023-05-11 21:40:03.938: [iter 28 : loss : 0.7817 = 0.3070 + 0.4724 + 0.0023, time: 23.434563]
2023-05-11 21:40:04.195: epoch 28:	0.02823555  	0.07597303  	0.06266826  
2023-05-11 21:40:04.195: Find a better model.
2023-05-11 21:40:27.816: [iter 29 : loss : 0.7481 = 0.2794 + 0.4661 + 0.0025, time: 23.616976]
2023-05-11 21:40:28.072: epoch 29:	0.02846506  	0.07697760  	0.06328170  
2023-05-11 21:40:28.072: Find a better model.
2023-05-11 21:40:51.321: [iter 30 : loss : 0.7307 = 0.2555 + 0.4724 + 0.0028, time: 23.244468]
2023-05-11 21:40:51.581: epoch 30:	0.02870197  	0.07790271  	0.06381480  
2023-05-11 21:40:51.582: Find a better model.
2023-05-11 21:41:15.193: [iter 31 : loss : 0.7042 = 0.2356 + 0.4655 + 0.0031, time: 23.607002]
2023-05-11 21:41:15.467: epoch 31:	0.02879822  	0.07838302  	0.06404921  
2023-05-11 21:41:15.467: Find a better model.
2023-05-11 21:41:39.051: [iter 32 : loss : 0.6936 = 0.2190 + 0.4713 + 0.0033, time: 23.579098]
2023-05-11 21:41:39.306: epoch 32:	0.02887964  	0.07842924  	0.06423426  
2023-05-11 21:41:39.306: Find a better model.
2023-05-11 21:42:03.000: [iter 33 : loss : 0.6724 = 0.2047 + 0.4642 + 0.0035, time: 23.689819]
2023-05-11 21:42:03.254: epoch 33:	0.02911655  	0.07930059  	0.06484959  
2023-05-11 21:42:03.254: Find a better model.
2023-05-11 21:42:26.484: [iter 34 : loss : 0.6654 = 0.1917 + 0.4699 + 0.0038, time: 23.226267]
2023-05-11 21:42:26.738: epoch 34:	0.02904992  	0.07916281  	0.06488045  
2023-05-11 21:42:50.335: [iter 35 : loss : 0.6464 = 0.1796 + 0.4628 + 0.0040, time: 23.593063]
2023-05-11 21:42:50.595: epoch 35:	0.02912395  	0.07942904  	0.06505053  
2023-05-11 21:42:50.595: Find a better model.
2023-05-11 21:43:14.016: [iter 36 : loss : 0.6422 = 0.1695 + 0.4685 + 0.0042, time: 23.417651]
2023-05-11 21:43:14.274: epoch 36:	0.02920538  	0.07939253  	0.06510491  
2023-05-11 21:43:37.955: [iter 37 : loss : 0.6268 = 0.1610 + 0.4614 + 0.0044, time: 23.677770]
2023-05-11 21:43:38.210: epoch 37:	0.02919797  	0.07928453  	0.06517825  
2023-05-11 21:44:01.696: [iter 38 : loss : 0.6243 = 0.1525 + 0.4672 + 0.0046, time: 23.481483]
2023-05-11 21:44:01.955: epoch 38:	0.02916095  	0.07872064  	0.06478418  
2023-05-11 21:44:25.566: [iter 39 : loss : 0.6103 = 0.1454 + 0.4601 + 0.0048, time: 23.607099]
2023-05-11 21:44:25.820: epoch 39:	0.02924238  	0.07881222  	0.06491766  
2023-05-11 21:44:49.234: [iter 40 : loss : 0.6092 = 0.1384 + 0.4659 + 0.0050, time: 23.410736]
2023-05-11 21:44:49.500: epoch 40:	0.02926459  	0.07884234  	0.06492906  
2023-05-11 21:45:13.114: [iter 41 : loss : 0.5962 = 0.1320 + 0.4591 + 0.0051, time: 23.609610]
2023-05-11 21:45:13.367: epoch 41:	0.02931641  	0.07883640  	0.06497823  
2023-05-11 21:45:36.963: [iter 42 : loss : 0.5966 = 0.1264 + 0.4649 + 0.0053, time: 23.592489]
2023-05-11 21:45:37.216: epoch 42:	0.02939785  	0.07918917  	0.06523593  
2023-05-11 21:46:01.089: [iter 43 : loss : 0.5846 = 0.1211 + 0.4580 + 0.0055, time: 23.869093]
2023-05-11 21:46:01.342: epoch 43:	0.02941267  	0.07887033  	0.06523320  
2023-05-11 21:46:25.000: [iter 44 : loss : 0.5871 = 0.1175 + 0.4640 + 0.0056, time: 23.654960]
2023-05-11 21:46:25.255: epoch 44:	0.02937564  	0.07890780  	0.06499017  
2023-05-11 21:46:49.069: [iter 45 : loss : 0.5754 = 0.1125 + 0.4571 + 0.0058, time: 23.809335]
2023-05-11 21:46:49.324: epoch 45:	0.02939786  	0.07884806  	0.06483123  
2023-05-11 21:47:13.137: [iter 46 : loss : 0.5776 = 0.1085 + 0.4632 + 0.0060, time: 23.809267]
2023-05-11 21:47:13.391: epoch 46:	0.02930901  	0.07836354  	0.06456233  
2023-05-11 21:47:37.274: [iter 47 : loss : 0.5670 = 0.1047 + 0.4562 + 0.0061, time: 23.880236]
2023-05-11 21:47:37.535: epoch 47:	0.02926459  	0.07782732  	0.06443032  
2023-05-11 21:48:01.200: [iter 48 : loss : 0.5699 = 0.1012 + 0.4625 + 0.0063, time: 23.660295]
2023-05-11 21:48:01.466: epoch 48:	0.02924978  	0.07835948  	0.06454609  
2023-05-11 21:48:25.446: [iter 49 : loss : 0.5601 = 0.0982 + 0.4554 + 0.0064, time: 23.975806]
2023-05-11 21:48:25.697: epoch 49:	0.02932382  	0.07840504  	0.06450592  
2023-05-11 21:48:49.340: [iter 50 : loss : 0.5632 = 0.0950 + 0.4617 + 0.0066, time: 23.638957]
2023-05-11 21:48:49.595: epoch 50:	0.02928679  	0.07843010  	0.06446254  
2023-05-11 21:49:13.436: [iter 51 : loss : 0.5535 = 0.0919 + 0.4548 + 0.0067, time: 23.835543]
2023-05-11 21:49:13.688: epoch 51:	0.02933863  	0.07877816  	0.06453966  
2023-05-11 21:49:37.496: [iter 52 : loss : 0.5567 = 0.0889 + 0.4610 + 0.0068, time: 23.804335]
2023-05-11 21:49:37.747: epoch 52:	0.02930161  	0.07864206  	0.06442538  
2023-05-11 21:50:01.445: [iter 53 : loss : 0.5485 = 0.0873 + 0.4542 + 0.0070, time: 23.694140]
2023-05-11 21:50:01.697: epoch 53:	0.02916834  	0.07794347  	0.06410127  
2023-05-11 21:50:25.323: [iter 54 : loss : 0.5524 = 0.0848 + 0.4605 + 0.0071, time: 23.620934]
2023-05-11 21:50:25.579: epoch 54:	0.02914614  	0.07796554  	0.06397104  
2023-05-11 21:50:49.571: [iter 55 : loss : 0.5434 = 0.0824 + 0.4537 + 0.0073, time: 23.988079]
2023-05-11 21:50:49.821: epoch 55:	0.02908690  	0.07758321  	0.06387483  
2023-05-11 21:51:13.468: [iter 56 : loss : 0.5474 = 0.0801 + 0.4600 + 0.0074, time: 23.642232]
2023-05-11 21:51:13.718: epoch 56:	0.02885741  	0.07747619  	0.06362906  
2023-05-11 21:51:37.430: [iter 57 : loss : 0.5382 = 0.0775 + 0.4533 + 0.0075, time: 23.708804]
2023-05-11 21:51:37.684: epoch 57:	0.02877597  	0.07709029  	0.06349976  
2023-05-11 21:52:01.322: [iter 58 : loss : 0.5434 = 0.0761 + 0.4597 + 0.0076, time: 23.635156]
2023-05-11 21:52:01.581: epoch 58:	0.02886481  	0.07706609  	0.06344489  
2023-05-11 21:52:25.563: [iter 59 : loss : 0.5350 = 0.0745 + 0.4528 + 0.0078, time: 23.977816]
2023-05-11 21:52:25.813: epoch 59:	0.02888702  	0.07688610  	0.06339312  
2023-05-11 21:52:49.465: [iter 60 : loss : 0.5399 = 0.0727 + 0.4593 + 0.0079, time: 23.647310]
2023-05-11 21:52:49.718: epoch 60:	0.02887221  	0.07680064  	0.06323271  
2023-05-11 21:52:49.718: Early stopping is trigger at epoch: 60
2023-05-11 21:52:49.718: best_result@epoch 35:

2023-05-11 21:52:49.718: 		0.0291      	0.0794      	0.0651      
2023-05-12 09:25:03.094: my pid: 13244
2023-05-12 09:25:03.094: model: model.general_recommender.SGL
2023-05-12 09:25:03.094: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 09:25:03.094: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 09:25:07.363: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 09:25:29.424: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.060284]
2023-05-12 09:25:29.697: epoch 1:	0.00091057  	0.00245355  	0.00166874  
2023-05-12 09:25:29.697: Find a better model.
2023-05-12 09:25:51.869: [iter 2 : loss : 1.1365 = 0.6930 + 0.4434 + 0.0000, time: 22.167222]
2023-05-12 09:25:52.169: epoch 2:	0.00119929  	0.00270589  	0.00212078  
2023-05-12 09:25:52.170: Find a better model.
2023-05-12 09:26:14.429: [iter 3 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 22.255925]
2023-05-12 09:26:14.705: epoch 3:	0.00181374  	0.00361922  	0.00296010  
2023-05-12 09:26:14.705: Find a better model.
2023-05-12 09:26:36.837: [iter 4 : loss : 1.1369 = 0.6929 + 0.4440 + 0.0000, time: 22.128659]
2023-05-12 09:26:37.144: epoch 4:	0.00185076  	0.00329829  	0.00266458  
2023-05-12 09:26:59.159: [iter 5 : loss : 1.1345 = 0.6928 + 0.4418 + 0.0000, time: 22.006844]
2023-05-12 09:26:59.447: epoch 5:	0.00237637  	0.00483285  	0.00384357  
2023-05-12 09:26:59.447: Find a better model.
2023-05-12 09:27:21.638: [iter 6 : loss : 1.1376 = 0.6927 + 0.4449 + 0.0000, time: 22.188029]
2023-05-12 09:27:21.935: epoch 6:	0.00227273  	0.00381484  	0.00335333  
2023-05-12 09:27:43.962: [iter 7 : loss : 1.1349 = 0.6925 + 0.4424 + 0.0000, time: 22.022760]
2023-05-12 09:27:44.265: epoch 7:	0.00299082  	0.00607152  	0.00458865  
2023-05-12 09:27:44.265: Find a better model.
2023-05-12 09:28:06.202: [iter 8 : loss : 1.1381 = 0.6924 + 0.4457 + 0.0000, time: 21.932031]
2023-05-12 09:28:06.498: epoch 8:	0.00296861  	0.00548416  	0.00468255  
2023-05-12 09:28:28.716: [iter 9 : loss : 1.1350 = 0.6921 + 0.4429 + 0.0000, time: 22.214634]
2023-05-12 09:28:29.008: epoch 9:	0.00360527  	0.00761230  	0.00591891  
2023-05-12 09:28:29.008: Find a better model.
2023-05-12 09:28:51.045: [iter 10 : loss : 1.1385 = 0.6918 + 0.4466 + 0.0000, time: 22.033912]
2023-05-12 09:28:51.345: epoch 10:	0.00387178  	0.00775784  	0.00630433  
2023-05-12 09:28:51.345: Find a better model.
2023-05-12 09:29:13.393: [iter 11 : loss : 1.1349 = 0.6914 + 0.4435 + 0.0000, time: 22.044928]
2023-05-12 09:29:13.680: epoch 11:	0.00490079  	0.01148658  	0.00911387  
2023-05-12 09:29:13.680: Find a better model.
2023-05-12 09:29:35.793: [iter 12 : loss : 1.1387 = 0.6909 + 0.4478 + 0.0000, time: 22.109237]
2023-05-12 09:29:36.085: epoch 12:	0.00467870  	0.01091894  	0.00860497  
2023-05-12 09:29:58.068: [iter 13 : loss : 1.1341 = 0.6900 + 0.4441 + 0.0000, time: 21.979626]
2023-05-12 09:29:58.368: epoch 13:	0.00597422  	0.01443345  	0.01190844  
2023-05-12 09:29:58.368: Find a better model.
2023-05-12 09:30:20.407: [iter 14 : loss : 1.1377 = 0.6885 + 0.4492 + 0.0000, time: 22.034999]
2023-05-12 09:30:20.696: epoch 14:	0.00642580  	0.01522361  	0.01261108  
2023-05-12 09:30:20.696: Find a better model.
2023-05-12 09:30:42.876: [iter 15 : loss : 1.1317 = 0.6867 + 0.4450 + 0.0001, time: 22.176537]
2023-05-12 09:30:43.162: epoch 15:	0.00854304  	0.02128586  	0.01800773  
2023-05-12 09:30:43.162: Find a better model.
2023-05-12 09:31:05.144: [iter 16 : loss : 1.1351 = 0.6840 + 0.4510 + 0.0001, time: 21.976871]
2023-05-12 09:31:05.437: epoch 16:	0.01021613  	0.02486454  	0.02142143  
2023-05-12 09:31:05.437: Find a better model.
2023-05-12 09:31:27.456: [iter 17 : loss : 1.1261 = 0.6801 + 0.4459 + 0.0001, time: 22.014686]
2023-05-12 09:31:27.739: epoch 17:	0.01326623  	0.03276693  	0.02856515  
2023-05-12 09:31:27.739: Find a better model.
2023-05-12 09:31:49.780: [iter 18 : loss : 1.1273 = 0.6735 + 0.4537 + 0.0001, time: 22.035577]
2023-05-12 09:31:50.064: epoch 18:	0.01590176  	0.04020553  	0.03479108  
2023-05-12 09:31:50.064: Find a better model.
2023-05-12 09:32:12.250: [iter 19 : loss : 1.1106 = 0.6630 + 0.4475 + 0.0002, time: 22.181318]
2023-05-12 09:32:12.535: epoch 19:	0.01954411  	0.04997041  	0.04335686  
2023-05-12 09:32:12.535: Find a better model.
2023-05-12 09:32:34.310: [iter 20 : loss : 1.1038 = 0.6463 + 0.4572 + 0.0003, time: 21.769299]
2023-05-12 09:32:34.596: epoch 20:	0.02223151  	0.05558183  	0.04907999  
2023-05-12 09:32:34.596: Find a better model.
2023-05-12 09:32:56.461: [iter 21 : loss : 1.0713 = 0.6209 + 0.4500 + 0.0004, time: 21.861901]
2023-05-12 09:32:56.735: epoch 21:	0.02477082  	0.06313022  	0.05424605  
2023-05-12 09:32:56.735: Find a better model.
2023-05-12 09:33:18.706: [iter 22 : loss : 1.0490 = 0.5862 + 0.4623 + 0.0005, time: 21.966551]
2023-05-12 09:33:18.984: epoch 22:	0.02611822  	0.06703470  	0.05728604  
2023-05-12 09:33:18.985: Find a better model.
2023-05-12 09:33:41.001: [iter 23 : loss : 0.9984 = 0.5424 + 0.4552 + 0.0008, time: 22.012376]
2023-05-12 09:33:41.279: epoch 23:	0.02731755  	0.07119559  	0.05980910  
2023-05-12 09:33:41.279: Find a better model.
2023-05-12 09:34:03.119: [iter 24 : loss : 0.9640 = 0.4941 + 0.4689 + 0.0010, time: 21.836050]
2023-05-12 09:34:03.400: epoch 24:	0.02771734  	0.07344331  	0.06103515  
2023-05-12 09:34:03.400: Find a better model.
2023-05-12 09:34:25.233: [iter 25 : loss : 0.9073 = 0.4447 + 0.4612 + 0.0013, time: 21.828790]
2023-05-12 09:34:25.508: epoch 25:	0.02807270  	0.07411000  	0.06196976  
2023-05-12 09:34:25.508: Find a better model.
2023-05-12 09:34:47.274: [iter 26 : loss : 0.8752 = 0.3991 + 0.4745 + 0.0016, time: 21.761933]
2023-05-12 09:34:47.553: epoch 26:	0.02839104  	0.07591837  	0.06275228  
2023-05-12 09:34:47.553: Find a better model.
2023-05-12 09:35:09.370: [iter 27 : loss : 0.8256 = 0.3584 + 0.4653 + 0.0019, time: 21.811923]
2023-05-12 09:35:09.654: epoch 27:	0.02865015  	0.07678044  	0.06323220  
2023-05-12 09:35:09.654: Find a better model.
2023-05-12 09:35:31.255: [iter 28 : loss : 0.8029 = 0.3235 + 0.4772 + 0.0021, time: 21.596561]
2023-05-12 09:35:31.523: epoch 28:	0.02866496  	0.07753753  	0.06344355  
2023-05-12 09:35:31.524: Find a better model.
2023-05-12 09:35:53.378: [iter 29 : loss : 0.7633 = 0.2940 + 0.4669 + 0.0024, time: 21.850823]
2023-05-12 09:35:53.653: epoch 29:	0.02893887  	0.07806825  	0.06394803  
2023-05-12 09:35:53.653: Find a better model.
2023-05-12 09:36:15.273: [iter 30 : loss : 0.7485 = 0.2683 + 0.4775 + 0.0027, time: 21.613129]
2023-05-12 09:36:15.541: epoch 30:	0.02907214  	0.07827246  	0.06419780  
2023-05-12 09:36:15.541: Find a better model.
2023-05-12 09:36:37.363: [iter 31 : loss : 0.7164 = 0.2468 + 0.4666 + 0.0030, time: 21.818099]
2023-05-12 09:36:37.634: epoch 31:	0.02919057  	0.07877269  	0.06449442  
2023-05-12 09:36:37.635: Find a better model.
2023-05-12 09:36:59.471: [iter 32 : loss : 0.7088 = 0.2287 + 0.4769 + 0.0032, time: 21.833319]
2023-05-12 09:36:59.740: epoch 32:	0.02919797  	0.07888982  	0.06477018  
2023-05-12 09:36:59.740: Find a better model.
2023-05-12 09:37:21.525: [iter 33 : loss : 0.6821 = 0.2131 + 0.4656 + 0.0034, time: 21.781050]
2023-05-12 09:37:21.776: epoch 33:	0.02927200  	0.07912818  	0.06489837  
2023-05-12 09:37:21.776: Find a better model.
2023-05-12 09:37:43.410: [iter 34 : loss : 0.6781 = 0.1991 + 0.4753 + 0.0037, time: 21.629252]
2023-05-12 09:37:43.676: epoch 34:	0.02951631  	0.07959552  	0.06523485  
2023-05-12 09:37:43.676: Find a better model.
2023-05-12 09:38:05.527: [iter 35 : loss : 0.6541 = 0.1861 + 0.4641 + 0.0039, time: 21.847656]
2023-05-12 09:38:05.797: epoch 35:	0.02952372  	0.07939915  	0.06512306  
2023-05-12 09:38:27.793: [iter 36 : loss : 0.6535 = 0.1755 + 0.4739 + 0.0041, time: 21.990678]
2023-05-12 09:38:28.064: epoch 36:	0.02952373  	0.07988708  	0.06529022  
2023-05-12 09:38:28.064: Find a better model.
2023-05-12 09:38:49.926: [iter 37 : loss : 0.6331 = 0.1659 + 0.4629 + 0.0043, time: 21.857450]
2023-05-12 09:38:50.196: epoch 37:	0.02959776  	0.08022273  	0.06559906  
2023-05-12 09:38:50.196: Find a better model.
2023-05-12 09:39:12.195: [iter 38 : loss : 0.6343 = 0.1570 + 0.4728 + 0.0045, time: 21.993990]
2023-05-12 09:39:12.462: epoch 38:	0.02967180  	0.08009779  	0.06566466  
2023-05-12 09:39:34.486: [iter 39 : loss : 0.6160 = 0.1498 + 0.4615 + 0.0047, time: 22.021014]
2023-05-12 09:39:34.750: epoch 39:	0.02969399  	0.08045648  	0.06577430  
2023-05-12 09:39:34.750: Find a better model.
2023-05-12 09:39:56.575: [iter 40 : loss : 0.6188 = 0.1425 + 0.4714 + 0.0049, time: 21.822515]
2023-05-12 09:39:56.843: epoch 40:	0.02975322  	0.08080540  	0.06591884  
2023-05-12 09:39:56.843: Find a better model.
2023-05-12 09:40:18.722: [iter 41 : loss : 0.6008 = 0.1354 + 0.4603 + 0.0051, time: 21.875158]
2023-05-12 09:40:19.006: epoch 41:	0.02967179  	0.08071543  	0.06587082  
2023-05-12 09:40:41.158: [iter 42 : loss : 0.6051 = 0.1296 + 0.4703 + 0.0052, time: 22.145020]
2023-05-12 09:40:41.411: epoch 42:	0.02963477  	0.08060671  	0.06582115  
2023-05-12 09:41:03.262: [iter 43 : loss : 0.5890 = 0.1243 + 0.4593 + 0.0054, time: 21.847312]
2023-05-12 09:41:03.528: epoch 43:	0.02960515  	0.08050158  	0.06582864  
2023-05-12 09:41:25.572: [iter 44 : loss : 0.5949 = 0.1198 + 0.4695 + 0.0056, time: 22.039926]
2023-05-12 09:41:25.836: epoch 44:	0.02964217  	0.08035553  	0.06575863  
2023-05-12 09:41:47.883: [iter 45 : loss : 0.5789 = 0.1147 + 0.4584 + 0.0058, time: 22.043347]
2023-05-12 09:41:48.157: epoch 45:	0.02970141  	0.08032205  	0.06584531  
2023-05-12 09:42:10.145: [iter 46 : loss : 0.5850 = 0.1105 + 0.4686 + 0.0059, time: 21.985346]
2023-05-12 09:42:10.408: epoch 46:	0.02957556  	0.07999562  	0.06561627  
2023-05-12 09:42:32.645: [iter 47 : loss : 0.5703 = 0.1065 + 0.4577 + 0.0061, time: 22.233610]
2023-05-12 09:42:32.906: epoch 47:	0.02953855  	0.08011103  	0.06564124  
2023-05-12 09:42:55.490: [iter 48 : loss : 0.5769 = 0.1029 + 0.4677 + 0.0062, time: 22.579325]
2023-05-12 09:42:55.753: epoch 48:	0.02957556  	0.08009168  	0.06573460  
2023-05-12 09:43:17.854: [iter 49 : loss : 0.5625 = 0.0994 + 0.4567 + 0.0064, time: 22.098413]
2023-05-12 09:43:18.118: epoch 49:	0.02955335  	0.07998845  	0.06567159  
2023-05-12 09:43:40.297: [iter 50 : loss : 0.5702 = 0.0967 + 0.4670 + 0.0065, time: 22.175641]
2023-05-12 09:43:40.563: epoch 50:	0.02938307  	0.07963081  	0.06537461  
2023-05-12 09:44:02.806: [iter 51 : loss : 0.5561 = 0.0932 + 0.4562 + 0.0067, time: 22.239576]
2023-05-12 09:44:03.069: epoch 51:	0.02943490  	0.07931314  	0.06536395  
2023-05-12 09:44:25.088: [iter 52 : loss : 0.5631 = 0.0900 + 0.4663 + 0.0068, time: 22.016740]
2023-05-12 09:44:25.360: epoch 52:	0.02938308  	0.07931396  	0.06526342  
2023-05-12 09:44:47.770: [iter 53 : loss : 0.5511 = 0.0886 + 0.4556 + 0.0069, time: 22.405410]
2023-05-12 09:44:48.035: epoch 53:	0.02950152  	0.07937648  	0.06531116  
2023-05-12 09:45:10.067: [iter 54 : loss : 0.5587 = 0.0859 + 0.4657 + 0.0071, time: 22.027664]
2023-05-12 09:45:10.340: epoch 54:	0.02930164  	0.07862989  	0.06502318  
2023-05-12 09:45:32.223: [iter 55 : loss : 0.5455 = 0.0833 + 0.4550 + 0.0072, time: 21.878492]
2023-05-12 09:45:32.490: epoch 55:	0.02930904  	0.07847469  	0.06486360  
2023-05-12 09:45:54.658: [iter 56 : loss : 0.5535 = 0.0807 + 0.4654 + 0.0073, time: 22.165576]
2023-05-12 09:45:54.932: epoch 56:	0.02917578  	0.07813884  	0.06465480  
2023-05-12 09:46:16.960: [iter 57 : loss : 0.5406 = 0.0786 + 0.4545 + 0.0075, time: 22.023529]
2023-05-12 09:46:17.227: epoch 57:	0.02918318  	0.07763270  	0.06462524  
2023-05-12 09:46:39.065: [iter 58 : loss : 0.5496 = 0.0770 + 0.4650 + 0.0076, time: 21.834279]
2023-05-12 09:46:39.328: epoch 58:	0.02907953  	0.07775441  	0.06459511  
2023-05-12 09:47:01.349: [iter 59 : loss : 0.5364 = 0.0746 + 0.4541 + 0.0077, time: 22.016973]
2023-05-12 09:47:01.619: epoch 59:	0.02904992  	0.07732780  	0.06444934  
2023-05-12 09:47:23.817: [iter 60 : loss : 0.5458 = 0.0734 + 0.4646 + 0.0078, time: 22.193102]
2023-05-12 09:47:24.081: epoch 60:	0.02891665  	0.07673515  	0.06420248  
2023-05-12 09:47:46.127: [iter 61 : loss : 0.5332 = 0.0715 + 0.4537 + 0.0080, time: 22.042534]
2023-05-12 09:47:46.389: epoch 61:	0.02882040  	0.07620224  	0.06384446  
2023-05-12 09:48:08.398: [iter 62 : loss : 0.5425 = 0.0704 + 0.4641 + 0.0081, time: 22.006464]
2023-05-12 09:48:08.662: epoch 62:	0.02876117  	0.07620383  	0.06379344  
2023-05-12 09:48:30.905: [iter 63 : loss : 0.5300 = 0.0685 + 0.4533 + 0.0082, time: 22.240207]
2023-05-12 09:48:31.168: epoch 63:	0.02871676  	0.07589094  	0.06356698  
2023-05-12 09:48:53.179: [iter 64 : loss : 0.5396 = 0.0675 + 0.4638 + 0.0083, time: 22.006487]
2023-05-12 09:48:53.441: epoch 64:	0.02862791  	0.07553513  	0.06338967  
2023-05-12 09:49:15.540: [iter 65 : loss : 0.5269 = 0.0655 + 0.4530 + 0.0084, time: 22.095382]
2023-05-12 09:49:15.804: epoch 65:	0.02853166  	0.07521958  	0.06329534  
2023-05-12 09:49:15.804: Early stopping is trigger at epoch: 65
2023-05-12 09:49:15.804: best_result@epoch 40:

2023-05-12 09:49:15.804: 		0.0298      	0.0808      	0.0659      
2023-05-12 09:51:49.095: my pid: 7784
2023-05-12 09:51:49.096: model: model.general_recommender.SGL
2023-05-12 09:51:49.096: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 09:51:49.096: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 09:51:52.373: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 09:52:13.667: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.293979]
2023-05-12 09:52:13.940: epoch 1:	0.00120669  	0.00258297  	0.00196111  
2023-05-12 09:52:13.940: Find a better model.
2023-05-12 09:52:35.690: [iter 2 : loss : 1.1365 = 0.6930 + 0.4435 + 0.0000, time: 21.745692]
2023-05-12 09:52:35.991: epoch 2:	0.00153243  	0.00267156  	0.00233532  
2023-05-12 09:52:35.991: Find a better model.
2023-05-12 09:52:57.794: [iter 3 : loss : 1.1344 = 0.6929 + 0.4415 + 0.0000, time: 21.800585]
2023-05-12 09:52:58.090: epoch 3:	0.00154723  	0.00341706  	0.00264966  
2023-05-12 09:52:58.090: Find a better model.
2023-05-12 09:53:19.871: [iter 4 : loss : 1.1370 = 0.6929 + 0.4442 + 0.0000, time: 21.776132]
2023-05-12 09:53:20.162: epoch 4:	0.00199882  	0.00371203  	0.00309249  
2023-05-12 09:53:20.163: Find a better model.
2023-05-12 09:53:41.987: [iter 5 : loss : 1.1350 = 0.6928 + 0.4422 + 0.0000, time: 21.821100]
2023-05-12 09:53:42.280: epoch 5:	0.00231715  	0.00526675  	0.00407422  
2023-05-12 09:53:42.280: Find a better model.
2023-05-12 09:54:04.251: [iter 6 : loss : 1.1377 = 0.6926 + 0.4450 + 0.0000, time: 21.967644]
2023-05-12 09:54:04.543: epoch 6:	0.00242819  	0.00465173  	0.00378643  
2023-05-12 09:54:26.612: [iter 7 : loss : 1.1353 = 0.6925 + 0.4428 + 0.0000, time: 22.065717]
2023-05-12 09:54:26.901: epoch 7:	0.00305004  	0.00617587  	0.00508733  
2023-05-12 09:54:26.901: Find a better model.
2023-05-12 09:54:48.624: [iter 8 : loss : 1.1382 = 0.6923 + 0.4458 + 0.0000, time: 21.718644]
2023-05-12 09:54:48.916: epoch 8:	0.00317590  	0.00592253  	0.00527067  
2023-05-12 09:55:10.744: [iter 9 : loss : 1.1355 = 0.6920 + 0.4434 + 0.0000, time: 21.825117]
2023-05-12 09:55:11.036: epoch 9:	0.00413829  	0.00873076  	0.00724675  
2023-05-12 09:55:11.036: Find a better model.
2023-05-12 09:55:32.464: [iter 10 : loss : 1.1385 = 0.6917 + 0.4468 + 0.0000, time: 21.423589]
2023-05-12 09:55:32.750: epoch 10:	0.00433076  	0.00917551  	0.00738248  
2023-05-12 09:55:32.750: Find a better model.
2023-05-12 09:55:54.531: [iter 11 : loss : 1.1353 = 0.6912 + 0.4440 + 0.0000, time: 21.776707]
2023-05-12 09:55:54.814: epoch 11:	0.00548562  	0.01311759  	0.01030477  
2023-05-12 09:55:54.815: Find a better model.
2023-05-12 09:56:16.388: [iter 12 : loss : 1.1386 = 0.6905 + 0.4480 + 0.0000, time: 21.570339]
2023-05-12 09:56:16.670: epoch 12:	0.00558186  	0.01327470  	0.01072023  
2023-05-12 09:56:16.670: Find a better model.
2023-05-12 09:56:38.510: [iter 13 : loss : 1.1343 = 0.6895 + 0.4449 + 0.0000, time: 21.835710]
2023-05-12 09:56:38.790: epoch 13:	0.00704024  	0.01716240  	0.01411951  
2023-05-12 09:56:38.790: Find a better model.
2023-05-12 09:57:00.227: [iter 14 : loss : 1.1374 = 0.6878 + 0.4495 + 0.0000, time: 21.432908]
2023-05-12 09:57:00.506: epoch 14:	0.00758806  	0.01842964  	0.01552545  
2023-05-12 09:57:00.506: Find a better model.
2023-05-12 09:57:22.107: [iter 15 : loss : 1.1315 = 0.6857 + 0.4458 + 0.0001, time: 21.597414]
2023-05-12 09:57:22.393: epoch 15:	0.01016432  	0.02566416  	0.02110310  
2023-05-12 09:57:22.394: Find a better model.
2023-05-12 09:57:43.946: [iter 16 : loss : 1.1341 = 0.6825 + 0.4515 + 0.0001, time: 21.548482]
2023-05-12 09:57:44.221: epoch 16:	0.01224458  	0.03034260  	0.02582566  
2023-05-12 09:57:44.221: Find a better model.
2023-05-12 09:58:05.687: [iter 17 : loss : 1.1248 = 0.6778 + 0.4468 + 0.0001, time: 21.461152]
2023-05-12 09:58:05.960: epoch 17:	0.01590915  	0.03955578  	0.03476036  
2023-05-12 09:58:05.960: Find a better model.
2023-05-12 09:58:27.560: [iter 18 : loss : 1.1242 = 0.6699 + 0.4542 + 0.0001, time: 21.594853]
2023-05-12 09:58:27.837: epoch 18:	0.01873717  	0.04740864  	0.04094888  
2023-05-12 09:58:27.837: Find a better model.
2023-05-12 09:58:49.477: [iter 19 : loss : 1.1057 = 0.6571 + 0.4484 + 0.0002, time: 21.636548]
2023-05-12 09:58:49.742: epoch 19:	0.02198719  	0.05610683  	0.04843528  
2023-05-12 09:58:49.742: Find a better model.
2023-05-12 09:59:11.141: [iter 20 : loss : 1.0957 = 0.6373 + 0.4581 + 0.0003, time: 21.395133]
2023-05-12 09:59:11.441: epoch 20:	0.02411193  	0.06120763  	0.05307469  
2023-05-12 09:59:11.441: Find a better model.
2023-05-12 09:59:32.887: [iter 21 : loss : 1.0600 = 0.6079 + 0.4516 + 0.0004, time: 21.441902]
2023-05-12 09:59:33.160: epoch 21:	0.02614783  	0.06734639  	0.05744751  
2023-05-12 09:59:33.160: Find a better model.
2023-05-12 09:59:54.728: [iter 22 : loss : 1.0331 = 0.5689 + 0.4636 + 0.0006, time: 21.564395]
2023-05-12 09:59:55.003: epoch 22:	0.02714724  	0.07136455  	0.05997908  
2023-05-12 09:59:55.003: Find a better model.
2023-05-12 10:00:16.456: [iter 23 : loss : 0.9798 = 0.5219 + 0.4571 + 0.0008, time: 21.447965]
2023-05-12 10:00:16.729: epoch 23:	0.02809488  	0.07393915  	0.06184967  
2023-05-12 10:00:16.729: Find a better model.
2023-05-12 10:00:38.112: [iter 24 : loss : 0.9436 = 0.4720 + 0.4705 + 0.0011, time: 21.379052]
2023-05-12 10:00:38.381: epoch 24:	0.02862790  	0.07632396  	0.06298545  
2023-05-12 10:00:38.381: Find a better model.
2023-05-12 10:00:59.812: [iter 25 : loss : 0.8875 = 0.4228 + 0.4633 + 0.0014, time: 21.427264]
2023-05-12 10:01:00.079: epoch 25:	0.02884260  	0.07740583  	0.06350824  
2023-05-12 10:01:00.079: Find a better model.
2023-05-12 10:01:21.333: [iter 26 : loss : 0.8563 = 0.3788 + 0.4759 + 0.0017, time: 21.249902]
2023-05-12 10:01:21.598: epoch 26:	0.02883521  	0.07770769  	0.06373014  
2023-05-12 10:01:21.598: Find a better model.
2023-05-12 10:01:43.036: [iter 27 : loss : 0.8087 = 0.3399 + 0.4669 + 0.0020, time: 21.435116]
2023-05-12 10:01:43.301: epoch 27:	0.02906471  	0.07888946  	0.06416235  
2023-05-12 10:01:43.301: Find a better model.
2023-05-12 10:02:04.474: [iter 28 : loss : 0.7873 = 0.3071 + 0.4780 + 0.0023, time: 21.167697]
2023-05-12 10:02:04.743: epoch 28:	0.02919057  	0.07960548  	0.06451062  
2023-05-12 10:02:04.743: Find a better model.
2023-05-12 10:02:26.228: [iter 29 : loss : 0.7501 = 0.2797 + 0.4678 + 0.0025, time: 21.479917]
2023-05-12 10:02:26.498: epoch 29:	0.02927200  	0.07954362  	0.06471859  
2023-05-12 10:02:47.878: [iter 30 : loss : 0.7366 = 0.2557 + 0.4781 + 0.0028, time: 21.377016]
2023-05-12 10:02:48.146: epoch 30:	0.02935344  	0.07999741  	0.06493369  
2023-05-12 10:02:48.146: Find a better model.
2023-05-12 10:03:09.584: [iter 31 : loss : 0.7060 = 0.2356 + 0.4673 + 0.0031, time: 21.434711]
2023-05-12 10:03:09.871: epoch 31:	0.02953113  	0.08086913  	0.06540566  
2023-05-12 10:03:09.871: Find a better model.
2023-05-12 10:03:31.077: [iter 32 : loss : 0.6988 = 0.2187 + 0.4768 + 0.0033, time: 21.201740]
2023-05-12 10:03:31.340: epoch 32:	0.02968659  	0.08115041  	0.06558916  
2023-05-12 10:03:31.340: Find a better model.
2023-05-12 10:03:53.362: [iter 33 : loss : 0.6736 = 0.2041 + 0.4660 + 0.0035, time: 22.017873]
2023-05-12 10:03:53.625: epoch 33:	0.02976062  	0.08172385  	0.06585769  
2023-05-12 10:03:53.625: Find a better model.
2023-05-12 10:04:15.067: [iter 34 : loss : 0.6699 = 0.1908 + 0.4754 + 0.0038, time: 21.438144]
2023-05-12 10:04:15.328: epoch 34:	0.02990128  	0.08231831  	0.06633565  
2023-05-12 10:04:15.328: Find a better model.
2023-05-12 10:04:36.996: [iter 35 : loss : 0.6473 = 0.1786 + 0.4647 + 0.0040, time: 21.665145]
2023-05-12 10:04:37.262: epoch 35:	0.02990128  	0.08213203  	0.06643067  
2023-05-12 10:04:59.014: [iter 36 : loss : 0.6473 = 0.1689 + 0.4741 + 0.0042, time: 21.749281]
2023-05-12 10:04:59.278: epoch 36:	0.02989388  	0.08203685  	0.06642982  
2023-05-12 10:05:20.930: [iter 37 : loss : 0.6274 = 0.1599 + 0.4631 + 0.0044, time: 21.648764]
2023-05-12 10:05:21.199: epoch 37:	0.02999752  	0.08177773  	0.06642208  
2023-05-12 10:05:42.792: [iter 38 : loss : 0.6285 = 0.1513 + 0.4727 + 0.0046, time: 21.589042]
2023-05-12 10:05:43.060: epoch 38:	0.02991609  	0.08147855  	0.06637621  
2023-05-12 10:06:04.737: [iter 39 : loss : 0.6113 = 0.1447 + 0.4618 + 0.0048, time: 21.674314]
2023-05-12 10:06:05.009: epoch 39:	0.02992349  	0.08168364  	0.06650775  
2023-05-12 10:06:26.559: [iter 40 : loss : 0.6138 = 0.1374 + 0.4714 + 0.0050, time: 21.545209]
2023-05-12 10:06:26.825: epoch 40:	0.02999752  	0.08166702  	0.06663909  
2023-05-12 10:06:48.336: [iter 41 : loss : 0.5968 = 0.1309 + 0.4608 + 0.0051, time: 21.508108]
2023-05-12 10:06:48.599: epoch 41:	0.03002713  	0.08136743  	0.06657901  
2023-05-12 10:07:10.371: [iter 42 : loss : 0.6012 = 0.1255 + 0.4704 + 0.0053, time: 21.767901]
2023-05-12 10:07:10.634: epoch 42:	0.02991607  	0.08130485  	0.06653271  
2023-05-12 10:07:32.303: [iter 43 : loss : 0.5853 = 0.1201 + 0.4597 + 0.0055, time: 21.664888]
2023-05-12 10:07:32.561: epoch 43:	0.02995309  	0.08111482  	0.06644849  
2023-05-12 10:07:54.352: [iter 44 : loss : 0.5915 = 0.1165 + 0.4694 + 0.0057, time: 21.786279]
2023-05-12 10:07:54.614: epoch 44:	0.02991608  	0.08097938  	0.06646809  
2023-05-12 10:08:16.470: [iter 45 : loss : 0.5760 = 0.1115 + 0.4588 + 0.0058, time: 21.852912]
2023-05-12 10:08:16.733: epoch 45:	0.02986425  	0.08098191  	0.06642930  
2023-05-12 10:08:38.343: [iter 46 : loss : 0.5820 = 0.1073 + 0.4687 + 0.0060, time: 21.607582]
2023-05-12 10:08:38.605: epoch 46:	0.02967917  	0.08046025  	0.06629922  
2023-05-12 10:09:00.295: [iter 47 : loss : 0.5677 = 0.1036 + 0.4580 + 0.0061, time: 21.685760]
2023-05-12 10:09:00.557: epoch 47:	0.02969398  	0.08022013  	0.06620855  
2023-05-12 10:09:22.310: [iter 48 : loss : 0.5746 = 0.1004 + 0.4679 + 0.0063, time: 21.749324]
2023-05-12 10:09:22.570: epoch 48:	0.02962735  	0.08008652  	0.06616303  
2023-05-12 10:09:44.271: [iter 49 : loss : 0.5606 = 0.0971 + 0.4571 + 0.0064, time: 21.695639]
2023-05-12 10:09:44.534: epoch 49:	0.02964956  	0.08002158  	0.06612344  
2023-05-12 10:10:06.120: [iter 50 : loss : 0.5677 = 0.0940 + 0.4671 + 0.0066, time: 21.582114]
2023-05-12 10:10:06.383: epoch 50:	0.02956813  	0.07981375  	0.06603668  
2023-05-12 10:10:28.033: [iter 51 : loss : 0.5543 = 0.0911 + 0.4564 + 0.0067, time: 21.646372]
2023-05-12 10:10:28.296: epoch 51:	0.02947930  	0.07913692  	0.06581293  
2023-05-12 10:10:49.682: [iter 52 : loss : 0.5613 = 0.0880 + 0.4665 + 0.0069, time: 21.382118]
2023-05-12 10:10:49.946: epoch 52:	0.02947930  	0.07875255  	0.06560545  
2023-05-12 10:11:11.616: [iter 53 : loss : 0.5495 = 0.0865 + 0.4560 + 0.0070, time: 21.667538]
2023-05-12 10:11:11.879: epoch 53:	0.02933863  	0.07787271  	0.06538396  
2023-05-12 10:11:33.486: [iter 54 : loss : 0.5567 = 0.0836 + 0.4660 + 0.0071, time: 21.602921]
2023-05-12 10:11:33.747: epoch 54:	0.02928682  	0.07742552  	0.06517468  
2023-05-12 10:11:55.211: [iter 55 : loss : 0.5442 = 0.0815 + 0.4554 + 0.0073, time: 21.461213]
2023-05-12 10:11:55.474: epoch 55:	0.02934604  	0.07735108  	0.06523412  
2023-05-12 10:12:16.690: [iter 56 : loss : 0.5519 = 0.0791 + 0.4654 + 0.0074, time: 21.211954]
2023-05-12 10:12:16.954: epoch 56:	0.02926461  	0.07695521  	0.06506199  
2023-05-12 10:12:38.587: [iter 57 : loss : 0.5394 = 0.0770 + 0.4549 + 0.0075, time: 21.629540]
2023-05-12 10:12:38.851: epoch 57:	0.02931643  	0.07729565  	0.06519339  
2023-05-12 10:13:00.088: [iter 58 : loss : 0.5479 = 0.0751 + 0.4651 + 0.0076, time: 21.233464]
2023-05-12 10:13:00.352: epoch 58:	0.02923500  	0.07694025  	0.06494534  
2023-05-12 10:13:21.801: [iter 59 : loss : 0.5359 = 0.0736 + 0.4546 + 0.0078, time: 21.445652]
2023-05-12 10:13:22.065: epoch 59:	0.02916097  	0.07687343  	0.06473713  
2023-05-12 10:13:22.065: Early stopping is trigger at epoch: 59
2023-05-12 10:13:22.065: best_result@epoch 34:

2023-05-12 10:13:22.065: 		0.0299      	0.0823      	0.0663      
2023-05-12 12:42:21.734: my pid: 7404
2023-05-12 12:42:21.734: model: model.general_recommender.SGL
2023-05-12 12:42:21.734: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 12:42:21.734: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 12:42:24.994: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 12:42:47.773: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.779313]
2023-05-12 12:42:48.044: epoch 1:	0.00100681  	0.00268721  	0.00190582  
2023-05-12 12:42:48.044: Find a better model.
2023-05-12 12:43:10.905: [iter 2 : loss : 1.1352 = 0.6930 + 0.4422 + 0.0000, time: 22.857094]
2023-05-12 12:43:11.192: epoch 2:	0.00087356  	0.00186221  	0.00141469  
2023-05-12 12:43:34.363: [iter 3 : loss : 1.1337 = 0.6930 + 0.4408 + 0.0000, time: 23.166189]
2023-05-12 12:43:34.654: epoch 3:	0.00131774  	0.00299114  	0.00226472  
2023-05-12 12:43:34.654: Find a better model.
2023-05-12 12:43:57.447: [iter 4 : loss : 1.1352 = 0.6929 + 0.4423 + 0.0000, time: 22.790593]
2023-05-12 12:43:57.735: epoch 4:	0.00145099  	0.00240185  	0.00211150  
2023-05-12 12:44:20.789: [iter 5 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 23.050336]
2023-05-12 12:44:21.085: epoch 5:	0.00168049  	0.00329557  	0.00249415  
2023-05-12 12:44:21.085: Find a better model.
2023-05-12 12:44:43.865: [iter 6 : loss : 1.1355 = 0.6927 + 0.4428 + 0.0000, time: 22.777099]
2023-05-12 12:44:44.154: epoch 6:	0.00191738  	0.00377582  	0.00296911  
2023-05-12 12:44:44.154: Find a better model.
2023-05-12 12:45:07.154: [iter 7 : loss : 1.1343 = 0.6926 + 0.4418 + 0.0000, time: 22.997372]
2023-05-12 12:45:07.445: epoch 7:	0.00224312  	0.00465728  	0.00362134  
2023-05-12 12:45:07.445: Find a better model.
2023-05-12 12:45:30.416: [iter 8 : loss : 1.1357 = 0.6924 + 0.4433 + 0.0000, time: 22.968462]
2023-05-12 12:45:30.703: epoch 8:	0.00246521  	0.00458566  	0.00395779  
2023-05-12 12:45:53.909: [iter 9 : loss : 1.1345 = 0.6922 + 0.4423 + 0.0000, time: 23.202837]
2023-05-12 12:45:54.210: epoch 9:	0.00323512  	0.00667415  	0.00541938  
2023-05-12 12:45:54.210: Find a better model.
2023-05-12 12:46:17.030: [iter 10 : loss : 1.1359 = 0.6919 + 0.4440 + 0.0000, time: 22.815205]
2023-05-12 12:46:17.319: epoch 10:	0.00342020  	0.00755002  	0.00590267  
2023-05-12 12:46:17.319: Find a better model.
2023-05-12 12:46:41.525: [iter 11 : loss : 1.1343 = 0.6915 + 0.4428 + 0.0000, time: 24.202450]
2023-05-12 12:46:41.806: epoch 11:	0.00437518  	0.01035073  	0.00817329  
2023-05-12 12:46:41.806: Find a better model.
2023-05-12 12:47:05.292: [iter 12 : loss : 1.1357 = 0.6910 + 0.4448 + 0.0000, time: 23.482602]
2023-05-12 12:47:05.585: epoch 12:	0.00475273  	0.01145207  	0.00898760  
2023-05-12 12:47:05.586: Find a better model.
2023-05-12 12:47:29.336: [iter 13 : loss : 1.1336 = 0.6901 + 0.4435 + 0.0000, time: 23.745878]
2023-05-12 12:47:29.606: epoch 13:	0.00648502  	0.01607647  	0.01260817  
2023-05-12 12:47:29.606: Find a better model.
2023-05-12 12:47:52.607: [iter 14 : loss : 1.1347 = 0.6889 + 0.4458 + 0.0000, time: 22.998058]
2023-05-12 12:47:52.890: epoch 14:	0.00721051  	0.01772871  	0.01424475  
2023-05-12 12:47:52.890: Find a better model.
2023-05-12 12:48:16.059: [iter 15 : loss : 1.1312 = 0.6867 + 0.4444 + 0.0001, time: 23.164210]
2023-05-12 12:48:16.344: epoch 15:	0.00947581  	0.02532660  	0.01971500  
2023-05-12 12:48:16.344: Find a better model.
2023-05-12 12:48:39.341: [iter 16 : loss : 1.1310 = 0.6834 + 0.4475 + 0.0001, time: 22.992873]
2023-05-12 12:48:39.618: epoch 16:	0.01087502  	0.02864087  	0.02335212  
2023-05-12 12:48:39.618: Find a better model.
2023-05-12 12:49:02.867: [iter 17 : loss : 1.1240 = 0.6784 + 0.4455 + 0.0001, time: 23.245889]
2023-05-12 12:49:03.144: epoch 17:	0.01456918  	0.03834370  	0.03199232  
2023-05-12 12:49:03.144: Find a better model.
2023-05-12 12:49:26.137: [iter 18 : loss : 1.1198 = 0.6699 + 0.4498 + 0.0001, time: 22.988831]
2023-05-12 12:49:26.412: epoch 18:	0.01726395  	0.04552225  	0.03858246  
2023-05-12 12:49:26.412: Find a better model.
2023-05-12 12:49:49.474: [iter 19 : loss : 1.1035 = 0.6559 + 0.4473 + 0.0002, time: 23.057775]
2023-05-12 12:49:49.748: epoch 19:	0.02064724  	0.05426849  	0.04548822  
2023-05-12 12:49:49.748: Find a better model.
2023-05-12 12:50:12.705: [iter 20 : loss : 1.0875 = 0.6337 + 0.4536 + 0.0003, time: 22.953217]
2023-05-12 12:50:12.976: epoch 20:	0.02289781  	0.05972875  	0.05058187  
2023-05-12 12:50:12.976: Find a better model.
2023-05-12 12:50:36.028: [iter 21 : loss : 1.0524 = 0.6011 + 0.4508 + 0.0005, time: 23.048295]
2023-05-12 12:50:36.299: epoch 21:	0.02526684  	0.06631485  	0.05526288  
2023-05-12 12:50:36.299: Find a better model.
2023-05-12 12:50:59.117: [iter 22 : loss : 1.0178 = 0.5582 + 0.4589 + 0.0007, time: 22.814149]
2023-05-12 12:50:59.400: epoch 22:	0.02652540  	0.06995018  	0.05774456  
2023-05-12 12:50:59.401: Find a better model.
2023-05-12 12:51:22.382: [iter 23 : loss : 0.9658 = 0.5085 + 0.4564 + 0.0009, time: 22.978136]
2023-05-12 12:51:22.652: epoch 23:	0.02722132  	0.07243480  	0.05937834  
2023-05-12 12:51:22.652: Find a better model.
2023-05-12 12:51:45.861: [iter 24 : loss : 0.9238 = 0.4567 + 0.4659 + 0.0012, time: 23.204960]
2023-05-12 12:51:46.135: epoch 24:	0.02780617  	0.07412016  	0.06041538  
2023-05-12 12:51:46.135: Find a better model.
2023-05-12 12:52:09.185: [iter 25 : loss : 0.8714 = 0.4074 + 0.4625 + 0.0015, time: 23.046067]
2023-05-12 12:52:09.458: epoch 25:	0.02825037  	0.07538725  	0.06140428  
2023-05-12 12:52:09.458: Find a better model.
2023-05-12 12:52:32.266: [iter 26 : loss : 0.8369 = 0.3644 + 0.4708 + 0.0018, time: 22.805455]
2023-05-12 12:52:32.541: epoch 26:	0.02842065  	0.07626499  	0.06197800  
2023-05-12 12:52:32.541: Find a better model.
2023-05-12 12:52:55.334: [iter 27 : loss : 0.7948 = 0.3270 + 0.4658 + 0.0021, time: 22.789694]
2023-05-12 12:52:55.607: epoch 27:	0.02867976  	0.07738036  	0.06231557  
2023-05-12 12:52:55.607: Find a better model.
2023-05-12 12:53:18.231: [iter 28 : loss : 0.7707 = 0.2958 + 0.4725 + 0.0023, time: 22.620745]
2023-05-12 12:53:18.502: epoch 28:	0.02890927  	0.07814108  	0.06285450  
2023-05-12 12:53:18.502: Find a better model.
2023-05-12 12:53:41.547: [iter 29 : loss : 0.7388 = 0.2697 + 0.4664 + 0.0026, time: 23.041801]
2023-05-12 12:53:41.819: epoch 29:	0.02897589  	0.07836457  	0.06312395  
2023-05-12 12:53:41.819: Find a better model.
2023-05-12 12:54:04.467: [iter 30 : loss : 0.7222 = 0.2470 + 0.4723 + 0.0029, time: 22.643242]
2023-05-12 12:54:04.734: epoch 30:	0.02916837  	0.07894836  	0.06365195  
2023-05-12 12:54:04.734: Find a better model.
2023-05-12 12:54:27.500: [iter 31 : loss : 0.6970 = 0.2281 + 0.4658 + 0.0031, time: 22.757056]
2023-05-12 12:54:27.779: epoch 31:	0.02938306  	0.07931306  	0.06374712  
2023-05-12 12:54:27.779: Find a better model.
2023-05-12 12:54:50.449: [iter 32 : loss : 0.6868 = 0.2121 + 0.4713 + 0.0034, time: 22.666239]
2023-05-12 12:54:50.711: epoch 32:	0.02947931  	0.07973803  	0.06401068  
2023-05-12 12:54:50.711: Find a better model.
2023-05-12 12:55:13.760: [iter 33 : loss : 0.6665 = 0.1984 + 0.4646 + 0.0036, time: 23.043933]
2023-05-12 12:55:14.027: epoch 33:	0.02960516  	0.08023422  	0.06431663  
2023-05-12 12:55:14.027: Find a better model.
2023-05-12 12:55:36.848: [iter 34 : loss : 0.6600 = 0.1861 + 0.4701 + 0.0038, time: 22.817171]
2023-05-12 12:55:37.119: epoch 34:	0.02962736  	0.07998788  	0.06451436  
2023-05-12 12:56:00.077: [iter 35 : loss : 0.6413 = 0.1743 + 0.4630 + 0.0040, time: 22.952578]
2023-05-12 12:56:00.338: epoch 35:	0.02963476  	0.08029747  	0.06473168  
2023-05-12 12:56:00.338: Find a better model.
2023-05-12 12:56:23.162: [iter 36 : loss : 0.6379 = 0.1650 + 0.4686 + 0.0042, time: 22.820189]
2023-05-12 12:56:23.426: epoch 36:	0.02972360  	0.08079559  	0.06506646  
2023-05-12 12:56:23.426: Find a better model.
2023-05-12 12:56:46.518: [iter 37 : loss : 0.6228 = 0.1566 + 0.4617 + 0.0044, time: 23.089295]
2023-05-12 12:56:46.780: epoch 37:	0.02972360  	0.08041602  	0.06498356  
2023-05-12 12:57:09.361: [iter 38 : loss : 0.6206 = 0.1486 + 0.4673 + 0.0046, time: 22.577541]
2023-05-12 12:57:09.623: epoch 38:	0.02974581  	0.08054031  	0.06517091  
2023-05-12 12:57:32.636: [iter 39 : loss : 0.6071 = 0.1419 + 0.4604 + 0.0048, time: 23.009108]
2023-05-12 12:57:32.899: epoch 39:	0.02968657  	0.08021228  	0.06516185  
2023-05-12 12:57:55.939: [iter 40 : loss : 0.6061 = 0.1350 + 0.4661 + 0.0050, time: 23.036724]
2023-05-12 12:57:56.214: epoch 40:	0.02976060  	0.08033291  	0.06520469  
2023-05-12 12:58:19.248: [iter 41 : loss : 0.5931 = 0.1287 + 0.4592 + 0.0052, time: 23.030771]
2023-05-12 12:58:19.521: epoch 41:	0.02986425  	0.08083322  	0.06534048  
2023-05-12 12:58:19.521: Find a better model.
2023-05-12 12:58:42.545: [iter 42 : loss : 0.5938 = 0.1234 + 0.4650 + 0.0054, time: 23.020617]
2023-05-12 12:58:42.809: epoch 42:	0.02990126  	0.08059867  	0.06553148  
2023-05-12 12:59:05.985: [iter 43 : loss : 0.5823 = 0.1185 + 0.4583 + 0.0055, time: 23.171409]
2023-05-12 12:59:06.246: epoch 43:	0.03003452  	0.08087677  	0.06554302  
2023-05-12 12:59:06.246: Find a better model.
2023-05-12 12:59:29.111: [iter 44 : loss : 0.5848 = 0.1149 + 0.4642 + 0.0057, time: 22.860682]
2023-05-12 12:59:29.384: epoch 44:	0.02997530  	0.08082461  	0.06545510  
2023-05-12 12:59:52.461: [iter 45 : loss : 0.5734 = 0.1101 + 0.4574 + 0.0059, time: 23.073227]
2023-05-12 12:59:52.724: epoch 45:	0.02992349  	0.08061957  	0.06533506  
2023-05-12 13:00:15.911: [iter 46 : loss : 0.5759 = 0.1065 + 0.4635 + 0.0060, time: 23.184881]
2023-05-12 13:00:16.174: epoch 46:	0.02975320  	0.07991502  	0.06515104  
2023-05-12 13:00:39.379: [iter 47 : loss : 0.5652 = 0.1024 + 0.4566 + 0.0062, time: 23.200329]
2023-05-12 13:00:39.642: epoch 47:	0.02978281  	0.07987242  	0.06509359  
2023-05-12 13:01:02.683: [iter 48 : loss : 0.5682 = 0.0993 + 0.4626 + 0.0063, time: 23.036275]
2023-05-12 13:01:02.945: epoch 48:	0.02975321  	0.07965433  	0.06508202  
2023-05-12 13:01:26.200: [iter 49 : loss : 0.5588 = 0.0964 + 0.4559 + 0.0065, time: 23.251975]
2023-05-12 13:01:26.472: epoch 49:	0.02970879  	0.07962353  	0.06500150  
2023-05-12 13:01:49.464: [iter 50 : loss : 0.5619 = 0.0934 + 0.4619 + 0.0066, time: 22.984042]
2023-05-12 13:01:49.725: epoch 50:	0.02968657  	0.07943875  	0.06496646  
2023-05-12 13:02:13.139: [iter 51 : loss : 0.5523 = 0.0903 + 0.4552 + 0.0068, time: 23.410493]
2023-05-12 13:02:13.400: epoch 51:	0.02965696  	0.07942749  	0.06489386  
2023-05-12 13:02:36.469: [iter 52 : loss : 0.5558 = 0.0876 + 0.4613 + 0.0069, time: 23.066163]
2023-05-12 13:02:36.737: epoch 52:	0.02956811  	0.07923986  	0.06473707  
2023-05-12 13:03:00.179: [iter 53 : loss : 0.5479 = 0.0862 + 0.4546 + 0.0070, time: 23.439180]
2023-05-12 13:03:00.442: epoch 53:	0.02952369  	0.07878190  	0.06447048  
2023-05-12 13:03:23.628: [iter 54 : loss : 0.5515 = 0.0834 + 0.4609 + 0.0072, time: 23.181749]
2023-05-12 13:03:23.890: epoch 54:	0.02958292  	0.07885616  	0.06446463  
2023-05-12 13:03:47.314: [iter 55 : loss : 0.5425 = 0.0811 + 0.4542 + 0.0073, time: 23.419596]
2023-05-12 13:03:47.577: epoch 55:	0.02953109  	0.07866711  	0.06444378  
2023-05-12 13:04:10.623: [iter 56 : loss : 0.5466 = 0.0788 + 0.4603 + 0.0074, time: 23.042072]
2023-05-12 13:04:10.887: epoch 56:	0.02947928  	0.07875345  	0.06451873  
2023-05-12 13:04:34.172: [iter 57 : loss : 0.5378 = 0.0765 + 0.4538 + 0.0076, time: 23.281925]
2023-05-12 13:04:34.434: epoch 57:	0.02944226  	0.07822039  	0.06431154  
2023-05-12 13:04:57.620: [iter 58 : loss : 0.5428 = 0.0750 + 0.4600 + 0.0077, time: 23.181641]
2023-05-12 13:04:57.886: epoch 58:	0.02936083  	0.07796550  	0.06409669  
2023-05-12 13:05:21.079: [iter 59 : loss : 0.5342 = 0.0730 + 0.4534 + 0.0078, time: 23.189113]
2023-05-12 13:05:21.341: epoch 59:	0.02927940  	0.07739715  	0.06402598  
2023-05-12 13:05:44.580: [iter 60 : loss : 0.5395 = 0.0719 + 0.4597 + 0.0079, time: 23.233596]
2023-05-12 13:05:44.842: epoch 60:	0.02919055  	0.07707516  	0.06372505  
2023-05-12 13:06:08.286: [iter 61 : loss : 0.5309 = 0.0699 + 0.4529 + 0.0080, time: 23.436844]
2023-05-12 13:06:08.549: epoch 61:	0.02921276  	0.07694021  	0.06374818  
2023-05-12 13:06:31.595: [iter 62 : loss : 0.5361 = 0.0688 + 0.4592 + 0.0082, time: 23.041719]
2023-05-12 13:06:31.858: epoch 62:	0.02912392  	0.07686496  	0.06353451  
2023-05-12 13:06:55.082: [iter 63 : loss : 0.5282 = 0.0674 + 0.4525 + 0.0083, time: 23.221368]
2023-05-12 13:06:55.345: epoch 63:	0.02921275  	0.07698944  	0.06360371  
2023-05-12 13:07:18.555: [iter 64 : loss : 0.5333 = 0.0661 + 0.4589 + 0.0084, time: 23.206034]
2023-05-12 13:07:18.819: epoch 64:	0.02896105  	0.07658498  	0.06333523  
2023-05-12 13:07:41.895: [iter 65 : loss : 0.5250 = 0.0642 + 0.4523 + 0.0085, time: 23.072187]
2023-05-12 13:07:42.161: epoch 65:	0.02902027  	0.07644704  	0.06322473  
2023-05-12 13:08:05.141: [iter 66 : loss : 0.5307 = 0.0634 + 0.4586 + 0.0086, time: 22.976191]
2023-05-12 13:08:05.406: epoch 66:	0.02896104  	0.07637776  	0.06311849  
2023-05-12 13:08:29.008: [iter 67 : loss : 0.5227 = 0.0621 + 0.4519 + 0.0087, time: 23.598819]
2023-05-12 13:08:29.272: epoch 67:	0.02896845  	0.07648302  	0.06317510  
2023-05-12 13:08:52.503: [iter 68 : loss : 0.5281 = 0.0610 + 0.4583 + 0.0088, time: 23.225730]
2023-05-12 13:08:52.766: epoch 68:	0.02888701  	0.07592694  	0.06299532  
2023-05-12 13:08:52.766: Early stopping is trigger at epoch: 68
2023-05-12 13:08:52.766: best_result@epoch 43:

2023-05-12 13:08:52.766: 		0.0300      	0.0809      	0.0655      
2023-05-12 16:25:43.081: my pid: 9840
2023-05-12 16:25:43.081: model: model.general_recommender.SGL
2023-05-12 16:25:43.081: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 16:25:43.081: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 16:25:46.451: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 16:26:10.619: [iter 1 : loss : 1.1339 = 0.6931 + 0.4409 + 0.0000, time: 24.167938]
2023-05-12 16:26:10.882: epoch 1:	0.00102162  	0.00213834  	0.00176633  
2023-05-12 16:26:10.883: Find a better model.
2023-05-12 16:26:35.462: [iter 2 : loss : 1.1347 = 0.6930 + 0.4416 + 0.0000, time: 24.574673]
2023-05-12 16:26:35.749: epoch 2:	0.00098460  	0.00216604  	0.00155859  
2023-05-12 16:26:35.749: Find a better model.
2023-05-12 16:27:00.755: [iter 3 : loss : 1.1330 = 0.6930 + 0.4401 + 0.0000, time: 25.002439]
2023-05-12 16:27:01.063: epoch 3:	0.00107344  	0.00217006  	0.00176729  
2023-05-12 16:27:01.063: Find a better model.
2023-05-12 16:27:25.858: [iter 4 : loss : 1.1347 = 0.6929 + 0.4418 + 0.0000, time: 24.791883]
2023-05-12 16:27:26.144: epoch 4:	0.00109565  	0.00208706  	0.00161378  
2023-05-12 16:27:50.949: [iter 5 : loss : 1.1333 = 0.6928 + 0.4404 + 0.0000, time: 24.801307]
2023-05-12 16:27:51.246: epoch 5:	0.00140657  	0.00323358  	0.00222608  
2023-05-12 16:27:51.246: Find a better model.
2023-05-12 16:28:16.047: [iter 6 : loss : 1.1349 = 0.6927 + 0.4421 + 0.0000, time: 24.797679]
2023-05-12 16:28:16.334: epoch 6:	0.00149541  	0.00324944  	0.00265854  
2023-05-12 16:28:16.335: Find a better model.
2023-05-12 16:28:40.971: [iter 7 : loss : 1.1335 = 0.6926 + 0.4408 + 0.0000, time: 24.633036]
2023-05-12 16:28:41.255: epoch 7:	0.00202843  	0.00446501  	0.00334627  
2023-05-12 16:28:41.255: Find a better model.
2023-05-12 16:29:05.813: [iter 8 : loss : 1.1352 = 0.6925 + 0.4427 + 0.0000, time: 24.553429]
2023-05-12 16:29:06.106: epoch 8:	0.00206544  	0.00418021  	0.00327941  
2023-05-12 16:29:30.739: [iter 9 : loss : 1.1335 = 0.6923 + 0.4412 + 0.0000, time: 24.629844]
2023-05-12 16:29:31.037: epoch 9:	0.00271691  	0.00533240  	0.00427638  
2023-05-12 16:29:31.037: Find a better model.
2023-05-12 16:29:55.828: [iter 10 : loss : 1.1353 = 0.6921 + 0.4432 + 0.0000, time: 24.786862]
2023-05-12 16:29:56.114: epoch 10:	0.00267249  	0.00496297  	0.00392678  
2023-05-12 16:30:20.908: [iter 11 : loss : 1.1334 = 0.6918 + 0.4416 + 0.0000, time: 24.790403]
2023-05-12 16:30:21.189: epoch 11:	0.00324993  	0.00699073  	0.00517279  
2023-05-12 16:30:21.189: Find a better model.
2023-05-12 16:30:45.613: [iter 12 : loss : 1.1353 = 0.6913 + 0.4440 + 0.0000, time: 24.419047]
2023-05-12 16:30:45.915: epoch 12:	0.00342020  	0.00719268  	0.00582782  
2023-05-12 16:30:45.915: Find a better model.
2023-05-12 16:31:10.281: [iter 13 : loss : 1.1329 = 0.6907 + 0.4421 + 0.0000, time: 24.363303]
2023-05-12 16:31:10.561: epoch 13:	0.00471572  	0.01104459  	0.00876311  
2023-05-12 16:31:10.561: Find a better model.
2023-05-12 16:31:34.961: [iter 14 : loss : 1.1347 = 0.6898 + 0.4449 + 0.0000, time: 24.396339]
2023-05-12 16:31:35.237: epoch 14:	0.00501924  	0.01229719  	0.00992827  
2023-05-12 16:31:35.237: Find a better model.
2023-05-12 16:31:59.896: [iter 15 : loss : 1.1311 = 0.6882 + 0.4428 + 0.0000, time: 24.656295]
2023-05-12 16:32:00.179: epoch 15:	0.00662568  	0.01706444  	0.01387289  
2023-05-12 16:32:00.179: Find a better model.
2023-05-12 16:32:24.552: [iter 16 : loss : 1.1321 = 0.6857 + 0.4464 + 0.0001, time: 24.368541]
2023-05-12 16:32:24.823: epoch 16:	0.00749182  	0.01992611  	0.01659285  
2023-05-12 16:32:24.823: Find a better model.
2023-05-12 16:32:49.298: [iter 17 : loss : 1.1257 = 0.6818 + 0.4438 + 0.0001, time: 24.471309]
2023-05-12 16:32:49.572: epoch 17:	0.01081579  	0.02921032  	0.02387179  
2023-05-12 16:32:49.572: Find a better model.
2023-05-12 16:33:13.940: [iter 18 : loss : 1.1241 = 0.6756 + 0.4485 + 0.0001, time: 24.363698]
2023-05-12 16:33:14.216: epoch 18:	0.01310335  	0.03512741  	0.02915421  
2023-05-12 16:33:14.216: Find a better model.
2023-05-12 16:33:38.649: [iter 19 : loss : 1.1104 = 0.6650 + 0.4452 + 0.0002, time: 24.427986]
2023-05-12 16:33:38.918: epoch 19:	0.01700485  	0.04536187  	0.03720904  
2023-05-12 16:33:38.918: Find a better model.
2023-05-12 16:34:03.117: [iter 20 : loss : 1.0995 = 0.6477 + 0.4516 + 0.0002, time: 24.194865]
2023-05-12 16:34:03.390: epoch 20:	0.01979586  	0.05291811  	0.04348499  
2023-05-12 16:34:03.390: Find a better model.
2023-05-12 16:34:28.019: [iter 21 : loss : 1.0693 = 0.6208 + 0.4482 + 0.0004, time: 24.624347]
2023-05-12 16:34:28.306: epoch 21:	0.02263130  	0.05999205  	0.04896401  
2023-05-12 16:34:28.306: Find a better model.
2023-05-12 16:34:52.476: [iter 22 : loss : 1.0405 = 0.5835 + 0.4564 + 0.0005, time: 24.167373]
2023-05-12 16:34:52.745: epoch 22:	0.02452652  	0.06420505  	0.05301008  
2023-05-12 16:34:52.745: Find a better model.
2023-05-12 16:35:17.007: [iter 23 : loss : 0.9912 = 0.5371 + 0.4534 + 0.0008, time: 24.257618]
2023-05-12 16:35:17.278: epoch 23:	0.02581471  	0.06738894  	0.05548301  
2023-05-12 16:35:17.278: Find a better model.
2023-05-12 16:35:41.505: [iter 24 : loss : 0.9504 = 0.4862 + 0.4632 + 0.0010, time: 24.223911]
2023-05-12 16:35:41.778: epoch 24:	0.02647360  	0.06967799  	0.05695683  
2023-05-12 16:35:41.778: Find a better model.
2023-05-12 16:36:06.198: [iter 25 : loss : 0.8970 = 0.4359 + 0.4598 + 0.0013, time: 24.415091]
2023-05-12 16:36:06.473: epoch 25:	0.02682156  	0.07032364  	0.05767623  
2023-05-12 16:36:06.473: Find a better model.
2023-05-12 16:36:30.721: [iter 26 : loss : 0.8605 = 0.3901 + 0.4688 + 0.0016, time: 24.243435]
2023-05-12 16:36:30.993: epoch 26:	0.02726576  	0.07179585  	0.05861957  
2023-05-12 16:36:30.994: Find a better model.
2023-05-12 16:36:55.188: [iter 27 : loss : 0.8158 = 0.3502 + 0.4637 + 0.0019, time: 24.191505]
2023-05-12 16:36:55.456: epoch 27:	0.02741382  	0.07218336  	0.05908826  
2023-05-12 16:36:55.456: Find a better model.
2023-05-12 16:37:19.652: [iter 28 : loss : 0.7896 = 0.3161 + 0.4713 + 0.0022, time: 24.190500]
2023-05-12 16:37:19.918: epoch 28:	0.02763592  	0.07330580  	0.05969607  
2023-05-12 16:37:19.918: Find a better model.
2023-05-12 16:37:44.181: [iter 29 : loss : 0.7555 = 0.2879 + 0.4651 + 0.0025, time: 24.258523]
2023-05-12 16:37:44.446: epoch 29:	0.02784321  	0.07433680  	0.06034261  
2023-05-12 16:37:44.446: Find a better model.
2023-05-12 16:38:08.621: [iter 30 : loss : 0.7371 = 0.2629 + 0.4715 + 0.0027, time: 24.172649]
2023-05-12 16:38:08.886: epoch 30:	0.02792464  	0.07466757  	0.06083113  
2023-05-12 16:38:08.886: Find a better model.
2023-05-12 16:38:33.294: [iter 31 : loss : 0.7107 = 0.2428 + 0.4649 + 0.0030, time: 24.403343]
2023-05-12 16:38:33.555: epoch 31:	0.02821337  	0.07542419  	0.06105911  
2023-05-12 16:38:33.556: Find a better model.
2023-05-12 16:38:58.025: [iter 32 : loss : 0.6991 = 0.2252 + 0.4706 + 0.0032, time: 24.465496]
2023-05-12 16:38:58.293: epoch 32:	0.02844285  	0.07592348  	0.06155587  
2023-05-12 16:38:58.293: Find a better model.
2023-05-12 16:39:22.743: [iter 33 : loss : 0.6777 = 0.2106 + 0.4636 + 0.0035, time: 24.445627]
2023-05-12 16:39:23.007: epoch 33:	0.02861313  	0.07620803  	0.06179937  
2023-05-12 16:39:23.007: Find a better model.
2023-05-12 16:39:47.218: [iter 34 : loss : 0.6698 = 0.1970 + 0.4691 + 0.0037, time: 24.206816]
2023-05-12 16:39:47.483: epoch 34:	0.02870937  	0.07663839  	0.06219953  
2023-05-12 16:39:47.483: Find a better model.
2023-05-12 16:40:11.926: [iter 35 : loss : 0.6510 = 0.1848 + 0.4623 + 0.0039, time: 24.438596]
2023-05-12 16:40:12.197: epoch 35:	0.02879082  	0.07683699  	0.06258279  
2023-05-12 16:40:12.197: Find a better model.
2023-05-12 16:40:36.618: [iter 36 : loss : 0.6464 = 0.1743 + 0.4679 + 0.0041, time: 24.417202]
2023-05-12 16:40:36.882: epoch 36:	0.02867236  	0.07656565  	0.06260227  
2023-05-12 16:41:01.333: [iter 37 : loss : 0.6305 = 0.1652 + 0.4610 + 0.0043, time: 24.447777]
2023-05-12 16:41:01.598: epoch 37:	0.02875379  	0.07705276  	0.06295546  
2023-05-12 16:41:01.598: Find a better model.
2023-05-12 16:41:26.009: [iter 38 : loss : 0.6277 = 0.1567 + 0.4665 + 0.0045, time: 24.405886]
2023-05-12 16:41:26.282: epoch 38:	0.02882783  	0.07726928  	0.06303509  
2023-05-12 16:41:26.282: Find a better model.
2023-05-12 16:41:51.079: [iter 39 : loss : 0.6141 = 0.1497 + 0.4597 + 0.0047, time: 24.793767]
2023-05-12 16:41:51.344: epoch 39:	0.02879082  	0.07740520  	0.06314534  
2023-05-12 16:41:51.344: Find a better model.
2023-05-12 16:42:15.753: [iter 40 : loss : 0.6125 = 0.1423 + 0.4653 + 0.0049, time: 24.403849]
2023-05-12 16:42:16.019: epoch 40:	0.02873899  	0.07727335  	0.06318278  
2023-05-12 16:42:40.809: [iter 41 : loss : 0.5989 = 0.1355 + 0.4584 + 0.0051, time: 24.786029]
2023-05-12 16:42:41.084: epoch 41:	0.02865755  	0.07696774  	0.06302883  
2023-05-12 16:43:05.540: [iter 42 : loss : 0.5995 = 0.1301 + 0.4642 + 0.0053, time: 24.446949]
2023-05-12 16:43:05.805: epoch 42:	0.02873159  	0.07671320  	0.06309835  
2023-05-12 16:43:30.638: [iter 43 : loss : 0.5873 = 0.1244 + 0.4575 + 0.0054, time: 24.829858]
2023-05-12 16:43:30.917: epoch 43:	0.02875379  	0.07667973  	0.06312496  
2023-05-12 16:43:55.538: [iter 44 : loss : 0.5894 = 0.1206 + 0.4633 + 0.0056, time: 24.616131]
2023-05-12 16:43:55.802: epoch 44:	0.02873899  	0.07658890  	0.06301429  
2023-05-12 16:44:20.589: [iter 45 : loss : 0.5779 = 0.1155 + 0.4566 + 0.0058, time: 24.782266]
2023-05-12 16:44:20.851: epoch 45:	0.02866496  	0.07644878  	0.06285260  
2023-05-12 16:44:45.515: [iter 46 : loss : 0.5799 = 0.1117 + 0.4623 + 0.0059, time: 24.659559]
2023-05-12 16:44:45.778: epoch 46:	0.02863533  	0.07636458  	0.06281643  
2023-05-12 16:45:10.654: [iter 47 : loss : 0.5688 = 0.1070 + 0.4557 + 0.0061, time: 24.870229]
2023-05-12 16:45:10.931: epoch 47:	0.02856871  	0.07641488  	0.06289809  
2023-05-12 16:45:35.693: [iter 48 : loss : 0.5716 = 0.1039 + 0.4615 + 0.0062, time: 24.759386]
2023-05-12 16:45:35.956: epoch 48:	0.02859091  	0.07636695  	0.06278338  
2023-05-12 16:46:00.975: [iter 49 : loss : 0.5620 = 0.1007 + 0.4549 + 0.0064, time: 25.013780]
2023-05-12 16:46:01.254: epoch 49:	0.02858352  	0.07603858  	0.06284212  
2023-05-12 16:46:25.869: [iter 50 : loss : 0.5650 = 0.0976 + 0.4608 + 0.0065, time: 24.610532]
2023-05-12 16:46:26.149: epoch 50:	0.02853170  	0.07606890  	0.06279230  
2023-05-12 16:46:50.985: [iter 51 : loss : 0.5550 = 0.0941 + 0.4542 + 0.0067, time: 24.830023]
2023-05-12 16:46:51.278: epoch 51:	0.02849468  	0.07600126  	0.06295775  
2023-05-12 16:47:15.872: [iter 52 : loss : 0.5580 = 0.0910 + 0.4602 + 0.0068, time: 24.589436]
2023-05-12 16:47:16.133: epoch 52:	0.02838363  	0.07572747  	0.06268636  
2023-05-12 16:47:40.934: [iter 53 : loss : 0.5500 = 0.0894 + 0.4537 + 0.0069, time: 24.795960]
2023-05-12 16:47:41.194: epoch 53:	0.02840584  	0.07589833  	0.06272363  
2023-05-12 16:48:05.859: [iter 54 : loss : 0.5536 = 0.0870 + 0.4596 + 0.0071, time: 24.660185]
2023-05-12 16:48:06.120: epoch 54:	0.02835401  	0.07561263  	0.06254649  
2023-05-12 16:48:31.136: [iter 55 : loss : 0.5449 = 0.0845 + 0.4531 + 0.0072, time: 25.010604]
2023-05-12 16:48:31.406: epoch 55:	0.02830960  	0.07544572  	0.06233576  
2023-05-12 16:48:56.018: [iter 56 : loss : 0.5481 = 0.0816 + 0.4592 + 0.0073, time: 24.607979]
2023-05-12 16:48:56.299: epoch 56:	0.02825038  	0.07525208  	0.06207930  
2023-05-12 16:49:20.931: [iter 57 : loss : 0.5395 = 0.0794 + 0.4526 + 0.0075, time: 24.627754]
2023-05-12 16:49:21.191: epoch 57:	0.02820595  	0.07508893  	0.06191809  
2023-05-12 16:49:45.598: [iter 58 : loss : 0.5442 = 0.0778 + 0.4588 + 0.0076, time: 24.402535]
2023-05-12 16:49:45.916: epoch 58:	0.02810230  	0.07452641  	0.06163169  
2023-05-12 16:50:10.487: [iter 59 : loss : 0.5359 = 0.0760 + 0.4522 + 0.0077, time: 24.567286]
2023-05-12 16:50:10.750: epoch 59:	0.02805049  	0.07455783  	0.06170354  
2023-05-12 16:50:35.243: [iter 60 : loss : 0.5404 = 0.0743 + 0.4583 + 0.0078, time: 24.488924]
2023-05-12 16:50:35.510: epoch 60:	0.02807269  	0.07461435  	0.06158318  
2023-05-12 16:51:00.323: [iter 61 : loss : 0.5323 = 0.0726 + 0.4518 + 0.0080, time: 24.809770]
2023-05-12 16:51:00.604: epoch 61:	0.02808010  	0.07465418  	0.06145296  
2023-05-12 16:51:25.197: [iter 62 : loss : 0.5373 = 0.0713 + 0.4579 + 0.0081, time: 24.588989]
2023-05-12 16:51:25.464: epoch 62:	0.02798385  	0.07437363  	0.06127647  
2023-05-12 16:51:50.088: [iter 63 : loss : 0.5292 = 0.0696 + 0.4514 + 0.0082, time: 24.620738]
2023-05-12 16:51:50.363: epoch 63:	0.02786540  	0.07369465  	0.06093791  
2023-05-12 16:52:14.954: [iter 64 : loss : 0.5344 = 0.0685 + 0.4576 + 0.0083, time: 24.587859]
2023-05-12 16:52:15.215: epoch 64:	0.02781358  	0.07347950  	0.06077279  
2023-05-12 16:52:15.215: Early stopping is trigger at epoch: 64
2023-05-12 16:52:15.215: best_result@epoch 39:

2023-05-12 16:52:15.215: 		0.0288      	0.0774      	0.0631      
2023-05-12 19:15:59.366: my pid: 9252
2023-05-12 19:15:59.366: model: model.general_recommender.SGL
2023-05-12 19:15:59.366: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 19:15:59.366: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 19:16:02.764: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 19:16:26.629: [iter 1 : loss : 1.1340 = 0.6931 + 0.4409 + 0.0000, time: 23.863736]
2023-05-12 19:16:26.915: epoch 1:	0.00095499  	0.00248145  	0.00178766  
2023-05-12 19:16:26.915: Find a better model.
2023-05-12 19:16:51.138: [iter 2 : loss : 1.1348 = 0.6930 + 0.4418 + 0.0000, time: 24.219340]
2023-05-12 19:16:51.422: epoch 2:	0.00096239  	0.00218210  	0.00171443  
2023-05-12 19:17:15.884: [iter 3 : loss : 1.1332 = 0.6930 + 0.4402 + 0.0000, time: 24.459229]
2023-05-12 19:17:16.170: epoch 3:	0.00170270  	0.00336193  	0.00277112  
2023-05-12 19:17:16.170: Find a better model.
2023-05-12 19:17:40.536: [iter 4 : loss : 1.1348 = 0.6929 + 0.4419 + 0.0000, time: 24.360984]
2023-05-12 19:17:40.821: epoch 4:	0.00132514  	0.00238381  	0.00205410  
2023-05-12 19:18:05.241: [iter 5 : loss : 1.1334 = 0.6928 + 0.4405 + 0.0000, time: 24.416961]
2023-05-12 19:18:05.522: epoch 5:	0.00142878  	0.00278316  	0.00230167  
2023-05-12 19:18:29.756: [iter 6 : loss : 1.1350 = 0.6927 + 0.4423 + 0.0000, time: 24.229416]
2023-05-12 19:18:30.043: epoch 6:	0.00157684  	0.00312634  	0.00244464  
2023-05-12 19:18:54.468: [iter 7 : loss : 1.1336 = 0.6926 + 0.4409 + 0.0000, time: 24.421587]
2023-05-12 19:18:54.752: epoch 7:	0.00209506  	0.00444320  	0.00352440  
2023-05-12 19:18:54.752: Find a better model.
2023-05-12 19:19:18.923: [iter 8 : loss : 1.1353 = 0.6925 + 0.4428 + 0.0000, time: 24.166913]
2023-05-12 19:19:19.207: epoch 8:	0.00220610  	0.00405394  	0.00328932  
2023-05-12 19:19:43.463: [iter 9 : loss : 1.1337 = 0.6923 + 0.4414 + 0.0000, time: 24.253105]
2023-05-12 19:19:43.757: epoch 9:	0.00268730  	0.00496545  	0.00422844  
2023-05-12 19:19:43.757: Find a better model.
2023-05-12 19:20:07.903: [iter 10 : loss : 1.1355 = 0.6921 + 0.4434 + 0.0000, time: 24.141883]
2023-05-12 19:20:08.180: epoch 10:	0.00271691  	0.00555282  	0.00439096  
2023-05-12 19:20:08.180: Find a better model.
2023-05-12 19:20:32.355: [iter 11 : loss : 1.1336 = 0.6917 + 0.4418 + 0.0000, time: 24.171726]
2023-05-12 19:20:32.635: epoch 11:	0.00345721  	0.00738184  	0.00590597  
2023-05-12 19:20:32.635: Find a better model.
2023-05-12 19:20:56.680: [iter 12 : loss : 1.1355 = 0.6913 + 0.4441 + 0.0000, time: 24.041094]
2023-05-12 19:20:56.966: epoch 12:	0.00341279  	0.00729619  	0.00590803  
2023-05-12 19:21:21.135: [iter 13 : loss : 1.1330 = 0.6907 + 0.4423 + 0.0000, time: 24.163435]
2023-05-12 19:21:21.411: epoch 13:	0.00484157  	0.01114803  	0.00883398  
2023-05-12 19:21:21.411: Find a better model.
2023-05-12 19:21:45.268: [iter 14 : loss : 1.1348 = 0.6897 + 0.4451 + 0.0000, time: 23.853265]
2023-05-12 19:21:45.540: epoch 14:	0.00501924  	0.01276766  	0.00965216  
2023-05-12 19:21:45.540: Find a better model.
2023-05-12 19:22:09.714: [iter 15 : loss : 1.1312 = 0.6881 + 0.4431 + 0.0000, time: 24.168462]
2023-05-12 19:22:10.005: epoch 15:	0.00668490  	0.01760124  	0.01341293  
2023-05-12 19:22:10.005: Find a better model.
2023-05-12 19:22:33.864: [iter 16 : loss : 1.1322 = 0.6855 + 0.4466 + 0.0001, time: 23.855725]
2023-05-12 19:22:34.138: epoch 16:	0.00716609  	0.01947296  	0.01555230  
2023-05-12 19:22:34.138: Find a better model.
2023-05-12 19:22:58.300: [iter 17 : loss : 1.1258 = 0.6817 + 0.4441 + 0.0001, time: 24.157062]
2023-05-12 19:22:58.575: epoch 17:	0.01065292  	0.02860977  	0.02306841  
2023-05-12 19:22:58.576: Find a better model.
2023-05-12 19:23:22.448: [iter 18 : loss : 1.1242 = 0.6754 + 0.4487 + 0.0001, time: 23.867773]
2023-05-12 19:23:22.717: epoch 18:	0.01359197  	0.03701606  	0.02963364  
2023-05-12 19:23:22.717: Find a better model.
2023-05-12 19:23:46.716: [iter 19 : loss : 1.1104 = 0.6647 + 0.4456 + 0.0002, time: 23.994467]
2023-05-12 19:23:46.998: epoch 19:	0.01756007  	0.04718286  	0.03855919  
2023-05-12 19:23:46.998: Find a better model.
2023-05-12 19:24:10.823: [iter 20 : loss : 1.0994 = 0.6473 + 0.4518 + 0.0002, time: 23.820548]
2023-05-12 19:24:11.095: epoch 20:	0.02038068  	0.05452685  	0.04499117  
2023-05-12 19:24:11.095: Find a better model.
2023-05-12 19:24:35.077: [iter 21 : loss : 1.0691 = 0.6203 + 0.4484 + 0.0004, time: 23.977464]
2023-05-12 19:24:35.348: epoch 21:	0.02312730  	0.06149182  	0.05073448  
2023-05-12 19:24:35.348: Find a better model.
2023-05-12 19:24:59.019: [iter 22 : loss : 1.0398 = 0.5826 + 0.4566 + 0.0005, time: 23.667062]
2023-05-12 19:24:59.305: epoch 22:	0.02467459  	0.06629919  	0.05429075  
2023-05-12 19:24:59.305: Find a better model.
2023-05-12 19:25:23.072: [iter 23 : loss : 0.9905 = 0.5360 + 0.4537 + 0.0008, time: 23.763762]
2023-05-12 19:25:23.339: epoch 23:	0.02642917  	0.07069127  	0.05759285  
2023-05-12 19:25:23.339: Find a better model.
2023-05-12 19:25:46.996: [iter 24 : loss : 0.9496 = 0.4851 + 0.4635 + 0.0010, time: 23.652233]
2023-05-12 19:25:47.259: epoch 24:	0.02716948  	0.07252087  	0.05920649  
2023-05-12 19:25:47.259: Find a better model.
2023-05-12 19:26:11.069: [iter 25 : loss : 0.8958 = 0.4345 + 0.4600 + 0.0013, time: 23.806353]
2023-05-12 19:26:11.334: epoch 25:	0.02792461  	0.07527284  	0.06087945  
2023-05-12 19:26:11.334: Find a better model.
2023-05-12 19:26:35.014: [iter 26 : loss : 0.8594 = 0.3888 + 0.4690 + 0.0016, time: 23.674836]
2023-05-12 19:26:35.293: epoch 26:	0.02791722  	0.07518196  	0.06120355  
2023-05-12 19:26:59.202: [iter 27 : loss : 0.8142 = 0.3482 + 0.4641 + 0.0019, time: 23.903982]
2023-05-12 19:26:59.481: epoch 27:	0.02827259  	0.07718699  	0.06192076  
2023-05-12 19:26:59.481: Find a better model.
2023-05-12 19:27:23.153: [iter 28 : loss : 0.7882 = 0.3145 + 0.4715 + 0.0022, time: 23.669678]
2023-05-12 19:27:23.419: epoch 28:	0.02838364  	0.07749127  	0.06234154  
2023-05-12 19:27:23.419: Find a better model.
2023-05-12 19:27:47.402: [iter 29 : loss : 0.7541 = 0.2863 + 0.4653 + 0.0025, time: 23.977512]
2023-05-12 19:27:47.663: epoch 29:	0.02868716  	0.07829193  	0.06278363  
2023-05-12 19:27:47.663: Find a better model.
2023-05-12 19:28:11.331: [iter 30 : loss : 0.7358 = 0.2613 + 0.4717 + 0.0027, time: 23.665232]
2023-05-12 19:28:11.593: epoch 30:	0.02870937  	0.07824680  	0.06289697  
2023-05-12 19:28:35.604: [iter 31 : loss : 0.7090 = 0.2410 + 0.4650 + 0.0030, time: 24.006373]
2023-05-12 19:28:35.866: epoch 31:	0.02876119  	0.07852558  	0.06309063  
2023-05-12 19:28:35.866: Find a better model.
2023-05-12 19:28:59.558: [iter 32 : loss : 0.6979 = 0.2238 + 0.4708 + 0.0032, time: 23.687654]
2023-05-12 19:28:59.819: epoch 32:	0.02885003  	0.07866725  	0.06323449  
2023-05-12 19:28:59.820: Find a better model.
2023-05-12 19:29:23.970: [iter 33 : loss : 0.6764 = 0.2092 + 0.4637 + 0.0035, time: 24.145922]
2023-05-12 19:29:24.234: epoch 33:	0.02897588  	0.07958240  	0.06352726  
2023-05-12 19:29:24.234: Find a better model.
2023-05-12 19:29:48.131: [iter 34 : loss : 0.6686 = 0.1955 + 0.4694 + 0.0037, time: 23.893488]
2023-05-12 19:29:48.392: epoch 34:	0.02902030  	0.07989860  	0.06367192  
2023-05-12 19:29:48.393: Find a better model.
2023-05-12 19:30:12.527: [iter 35 : loss : 0.6493 = 0.1829 + 0.4625 + 0.0039, time: 24.131640]
2023-05-12 19:30:12.788: epoch 35:	0.02908693  	0.08051535  	0.06390225  
2023-05-12 19:30:12.788: Find a better model.
2023-05-12 19:30:36.896: [iter 36 : loss : 0.6456 = 0.1733 + 0.4682 + 0.0041, time: 24.104038]
2023-05-12 19:30:37.169: epoch 36:	0.02919799  	0.08106539  	0.06423408  
2023-05-12 19:30:37.169: Find a better model.
2023-05-12 19:31:01.137: [iter 37 : loss : 0.6296 = 0.1642 + 0.4611 + 0.0043, time: 23.964050]
2023-05-12 19:31:01.396: epoch 37:	0.02922761  	0.08132781  	0.06434498  
2023-05-12 19:31:01.396: Find a better model.
2023-05-12 19:31:25.270: [iter 38 : loss : 0.6266 = 0.1554 + 0.4667 + 0.0045, time: 23.870379]
2023-05-12 19:31:25.530: epoch 38:	0.02919059  	0.08078266  	0.06426054  
2023-05-12 19:31:49.749: [iter 39 : loss : 0.6127 = 0.1483 + 0.4597 + 0.0047, time: 24.215680]
2023-05-12 19:31:50.047: epoch 39:	0.02915357  	0.08097248  	0.06440438  
2023-05-12 19:32:13.890: [iter 40 : loss : 0.6115 = 0.1411 + 0.4655 + 0.0049, time: 23.839401]
2023-05-12 19:32:14.159: epoch 40:	0.02916838  	0.08055948  	0.06434258  
2023-05-12 19:32:38.307: [iter 41 : loss : 0.5982 = 0.1344 + 0.4586 + 0.0051, time: 24.144807]
2023-05-12 19:32:38.568: epoch 41:	0.02916098  	0.08063698  	0.06417833  
2023-05-12 19:33:02.664: [iter 42 : loss : 0.5987 = 0.1289 + 0.4645 + 0.0053, time: 24.092034]
2023-05-12 19:33:02.922: epoch 42:	0.02927943  	0.08065413  	0.06429548  
2023-05-12 19:33:26.913: [iter 43 : loss : 0.5865 = 0.1234 + 0.4576 + 0.0054, time: 23.986518]
2023-05-12 19:33:27.178: epoch 43:	0.02922760  	0.08048458  	0.06433796  
2023-05-12 19:33:51.041: [iter 44 : loss : 0.5885 = 0.1194 + 0.4635 + 0.0056, time: 23.858942]
2023-05-12 19:33:51.301: epoch 44:	0.02929422  	0.08039210  	0.06440774  
2023-05-12 19:34:15.493: [iter 45 : loss : 0.5767 = 0.1144 + 0.4565 + 0.0058, time: 24.187658]
2023-05-12 19:34:15.756: epoch 45:	0.02922018  	0.08017539  	0.06430499  
2023-05-12 19:34:39.645: [iter 46 : loss : 0.5788 = 0.1103 + 0.4626 + 0.0059, time: 23.885051]
2023-05-12 19:34:39.905: epoch 46:	0.02924240  	0.07979687  	0.06413770  
2023-05-12 19:35:04.077: [iter 47 : loss : 0.5685 = 0.1066 + 0.4558 + 0.0061, time: 24.168024]
2023-05-12 19:35:04.336: epoch 47:	0.02916837  	0.07970320  	0.06399839  
2023-05-12 19:35:28.375: [iter 48 : loss : 0.5711 = 0.1030 + 0.4619 + 0.0062, time: 24.033375]
2023-05-12 19:35:28.634: epoch 48:	0.02924240  	0.07945231  	0.06391217  
2023-05-12 19:35:52.673: [iter 49 : loss : 0.5613 = 0.0998 + 0.4551 + 0.0064, time: 24.033812]
2023-05-12 19:35:52.933: epoch 49:	0.02904251  	0.07899864  	0.06375603  
2023-05-12 19:36:16.774: [iter 50 : loss : 0.5643 = 0.0967 + 0.4611 + 0.0065, time: 23.838789]
2023-05-12 19:36:17.040: epoch 50:	0.02893887  	0.07864849  	0.06359005  
2023-05-12 19:36:41.238: [iter 51 : loss : 0.5543 = 0.0932 + 0.4544 + 0.0067, time: 24.186264]
2023-05-12 19:36:41.515: epoch 51:	0.02895367  	0.07853311  	0.06364217  
2023-05-12 19:37:05.357: [iter 52 : loss : 0.5578 = 0.0904 + 0.4606 + 0.0068, time: 23.837146]
2023-05-12 19:37:05.616: epoch 52:	0.02893146  	0.07809928  	0.06341556  
2023-05-12 19:37:29.843: [iter 53 : loss : 0.5494 = 0.0886 + 0.4538 + 0.0070, time: 24.223104]
2023-05-12 19:37:30.131: epoch 53:	0.02892405  	0.07814117  	0.06327546  
2023-05-12 19:37:54.327: [iter 54 : loss : 0.5533 = 0.0863 + 0.4599 + 0.0071, time: 24.190986]
2023-05-12 19:37:54.589: epoch 54:	0.02878338  	0.07699291  	0.06275589  
2023-05-12 19:38:18.793: [iter 55 : loss : 0.5442 = 0.0837 + 0.4533 + 0.0072, time: 24.200213]
2023-05-12 19:38:19.062: epoch 55:	0.02870195  	0.07695162  	0.06294633  
2023-05-12 19:38:42.940: [iter 56 : loss : 0.5479 = 0.0811 + 0.4594 + 0.0074, time: 23.867008]
2023-05-12 19:38:43.208: epoch 56:	0.02858349  	0.07686331  	0.06280185  
2023-05-12 19:39:07.387: [iter 57 : loss : 0.5387 = 0.0784 + 0.4528 + 0.0075, time: 24.175731]
2023-05-12 19:39:07.648: epoch 57:	0.02859090  	0.07712881  	0.06284735  
2023-05-12 19:39:31.497: [iter 58 : loss : 0.5439 = 0.0772 + 0.4591 + 0.0076, time: 23.844563]
2023-05-12 19:39:31.759: epoch 58:	0.02865012  	0.07669649  	0.06272199  
2023-05-12 19:39:55.937: [iter 59 : loss : 0.5355 = 0.0752 + 0.4525 + 0.0077, time: 24.174643]
2023-05-12 19:39:56.204: epoch 59:	0.02865012  	0.07694292  	0.06273087  
2023-05-12 19:40:20.289: [iter 60 : loss : 0.5403 = 0.0738 + 0.4588 + 0.0078, time: 24.080290]
2023-05-12 19:40:20.550: epoch 60:	0.02856868  	0.07637069  	0.06241730  
2023-05-12 19:40:44.540: [iter 61 : loss : 0.5321 = 0.0720 + 0.4521 + 0.0080, time: 23.986008]
2023-05-12 19:40:44.817: epoch 61:	0.02849465  	0.07611547  	0.06229228  
2023-05-12 19:41:08.872: [iter 62 : loss : 0.5369 = 0.0706 + 0.4582 + 0.0081, time: 24.052079]
2023-05-12 19:41:09.148: epoch 62:	0.02842062  	0.07573565  	0.06203959  
2023-05-12 19:41:09.148: Early stopping is trigger at epoch: 62
2023-05-12 19:41:09.148: best_result@epoch 37:

2023-05-12 19:41:09.148: 		0.0292      	0.0813      	0.0643      
2023-05-12 20:43:38.514: my pid: 8068
2023-05-12 20:43:38.514: model: model.general_recommender.SGL
2023-05-12 20:43:38.514: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 20:43:38.514: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 20:43:41.930: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 20:44:03.486: [iter 1 : loss : 1.1338 = 0.6931 + 0.4408 + 0.0000, time: 21.554233]
2023-05-12 20:44:03.753: epoch 1:	0.00119189  	0.00219675  	0.00197558  
2023-05-12 20:44:03.754: Find a better model.
2023-05-12 20:44:25.547: [iter 2 : loss : 1.1357 = 0.6930 + 0.4426 + 0.0000, time: 21.790521]
2023-05-12 20:44:25.834: epoch 2:	0.00144359  	0.00266317  	0.00231131  
2023-05-12 20:44:25.834: Find a better model.
2023-05-12 20:44:47.852: [iter 3 : loss : 1.1334 = 0.6930 + 0.4405 + 0.0000, time: 22.012202]
2023-05-12 20:44:48.139: epoch 3:	0.00189517  	0.00436671  	0.00324113  
2023-05-12 20:44:48.139: Find a better model.
2023-05-12 20:45:10.243: [iter 4 : loss : 1.1361 = 0.6929 + 0.4432 + 0.0000, time: 22.100060]
2023-05-12 20:45:10.543: epoch 4:	0.00216909  	0.00377718  	0.00299909  
2023-05-12 20:45:32.630: [iter 5 : loss : 1.1338 = 0.6928 + 0.4410 + 0.0000, time: 22.084078]
2023-05-12 20:45:32.917: epoch 5:	0.00269470  	0.00594254  	0.00435268  
2023-05-12 20:45:32.917: Find a better model.
2023-05-12 20:45:54.881: [iter 6 : loss : 1.1366 = 0.6927 + 0.4439 + 0.0000, time: 21.959936]
2023-05-12 20:45:55.166: epoch 6:	0.00283536  	0.00552284  	0.00446558  
2023-05-12 20:46:17.272: [iter 7 : loss : 1.1340 = 0.6925 + 0.4415 + 0.0000, time: 22.103695]
2023-05-12 20:46:17.590: epoch 7:	0.00356826  	0.00770603  	0.00592091  
2023-05-12 20:46:17.590: Find a better model.
2023-05-12 20:46:39.434: [iter 8 : loss : 1.1370 = 0.6923 + 0.4447 + 0.0000, time: 21.840197]
2023-05-12 20:46:39.719: epoch 8:	0.00356085  	0.00659059  	0.00561132  
2023-05-12 20:47:01.614: [iter 9 : loss : 1.1341 = 0.6921 + 0.4420 + 0.0000, time: 21.891557]
2023-05-12 20:47:01.893: epoch 9:	0.00472312  	0.01064250  	0.00820404  
2023-05-12 20:47:01.893: Find a better model.
2023-05-12 20:47:23.862: [iter 10 : loss : 1.1373 = 0.6917 + 0.4456 + 0.0000, time: 21.965235]
2023-05-12 20:47:24.167: epoch 10:	0.00476013  	0.01006592  	0.00840048  
2023-05-12 20:47:45.980: [iter 11 : loss : 1.1336 = 0.6911 + 0.4425 + 0.0000, time: 21.807982]
2023-05-12 20:47:46.258: epoch 11:	0.00586317  	0.01300435  	0.01076008  
2023-05-12 20:47:46.259: Find a better model.
2023-05-12 20:48:07.808: [iter 12 : loss : 1.1370 = 0.6902 + 0.4468 + 0.0000, time: 21.545366]
2023-05-12 20:48:08.087: epoch 12:	0.00599643  	0.01261097  	0.01092712  
2023-05-12 20:48:29.833: [iter 13 : loss : 1.1320 = 0.6889 + 0.4431 + 0.0000, time: 21.741849]
2023-05-12 20:48:30.124: epoch 13:	0.00769170  	0.01789446  	0.01527336  
2023-05-12 20:48:30.124: Find a better model.
2023-05-12 20:48:51.586: [iter 14 : loss : 1.1357 = 0.6872 + 0.4485 + 0.0000, time: 21.455639]
2023-05-12 20:48:51.861: epoch 14:	0.00936477  	0.02171070  	0.01873622  
2023-05-12 20:48:51.861: Find a better model.
2023-05-12 20:49:13.384: [iter 15 : loss : 1.1287 = 0.6849 + 0.4438 + 0.0001, time: 21.519059]
2023-05-12 20:49:13.666: epoch 15:	0.01170415  	0.02873404  	0.02500234  
2023-05-12 20:49:13.666: Find a better model.
2023-05-12 20:49:35.201: [iter 16 : loss : 1.1317 = 0.6810 + 0.4506 + 0.0001, time: 21.530653]
2023-05-12 20:49:35.472: epoch 16:	0.01462100  	0.03539020  	0.03133474  
2023-05-12 20:49:35.472: Find a better model.
2023-05-12 20:49:57.195: [iter 17 : loss : 1.1198 = 0.6751 + 0.4446 + 0.0001, time: 21.718827]
2023-05-12 20:49:57.484: epoch 17:	0.01777476  	0.04524767  	0.03953227  
2023-05-12 20:49:57.485: Find a better model.
2023-05-12 20:50:18.810: [iter 18 : loss : 1.1184 = 0.6647 + 0.4535 + 0.0002, time: 21.321730]
2023-05-12 20:50:19.082: epoch 18:	0.02045472  	0.05186346  	0.04606821  
2023-05-12 20:50:19.083: Find a better model.
2023-05-12 20:50:40.699: [iter 19 : loss : 1.0948 = 0.6481 + 0.4464 + 0.0002, time: 21.611720]
2023-05-12 20:50:40.970: epoch 19:	0.02321613  	0.05940579  	0.05186115  
2023-05-12 20:50:40.970: Find a better model.
2023-05-12 20:51:02.367: [iter 20 : loss : 1.0810 = 0.6229 + 0.4578 + 0.0004, time: 21.393306]
2023-05-12 20:51:02.665: epoch 20:	0.02494849  	0.06496510  	0.05602055  
2023-05-12 20:51:02.665: Find a better model.
2023-05-12 20:51:24.358: [iter 21 : loss : 1.0380 = 0.5872 + 0.4502 + 0.0005, time: 21.689065]
2023-05-12 20:51:24.661: epoch 21:	0.02648837  	0.06961251  	0.05948472  
2023-05-12 20:51:24.662: Find a better model.
2023-05-12 20:51:45.972: [iter 22 : loss : 1.0072 = 0.5429 + 0.4636 + 0.0007, time: 21.306797]
2023-05-12 20:51:46.239: epoch 22:	0.02736196  	0.07330659  	0.06113596  
2023-05-12 20:51:46.239: Find a better model.
2023-05-12 20:52:07.743: [iter 23 : loss : 0.9498 = 0.4929 + 0.4559 + 0.0010, time: 21.499763]
2023-05-12 20:52:08.007: epoch 23:	0.02806525  	0.07499782  	0.06276333  
2023-05-12 20:52:08.007: Find a better model.
2023-05-12 20:52:29.513: [iter 24 : loss : 0.9141 = 0.4428 + 0.4700 + 0.0013, time: 21.500933]
2023-05-12 20:52:29.785: epoch 24:	0.02821332  	0.07600722  	0.06343865  
2023-05-12 20:52:29.785: Find a better model.
2023-05-12 20:52:51.340: [iter 25 : loss : 0.8585 = 0.3957 + 0.4612 + 0.0016, time: 21.550185]
2023-05-12 20:52:51.640: epoch 25:	0.02878338  	0.07785026  	0.06428356  
2023-05-12 20:52:51.641: Find a better model.
2023-05-12 20:53:13.305: [iter 26 : loss : 0.8309 = 0.3548 + 0.4742 + 0.0019, time: 21.656629]
2023-05-12 20:53:13.567: epoch 26:	0.02880558  	0.07849332  	0.06435138  
2023-05-12 20:53:13.567: Find a better model.
2023-05-12 20:53:35.259: [iter 27 : loss : 0.7854 = 0.3192 + 0.4640 + 0.0022, time: 21.688644]
2023-05-12 20:53:35.523: epoch 27:	0.02890924  	0.07863020  	0.06453300  
2023-05-12 20:53:35.523: Find a better model.
2023-05-12 20:53:57.093: [iter 28 : loss : 0.7676 = 0.2896 + 0.4756 + 0.0024, time: 21.565314]
2023-05-12 20:53:57.356: epoch 28:	0.02909431  	0.07951187  	0.06488182  
2023-05-12 20:53:57.356: Find a better model.
2023-05-12 20:54:19.249: [iter 29 : loss : 0.7316 = 0.2645 + 0.4644 + 0.0027, time: 21.888055]
2023-05-12 20:54:19.512: epoch 29:	0.02943487  	0.08013026  	0.06538939  
2023-05-12 20:54:19.512: Find a better model.
2023-05-12 20:54:41.468: [iter 30 : loss : 0.7208 = 0.2424 + 0.4754 + 0.0030, time: 21.951606]
2023-05-12 20:54:41.757: epoch 30:	0.02950150  	0.08038571  	0.06545729  
2023-05-12 20:54:41.757: Find a better model.
2023-05-12 20:55:03.655: [iter 31 : loss : 0.6910 = 0.2239 + 0.4639 + 0.0032, time: 21.893912]
2023-05-12 20:55:03.919: epoch 31:	0.02960515  	0.08058133  	0.06552547  
2023-05-12 20:55:03.919: Find a better model.
2023-05-12 20:55:25.833: [iter 32 : loss : 0.6859 = 0.2082 + 0.4742 + 0.0035, time: 21.910412]
2023-05-12 20:55:26.093: epoch 32:	0.02964956  	0.08088619  	0.06556445  
2023-05-12 20:55:26.094: Find a better model.
2023-05-12 20:55:47.853: [iter 33 : loss : 0.6613 = 0.1951 + 0.4626 + 0.0037, time: 21.754766]
2023-05-12 20:55:48.110: epoch 33:	0.02972360  	0.08110154  	0.06584194  
2023-05-12 20:55:48.110: Find a better model.
2023-05-12 20:56:09.848: [iter 34 : loss : 0.6596 = 0.1827 + 0.4730 + 0.0039, time: 21.734694]
2023-05-12 20:56:10.112: epoch 34:	0.02973840  	0.08062091  	0.06583583  
2023-05-12 20:56:32.167: [iter 35 : loss : 0.6366 = 0.1712 + 0.4613 + 0.0041, time: 22.051246]
2023-05-12 20:56:32.428: epoch 35:	0.02985685  	0.08113356  	0.06596640  
2023-05-12 20:56:32.429: Find a better model.
2023-05-12 20:56:54.437: [iter 36 : loss : 0.6382 = 0.1623 + 0.4716 + 0.0043, time: 22.003933]
2023-05-12 20:56:54.714: epoch 36:	0.02979022  	0.08087900  	0.06592847  
2023-05-12 20:57:16.789: [iter 37 : loss : 0.6184 = 0.1538 + 0.4600 + 0.0045, time: 22.070354]
2023-05-12 20:57:17.049: epoch 37:	0.02983465  	0.08098640  	0.06589764  
2023-05-12 20:57:39.206: [iter 38 : loss : 0.6212 = 0.1461 + 0.4704 + 0.0047, time: 22.153170]
2023-05-12 20:57:39.472: epoch 38:	0.02990128  	0.08148108  	0.06595388  
2023-05-12 20:57:39.472: Find a better model.
2023-05-12 20:58:01.585: [iter 39 : loss : 0.6030 = 0.1393 + 0.4588 + 0.0049, time: 22.107883]
2023-05-12 20:58:01.851: epoch 39:	0.02990868  	0.08176038  	0.06622815  
2023-05-12 20:58:01.851: Find a better model.
2023-05-12 20:58:24.010: [iter 40 : loss : 0.6072 = 0.1328 + 0.4693 + 0.0051, time: 22.153869]
2023-05-12 20:58:24.274: epoch 40:	0.02987167  	0.08169516  	0.06624965  
2023-05-12 20:58:46.371: [iter 41 : loss : 0.5895 = 0.1265 + 0.4577 + 0.0053, time: 22.092703]
2023-05-12 20:58:46.631: epoch 41:	0.02992349  	0.08176649  	0.06641286  
2023-05-12 20:58:46.631: Find a better model.
2023-05-12 20:59:08.633: [iter 42 : loss : 0.5953 = 0.1215 + 0.4683 + 0.0054, time: 21.998587]
2023-05-12 20:59:08.897: epoch 42:	0.02988647  	0.08135715  	0.06632328  
2023-05-12 20:59:30.940: [iter 43 : loss : 0.5788 = 0.1164 + 0.4567 + 0.0056, time: 22.038902]
2023-05-12 20:59:31.202: epoch 43:	0.02987166  	0.08129552  	0.06621931  
2023-05-12 20:59:53.133: [iter 44 : loss : 0.5860 = 0.1128 + 0.4674 + 0.0058, time: 21.925574]
2023-05-12 20:59:53.396: epoch 44:	0.02973842  	0.08118390  	0.06607118  
2023-05-12 21:00:15.359: [iter 45 : loss : 0.5701 = 0.1083 + 0.4558 + 0.0059, time: 21.958171]
2023-05-12 21:00:15.640: epoch 45:	0.02967918  	0.08099956  	0.06604975  
2023-05-12 21:00:37.552: [iter 46 : loss : 0.5770 = 0.1044 + 0.4665 + 0.0061, time: 21.909220]
2023-05-12 21:00:37.824: epoch 46:	0.02955333  	0.08092749  	0.06581605  
2023-05-12 21:00:59.978: [iter 47 : loss : 0.5621 = 0.1008 + 0.4551 + 0.0063, time: 22.149526]
2023-05-12 21:01:00.261: epoch 47:	0.02953851  	0.08088908  	0.06580305  
2023-05-12 21:01:21.963: [iter 48 : loss : 0.5699 = 0.0976 + 0.4659 + 0.0064, time: 21.697153]
2023-05-12 21:01:22.226: epoch 48:	0.02950150  	0.08071087  	0.06566180  
2023-05-12 21:01:44.161: [iter 49 : loss : 0.5554 = 0.0945 + 0.4544 + 0.0066, time: 21.931275]
2023-05-12 21:01:44.444: epoch 49:	0.02950151  	0.08065701  	0.06556339  
2023-05-12 21:02:05.960: [iter 50 : loss : 0.5636 = 0.0917 + 0.4652 + 0.0067, time: 21.510834]
2023-05-12 21:02:06.238: epoch 50:	0.02946450  	0.08060046  	0.06546187  
2023-05-12 21:02:28.079: [iter 51 : loss : 0.5494 = 0.0888 + 0.4538 + 0.0068, time: 21.837910]
2023-05-12 21:02:28.339: epoch 51:	0.02934605  	0.08032758  	0.06524330  
2023-05-12 21:02:50.278: [iter 52 : loss : 0.5575 = 0.0857 + 0.4648 + 0.0070, time: 21.933592]
2023-05-12 21:02:50.558: epoch 52:	0.02928682  	0.08016947  	0.06519926  
2023-05-12 21:03:12.271: [iter 53 : loss : 0.5449 = 0.0846 + 0.4531 + 0.0071, time: 21.709583]
2023-05-12 21:03:12.535: epoch 53:	0.02927942  	0.08019532  	0.06525423  
2023-05-12 21:03:34.321: [iter 54 : loss : 0.5534 = 0.0820 + 0.4641 + 0.0073, time: 21.783375]
2023-05-12 21:03:34.581: epoch 54:	0.02921278  	0.08001929  	0.06519631  
2023-05-12 21:03:56.687: [iter 55 : loss : 0.5398 = 0.0798 + 0.4526 + 0.0074, time: 22.101799]
2023-05-12 21:03:56.974: epoch 55:	0.02933863  	0.08006500  	0.06536499  
2023-05-12 21:04:18.675: [iter 56 : loss : 0.5484 = 0.0772 + 0.4636 + 0.0075, time: 21.697615]
2023-05-12 21:04:18.943: epoch 56:	0.02924979  	0.07985348  	0.06504422  
2023-05-12 21:04:40.844: [iter 57 : loss : 0.5351 = 0.0753 + 0.4522 + 0.0076, time: 21.895936]
2023-05-12 21:04:41.106: epoch 57:	0.02924239  	0.07964991  	0.06505088  
2023-05-12 21:05:02.873: [iter 58 : loss : 0.5448 = 0.0738 + 0.4633 + 0.0078, time: 21.763536]
2023-05-12 21:05:03.135: epoch 58:	0.02922758  	0.07960229  	0.06503931  
2023-05-12 21:05:24.848: [iter 59 : loss : 0.5315 = 0.0719 + 0.4517 + 0.0079, time: 21.707560]
2023-05-12 21:05:25.131: epoch 59:	0.02919056  	0.07946688  	0.06492438  
2023-05-12 21:05:46.632: [iter 60 : loss : 0.5412 = 0.0702 + 0.4630 + 0.0080, time: 21.497558]
2023-05-12 21:05:46.900: epoch 60:	0.02900548  	0.07855553  	0.06453837  
2023-05-12 21:06:08.810: [iter 61 : loss : 0.5282 = 0.0687 + 0.4514 + 0.0081, time: 21.906793]
2023-05-12 21:06:09.072: epoch 61:	0.02893144  	0.07853679  	0.06439159  
2023-05-12 21:06:30.866: [iter 62 : loss : 0.5387 = 0.0679 + 0.4626 + 0.0082, time: 21.790725]
2023-05-12 21:06:31.128: epoch 62:	0.02889443  	0.07821979  	0.06425269  
2023-05-12 21:06:53.032: [iter 63 : loss : 0.5254 = 0.0660 + 0.4511 + 0.0083, time: 21.900058]
2023-05-12 21:06:53.295: epoch 63:	0.02880559  	0.07811249  	0.06417278  
2023-05-12 21:07:15.033: [iter 64 : loss : 0.5357 = 0.0650 + 0.4623 + 0.0084, time: 21.732743]
2023-05-12 21:07:15.294: epoch 64:	0.02870935  	0.07754155  	0.06369440  
2023-05-12 21:07:37.027: [iter 65 : loss : 0.5228 = 0.0635 + 0.4507 + 0.0085, time: 21.729638]
2023-05-12 21:07:37.288: epoch 65:	0.02867974  	0.07723703  	0.06370550  
2023-05-12 21:07:58.846: [iter 66 : loss : 0.5328 = 0.0621 + 0.4620 + 0.0087, time: 21.553859]
2023-05-12 21:07:59.106: epoch 66:	0.02868715  	0.07713906  	0.06362798  
2023-05-12 21:07:59.106: Early stopping is trigger at epoch: 66
2023-05-12 21:07:59.106: best_result@epoch 41:

2023-05-12 21:07:59.106: 		0.0299      	0.0818      	0.0664      
2023-05-12 21:13:57.697: my pid: 4472
2023-05-12 21:13:57.697: model: model.general_recommender.SGL
2023-05-12 21:13:57.697: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 21:13:57.697: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 21:14:01.093: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 21:14:23.070: [iter 1 : loss : 1.1334 = 0.6931 + 0.4403 + 0.0000, time: 21.976455]
2023-05-12 21:14:23.342: epoch 1:	0.00119929  	0.00214631  	0.00184602  
2023-05-12 21:14:23.342: Find a better model.
2023-05-12 21:14:45.903: [iter 2 : loss : 1.1347 = 0.6930 + 0.4417 + 0.0000, time: 22.556462]
2023-05-12 21:14:46.196: epoch 2:	0.00160646  	0.00296063  	0.00236266  
2023-05-12 21:14:46.196: Find a better model.
2023-05-12 21:15:08.855: [iter 3 : loss : 1.1324 = 0.6930 + 0.4394 + 0.0000, time: 22.655714]
2023-05-12 21:15:09.164: epoch 3:	0.00198401  	0.00424803  	0.00351537  
2023-05-12 21:15:09.164: Find a better model.
2023-05-12 21:15:31.647: [iter 4 : loss : 1.1350 = 0.6929 + 0.4421 + 0.0000, time: 22.478974]
2023-05-12 21:15:31.954: epoch 4:	0.00216909  	0.00439268  	0.00364409  
2023-05-12 21:15:31.954: Find a better model.
2023-05-12 21:15:54.816: [iter 5 : loss : 1.1325 = 0.6928 + 0.4397 + 0.0000, time: 22.858437]
2023-05-12 21:15:55.107: epoch 5:	0.00276133  	0.00578169  	0.00436444  
2023-05-12 21:15:55.107: Find a better model.
2023-05-12 21:16:17.411: [iter 6 : loss : 1.1354 = 0.6927 + 0.4426 + 0.0000, time: 22.300777]
2023-05-12 21:16:17.724: epoch 6:	0.00314628  	0.00555876  	0.00484337  
2023-05-12 21:16:40.190: [iter 7 : loss : 1.1326 = 0.6926 + 0.4400 + 0.0000, time: 22.462907]
2023-05-12 21:16:40.481: epoch 7:	0.00390880  	0.00860729  	0.00676550  
2023-05-12 21:16:40.481: Find a better model.
2023-05-12 21:17:03.016: [iter 8 : loss : 1.1356 = 0.6924 + 0.4432 + 0.0000, time: 22.531606]
2023-05-12 21:17:03.307: epoch 8:	0.00405685  	0.00833979  	0.00700703  
2023-05-12 21:17:25.961: [iter 9 : loss : 1.1325 = 0.6922 + 0.4403 + 0.0000, time: 22.649887]
2023-05-12 21:17:26.252: epoch 9:	0.00501184  	0.01077850  	0.00898779  
2023-05-12 21:17:26.252: Find a better model.
2023-05-12 21:17:48.559: [iter 10 : loss : 1.1359 = 0.6919 + 0.4440 + 0.0000, time: 22.303792]
2023-05-12 21:17:48.847: epoch 10:	0.00544861  	0.01113490  	0.00948223  
2023-05-12 21:17:48.847: Find a better model.
2023-05-12 21:18:11.592: [iter 11 : loss : 1.1321 = 0.6914 + 0.4406 + 0.0000, time: 22.740600]
2023-05-12 21:18:11.911: epoch 11:	0.00669971  	0.01545716  	0.01273822  
2023-05-12 21:18:11.912: Find a better model.
2023-05-12 21:18:34.190: [iter 12 : loss : 1.1357 = 0.6907 + 0.4450 + 0.0000, time: 22.270014]
2023-05-12 21:18:34.481: epoch 12:	0.00730675  	0.01587288  	0.01304009  
2023-05-12 21:18:34.481: Find a better model.
2023-05-12 21:18:56.993: [iter 13 : loss : 1.1306 = 0.6896 + 0.4410 + 0.0000, time: 22.507715]
2023-05-12 21:18:57.275: epoch 13:	0.00820991  	0.01977818  	0.01632828  
2023-05-12 21:18:57.275: Find a better model.
2023-05-12 21:19:19.746: [iter 14 : loss : 1.1344 = 0.6877 + 0.4466 + 0.0000, time: 22.466144]
2023-05-12 21:19:20.041: epoch 14:	0.01007548  	0.02329292  	0.02004026  
2023-05-12 21:19:20.041: Find a better model.
2023-05-12 21:19:42.372: [iter 15 : loss : 1.1269 = 0.6853 + 0.4416 + 0.0001, time: 22.326591]
2023-05-12 21:19:42.653: epoch 15:	0.01174856  	0.02969996  	0.02525992  
2023-05-12 21:19:42.653: Find a better model.
2023-05-12 21:20:05.119: [iter 16 : loss : 1.1301 = 0.6813 + 0.4487 + 0.0001, time: 22.460788]
2023-05-12 21:20:05.401: epoch 16:	0.01478387  	0.03613199  	0.03200294  
2023-05-12 21:20:05.401: Find a better model.
2023-05-12 21:20:27.907: [iter 17 : loss : 1.1174 = 0.6750 + 0.4423 + 0.0001, time: 22.501817]
2023-05-12 21:20:28.190: epoch 17:	0.01750085  	0.04511201  	0.03928491  
2023-05-12 21:20:28.190: Find a better model.
2023-05-12 21:20:50.385: [iter 18 : loss : 1.1155 = 0.6636 + 0.4517 + 0.0002, time: 22.190888]
2023-05-12 21:20:50.664: epoch 18:	0.02126169  	0.05401567  	0.04723943  
2023-05-12 21:20:50.664: Find a better model.
2023-05-12 21:21:13.098: [iter 19 : loss : 1.0896 = 0.6452 + 0.4442 + 0.0002, time: 22.429175]
2023-05-12 21:21:13.397: epoch 19:	0.02358631  	0.06128725  	0.05293817  
2023-05-12 21:21:13.397: Find a better model.
2023-05-12 21:21:35.500: [iter 20 : loss : 1.0732 = 0.6170 + 0.4559 + 0.0004, time: 22.098910]
2023-05-12 21:21:35.777: epoch 20:	0.02530387  	0.06605205  	0.05738435  
2023-05-12 21:21:35.777: Find a better model.
2023-05-12 21:21:58.313: [iter 21 : loss : 1.0267 = 0.5778 + 0.4483 + 0.0006, time: 22.531630]
2023-05-12 21:21:58.610: epoch 21:	0.02656242  	0.06945791  	0.06056810  
2023-05-12 21:21:58.610: Find a better model.
2023-05-12 21:22:20.940: [iter 22 : loss : 0.9924 = 0.5300 + 0.4616 + 0.0008, time: 22.325688]
2023-05-12 21:22:21.225: epoch 22:	0.02722871  	0.07238159  	0.06215041  
2023-05-12 21:22:21.225: Find a better model.
2023-05-12 21:22:43.543: [iter 23 : loss : 0.9335 = 0.4780 + 0.4544 + 0.0011, time: 22.313063]
2023-05-12 21:22:43.839: epoch 23:	0.02810969  	0.07507728  	0.06406011  
2023-05-12 21:22:43.840: Find a better model.
2023-05-12 21:23:06.301: [iter 24 : loss : 0.8966 = 0.4275 + 0.4678 + 0.0014, time: 22.455347]
2023-05-12 21:23:06.600: epoch 24:	0.02835399  	0.07568663  	0.06456348  
2023-05-12 21:23:06.600: Find a better model.
2023-05-12 21:23:29.077: [iter 25 : loss : 0.8424 = 0.3813 + 0.4595 + 0.0017, time: 22.473758]
2023-05-12 21:23:29.355: epoch 25:	0.02869456  	0.07690373  	0.06506450  
2023-05-12 21:23:29.355: Find a better model.
2023-05-12 21:23:51.485: [iter 26 : loss : 0.8151 = 0.3417 + 0.4715 + 0.0019, time: 22.126132]
2023-05-12 21:23:51.764: epoch 26:	0.02870197  	0.07745668  	0.06532314  
2023-05-12 21:23:51.764: Find a better model.
2023-05-12 21:24:14.051: [iter 27 : loss : 0.7721 = 0.3080 + 0.4618 + 0.0022, time: 22.281356]
2023-05-12 21:24:14.330: epoch 27:	0.02889446  	0.07783687  	0.06546991  
2023-05-12 21:24:14.330: Find a better model.
2023-05-12 21:24:36.651: [iter 28 : loss : 0.7546 = 0.2797 + 0.4724 + 0.0025, time: 22.317899]
2023-05-12 21:24:36.935: epoch 28:	0.02886485  	0.07799762  	0.06553800  
2023-05-12 21:24:36.936: Find a better model.
2023-05-12 21:24:59.239: [iter 29 : loss : 0.7208 = 0.2559 + 0.4620 + 0.0028, time: 22.298086]
2023-05-12 21:24:59.536: epoch 29:	0.02915356  	0.07855430  	0.06611955  
2023-05-12 21:24:59.536: Find a better model.
2023-05-12 21:25:21.805: [iter 30 : loss : 0.7101 = 0.2350 + 0.4720 + 0.0030, time: 22.264250]
2023-05-12 21:25:22.116: epoch 30:	0.02916097  	0.07857148  	0.06604289  
2023-05-12 21:25:22.117: Find a better model.
2023-05-12 21:25:44.436: [iter 31 : loss : 0.6822 = 0.2176 + 0.4613 + 0.0033, time: 22.315541]
2023-05-12 21:25:44.736: epoch 31:	0.02938307  	0.07920614  	0.06651314  
2023-05-12 21:25:44.736: Find a better model.
2023-05-12 21:26:07.010: [iter 32 : loss : 0.6770 = 0.2026 + 0.4709 + 0.0035, time: 22.269528]
2023-05-12 21:26:07.301: epoch 32:	0.02933124  	0.07953279  	0.06660005  
2023-05-12 21:26:07.301: Find a better model.
2023-05-12 21:26:29.623: [iter 33 : loss : 0.6539 = 0.1901 + 0.4600 + 0.0038, time: 22.317700]
2023-05-12 21:26:29.929: epoch 33:	0.02943488  	0.07950422  	0.06656075  
2023-05-12 21:26:52.392: [iter 34 : loss : 0.6516 = 0.1781 + 0.4695 + 0.0040, time: 22.450826]
2023-05-12 21:26:52.669: epoch 34:	0.02939786  	0.07946790  	0.06657324  
2023-05-12 21:27:14.990: [iter 35 : loss : 0.6302 = 0.1674 + 0.4586 + 0.0042, time: 22.316007]
2023-05-12 21:27:15.271: epoch 35:	0.02941266  	0.07962087  	0.06667544  
2023-05-12 21:27:15.271: Find a better model.
2023-05-12 21:27:37.371: [iter 36 : loss : 0.6315 = 0.1589 + 0.4682 + 0.0044, time: 22.096951]
2023-05-12 21:27:37.663: epoch 36:	0.02935343  	0.07997099  	0.06664302  
2023-05-12 21:27:37.663: Find a better model.
2023-05-12 21:28:00.148: [iter 37 : loss : 0.6132 = 0.1512 + 0.4574 + 0.0046, time: 22.480252]
2023-05-12 21:28:00.437: epoch 37:	0.02932382  	0.07980448  	0.06685823  
2023-05-12 21:28:22.575: [iter 38 : loss : 0.6148 = 0.1431 + 0.4670 + 0.0048, time: 22.132449]
2023-05-12 21:28:22.854: epoch 38:	0.02947189  	0.08022303  	0.06697229  
2023-05-12 21:28:22.854: Find a better model.
2023-05-12 21:28:45.339: [iter 39 : loss : 0.5982 = 0.1370 + 0.4562 + 0.0050, time: 22.480557]
2023-05-12 21:28:45.619: epoch 39:	0.02949410  	0.08036827  	0.06697699  
2023-05-12 21:28:45.619: Find a better model.
2023-05-12 21:29:08.139: [iter 40 : loss : 0.6015 = 0.1305 + 0.4659 + 0.0052, time: 22.515613]
2023-05-12 21:29:08.438: epoch 40:	0.02945708  	0.08006403  	0.06679118  
2023-05-12 21:29:30.953: [iter 41 : loss : 0.5849 = 0.1245 + 0.4551 + 0.0053, time: 22.510671]
2023-05-12 21:29:31.246: epoch 41:	0.02956073  	0.08025222  	0.06693307  
2023-05-12 21:29:53.774: [iter 42 : loss : 0.5901 = 0.1196 + 0.4650 + 0.0055, time: 22.524842]
2023-05-12 21:29:54.070: epoch 42:	0.02944968  	0.08030330  	0.06698962  
2023-05-12 21:30:16.580: [iter 43 : loss : 0.5744 = 0.1145 + 0.4542 + 0.0057, time: 22.505432]
2023-05-12 21:30:16.873: epoch 43:	0.02935344  	0.07974058  	0.06665660  
2023-05-12 21:30:39.519: [iter 44 : loss : 0.5810 = 0.1111 + 0.4641 + 0.0058, time: 22.643452]
2023-05-12 21:30:39.816: epoch 44:	0.02935344  	0.07986146  	0.06666885  
2023-05-12 21:31:02.351: [iter 45 : loss : 0.5660 = 0.1067 + 0.4533 + 0.0060, time: 22.530982]
2023-05-12 21:31:02.648: epoch 45:	0.02942006  	0.08005783  	0.06687741  
2023-05-12 21:31:25.292: [iter 46 : loss : 0.5725 = 0.1031 + 0.4632 + 0.0062, time: 22.640741]
2023-05-12 21:31:25.570: epoch 46:	0.02927200  	0.07974374  	0.06654672  
2023-05-12 21:31:48.076: [iter 47 : loss : 0.5581 = 0.0993 + 0.4525 + 0.0063, time: 22.503306]
2023-05-12 21:31:48.354: epoch 47:	0.02920537  	0.07946318  	0.06649114  
2023-05-12 21:32:10.909: [iter 48 : loss : 0.5656 = 0.0966 + 0.4626 + 0.0065, time: 22.550036]
2023-05-12 21:32:11.200: epoch 48:	0.02917575  	0.07900709  	0.06622356  
2023-05-12 21:32:33.857: [iter 49 : loss : 0.5515 = 0.0931 + 0.4518 + 0.0066, time: 22.652149]
2023-05-12 21:32:34.138: epoch 49:	0.02924979  	0.07887460  	0.06624150  
2023-05-12 21:32:56.480: [iter 50 : loss : 0.5596 = 0.0909 + 0.4619 + 0.0068, time: 22.334018]
2023-05-12 21:32:56.752: epoch 50:	0.02920537  	0.07890534  	0.06610078  
2023-05-12 21:33:19.492: [iter 51 : loss : 0.5457 = 0.0876 + 0.4512 + 0.0069, time: 22.737186]
2023-05-12 21:33:19.766: epoch 51:	0.02922017  	0.07895599  	0.06612574  
2023-05-12 21:33:42.294: [iter 52 : loss : 0.5534 = 0.0850 + 0.4614 + 0.0070, time: 22.523205]
2023-05-12 21:33:42.587: epoch 52:	0.02928680  	0.07886199  	0.06599127  
2023-05-12 21:34:05.071: [iter 53 : loss : 0.5412 = 0.0834 + 0.4506 + 0.0072, time: 22.478956]
2023-05-12 21:34:05.354: epoch 53:	0.02919056  	0.07875951  	0.06592648  
2023-05-12 21:34:27.882: [iter 54 : loss : 0.5495 = 0.0814 + 0.4609 + 0.0073, time: 22.523596]
2023-05-12 21:34:28.160: epoch 54:	0.02907951  	0.07857886  	0.06583507  
2023-05-12 21:34:51.030: [iter 55 : loss : 0.5364 = 0.0789 + 0.4501 + 0.0074, time: 22.866681]
2023-05-12 21:34:51.315: epoch 55:	0.02918315  	0.07841352  	0.06581635  
2023-05-12 21:35:13.760: [iter 56 : loss : 0.5446 = 0.0766 + 0.4605 + 0.0076, time: 22.441651]
2023-05-12 21:35:14.033: epoch 56:	0.02904250  	0.07786535  	0.06544530  
2023-05-12 21:35:36.849: [iter 57 : loss : 0.5317 = 0.0743 + 0.4497 + 0.0077, time: 22.812515]
2023-05-12 21:35:37.124: epoch 57:	0.02910172  	0.07804965  	0.06548399  
2023-05-12 21:35:59.843: [iter 58 : loss : 0.5408 = 0.0728 + 0.4601 + 0.0078, time: 22.715721]
2023-05-12 21:36:00.141: epoch 58:	0.02905730  	0.07788000  	0.06542411  
2023-05-12 21:36:22.985: [iter 59 : loss : 0.5280 = 0.0708 + 0.4492 + 0.0079, time: 22.838954]
2023-05-12 21:36:23.279: epoch 59:	0.02900548  	0.07776118  	0.06547642  
2023-05-12 21:36:46.179: [iter 60 : loss : 0.5378 = 0.0699 + 0.4598 + 0.0080, time: 22.895262]
2023-05-12 21:36:46.460: epoch 60:	0.02895365  	0.07759076  	0.06511833  
2023-05-12 21:37:09.604: [iter 61 : loss : 0.5250 = 0.0679 + 0.4489 + 0.0082, time: 23.138980]
2023-05-12 21:37:09.880: epoch 61:	0.02890923  	0.07734096  	0.06495415  
2023-05-12 21:37:32.786: [iter 62 : loss : 0.5347 = 0.0670 + 0.4594 + 0.0083, time: 22.903228]
2023-05-12 21:37:33.062: epoch 62:	0.02886481  	0.07711114  	0.06487805  
2023-05-12 21:37:55.805: [iter 63 : loss : 0.5227 = 0.0658 + 0.4485 + 0.0084, time: 22.734451]
2023-05-12 21:37:56.082: epoch 63:	0.02882038  	0.07691260  	0.06483808  
2023-05-12 21:38:18.999: [iter 64 : loss : 0.5321 = 0.0644 + 0.4592 + 0.0085, time: 22.913289]
2023-05-12 21:38:19.295: epoch 64:	0.02875375  	0.07689432  	0.06455728  
2023-05-12 21:38:19.295: Early stopping is trigger at epoch: 64
2023-05-12 21:38:19.296: best_result@epoch 39:

2023-05-12 21:38:19.296: 		0.0295      	0.0804      	0.0670      
2023-05-12 21:47:30.238: my pid: 9020
2023-05-12 21:47:30.239: model: model.general_recommender.SGL
2023-05-12 21:47:30.239: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-12 21:47:30.239: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-12 21:47:33.636: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-12 21:47:55.872: [iter 1 : loss : 1.1328 = 0.6931 + 0.4397 + 0.0000, time: 22.236177]
2023-05-12 21:47:56.149: epoch 1:	0.00132514  	0.00263118  	0.00238876  
2023-05-12 21:47:56.149: Find a better model.
2023-05-12 21:48:18.638: [iter 2 : loss : 1.1342 = 0.6930 + 0.4412 + 0.0000, time: 22.485592]
2023-05-12 21:48:18.937: epoch 2:	0.00203583  	0.00363354  	0.00298174  
2023-05-12 21:48:18.937: Find a better model.
2023-05-12 21:48:41.641: [iter 3 : loss : 1.1315 = 0.6930 + 0.4385 + 0.0000, time: 22.699844]
2023-05-12 21:48:41.943: epoch 3:	0.00213207  	0.00420015  	0.00352042  
2023-05-12 21:48:41.943: Find a better model.
2023-05-12 21:49:04.423: [iter 4 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 22.476862]
2023-05-12 21:49:04.720: epoch 4:	0.00262067  	0.00472055  	0.00400225  
2023-05-12 21:49:04.720: Find a better model.
2023-05-12 21:49:27.427: [iter 5 : loss : 1.1315 = 0.6929 + 0.4386 + 0.0000, time: 22.702384]
2023-05-12 21:49:27.724: epoch 5:	0.00300563  	0.00601113  	0.00473812  
2023-05-12 21:49:27.724: Find a better model.
2023-05-12 21:49:50.381: [iter 6 : loss : 1.1345 = 0.6928 + 0.4418 + 0.0000, time: 22.652360]
2023-05-12 21:49:50.682: epoch 6:	0.00350163  	0.00580677  	0.00523908  
2023-05-12 21:50:13.377: [iter 7 : loss : 1.1314 = 0.6927 + 0.4388 + 0.0000, time: 22.692118]
2023-05-12 21:50:13.671: epoch 7:	0.00405685  	0.00834962  	0.00677531  
2023-05-12 21:50:13.671: Find a better model.
2023-05-12 21:50:36.160: [iter 8 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 22.485446]
2023-05-12 21:50:36.468: epoch 8:	0.00425673  	0.00675762  	0.00664553  
2023-05-12 21:50:59.137: [iter 9 : loss : 1.1313 = 0.6923 + 0.4389 + 0.0000, time: 22.666364]
2023-05-12 21:50:59.430: epoch 9:	0.00510067  	0.01073477  	0.00870001  
2023-05-12 21:50:59.430: Find a better model.
2023-05-12 21:51:21.924: [iter 10 : loss : 1.1350 = 0.6920 + 0.4429 + 0.0000, time: 22.488538]
2023-05-12 21:51:22.218: epoch 10:	0.00616669  	0.01214983  	0.01016172  
2023-05-12 21:51:22.218: Find a better model.
2023-05-12 21:51:44.997: [iter 11 : loss : 1.1308 = 0.6917 + 0.4391 + 0.0000, time: 22.774731]
2023-05-12 21:51:45.285: epoch 11:	0.00708466  	0.01657375  	0.01332964  
2023-05-12 21:51:45.285: Find a better model.
2023-05-12 21:52:07.696: [iter 12 : loss : 1.1349 = 0.6910 + 0.4438 + 0.0000, time: 22.407425]
2023-05-12 21:52:07.986: epoch 12:	0.00855044  	0.01760687  	0.01536322  
2023-05-12 21:52:07.986: Find a better model.
2023-05-12 21:52:30.546: [iter 13 : loss : 1.1295 = 0.6902 + 0.4393 + 0.0000, time: 22.555658]
2023-05-12 21:52:30.845: epoch 13:	0.00895020  	0.02062534  	0.01773791  
2023-05-12 21:52:30.845: Find a better model.
2023-05-12 21:52:53.094: [iter 14 : loss : 1.1337 = 0.6885 + 0.4451 + 0.0000, time: 22.240004]
2023-05-12 21:52:53.386: epoch 14:	0.01054929  	0.02325168  	0.02060635  
2023-05-12 21:52:53.386: Find a better model.
2023-05-12 21:53:15.933: [iter 15 : loss : 1.1259 = 0.6862 + 0.4397 + 0.0000, time: 22.542369]
2023-05-12 21:53:16.218: epoch 15:	0.01173376  	0.02844290  	0.02487759  
2023-05-12 21:53:16.218: Find a better model.
2023-05-12 21:53:38.538: [iter 16 : loss : 1.1295 = 0.6822 + 0.4472 + 0.0001, time: 22.315826]
2023-05-12 21:53:38.827: epoch 16:	0.01522806  	0.03729604  	0.03272469  
2023-05-12 21:53:38.827: Find a better model.
2023-05-12 21:54:01.265: [iter 17 : loss : 1.1166 = 0.6762 + 0.4403 + 0.0001, time: 22.434853]
2023-05-12 21:54:01.552: epoch 17:	0.01693822  	0.04399155  	0.03843307  
2023-05-12 21:54:01.552: Find a better model.
2023-05-12 21:54:23.860: [iter 18 : loss : 1.1150 = 0.6649 + 0.4500 + 0.0002, time: 22.304693]
2023-05-12 21:54:24.151: epoch 18:	0.02026225  	0.05279547  	0.04639569  
2023-05-12 21:54:24.151: Find a better model.
2023-05-12 21:54:46.508: [iter 19 : loss : 1.0888 = 0.6465 + 0.4420 + 0.0002, time: 22.352788]
2023-05-12 21:54:46.795: epoch 19:	0.02257206  	0.05966447  	0.05199577  
2023-05-12 21:54:46.795: Find a better model.
2023-05-12 21:55:09.276: [iter 20 : loss : 1.0720 = 0.6177 + 0.4539 + 0.0004, time: 22.477910]
2023-05-12 21:55:09.563: epoch 20:	0.02486709  	0.06586855  	0.05734164  
2023-05-12 21:55:09.563: Find a better model.
2023-05-12 21:55:31.862: [iter 21 : loss : 1.0245 = 0.5778 + 0.4461 + 0.0006, time: 22.295037]
2023-05-12 21:55:32.147: epoch 21:	0.02648099  	0.07128394  	0.06080073  
2023-05-12 21:55:32.147: Find a better model.
2023-05-12 21:55:54.494: [iter 22 : loss : 0.9890 = 0.5290 + 0.4592 + 0.0008, time: 22.341580]
2023-05-12 21:55:54.779: epoch 22:	0.02732496  	0.07357197  	0.06218836  
2023-05-12 21:55:54.779: Find a better model.
2023-05-12 21:56:17.027: [iter 23 : loss : 0.9294 = 0.4761 + 0.4523 + 0.0011, time: 22.243840]
2023-05-12 21:56:17.307: epoch 23:	0.02767291  	0.07502708  	0.06328664  
2023-05-12 21:56:17.307: Find a better model.
2023-05-12 21:56:39.416: [iter 24 : loss : 0.8916 = 0.4253 + 0.4649 + 0.0014, time: 22.105479]
2023-05-12 21:56:39.698: epoch 24:	0.02797644  	0.07670747  	0.06407269  
2023-05-12 21:56:39.698: Find a better model.
2023-05-12 21:57:02.048: [iter 25 : loss : 0.8379 = 0.3790 + 0.4573 + 0.0017, time: 22.346308]
2023-05-12 21:57:02.327: epoch 25:	0.02855390  	0.07806151  	0.06490585  
2023-05-12 21:57:02.327: Find a better model.
2023-05-12 21:57:24.578: [iter 26 : loss : 0.8098 = 0.3395 + 0.4683 + 0.0020, time: 22.248249]
2023-05-12 21:57:24.864: epoch 26:	0.02868716  	0.07917470  	0.06524026  
2023-05-12 21:57:24.864: Find a better model.
2023-05-12 21:57:47.409: [iter 27 : loss : 0.7676 = 0.3058 + 0.4595 + 0.0023, time: 22.541788]
2023-05-12 21:57:47.687: epoch 27:	0.02869456  	0.07898608  	0.06508584  
2023-05-12 21:58:09.939: [iter 28 : loss : 0.7496 = 0.2778 + 0.4693 + 0.0025, time: 22.248476]
2023-05-12 21:58:10.219: epoch 28:	0.02866494  	0.07909591  	0.06524656  
2023-05-12 21:58:32.805: [iter 29 : loss : 0.7167 = 0.2542 + 0.4597 + 0.0028, time: 22.581334]
2023-05-12 21:58:33.083: epoch 29:	0.02886484  	0.07987801  	0.06563225  
2023-05-12 21:58:33.083: Find a better model.
2023-05-12 21:58:55.574: [iter 30 : loss : 0.7050 = 0.2331 + 0.4689 + 0.0031, time: 22.486472]
2023-05-12 21:58:55.854: epoch 30:	0.02899070  	0.08047666  	0.06566176  
2023-05-12 21:58:55.854: Find a better model.
2023-05-12 21:59:18.153: [iter 31 : loss : 0.6782 = 0.2159 + 0.4590 + 0.0033, time: 22.295436]
2023-05-12 21:59:18.435: epoch 31:	0.02921279  	0.08087132  	0.06583884  
2023-05-12 21:59:18.435: Find a better model.
2023-05-12 21:59:40.753: [iter 32 : loss : 0.6726 = 0.2013 + 0.4678 + 0.0036, time: 22.311362]
2023-05-12 21:59:41.028: epoch 32:	0.02942749  	0.08143473  	0.06599904  
2023-05-12 21:59:41.028: Find a better model.
2023-05-12 22:00:03.550: [iter 33 : loss : 0.6503 = 0.1888 + 0.4578 + 0.0038, time: 22.518048]
2023-05-12 22:00:03.830: epoch 33:	0.02947931  	0.08163305  	0.06627993  
2023-05-12 22:00:03.830: Find a better model.
2023-05-12 22:00:25.915: [iter 34 : loss : 0.6476 = 0.1771 + 0.4665 + 0.0040, time: 22.081621]
2023-05-12 22:00:26.193: epoch 34:	0.02942008  	0.08189565  	0.06640216  
2023-05-12 22:00:26.193: Find a better model.
2023-05-12 22:00:48.742: [iter 35 : loss : 0.6271 = 0.1664 + 0.4565 + 0.0042, time: 22.544083]
2023-05-12 22:00:49.021: epoch 35:	0.02958294  	0.08215506  	0.06648785  
2023-05-12 22:00:49.021: Find a better model.
2023-05-12 22:01:11.312: [iter 36 : loss : 0.6275 = 0.1578 + 0.4653 + 0.0044, time: 22.288319]
2023-05-12 22:01:11.593: epoch 36:	0.02953112  	0.08180505  	0.06639621  
2023-05-12 22:01:34.136: [iter 37 : loss : 0.6101 = 0.1503 + 0.4552 + 0.0046, time: 22.539643]
2023-05-12 22:01:34.414: epoch 37:	0.02953113  	0.08195069  	0.06648584  
2023-05-12 22:01:56.873: [iter 38 : loss : 0.6113 = 0.1424 + 0.4641 + 0.0048, time: 22.454115]
2023-05-12 22:01:57.152: epoch 38:	0.02947932  	0.08187035  	0.06634558  
2023-05-12 22:02:19.720: [iter 39 : loss : 0.5952 = 0.1361 + 0.4541 + 0.0050, time: 22.565397]
2023-05-12 22:02:19.998: epoch 39:	0.02947931  	0.08183723  	0.06646512  
2023-05-12 22:02:42.456: [iter 40 : loss : 0.5981 = 0.1298 + 0.4631 + 0.0052, time: 22.454077]
2023-05-12 22:02:42.744: epoch 40:	0.02947190  	0.08174931  	0.06653888  
2023-05-12 22:03:05.330: [iter 41 : loss : 0.5820 = 0.1237 + 0.4530 + 0.0054, time: 22.583327]
2023-05-12 22:03:05.614: epoch 41:	0.02942007  	0.08165497  	0.06642614  
2023-05-12 22:03:28.119: [iter 42 : loss : 0.5864 = 0.1187 + 0.4621 + 0.0055, time: 22.501101]
2023-05-12 22:03:28.399: epoch 42:	0.02950151  	0.08173893  	0.06644354  
2023-05-12 22:03:51.049: [iter 43 : loss : 0.5719 = 0.1141 + 0.4521 + 0.0057, time: 22.647703]
2023-05-12 22:03:51.329: epoch 43:	0.02940527  	0.08126525  	0.06628949  
2023-05-12 22:04:13.828: [iter 44 : loss : 0.5779 = 0.1107 + 0.4614 + 0.0059, time: 22.495013]
2023-05-12 22:04:14.108: epoch 44:	0.02929422  	0.08112081  	0.06622603  
2023-05-12 22:04:36.853: [iter 45 : loss : 0.5635 = 0.1063 + 0.4513 + 0.0060, time: 22.742106]
2023-05-12 22:04:37.133: epoch 45:	0.02931643  	0.08093721  	0.06607383  
2023-05-12 22:04:59.615: [iter 46 : loss : 0.5693 = 0.1025 + 0.4606 + 0.0062, time: 22.479288]
2023-05-12 22:04:59.903: epoch 46:	0.02927941  	0.08063441  	0.06594981  
2023-05-12 22:05:22.862: [iter 47 : loss : 0.5557 = 0.0990 + 0.4505 + 0.0063, time: 22.953427]
2023-05-12 22:05:23.145: epoch 47:	0.02922760  	0.08035579  	0.06582715  
2023-05-12 22:05:45.612: [iter 48 : loss : 0.5625 = 0.0961 + 0.4599 + 0.0065, time: 22.463141]
2023-05-12 22:05:45.902: epoch 48:	0.02916096  	0.08000617  	0.06579244  
2023-05-12 22:06:08.279: [iter 49 : loss : 0.5493 = 0.0929 + 0.4498 + 0.0066, time: 22.368407]
2023-05-12 22:06:08.562: epoch 49:	0.02916096  	0.07965097  	0.06564355  
2023-05-12 22:06:31.000: [iter 50 : loss : 0.5563 = 0.0902 + 0.4593 + 0.0068, time: 22.434360]
2023-05-12 22:06:31.282: epoch 50:	0.02898328  	0.07931510  	0.06537743  
2023-05-12 22:06:54.026: [iter 51 : loss : 0.5433 = 0.0872 + 0.4492 + 0.0069, time: 22.740388]
2023-05-12 22:06:54.309: epoch 51:	0.02899810  	0.07929685  	0.06556450  
2023-05-12 22:07:16.765: [iter 52 : loss : 0.5506 = 0.0847 + 0.4588 + 0.0070, time: 22.452281]
2023-05-12 22:07:17.059: epoch 52:	0.02899068  	0.07913731  	0.06546119  
2023-05-12 22:07:39.822: [iter 53 : loss : 0.5393 = 0.0835 + 0.4486 + 0.0072, time: 22.758456]
2023-05-12 22:07:40.112: epoch 53:	0.02900549  	0.07909905  	0.06554209  
2023-05-12 22:08:02.591: [iter 54 : loss : 0.5464 = 0.0807 + 0.4583 + 0.0073, time: 22.475288]
2023-05-12 22:08:02.875: epoch 54:	0.02903510  	0.07916892  	0.06542034  
2023-05-12 22:08:25.376: [iter 55 : loss : 0.5343 = 0.0787 + 0.4481 + 0.0075, time: 22.496892]
2023-05-12 22:08:25.661: epoch 55:	0.02908693  	0.07934128  	0.06542642  
2023-05-12 22:08:48.148: [iter 56 : loss : 0.5418 = 0.0763 + 0.4579 + 0.0076, time: 22.484071]
2023-05-12 22:08:48.433: epoch 56:	0.02901289  	0.07924502  	0.06524698  
2023-05-12 22:09:10.980: [iter 57 : loss : 0.5296 = 0.0742 + 0.4477 + 0.0077, time: 22.543472]
2023-05-12 22:09:11.264: epoch 57:	0.02915356  	0.07937402  	0.06531366  
2023-05-12 22:09:33.723: [iter 58 : loss : 0.5379 = 0.0725 + 0.4576 + 0.0078, time: 22.453706]
2023-05-12 22:09:34.010: epoch 58:	0.02908693  	0.07911407  	0.06504095  
2023-05-12 22:09:56.563: [iter 59 : loss : 0.5264 = 0.0712 + 0.4472 + 0.0079, time: 22.549303]
2023-05-12 22:09:56.853: epoch 59:	0.02897587  	0.07872754  	0.06492754  
2023-05-12 22:10:19.342: [iter 60 : loss : 0.5348 = 0.0694 + 0.4573 + 0.0081, time: 22.484818]
2023-05-12 22:10:19.631: epoch 60:	0.02890925  	0.07846241  	0.06482989  
2023-05-12 22:10:19.631: Early stopping is trigger at epoch: 60
2023-05-12 22:10:19.631: best_result@epoch 35:

2023-05-12 22:10:19.631: 		0.0296      	0.0822      	0.0665      
2023-05-13 10:12:23.930: my pid: 11548
2023-05-13 10:12:23.930: model: model.general_recommender.SGL
2023-05-13 10:12:23.930: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-13 10:12:23.930: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-13 10:12:27.228: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-13 10:12:51.758: [iter 1 : loss : 1.1321 = 0.6931 + 0.4391 + 0.0000, time: 24.530224]
2023-05-13 10:12:52.033: epoch 1:	0.00137696  	0.00286236  	0.00244759  
2023-05-13 10:12:52.033: Find a better model.
2023-05-13 10:13:17.066: [iter 2 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 25.029995]
2023-05-13 10:13:17.348: epoch 2:	0.00146580  	0.00241023  	0.00230733  
2023-05-13 10:13:42.345: [iter 3 : loss : 1.1307 = 0.6930 + 0.4377 + 0.0000, time: 24.992767]
2023-05-13 10:13:42.629: epoch 3:	0.00176932  	0.00385333  	0.00312932  
2023-05-13 10:13:42.629: Find a better model.
2023-05-13 10:14:07.515: [iter 4 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 24.882919]
2023-05-13 10:14:07.803: epoch 4:	0.00225052  	0.00395744  	0.00347253  
2023-05-13 10:14:07.803: Find a better model.
2023-05-13 10:14:33.127: [iter 5 : loss : 1.1306 = 0.6929 + 0.4377 + 0.0000, time: 25.320863]
2023-05-13 10:14:33.410: epoch 5:	0.00264288  	0.00561816  	0.00440120  
2023-05-13 10:14:33.410: Find a better model.
2023-05-13 10:14:58.431: [iter 6 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 25.017532]
2023-05-13 10:14:58.717: epoch 6:	0.00280575  	0.00475379  	0.00443715  
2023-05-13 10:15:23.925: [iter 7 : loss : 1.1304 = 0.6927 + 0.4377 + 0.0000, time: 25.205561]
2023-05-13 10:15:24.214: epoch 7:	0.00319811  	0.00671348  	0.00533237  
2023-05-13 10:15:24.214: Find a better model.
2023-05-13 10:15:49.268: [iter 8 : loss : 1.1341 = 0.6926 + 0.4415 + 0.0000, time: 25.050285]
2023-05-13 10:15:49.551: epoch 8:	0.00390139  	0.00709650  	0.00627580  
2023-05-13 10:15:49.551: Find a better model.
2023-05-13 10:16:14.681: [iter 9 : loss : 1.1302 = 0.6924 + 0.4378 + 0.0000, time: 25.127273]
2023-05-13 10:16:14.959: epoch 9:	0.00476013  	0.00966038  	0.00836298  
2023-05-13 10:16:14.959: Find a better model.
2023-05-13 10:16:39.810: [iter 10 : loss : 1.1343 = 0.6922 + 0.4421 + 0.0000, time: 24.847804]
2023-05-13 10:16:40.089: epoch 10:	0.00589279  	0.01102770  	0.00936996  
2023-05-13 10:16:40.089: Find a better model.
2023-05-13 10:17:05.104: [iter 11 : loss : 1.1297 = 0.6919 + 0.4379 + 0.0000, time: 25.011009]
2023-05-13 10:17:05.384: epoch 11:	0.00648502  	0.01475050  	0.01198106  
2023-05-13 10:17:05.384: Find a better model.
2023-05-13 10:17:30.223: [iter 12 : loss : 1.1342 = 0.6913 + 0.4429 + 0.0000, time: 24.835736]
2023-05-13 10:17:30.500: epoch 12:	0.00777313  	0.01614648  	0.01401792  
2023-05-13 10:17:30.500: Find a better model.
2023-05-13 10:17:55.630: [iter 13 : loss : 1.1287 = 0.6907 + 0.4380 + 0.0000, time: 25.124362]
2023-05-13 10:17:55.903: epoch 13:	0.00876512  	0.02060369  	0.01744140  
2023-05-13 10:17:55.903: Find a better model.
2023-05-13 10:18:20.558: [iter 14 : loss : 1.1334 = 0.6895 + 0.4440 + 0.0000, time: 24.652789]
2023-05-13 10:18:20.829: epoch 14:	0.01077878  	0.02408905  	0.02077941  
2023-05-13 10:18:20.830: Find a better model.
2023-05-13 10:18:45.677: [iter 15 : loss : 1.1258 = 0.6875 + 0.4382 + 0.0000, time: 24.843673]
2023-05-13 10:18:45.949: epoch 15:	0.01083800  	0.02731183  	0.02303040  
2023-05-13 10:18:45.949: Find a better model.
2023-05-13 10:19:10.581: [iter 16 : loss : 1.1298 = 0.6838 + 0.4459 + 0.0001, time: 24.628754]
2023-05-13 10:19:10.853: epoch 16:	0.01413979  	0.03423168  	0.03028526  
2023-05-13 10:19:10.853: Find a better model.
2023-05-13 10:19:35.640: [iter 17 : loss : 1.1173 = 0.6785 + 0.4387 + 0.0001, time: 24.782560]
2023-05-13 10:19:35.911: epoch 17:	0.01534651  	0.03933275  	0.03466610  
2023-05-13 10:19:35.911: Find a better model.
2023-05-13 10:20:00.558: [iter 18 : loss : 1.1168 = 0.6682 + 0.4485 + 0.0001, time: 24.644062]
2023-05-13 10:20:00.838: epoch 18:	0.01934425  	0.04980650  	0.04330172  
2023-05-13 10:20:00.838: Find a better model.
2023-05-13 10:20:25.675: [iter 19 : loss : 1.0919 = 0.6515 + 0.4402 + 0.0002, time: 24.833340]
2023-05-13 10:20:25.947: epoch 19:	0.02100997  	0.05619914  	0.04819028  
2023-05-13 10:20:25.947: Find a better model.
2023-05-13 10:20:50.405: [iter 20 : loss : 1.0765 = 0.6242 + 0.4520 + 0.0003, time: 24.453509]
2023-05-13 10:20:50.677: epoch 20:	0.02362332  	0.06349194  	0.05421214  
2023-05-13 10:20:50.678: Find a better model.
2023-05-13 10:21:15.606: [iter 21 : loss : 1.0304 = 0.5859 + 0.4440 + 0.0005, time: 24.925678]
2023-05-13 10:21:15.878: epoch 21:	0.02513359  	0.06808414  	0.05781716  
2023-05-13 10:21:15.879: Find a better model.
2023-05-13 10:21:40.549: [iter 22 : loss : 0.9951 = 0.5375 + 0.4569 + 0.0007, time: 24.666736]
2023-05-13 10:21:40.827: epoch 22:	0.02631812  	0.07200537  	0.06002211  
2023-05-13 10:21:40.827: Find a better model.
2023-05-13 10:22:05.613: [iter 23 : loss : 0.9356 = 0.4845 + 0.4501 + 0.0010, time: 24.781660]
2023-05-13 10:22:05.883: epoch 23:	0.02712505  	0.07400088  	0.06128297  
2023-05-13 10:22:05.883: Find a better model.
2023-05-13 10:22:30.524: [iter 24 : loss : 0.8964 = 0.4328 + 0.4623 + 0.0013, time: 24.638630]
2023-05-13 10:22:30.792: epoch 24:	0.02741379  	0.07536392  	0.06212129  
2023-05-13 10:22:30.792: Find a better model.
2023-05-13 10:22:55.544: [iter 25 : loss : 0.8420 = 0.3851 + 0.4552 + 0.0016, time: 24.747595]
2023-05-13 10:22:55.813: epoch 25:	0.02802825  	0.07747909  	0.06321268  
2023-05-13 10:22:55.813: Find a better model.
2023-05-13 10:23:20.469: [iter 26 : loss : 0.8123 = 0.3445 + 0.4659 + 0.0019, time: 24.653298]
2023-05-13 10:23:20.740: epoch 26:	0.02820593  	0.07854092  	0.06381320  
2023-05-13 10:23:20.740: Find a better model.
2023-05-13 10:23:45.523: [iter 27 : loss : 0.7699 = 0.3100 + 0.4577 + 0.0022, time: 24.779908]
2023-05-13 10:23:45.789: epoch 27:	0.02856869  	0.07979656  	0.06442501  
2023-05-13 10:23:45.789: Find a better model.
2023-05-13 10:24:10.296: [iter 28 : loss : 0.7505 = 0.2810 + 0.4669 + 0.0025, time: 24.503370]
2023-05-13 10:24:10.562: epoch 28:	0.02866493  	0.07996137  	0.06462742  
2023-05-13 10:24:10.562: Find a better model.
2023-05-13 10:24:35.307: [iter 29 : loss : 0.7176 = 0.2568 + 0.4580 + 0.0028, time: 24.741874]
2023-05-13 10:24:35.572: epoch 29:	0.02878338  	0.08023657  	0.06489776  
2023-05-13 10:24:35.572: Find a better model.
2023-05-13 10:25:00.243: [iter 30 : loss : 0.7053 = 0.2357 + 0.4666 + 0.0030, time: 24.667649]
2023-05-13 10:25:00.509: epoch 30:	0.02885741  	0.08093768  	0.06507220  
2023-05-13 10:25:00.509: Find a better model.
2023-05-13 10:25:25.518: [iter 31 : loss : 0.6786 = 0.2179 + 0.4574 + 0.0033, time: 25.005222]
2023-05-13 10:25:25.788: epoch 31:	0.02910913  	0.08147556  	0.06544812  
2023-05-13 10:25:25.788: Find a better model.
2023-05-13 10:25:50.692: [iter 32 : loss : 0.6722 = 0.2031 + 0.4656 + 0.0035, time: 24.901351]
2023-05-13 10:25:50.955: epoch 32:	0.02919057  	0.08173486  	0.06572673  
2023-05-13 10:25:50.955: Find a better model.
2023-05-13 10:26:16.068: [iter 33 : loss : 0.6504 = 0.1904 + 0.4562 + 0.0038, time: 25.108734]
2023-05-13 10:26:16.337: epoch 33:	0.02935342  	0.08215701  	0.06594878  
2023-05-13 10:26:16.337: Find a better model.
2023-05-13 10:26:41.201: [iter 34 : loss : 0.6470 = 0.1786 + 0.4644 + 0.0040, time: 24.861542]
2023-05-13 10:26:41.464: epoch 34:	0.02920537  	0.08190411  	0.06606183  
2023-05-13 10:27:06.504: [iter 35 : loss : 0.6268 = 0.1677 + 0.4549 + 0.0042, time: 25.036222]
2023-05-13 10:27:06.777: epoch 35:	0.02944228  	0.08205284  	0.06623241  
2023-05-13 10:27:31.835: [iter 36 : loss : 0.6266 = 0.1590 + 0.4632 + 0.0044, time: 25.054011]
2023-05-13 10:27:32.096: epoch 36:	0.02942747  	0.08171397  	0.06611925  
2023-05-13 10:27:57.265: [iter 37 : loss : 0.6092 = 0.1508 + 0.4538 + 0.0046, time: 25.164506]
2023-05-13 10:27:57.526: epoch 37:	0.02947189  	0.08157797  	0.06611930  
2023-05-13 10:28:22.405: [iter 38 : loss : 0.6101 = 0.1433 + 0.4621 + 0.0048, time: 24.875345]
2023-05-13 10:28:22.668: epoch 38:	0.02939044  	0.08160584  	0.06609052  
2023-05-13 10:28:47.692: [iter 39 : loss : 0.5946 = 0.1370 + 0.4526 + 0.0050, time: 25.020164]
2023-05-13 10:28:47.965: epoch 39:	0.02931643  	0.08137719  	0.06602736  
2023-05-13 10:29:13.190: [iter 40 : loss : 0.5968 = 0.1305 + 0.4611 + 0.0052, time: 25.222063]
2023-05-13 10:29:13.454: epoch 40:	0.02936085  	0.08163048  	0.06610037  
2023-05-13 10:29:38.619: [iter 41 : loss : 0.5813 = 0.1244 + 0.4516 + 0.0053, time: 25.162503]
2023-05-13 10:29:38.880: epoch 41:	0.02944227  	0.08147292  	0.06617042  
2023-05-13 10:30:03.776: [iter 42 : loss : 0.5850 = 0.1194 + 0.4601 + 0.0055, time: 24.891602]
2023-05-13 10:30:04.043: epoch 42:	0.02933122  	0.08120405  	0.06610576  
2023-05-13 10:30:29.208: [iter 43 : loss : 0.5711 = 0.1148 + 0.4507 + 0.0057, time: 25.161913]
2023-05-13 10:30:29.472: epoch 43:	0.02945707  	0.08163091  	0.06621065  
2023-05-13 10:30:54.346: [iter 44 : loss : 0.5767 = 0.1114 + 0.4594 + 0.0059, time: 24.870535]
2023-05-13 10:30:54.607: epoch 44:	0.02921276  	0.08079906  	0.06576832  
2023-05-13 10:31:19.816: [iter 45 : loss : 0.5629 = 0.1071 + 0.4498 + 0.0060, time: 25.205570]
2023-05-13 10:31:20.084: epoch 45:	0.02931641  	0.08072691  	0.06580617  
2023-05-13 10:31:45.729: [iter 46 : loss : 0.5676 = 0.1028 + 0.4586 + 0.0062, time: 25.641448]
2023-05-13 10:31:45.993: epoch 46:	0.02921277  	0.08041316  	0.06572006  
2023-05-13 10:32:11.244: [iter 47 : loss : 0.5549 = 0.0995 + 0.4490 + 0.0063, time: 25.247144]
2023-05-13 10:32:11.510: epoch 47:	0.02925719  	0.08075550  	0.06582873  
2023-05-13 10:32:36.581: [iter 48 : loss : 0.5608 = 0.0964 + 0.4579 + 0.0065, time: 25.067558]
2023-05-13 10:32:36.848: epoch 48:	0.02919797  	0.08049961  	0.06565341  
2023-05-13 10:33:02.154: [iter 49 : loss : 0.5482 = 0.0932 + 0.4484 + 0.0066, time: 25.302403]
2023-05-13 10:33:02.421: epoch 49:	0.02923499  	0.08054244  	0.06582722  
2023-05-13 10:33:27.702: [iter 50 : loss : 0.5549 = 0.0907 + 0.4574 + 0.0068, time: 25.278088]
2023-05-13 10:33:27.967: epoch 50:	0.02910913  	0.08021367  	0.06550977  
2023-05-13 10:33:53.212: [iter 51 : loss : 0.5426 = 0.0879 + 0.4478 + 0.0069, time: 25.242270]
2023-05-13 10:33:53.485: epoch 51:	0.02917576  	0.08040840  	0.06565703  
2023-05-13 10:34:18.722: [iter 52 : loss : 0.5489 = 0.0850 + 0.4569 + 0.0070, time: 25.233603]
2023-05-13 10:34:18.986: epoch 52:	0.02914615  	0.08080618  	0.06551843  
2023-05-13 10:34:44.361: [iter 53 : loss : 0.5380 = 0.0836 + 0.4472 + 0.0072, time: 25.371337]
2023-05-13 10:34:44.625: epoch 53:	0.02910174  	0.08051021  	0.06538534  
2023-05-13 10:35:09.662: [iter 54 : loss : 0.5448 = 0.0811 + 0.4564 + 0.0073, time: 25.032610]
2023-05-13 10:35:09.927: epoch 54:	0.02892405  	0.07971703  	0.06511511  
2023-05-13 10:35:35.369: [iter 55 : loss : 0.5331 = 0.0790 + 0.4467 + 0.0074, time: 25.438837]
2023-05-13 10:35:35.631: epoch 55:	0.02885742  	0.07938500  	0.06491048  
2023-05-13 10:36:00.897: [iter 56 : loss : 0.5401 = 0.0765 + 0.4561 + 0.0076, time: 25.262885]
2023-05-13 10:36:01.170: epoch 56:	0.02884262  	0.07935419  	0.06496161  
2023-05-13 10:36:26.494: [iter 57 : loss : 0.5284 = 0.0744 + 0.4463 + 0.0077, time: 25.320283]
2023-05-13 10:36:26.761: epoch 57:	0.02882782  	0.07922666  	0.06498253  
2023-05-13 10:36:52.018: [iter 58 : loss : 0.5367 = 0.0731 + 0.4557 + 0.0078, time: 25.253633]
2023-05-13 10:36:52.283: epoch 58:	0.02877600  	0.07901078  	0.06483656  
2023-05-13 10:36:52.283: Early stopping is trigger at epoch: 58
2023-05-13 10:36:52.283: best_result@epoch 33:

2023-05-13 10:36:52.283: 		0.0294      	0.0822      	0.0659      
2023-05-13 10:48:53.174: my pid: 12556
2023-05-13 10:48:53.174: model: model.general_recommender.SGL
2023-05-13 10:48:53.174: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-13 10:48:53.174: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-13 10:48:56.479: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-13 10:49:16.452: [iter 1 : loss : 1.1355 = 0.6931 + 0.4424 + 0.0000, time: 19.972489]
2023-05-13 10:49:16.720: epoch 1:	0.00117708  	0.00236195  	0.00180349  
2023-05-13 10:49:16.720: Find a better model.
2023-05-13 10:49:37.310: [iter 2 : loss : 1.1385 = 0.6930 + 0.4455 + 0.0000, time: 20.586079]
2023-05-13 10:49:37.598: epoch 2:	0.00156944  	0.00313628  	0.00258980  
2023-05-13 10:49:37.598: Find a better model.
2023-05-13 10:49:58.185: [iter 3 : loss : 1.1369 = 0.6929 + 0.4440 + 0.0000, time: 20.583630]
2023-05-13 10:49:58.472: epoch 3:	0.00179153  	0.00318649  	0.00276580  
2023-05-13 10:49:58.472: Find a better model.
2023-05-13 10:50:19.274: [iter 4 : loss : 1.1400 = 0.6928 + 0.4472 + 0.0000, time: 20.798573]
2023-05-13 10:50:19.559: epoch 4:	0.00204323  	0.00366442  	0.00295759  
2023-05-13 10:50:19.559: Find a better model.
2023-05-13 10:50:40.373: [iter 5 : loss : 1.1380 = 0.6927 + 0.4453 + 0.0000, time: 20.810869]
2023-05-13 10:50:40.661: epoch 5:	0.00261327  	0.00513436  	0.00408805  
2023-05-13 10:50:40.661: Find a better model.
2023-05-13 10:51:01.136: [iter 6 : loss : 1.1413 = 0.6925 + 0.4487 + 0.0000, time: 20.470974]
2023-05-13 10:51:01.422: epoch 6:	0.00282055  	0.00556664  	0.00422616  
2023-05-13 10:51:01.422: Find a better model.
2023-05-13 10:51:22.179: [iter 7 : loss : 1.1387 = 0.6923 + 0.4464 + 0.0000, time: 20.754101]
2023-05-13 10:51:22.463: epoch 7:	0.00333136  	0.00758975  	0.00578417  
2023-05-13 10:51:22.463: Find a better model.
2023-05-13 10:51:43.079: [iter 8 : loss : 1.1422 = 0.6921 + 0.4501 + 0.0000, time: 20.611859]
2023-05-13 10:51:43.362: epoch 8:	0.00344981  	0.00673474  	0.00559697  
2023-05-13 10:52:03.927: [iter 9 : loss : 1.1392 = 0.6918 + 0.4474 + 0.0000, time: 20.559784]
2023-05-13 10:52:04.208: epoch 9:	0.00396062  	0.00858688  	0.00716214  
2023-05-13 10:52:04.208: Find a better model.
2023-05-13 10:52:24.658: [iter 10 : loss : 1.1429 = 0.6914 + 0.4515 + 0.0000, time: 20.446130]
2023-05-13 10:52:24.953: epoch 10:	0.00447142  	0.00985143  	0.00763050  
2023-05-13 10:52:24.953: Find a better model.
2023-05-13 10:52:45.507: [iter 11 : loss : 1.1391 = 0.6907 + 0.4484 + 0.0000, time: 20.548365]
2023-05-13 10:52:45.790: epoch 11:	0.00532276  	0.01199383  	0.01001965  
2023-05-13 10:52:45.790: Find a better model.
2023-05-13 10:53:06.232: [iter 12 : loss : 1.1431 = 0.6898 + 0.4533 + 0.0000, time: 20.439385]
2023-05-13 10:53:06.510: epoch 12:	0.00560407  	0.01256321  	0.00995094  
2023-05-13 10:53:06.510: Find a better model.
2023-05-13 10:53:26.925: [iter 13 : loss : 1.1381 = 0.6886 + 0.4495 + 0.0000, time: 20.411618]
2023-05-13 10:53:27.204: epoch 13:	0.00705505  	0.01741713  	0.01468371  
2023-05-13 10:53:27.204: Find a better model.
2023-05-13 10:53:47.789: [iter 14 : loss : 1.1424 = 0.6871 + 0.4552 + 0.0001, time: 20.580709]
2023-05-13 10:53:48.081: epoch 14:	0.00826913  	0.01949424  	0.01616122  
2023-05-13 10:53:48.081: Find a better model.
2023-05-13 10:54:08.497: [iter 15 : loss : 1.1357 = 0.6851 + 0.4505 + 0.0001, time: 20.410642]
2023-05-13 10:54:08.771: epoch 15:	0.01081579  	0.02639671  	0.02305241  
2023-05-13 10:54:08.771: Find a better model.
2023-05-13 10:54:29.341: [iter 16 : loss : 1.1399 = 0.6822 + 0.4576 + 0.0001, time: 20.566195]
2023-05-13 10:54:29.622: epoch 16:	0.01265916  	0.03045776  	0.02685946  
2023-05-13 10:54:29.623: Find a better model.
2023-05-13 10:54:50.133: [iter 17 : loss : 1.1299 = 0.6780 + 0.4518 + 0.0001, time: 20.504089]
2023-05-13 10:54:50.410: epoch 17:	0.01607203  	0.04033539  	0.03511021  
2023-05-13 10:54:50.410: Find a better model.
2023-05-13 10:55:10.777: [iter 18 : loss : 1.1318 = 0.6713 + 0.4604 + 0.0001, time: 20.362864]
2023-05-13 10:55:11.053: epoch 18:	0.01873717  	0.04798791  	0.04212102  
2023-05-13 10:55:11.053: Find a better model.
2023-05-13 10:55:31.532: [iter 19 : loss : 1.1146 = 0.6609 + 0.4535 + 0.0002, time: 20.475703]
2023-05-13 10:55:31.806: epoch 19:	0.02189095  	0.05634196  	0.04885756  
2023-05-13 10:55:31.807: Find a better model.
2023-05-13 10:55:51.965: [iter 20 : loss : 1.1096 = 0.6451 + 0.4643 + 0.0003, time: 20.154388]
2023-05-13 10:55:52.253: epoch 20:	0.02408233  	0.06195600  	0.05365973  
2023-05-13 10:55:52.253: Find a better model.
2023-05-13 10:56:12.297: [iter 21 : loss : 1.0784 = 0.6214 + 0.4567 + 0.0004, time: 20.039953]
2023-05-13 10:56:12.568: epoch 21:	0.02605898  	0.06684567  	0.05747935  
2023-05-13 10:56:12.568: Find a better model.
2023-05-13 10:56:32.936: [iter 22 : loss : 1.0589 = 0.5890 + 0.4695 + 0.0005, time: 20.364357]
2023-05-13 10:56:33.208: epoch 22:	0.02721387  	0.07067950  	0.06005416  
2023-05-13 10:56:33.208: Find a better model.
2023-05-13 10:56:53.692: [iter 23 : loss : 1.0099 = 0.5475 + 0.4617 + 0.0007, time: 20.478810]
2023-05-13 10:56:53.960: epoch 23:	0.02801342  	0.07325308  	0.06166296  
2023-05-13 10:56:53.961: Find a better model.
2023-05-13 10:57:14.588: [iter 24 : loss : 0.9782 = 0.5008 + 0.4764 + 0.0010, time: 20.624140]
2023-05-13 10:57:14.851: epoch 24:	0.02830216  	0.07475621  	0.06238319  
2023-05-13 10:57:14.851: Find a better model.
2023-05-13 10:57:34.965: [iter 25 : loss : 0.9211 = 0.4519 + 0.4680 + 0.0012, time: 20.111256]
2023-05-13 10:57:35.226: epoch 25:	0.02875376  	0.07589059  	0.06295218  
2023-05-13 10:57:35.226: Find a better model.
2023-05-13 10:57:55.485: [iter 26 : loss : 0.8902 = 0.4060 + 0.4826 + 0.0015, time: 20.255121]
2023-05-13 10:57:55.747: epoch 26:	0.02886483  	0.07697321  	0.06335953  
2023-05-13 10:57:55.747: Find a better model.
2023-05-13 10:58:15.953: [iter 27 : loss : 0.8387 = 0.3644 + 0.4725 + 0.0018, time: 20.202288]
2023-05-13 10:58:16.214: epoch 27:	0.02904250  	0.07748391  	0.06363712  
2023-05-13 10:58:16.214: Find a better model.
2023-05-13 10:58:36.474: [iter 28 : loss : 0.8162 = 0.3286 + 0.4856 + 0.0021, time: 20.255987]
2023-05-13 10:58:36.735: epoch 28:	0.02924238  	0.07808539  	0.06398004  
2023-05-13 10:58:36.735: Find a better model.
2023-05-13 10:58:56.949: [iter 29 : loss : 0.7748 = 0.2983 + 0.4742 + 0.0023, time: 20.209720]
2023-05-13 10:58:57.210: epoch 29:	0.02938305  	0.07858273  	0.06420682  
2023-05-13 10:58:57.210: Find a better model.
2023-05-13 10:59:17.451: [iter 30 : loss : 0.7603 = 0.2716 + 0.4861 + 0.0026, time: 20.237109]
2023-05-13 10:59:17.712: epoch 30:	0.02932382  	0.07853483  	0.06433914  
2023-05-13 10:59:37.951: [iter 31 : loss : 0.7263 = 0.2495 + 0.4739 + 0.0029, time: 20.235876]
2023-05-13 10:59:38.211: epoch 31:	0.02939045  	0.07854735  	0.06452589  
2023-05-13 10:59:58.472: [iter 32 : loss : 0.7189 = 0.2307 + 0.4851 + 0.0031, time: 20.257740]
2023-05-13 10:59:58.735: epoch 32:	0.02955332  	0.07904408  	0.06468550  
2023-05-13 10:59:58.736: Find a better model.
2023-05-13 11:00:19.116: [iter 33 : loss : 0.6906 = 0.2146 + 0.4727 + 0.0033, time: 20.377514]
2023-05-13 11:00:19.375: epoch 33:	0.02973841  	0.07927814  	0.06489735  
2023-05-13 11:00:19.375: Find a better model.
2023-05-13 11:00:39.638: [iter 34 : loss : 0.6875 = 0.2002 + 0.4838 + 0.0036, time: 20.260795]
2023-05-13 11:00:39.896: epoch 34:	0.02976062  	0.07974207  	0.06505150  
2023-05-13 11:00:39.896: Find a better model.
2023-05-13 11:01:00.302: [iter 35 : loss : 0.6622 = 0.1872 + 0.4713 + 0.0038, time: 20.400809]
2023-05-13 11:01:00.564: epoch 35:	0.02983464  	0.07990150  	0.06537294  
2023-05-13 11:01:00.564: Find a better model.
2023-05-13 11:01:20.822: [iter 36 : loss : 0.6623 = 0.1761 + 0.4822 + 0.0040, time: 20.253989]
2023-05-13 11:01:21.083: epoch 36:	0.02991608  	0.08036417  	0.06562219  
2023-05-13 11:01:21.083: Find a better model.
2023-05-13 11:01:41.288: [iter 37 : loss : 0.6405 = 0.1666 + 0.4697 + 0.0042, time: 20.200533]
2023-05-13 11:01:41.545: epoch 37:	0.02987906  	0.08019604  	0.06571368  
2023-05-13 11:02:01.795: [iter 38 : loss : 0.6426 = 0.1571 + 0.4811 + 0.0044, time: 20.246964]
2023-05-13 11:02:02.055: epoch 38:	0.02993088  	0.08033358  	0.06581061  
2023-05-13 11:02:22.288: [iter 39 : loss : 0.6229 = 0.1499 + 0.4684 + 0.0046, time: 20.229063]
2023-05-13 11:02:22.546: epoch 39:	0.03003452  	0.08033203  	0.06602551  
2023-05-13 11:02:42.785: [iter 40 : loss : 0.6269 = 0.1423 + 0.4798 + 0.0048, time: 20.235103]
2023-05-13 11:02:43.048: epoch 40:	0.03000492  	0.08035482  	0.06594799  
2023-05-13 11:03:03.263: [iter 41 : loss : 0.6076 = 0.1355 + 0.4671 + 0.0050, time: 20.211176]
2023-05-13 11:03:03.523: epoch 41:	0.02998271  	0.08037091  	0.06584528  
2023-05-13 11:03:03.523: Find a better model.
2023-05-13 11:03:23.607: [iter 42 : loss : 0.6131 = 0.1295 + 0.4784 + 0.0052, time: 20.079667]
2023-05-13 11:03:23.866: epoch 42:	0.02990127  	0.08067979  	0.06603052  
2023-05-13 11:03:23.866: Find a better model.
2023-05-13 11:03:44.033: [iter 43 : loss : 0.5953 = 0.1239 + 0.4661 + 0.0053, time: 20.163402]
2023-05-13 11:03:44.291: epoch 43:	0.03004933  	0.08071326  	0.06616153  
2023-05-13 11:03:44.291: Find a better model.
2023-05-13 11:04:04.561: [iter 44 : loss : 0.6025 = 0.1196 + 0.4775 + 0.0055, time: 20.266194]
2023-05-13 11:04:04.818: epoch 44:	0.02991609  	0.08049990  	0.06607722  
2023-05-13 11:04:25.240: [iter 45 : loss : 0.5854 = 0.1147 + 0.4650 + 0.0057, time: 20.417523]
2023-05-13 11:04:25.498: epoch 45:	0.02987906  	0.08071450  	0.06616016  
2023-05-13 11:04:25.498: Find a better model.
2023-05-13 11:04:45.581: [iter 46 : loss : 0.5925 = 0.1101 + 0.4765 + 0.0058, time: 20.079312]
2023-05-13 11:04:45.838: epoch 46:	0.02976801  	0.08042550  	0.06599034  
2023-05-13 11:05:06.226: [iter 47 : loss : 0.5760 = 0.1060 + 0.4640 + 0.0060, time: 20.384847]
2023-05-13 11:05:06.484: epoch 47:	0.02984945  	0.08059057  	0.06602677  
2023-05-13 11:05:26.576: [iter 48 : loss : 0.5848 = 0.1028 + 0.4759 + 0.0061, time: 20.087686]
2023-05-13 11:05:26.835: epoch 48:	0.02973839  	0.08023503  	0.06594366  
2023-05-13 11:05:46.826: [iter 49 : loss : 0.5688 = 0.0992 + 0.4632 + 0.0063, time: 19.987428]
2023-05-13 11:05:47.084: epoch 49:	0.02979022  	0.08024868  	0.06590299  
2023-05-13 11:06:07.322: [iter 50 : loss : 0.5778 = 0.0962 + 0.4751 + 0.0064, time: 20.233937]
2023-05-13 11:06:07.579: epoch 50:	0.02963475  	0.07981455  	0.06577326  
2023-05-13 11:06:27.787: [iter 51 : loss : 0.5621 = 0.0929 + 0.4626 + 0.0066, time: 20.204113]
2023-05-13 11:06:28.045: epoch 51:	0.02961253  	0.07984477  	0.06591815  
2023-05-13 11:06:48.286: [iter 52 : loss : 0.5712 = 0.0898 + 0.4747 + 0.0067, time: 20.237704]
2023-05-13 11:06:48.543: epoch 52:	0.02937564  	0.07937496  	0.06552040  
2023-05-13 11:07:08.768: [iter 53 : loss : 0.5570 = 0.0883 + 0.4619 + 0.0069, time: 20.219609]
2023-05-13 11:07:09.026: epoch 53:	0.02937564  	0.07933187  	0.06551219  
2023-05-13 11:07:29.294: [iter 54 : loss : 0.5662 = 0.0854 + 0.4738 + 0.0070, time: 20.264407]
2023-05-13 11:07:29.555: epoch 54:	0.02928679  	0.07902431  	0.06526792  
2023-05-13 11:07:49.750: [iter 55 : loss : 0.5518 = 0.0834 + 0.4612 + 0.0071, time: 20.192261]
2023-05-13 11:07:50.008: epoch 55:	0.02923498  	0.07887626  	0.06528138  
2023-05-13 11:08:10.072: [iter 56 : loss : 0.5609 = 0.0803 + 0.4734 + 0.0073, time: 20.059437]
2023-05-13 11:08:10.330: epoch 56:	0.02909432  	0.07857063  	0.06513875  
2023-05-13 11:08:30.541: [iter 57 : loss : 0.5459 = 0.0777 + 0.4607 + 0.0074, time: 20.208651]
2023-05-13 11:08:30.800: epoch 57:	0.02910172  	0.07839127  	0.06512392  
2023-05-13 11:08:50.853: [iter 58 : loss : 0.5573 = 0.0768 + 0.4730 + 0.0075, time: 20.048904]
2023-05-13 11:08:51.112: epoch 58:	0.02907952  	0.07786652  	0.06494159  
2023-05-13 11:09:11.331: [iter 59 : loss : 0.5426 = 0.0746 + 0.4604 + 0.0076, time: 20.214988]
2023-05-13 11:09:11.598: epoch 59:	0.02907211  	0.07802457  	0.06498813  
2023-05-13 11:09:31.667: [iter 60 : loss : 0.5536 = 0.0733 + 0.4726 + 0.0077, time: 20.060594]
2023-05-13 11:09:31.924: epoch 60:	0.02902029  	0.07781392  	0.06473150  
2023-05-13 11:09:51.937: [iter 61 : loss : 0.5390 = 0.0713 + 0.4599 + 0.0079, time: 20.007546]
2023-05-13 11:09:52.194: epoch 61:	0.02905730  	0.07812926  	0.06469221  
2023-05-13 11:10:12.259: [iter 62 : loss : 0.5502 = 0.0700 + 0.4722 + 0.0080, time: 20.060859]
2023-05-13 11:10:12.518: epoch 62:	0.02901287  	0.07796854  	0.06473964  
2023-05-13 11:10:32.531: [iter 63 : loss : 0.5364 = 0.0687 + 0.4596 + 0.0081, time: 20.008584]
2023-05-13 11:10:32.801: epoch 63:	0.02901288  	0.07796383  	0.06464564  
2023-05-13 11:10:52.819: [iter 64 : loss : 0.5470 = 0.0670 + 0.4718 + 0.0082, time: 20.014600]
2023-05-13 11:10:53.077: epoch 64:	0.02892404  	0.07771482  	0.06458159  
2023-05-13 11:11:13.275: [iter 65 : loss : 0.5332 = 0.0656 + 0.4592 + 0.0083, time: 20.193331]
2023-05-13 11:11:13.533: epoch 65:	0.02885740  	0.07739377  	0.06446423  
2023-05-13 11:11:33.627: [iter 66 : loss : 0.5443 = 0.0646 + 0.4713 + 0.0084, time: 20.088979]
2023-05-13 11:11:33.883: epoch 66:	0.02876115  	0.07726179  	0.06428783  
2023-05-13 11:11:53.893: [iter 67 : loss : 0.5302 = 0.0629 + 0.4589 + 0.0085, time: 20.006849]
2023-05-13 11:11:54.160: epoch 67:	0.02876857  	0.07692523  	0.06420632  
2023-05-13 11:12:14.215: [iter 68 : loss : 0.5417 = 0.0619 + 0.4712 + 0.0086, time: 20.050705]
2023-05-13 11:12:14.472: epoch 68:	0.02874635  	0.07717033  	0.06414979  
2023-05-13 11:12:34.497: [iter 69 : loss : 0.5282 = 0.0608 + 0.4587 + 0.0087, time: 20.021258]
2023-05-13 11:12:34.756: epoch 69:	0.02865011  	0.07666895  	0.06394207  
2023-05-13 11:12:54.967: [iter 70 : loss : 0.5392 = 0.0594 + 0.4709 + 0.0088, time: 20.206340]
2023-05-13 11:12:55.224: epoch 70:	0.02853907  	0.07622178  	0.06370584  
2023-05-13 11:12:55.224: Early stopping is trigger at epoch: 70
2023-05-13 11:12:55.224: best_result@epoch 45:

2023-05-13 11:12:55.224: 		0.0299      	0.0807      	0.0662      
2023-05-22 11:02:30.038: my pid: 15260
2023-05-22 11:02:30.038: model: model.general_recommender.SGL
2023-05-22 11:02:30.038: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 11:02:30.038: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 11:02:34.259: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 11:02:55.015: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.755950]
2023-05-22 11:02:55.281: epoch 1:	0.00164347  	0.00300149  	0.00258104  
2023-05-22 11:02:55.281: Find a better model.
2023-05-22 11:03:15.827: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.542554]
2023-05-22 11:03:16.132: epoch 2:	0.00205064  	0.00378048  	0.00337046  
2023-05-22 11:03:16.132: Find a better model.
2023-05-22 11:03:36.774: [iter 3 : loss : 1.1343 = 0.6929 + 0.4414 + 0.0000, time: 20.637112]
2023-05-22 11:03:37.079: epoch 3:	0.00239858  	0.00471779  	0.00401059  
2023-05-22 11:03:37.079: Find a better model.
2023-05-22 11:03:57.573: [iter 4 : loss : 1.1345 = 0.6928 + 0.4417 + 0.0000, time: 20.489550]
2023-05-22 11:03:57.859: epoch 4:	0.00306485  	0.00620850  	0.00500110  
2023-05-22 11:03:57.859: Find a better model.
2023-05-22 11:04:18.385: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.522874]
2023-05-22 11:04:18.666: epoch 5:	0.00373853  	0.00796690  	0.00670043  
2023-05-22 11:04:18.667: Find a better model.
2023-05-22 11:04:39.127: [iter 6 : loss : 1.1348 = 0.6924 + 0.4424 + 0.0000, time: 20.456378]
2023-05-22 11:04:39.409: epoch 6:	0.00450103  	0.01061134  	0.00858048  
2023-05-22 11:04:39.409: Find a better model.
2023-05-22 11:04:59.942: [iter 7 : loss : 1.1349 = 0.6921 + 0.4427 + 0.0000, time: 20.529841]
2023-05-22 11:05:00.235: epoch 7:	0.00533016  	0.01338300  	0.01098185  
2023-05-22 11:05:00.235: Find a better model.
2023-05-22 11:05:20.719: [iter 8 : loss : 1.1349 = 0.6918 + 0.4431 + 0.0000, time: 20.480365]
2023-05-22 11:05:21.003: epoch 8:	0.00625553  	0.01568670  	0.01349993  
2023-05-22 11:05:21.003: Find a better model.
2023-05-22 11:05:41.312: [iter 9 : loss : 1.1347 = 0.6911 + 0.4435 + 0.0000, time: 20.304786]
2023-05-22 11:05:41.611: epoch 9:	0.00706986  	0.01726864  	0.01505644  
2023-05-22 11:05:41.611: Find a better model.
2023-05-22 11:06:01.947: [iter 10 : loss : 1.1341 = 0.6900 + 0.4441 + 0.0000, time: 20.333151]
2023-05-22 11:06:02.254: epoch 10:	0.00761027  	0.01830139  	0.01621857  
2023-05-22 11:06:02.254: Find a better model.
2023-05-22 11:06:22.747: [iter 11 : loss : 1.1332 = 0.6884 + 0.4447 + 0.0000, time: 20.490117]
2023-05-22 11:06:23.025: epoch 11:	0.00927594  	0.02223473  	0.01940631  
2023-05-22 11:06:23.025: Find a better model.
2023-05-22 11:06:43.327: [iter 12 : loss : 1.1315 = 0.6861 + 0.4454 + 0.0000, time: 20.297398]
2023-05-22 11:06:43.607: epoch 12:	0.01143024  	0.02839770  	0.02461976  
2023-05-22 11:06:43.607: Find a better model.
2023-05-22 11:07:04.051: [iter 13 : loss : 1.1288 = 0.6826 + 0.4461 + 0.0001, time: 20.441308]
2023-05-22 11:07:04.343: epoch 13:	0.01400654  	0.03601438  	0.03069845  
2023-05-22 11:07:04.343: Find a better model.
2023-05-22 11:07:24.901: [iter 14 : loss : 1.1234 = 0.6764 + 0.4469 + 0.0001, time: 20.553648]
2023-05-22 11:07:25.196: epoch 14:	0.01708624  	0.04445603  	0.03837159  
2023-05-22 11:07:25.196: Find a better model.
2023-05-22 11:07:45.496: [iter 15 : loss : 1.1136 = 0.6656 + 0.4478 + 0.0001, time: 20.294515]
2023-05-22 11:07:45.771: epoch 15:	0.02050654  	0.05381757  	0.04609435  
2023-05-22 11:07:45.771: Find a better model.
2023-05-22 11:08:06.002: [iter 16 : loss : 1.0968 = 0.6476 + 0.4490 + 0.0002, time: 20.225878]
2023-05-22 11:08:06.372: epoch 16:	0.02322353  	0.06086848  	0.05266913  
2023-05-22 11:08:06.373: Find a better model.
2023-05-22 11:08:27.151: [iter 17 : loss : 1.0706 = 0.6196 + 0.4507 + 0.0003, time: 20.774499]
2023-05-22 11:08:27.429: epoch 17:	0.02580728  	0.06780189  	0.05799351  
2023-05-22 11:08:27.429: Find a better model.
2023-05-22 11:08:47.667: [iter 18 : loss : 1.0332 = 0.5799 + 0.4527 + 0.0005, time: 20.233746]
2023-05-22 11:08:47.937: epoch 18:	0.02747303  	0.07308269  	0.06152439  
2023-05-22 11:08:47.937: Find a better model.
2023-05-22 11:09:08.035: [iter 19 : loss : 0.9876 = 0.5313 + 0.4555 + 0.0007, time: 20.093173]
2023-05-22 11:09:08.323: epoch 19:	0.02892406  	0.07688966  	0.06428976  
2023-05-22 11:09:08.323: Find a better model.
2023-05-22 11:09:28.687: [iter 20 : loss : 0.9378 = 0.4781 + 0.4587 + 0.0010, time: 20.359704]
2023-05-22 11:09:28.958: epoch 20:	0.02967920  	0.07931972  	0.06541023  
2023-05-22 11:09:28.959: Find a better model.
2023-05-22 11:09:49.074: [iter 21 : loss : 0.8890 = 0.4258 + 0.4619 + 0.0013, time: 20.111125]
2023-05-22 11:09:49.352: epoch 21:	0.02988650  	0.08053865  	0.06601702  
2023-05-22 11:09:49.352: Find a better model.
2023-05-22 11:10:09.660: [iter 22 : loss : 0.8434 = 0.3776 + 0.4643 + 0.0016, time: 20.305112]
2023-05-22 11:10:09.928: epoch 22:	0.03009379  	0.08155520  	0.06636399  
2023-05-22 11:10:09.928: Find a better model.
2023-05-22 11:10:30.141: [iter 23 : loss : 0.8047 = 0.3368 + 0.4660 + 0.0019, time: 20.207984]
2023-05-22 11:10:30.411: epoch 23:	0.03001235  	0.08188091  	0.06620792  
2023-05-22 11:10:30.411: Find a better model.
2023-05-22 11:10:51.087: [iter 24 : loss : 0.7718 = 0.3025 + 0.4670 + 0.0022, time: 20.672955]
2023-05-22 11:10:51.368: epoch 24:	0.03006417  	0.08263879  	0.06658728  
2023-05-22 11:10:51.368: Find a better model.
2023-05-22 11:11:12.158: [iter 25 : loss : 0.7433 = 0.2735 + 0.4673 + 0.0025, time: 20.786539]
2023-05-22 11:11:12.431: epoch 25:	0.03021964  	0.08362439  	0.06701344  
2023-05-22 11:11:12.431: Find a better model.
2023-05-22 11:11:32.907: [iter 26 : loss : 0.7200 = 0.2497 + 0.4675 + 0.0028, time: 20.471340]
2023-05-22 11:11:33.179: epoch 26:	0.03040471  	0.08403242  	0.06730983  
2023-05-22 11:11:33.179: Find a better model.
2023-05-22 11:11:53.698: [iter 27 : loss : 0.6990 = 0.2291 + 0.4669 + 0.0030, time: 20.515544]
2023-05-22 11:11:53.968: epoch 27:	0.03043433  	0.08407462  	0.06727977  
2023-05-22 11:11:53.968: Find a better model.
2023-05-22 11:12:14.469: [iter 28 : loss : 0.6820 = 0.2121 + 0.4666 + 0.0033, time: 20.496949]
2023-05-22 11:12:14.737: epoch 28:	0.03038991  	0.08435205  	0.06748859  
2023-05-22 11:12:14.737: Find a better model.
2023-05-22 11:12:34.977: [iter 29 : loss : 0.6669 = 0.1974 + 0.4660 + 0.0035, time: 20.234909]
2023-05-22 11:12:35.257: epoch 29:	0.03035288  	0.08395666  	0.06756894  
2023-05-22 11:12:55.899: [iter 30 : loss : 0.6532 = 0.1840 + 0.4654 + 0.0037, time: 20.637978]
2023-05-22 11:12:56.174: epoch 30:	0.03050095  	0.08420390  	0.06771825  
2023-05-22 11:13:16.427: [iter 31 : loss : 0.6412 = 0.1725 + 0.4647 + 0.0040, time: 20.249346]
2023-05-22 11:13:16.697: epoch 31:	0.03041952  	0.08396658  	0.06785947  
2023-05-22 11:13:37.281: [iter 32 : loss : 0.6312 = 0.1630 + 0.4640 + 0.0042, time: 20.579556]
2023-05-22 11:13:37.556: epoch 32:	0.03054538  	0.08432647  	0.06806278  
2023-05-22 11:13:58.276: [iter 33 : loss : 0.6225 = 0.1546 + 0.4635 + 0.0044, time: 20.716401]
2023-05-22 11:13:58.540: epoch 33:	0.03047135  	0.08428163  	0.06803625  
2023-05-22 11:14:19.215: [iter 34 : loss : 0.6139 = 0.1464 + 0.4629 + 0.0046, time: 20.670636]
2023-05-22 11:14:19.482: epoch 34:	0.03055279  	0.08457140  	0.06809018  
2023-05-22 11:14:19.482: Find a better model.
2023-05-22 11:14:39.871: [iter 35 : loss : 0.6058 = 0.1388 + 0.4623 + 0.0048, time: 20.384849]
2023-05-22 11:14:40.133: epoch 35:	0.03053798  	0.08456862  	0.06814045  
2023-05-22 11:15:00.652: [iter 36 : loss : 0.5993 = 0.1327 + 0.4617 + 0.0049, time: 20.513995]
2023-05-22 11:15:00.931: epoch 36:	0.03048614  	0.08423693  	0.06800906  
2023-05-22 11:15:21.169: [iter 37 : loss : 0.5934 = 0.1271 + 0.4612 + 0.0051, time: 20.234682]
2023-05-22 11:15:21.439: epoch 37:	0.03033809  	0.08400570  	0.06777783  
2023-05-22 11:15:41.823: [iter 38 : loss : 0.5873 = 0.1213 + 0.4607 + 0.0053, time: 20.379039]
2023-05-22 11:15:42.104: epoch 38:	0.03039731  	0.08365903  	0.06781426  
2023-05-22 11:16:02.508: [iter 39 : loss : 0.5828 = 0.1171 + 0.4602 + 0.0055, time: 20.400696]
2023-05-22 11:16:02.799: epoch 39:	0.03033808  	0.08351649  	0.06770623  
2023-05-22 11:16:23.442: [iter 40 : loss : 0.5776 = 0.1121 + 0.4598 + 0.0056, time: 20.640067]
2023-05-22 11:16:23.714: epoch 40:	0.03040472  	0.08325500  	0.06776442  
2023-05-22 11:16:44.243: [iter 41 : loss : 0.5728 = 0.1077 + 0.4593 + 0.0058, time: 20.524514]
2023-05-22 11:16:44.506: epoch 41:	0.03047875  	0.08336892  	0.06796784  
2023-05-22 11:17:05.149: [iter 42 : loss : 0.5688 = 0.1039 + 0.4589 + 0.0060, time: 20.637876]
2023-05-22 11:17:05.419: epoch 42:	0.03040472  	0.08292863  	0.06786208  
2023-05-22 11:17:26.232: [iter 43 : loss : 0.5646 = 0.0999 + 0.4586 + 0.0061, time: 20.810553]
2023-05-22 11:17:26.504: epoch 43:	0.03023443  	0.08265674  	0.06768150  
2023-05-22 11:17:46.938: [iter 44 : loss : 0.5622 = 0.0976 + 0.4583 + 0.0063, time: 20.428700]
2023-05-22 11:17:47.261: epoch 44:	0.03025664  	0.08242583  	0.06775089  
2023-05-22 11:18:07.716: [iter 45 : loss : 0.5584 = 0.0941 + 0.4579 + 0.0064, time: 20.444381]
2023-05-22 11:18:07.986: epoch 45:	0.03033808  	0.08256442  	0.06772384  
2023-05-22 11:18:28.380: [iter 46 : loss : 0.5552 = 0.0911 + 0.4575 + 0.0066, time: 20.390030]
2023-05-22 11:18:28.659: epoch 46:	0.03032327  	0.08280331  	0.06774014  
2023-05-22 11:18:48.833: [iter 47 : loss : 0.5523 = 0.0883 + 0.4573 + 0.0067, time: 20.171382]
2023-05-22 11:18:49.095: epoch 47:	0.03021962  	0.08258342  	0.06750610  
2023-05-22 11:19:09.640: [iter 48 : loss : 0.5496 = 0.0858 + 0.4570 + 0.0069, time: 20.540651]
2023-05-22 11:19:09.927: epoch 48:	0.03021221  	0.08257928  	0.06753458  
2023-05-22 11:19:30.779: [iter 49 : loss : 0.5470 = 0.0834 + 0.4566 + 0.0070, time: 20.849057]
2023-05-22 11:19:31.048: epoch 49:	0.03016040  	0.08233998  	0.06745244  
2023-05-22 11:19:53.606: [iter 50 : loss : 0.5449 = 0.0814 + 0.4564 + 0.0071, time: 22.553205]
2023-05-22 11:19:53.891: epoch 50:	0.03012338  	0.08203424  	0.06725069  
2023-05-22 11:20:16.415: [iter 51 : loss : 0.5424 = 0.0791 + 0.4560 + 0.0073, time: 22.519173]
2023-05-22 11:20:16.764: epoch 51:	0.03004933  	0.08145776  	0.06703571  
2023-05-22 11:20:39.794: [iter 52 : loss : 0.5397 = 0.0765 + 0.4558 + 0.0074, time: 23.027150]
2023-05-22 11:20:40.129: epoch 52:	0.02990128  	0.08080266  	0.06673893  
2023-05-22 11:21:33.875: my pid: 6184
2023-05-22 11:21:33.875: model: model.general_recommender.SGL
2023-05-22 11:21:33.875: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 11:21:33.875: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 11:21:38.247: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 11:22:01.847: [iter 1 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 23.600110]
2023-05-22 11:22:02.154: epoch 1:	0.00166568  	0.00314292  	0.00269234  
2023-05-22 11:22:02.154: Find a better model.
2023-05-22 11:22:25.807: [iter 2 : loss : 1.1330 = 0.6930 + 0.4400 + 0.0000, time: 23.649765]
2023-05-22 11:22:26.173: epoch 2:	0.00208765  	0.00420850  	0.00341123  
2023-05-22 11:22:26.173: Find a better model.
2023-05-22 11:22:49.720: [iter 3 : loss : 1.1330 = 0.6929 + 0.4401 + 0.0000, time: 23.543820]
2023-05-22 11:22:50.057: epoch 3:	0.00233936  	0.00511328  	0.00395821  
2023-05-22 11:22:50.058: Find a better model.
2023-05-22 11:23:13.344: [iter 4 : loss : 1.1331 = 0.6928 + 0.4402 + 0.0000, time: 23.278753]
2023-05-22 11:23:13.695: epoch 4:	0.00303524  	0.00676998  	0.00531937  
2023-05-22 11:23:13.695: Find a better model.
2023-05-22 11:23:37.201: [iter 5 : loss : 1.1331 = 0.6927 + 0.4404 + 0.0000, time: 23.501240]
2023-05-22 11:23:37.535: epoch 5:	0.00359787  	0.00852355  	0.00709956  
2023-05-22 11:23:37.535: Find a better model.
2023-05-22 11:24:00.959: [iter 6 : loss : 1.1332 = 0.6925 + 0.4406 + 0.0000, time: 23.418095]
2023-05-22 11:24:01.326: epoch 6:	0.00461948  	0.01118831  	0.00857149  
2023-05-22 11:24:01.326: Find a better model.
2023-05-22 11:24:24.710: [iter 7 : loss : 1.1331 = 0.6923 + 0.4408 + 0.0000, time: 23.376735]
2023-05-22 11:24:25.041: epoch 7:	0.00538938  	0.01303906  	0.01092593  
2023-05-22 11:24:25.041: Find a better model.
2023-05-22 11:24:48.433: [iter 8 : loss : 1.1331 = 0.6920 + 0.4411 + 0.0000, time: 23.385653]
2023-05-22 11:24:48.773: epoch 8:	0.00624072  	0.01555157  	0.01315863  
2023-05-22 11:24:48.773: Find a better model.
2023-05-22 11:25:12.158: [iter 9 : loss : 1.1328 = 0.6914 + 0.4414 + 0.0000, time: 23.380387]
2023-05-22 11:25:12.503: epoch 9:	0.00713648  	0.01827147  	0.01489207  
2023-05-22 11:25:12.503: Find a better model.
2023-05-22 11:25:36.018: [iter 10 : loss : 1.1323 = 0.6905 + 0.4418 + 0.0000, time: 23.508221]
2023-05-22 11:25:36.382: epoch 10:	0.00781015  	0.01955487  	0.01585014  
2023-05-22 11:25:36.382: Find a better model.
2023-05-22 11:25:59.681: [iter 11 : loss : 1.1312 = 0.6890 + 0.4423 + 0.0000, time: 23.292936]
2023-05-22 11:26:00.037: epoch 11:	0.00927593  	0.02304214  	0.01976286  
2023-05-22 11:26:00.037: Find a better model.
2023-05-22 11:26:23.429: [iter 12 : loss : 1.1294 = 0.6866 + 0.4428 + 0.0000, time: 23.387721]
2023-05-22 11:26:23.786: epoch 12:	0.01114151  	0.02823068  	0.02425650  
2023-05-22 11:26:23.786: Find a better model.
2023-05-22 11:26:47.273: [iter 13 : loss : 1.1262 = 0.6828 + 0.4433 + 0.0001, time: 23.483167]
2023-05-22 11:26:47.616: epoch 13:	0.01376963  	0.03586553  	0.03096500  
2023-05-22 11:26:47.617: Find a better model.
2023-05-22 11:27:11.038: [iter 14 : loss : 1.1202 = 0.6762 + 0.4440 + 0.0001, time: 23.416194]
2023-05-22 11:27:11.398: epoch 14:	0.01658284  	0.04414487  	0.03800284  
2023-05-22 11:27:11.398: Find a better model.
2023-05-22 11:27:34.812: [iter 15 : loss : 1.1092 = 0.6643 + 0.4448 + 0.0001, time: 23.410117]
2023-05-22 11:27:35.163: epoch 15:	0.01970698  	0.05229362  	0.04468898  
2023-05-22 11:27:35.163: Find a better model.
2023-05-22 11:27:58.580: [iter 16 : loss : 1.0900 = 0.6440 + 0.4458 + 0.0002, time: 23.410188]
2023-05-22 11:27:58.900: epoch 16:	0.02286077  	0.06069425  	0.05193013  
2023-05-22 11:27:58.900: Find a better model.
2023-05-22 11:28:21.949: [iter 17 : loss : 1.0606 = 0.6128 + 0.4474 + 0.0004, time: 23.043427]
2023-05-22 11:28:22.307: epoch 17:	0.02488187  	0.06638756  	0.05653061  
2023-05-22 11:28:22.307: Find a better model.
2023-05-22 11:28:43.010: [iter 18 : loss : 1.0197 = 0.5697 + 0.4495 + 0.0006, time: 20.698620]
2023-05-22 11:28:43.297: epoch 18:	0.02688076  	0.07243787  	0.06094836  
2023-05-22 11:28:43.297: Find a better model.
2023-05-22 11:29:03.933: [iter 19 : loss : 0.9715 = 0.5185 + 0.4522 + 0.0008, time: 20.632227]
2023-05-22 11:29:04.213: epoch 19:	0.02805045  	0.07566701  	0.06354605  
2023-05-22 11:29:04.213: Find a better model.
2023-05-22 11:29:24.411: [iter 20 : loss : 0.9205 = 0.4641 + 0.4553 + 0.0011, time: 20.194310]
2023-05-22 11:29:24.690: epoch 20:	0.02875379  	0.07791923  	0.06498494  
2023-05-22 11:29:24.690: Find a better model.
2023-05-22 11:29:45.141: [iter 21 : loss : 0.8720 = 0.4123 + 0.4583 + 0.0014, time: 20.446868]
2023-05-22 11:29:45.419: epoch 21:	0.02922019  	0.07947195  	0.06557427  
2023-05-22 11:29:45.419: Find a better model.
2023-05-22 11:30:05.921: [iter 22 : loss : 0.8277 = 0.3654 + 0.4606 + 0.0017, time: 20.498878]
2023-05-22 11:30:06.198: epoch 22:	0.02933126  	0.08025172  	0.06601068  
2023-05-22 11:30:06.198: Find a better model.
2023-05-22 11:30:26.744: [iter 23 : loss : 0.7903 = 0.3260 + 0.4623 + 0.0020, time: 20.542915]
2023-05-22 11:30:27.022: epoch 23:	0.02948673  	0.08032805  	0.06632464  
2023-05-22 11:30:27.022: Find a better model.
2023-05-22 11:30:47.492: [iter 24 : loss : 0.7587 = 0.2932 + 0.4631 + 0.0023, time: 20.465766]
2023-05-22 11:30:47.778: epoch 24:	0.02958297  	0.08129058  	0.06654544  
2023-05-22 11:30:47.778: Find a better model.
2023-05-22 11:31:08.319: [iter 25 : loss : 0.7314 = 0.2655 + 0.4634 + 0.0026, time: 20.537157]
2023-05-22 11:31:08.596: epoch 25:	0.02973843  	0.08196471  	0.06692484  
2023-05-22 11:31:08.596: Find a better model.
2023-05-22 11:31:29.099: [iter 26 : loss : 0.7090 = 0.2427 + 0.4635 + 0.0028, time: 20.499257]
2023-05-22 11:31:29.374: epoch 26:	0.02985689  	0.08212835  	0.06695601  
2023-05-22 11:31:29.374: Find a better model.
2023-05-22 11:31:49.911: [iter 27 : loss : 0.6896 = 0.2235 + 0.4630 + 0.0031, time: 20.533487]
2023-05-22 11:31:50.187: epoch 27:	0.02993833  	0.08255180  	0.06713423  
2023-05-22 11:31:50.187: Find a better model.
2023-05-22 11:32:10.688: [iter 28 : loss : 0.6731 = 0.2070 + 0.4627 + 0.0033, time: 20.495829]
2023-05-22 11:32:10.968: epoch 28:	0.02999755  	0.08298846  	0.06739258  
2023-05-22 11:32:10.968: Find a better model.
2023-05-22 11:32:31.466: [iter 29 : loss : 0.6589 = 0.1932 + 0.4621 + 0.0036, time: 20.494060]
2023-05-22 11:32:31.739: epoch 29:	0.02989390  	0.08290310  	0.06743348  
2023-05-22 11:32:52.037: [iter 30 : loss : 0.6457 = 0.1803 + 0.4615 + 0.0038, time: 20.293614]
2023-05-22 11:32:52.310: epoch 30:	0.03004197  	0.08346689  	0.06756815  
2023-05-22 11:32:52.310: Find a better model.
2023-05-22 11:33:12.485: [iter 31 : loss : 0.6342 = 0.1692 + 0.4610 + 0.0040, time: 20.169472]
2023-05-22 11:33:12.753: epoch 31:	0.02999015  	0.08351061  	0.06766604  
2023-05-22 11:33:12.753: Find a better model.
2023-05-22 11:33:33.081: [iter 32 : loss : 0.6245 = 0.1599 + 0.4603 + 0.0042, time: 20.324011]
2023-05-22 11:33:33.348: epoch 32:	0.03002717  	0.08344258  	0.06766920  
2023-05-22 11:33:53.658: [iter 33 : loss : 0.6161 = 0.1520 + 0.4598 + 0.0044, time: 20.306106]
2023-05-22 11:33:53.937: epoch 33:	0.03001976  	0.08329479  	0.06763779  
2023-05-22 11:34:14.424: [iter 34 : loss : 0.6077 = 0.1439 + 0.4592 + 0.0046, time: 20.483158]
2023-05-22 11:34:14.689: epoch 34:	0.03007157  	0.08350976  	0.06770233  
2023-05-22 11:34:35.031: [iter 35 : loss : 0.5998 = 0.1364 + 0.4586 + 0.0048, time: 20.338855]
2023-05-22 11:34:35.298: epoch 35:	0.03018260  	0.08376887  	0.06785025  
2023-05-22 11:34:35.298: Find a better model.
2023-05-22 11:34:55.860: [iter 36 : loss : 0.5937 = 0.1306 + 0.4581 + 0.0050, time: 20.559626]
2023-05-22 11:34:56.136: epoch 36:	0.03023441  	0.08343851  	0.06758609  
2023-05-22 11:35:16.644: [iter 37 : loss : 0.5880 = 0.1252 + 0.4576 + 0.0052, time: 20.503942]
2023-05-22 11:35:16.909: epoch 37:	0.03016778  	0.08287400  	0.06733507  
2023-05-22 11:35:37.383: [iter 38 : loss : 0.5822 = 0.1198 + 0.4570 + 0.0054, time: 20.471044]
2023-05-22 11:35:37.651: epoch 38:	0.03016037  	0.08262240  	0.06741679  
2023-05-22 11:35:58.047: [iter 39 : loss : 0.5775 = 0.1154 + 0.4566 + 0.0055, time: 20.392847]
2023-05-22 11:35:58.319: epoch 39:	0.03017518  	0.08247180  	0.06742824  
2023-05-22 11:36:18.826: [iter 40 : loss : 0.5721 = 0.1102 + 0.4562 + 0.0057, time: 20.502005]
2023-05-22 11:36:19.090: epoch 40:	0.03000490  	0.08180019  	0.06718914  
2023-05-22 11:36:39.770: [iter 41 : loss : 0.5680 = 0.1064 + 0.4557 + 0.0059, time: 20.677789]
2023-05-22 11:36:40.035: epoch 41:	0.02997530  	0.08188008  	0.06726068  
2023-05-22 11:37:00.570: [iter 42 : loss : 0.5637 = 0.1023 + 0.4554 + 0.0060, time: 20.531032]
2023-05-22 11:37:00.833: epoch 42:	0.02987165  	0.08129809  	0.06716920  
2023-05-22 11:37:21.188: [iter 43 : loss : 0.5601 = 0.0990 + 0.4550 + 0.0062, time: 20.352777]
2023-05-22 11:37:21.458: epoch 43:	0.02976800  	0.08095583  	0.06704196  
2023-05-22 11:37:42.174: [iter 44 : loss : 0.5574 = 0.0964 + 0.4547 + 0.0063, time: 20.711320]
2023-05-22 11:37:42.441: epoch 44:	0.02973838  	0.08070088  	0.06678403  
2023-05-22 11:38:03.193: [iter 45 : loss : 0.5538 = 0.0930 + 0.4543 + 0.0065, time: 20.748901]
2023-05-22 11:38:03.469: epoch 45:	0.02993087  	0.08117068  	0.06680947  
2023-05-22 11:38:23.974: [iter 46 : loss : 0.5509 = 0.0903 + 0.4540 + 0.0066, time: 20.501466]
2023-05-22 11:38:24.236: epoch 46:	0.02970137  	0.08052652  	0.06659272  
2023-05-22 11:38:44.784: [iter 47 : loss : 0.5477 = 0.0873 + 0.4536 + 0.0068, time: 20.542204]
2023-05-22 11:38:45.046: epoch 47:	0.02967175  	0.08040825  	0.06660891  
2023-05-22 11:39:05.561: [iter 48 : loss : 0.5452 = 0.0849 + 0.4534 + 0.0069, time: 20.512410]
2023-05-22 11:39:05.825: epoch 48:	0.02960512  	0.08032744  	0.06652142  
2023-05-22 11:39:26.321: [iter 49 : loss : 0.5427 = 0.0826 + 0.4530 + 0.0071, time: 20.492559]
2023-05-22 11:39:26.599: epoch 49:	0.02960512  	0.08019038  	0.06642544  
2023-05-22 11:39:47.313: [iter 50 : loss : 0.5404 = 0.0804 + 0.4528 + 0.0072, time: 20.711696]
2023-05-22 11:39:47.592: epoch 50:	0.02952369  	0.07987264  	0.06627919  
2023-05-22 11:40:08.308: [iter 51 : loss : 0.5381 = 0.0782 + 0.4525 + 0.0073, time: 20.712051]
2023-05-22 11:40:08.573: epoch 51:	0.02934601  	0.07920450  	0.06593297  
2023-05-22 11:40:29.101: [iter 52 : loss : 0.5353 = 0.0756 + 0.4523 + 0.0074, time: 20.523507]
2023-05-22 11:40:29.366: epoch 52:	0.02928678  	0.07907096  	0.06596369  
2023-05-22 11:40:50.074: [iter 53 : loss : 0.5348 = 0.0752 + 0.4521 + 0.0076, time: 20.704451]
2023-05-22 11:40:50.339: epoch 53:	0.02928678  	0.07862251  	0.06577155  
2023-05-22 11:41:10.885: [iter 54 : loss : 0.5325 = 0.0730 + 0.4518 + 0.0077, time: 20.541662]
2023-05-22 11:41:11.149: epoch 54:	0.02924236  	0.07842751  	0.06565347  
2023-05-22 11:41:31.681: [iter 55 : loss : 0.5308 = 0.0713 + 0.4517 + 0.0078, time: 20.528053]
2023-05-22 11:41:31.944: epoch 55:	0.02914612  	0.07792249  	0.06536275  
2023-05-22 11:41:52.529: [iter 56 : loss : 0.5286 = 0.0692 + 0.4514 + 0.0079, time: 20.581276]
2023-05-22 11:41:52.798: epoch 56:	0.02913132  	0.07796494  	0.06535835  
2023-05-22 11:42:13.479: [iter 57 : loss : 0.5271 = 0.0678 + 0.4513 + 0.0081, time: 20.676197]
2023-05-22 11:42:13.750: epoch 57:	0.02904988  	0.07776009  	0.06532556  
2023-05-22 11:42:34.304: [iter 58 : loss : 0.5255 = 0.0662 + 0.4511 + 0.0082, time: 20.544880]
2023-05-22 11:42:34.567: epoch 58:	0.02899066  	0.07747212  	0.06518308  
2023-05-22 11:42:55.105: [iter 59 : loss : 0.5243 = 0.0650 + 0.4510 + 0.0083, time: 20.534314]
2023-05-22 11:42:55.369: epoch 59:	0.02896846  	0.07735790  	0.06498051  
2023-05-22 11:43:16.026: [iter 60 : loss : 0.5228 = 0.0636 + 0.4508 + 0.0084, time: 20.652423]
2023-05-22 11:43:16.291: epoch 60:	0.02895365  	0.07720801  	0.06500695  
2023-05-22 11:43:16.291: Early stopping is trigger at epoch: 60
2023-05-22 11:43:16.291: best_result@epoch 35:

2023-05-22 11:43:16.291: 		0.0302      	0.0838      	0.0679      
2023-05-22 14:31:33.302: my pid: 8600
2023-05-22 14:31:33.303: model: model.general_recommender.SGL
2023-05-22 14:31:33.303: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 14:31:33.303: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 14:31:37.679: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 14:32:00.051: [iter 1 : loss : 1.1334 = 0.6931 + 0.4403 + 0.0000, time: 22.371898]
2023-05-22 14:32:00.315: epoch 1:	0.00173231  	0.00320836  	0.00286682  
2023-05-22 14:32:00.315: Find a better model.
2023-05-22 14:32:22.207: [iter 2 : loss : 1.1321 = 0.6930 + 0.4391 + 0.0000, time: 21.888320]
2023-05-22 14:32:22.521: epoch 2:	0.00216909  	0.00438909  	0.00382662  
2023-05-22 14:32:22.521: Find a better model.
2023-05-22 14:32:44.453: [iter 3 : loss : 1.1320 = 0.6930 + 0.4391 + 0.0000, time: 21.925065]
2023-05-22 14:32:44.762: epoch 3:	0.00264288  	0.00517653  	0.00437739  
2023-05-22 14:32:44.762: Find a better model.
2023-05-22 14:33:07.022: [iter 4 : loss : 1.1320 = 0.6929 + 0.4391 + 0.0000, time: 22.257020]
2023-05-22 14:33:07.340: epoch 4:	0.00330915  	0.00711729  	0.00554805  
2023-05-22 14:33:07.340: Find a better model.
2023-05-22 14:33:30.776: [iter 5 : loss : 1.1319 = 0.6928 + 0.4392 + 0.0000, time: 23.430047]
2023-05-22 14:33:31.093: epoch 5:	0.00384217  	0.00882275  	0.00700601  
2023-05-22 14:33:31.093: Find a better model.
2023-05-22 14:33:54.646: [iter 6 : loss : 1.1319 = 0.6926 + 0.4393 + 0.0000, time: 23.548406]
2023-05-22 14:33:54.940: epoch 6:	0.00467130  	0.01054844  	0.00861767  
2023-05-22 14:33:54.940: Find a better model.
2023-05-22 14:34:18.502: [iter 7 : loss : 1.1318 = 0.6924 + 0.4394 + 0.0000, time: 23.557062]
2023-05-22 14:34:18.859: epoch 7:	0.00564849  	0.01397439  	0.01144404  
2023-05-22 14:34:18.859: Find a better model.
2023-05-22 14:34:42.463: [iter 8 : loss : 1.1317 = 0.6921 + 0.4395 + 0.0000, time: 23.597375]
2023-05-22 14:34:42.785: epoch 8:	0.00660347  	0.01574131  	0.01341194  
2023-05-22 14:34:42.785: Find a better model.
2023-05-22 14:35:06.429: [iter 9 : loss : 1.1314 = 0.6917 + 0.4397 + 0.0000, time: 23.639464]
2023-05-22 14:35:06.764: epoch 9:	0.00770650  	0.01928624  	0.01643682  
2023-05-22 14:35:06.764: Find a better model.
2023-05-22 14:35:30.326: [iter 10 : loss : 1.1310 = 0.6910 + 0.4400 + 0.0000, time: 23.558114]
2023-05-22 14:35:30.626: epoch 10:	0.00883175  	0.02273118  	0.01855037  
2023-05-22 14:35:30.626: Find a better model.
2023-05-22 14:35:53.977: [iter 11 : loss : 1.1300 = 0.6898 + 0.4402 + 0.0000, time: 23.346839]
2023-05-22 14:35:54.316: epoch 11:	0.00943880  	0.02386959  	0.01999252  
2023-05-22 14:35:54.316: Find a better model.
2023-05-22 14:36:17.738: [iter 12 : loss : 1.1281 = 0.6875 + 0.4406 + 0.0000, time: 23.417085]
2023-05-22 14:36:18.079: epoch 12:	0.01097125  	0.02874049  	0.02407111  
2023-05-22 14:36:18.079: Find a better model.
2023-05-22 14:36:41.523: [iter 13 : loss : 1.1250 = 0.6839 + 0.4410 + 0.0000, time: 23.439882]
2023-05-22 14:36:41.888: epoch 13:	0.01300710  	0.03443241  	0.02945426  
2023-05-22 14:36:41.888: Find a better model.
2023-05-22 14:37:05.380: [iter 14 : loss : 1.1192 = 0.6776 + 0.4416 + 0.0001, time: 23.482327]
2023-05-22 14:37:05.690: epoch 14:	0.01563523  	0.04175432  	0.03599825  
2023-05-22 14:37:05.690: Find a better model.
2023-05-22 14:37:29.112: [iter 15 : loss : 1.1084 = 0.6661 + 0.4422 + 0.0001, time: 23.418468]
2023-05-22 14:37:29.434: epoch 15:	0.01880380  	0.04911765  	0.04331844  
2023-05-22 14:37:29.434: Find a better model.
2023-05-22 14:37:52.700: [iter 16 : loss : 1.0895 = 0.6462 + 0.4431 + 0.0002, time: 23.261100]
2023-05-22 14:37:52.991: epoch 16:	0.02165408  	0.05738262  	0.05014645  
2023-05-22 14:37:52.991: Find a better model.
2023-05-22 14:38:16.346: [iter 17 : loss : 1.0600 = 0.6152 + 0.4445 + 0.0003, time: 23.351952]
2023-05-22 14:38:16.640: epoch 17:	0.02420078  	0.06475939  	0.05575235  
2023-05-22 14:38:16.640: Find a better model.
2023-05-22 14:38:39.827: [iter 18 : loss : 1.0189 = 0.5719 + 0.4464 + 0.0005, time: 23.183639]
2023-05-22 14:38:40.115: epoch 18:	0.02599238  	0.06984849  	0.06004484  
2023-05-22 14:38:40.115: Find a better model.
2023-05-22 14:39:03.422: [iter 19 : loss : 0.9705 = 0.5208 + 0.4490 + 0.0008, time: 23.303561]
2023-05-22 14:39:03.723: epoch 19:	0.02756926  	0.07375997  	0.06264313  
2023-05-22 14:39:03.723: Find a better model.
2023-05-22 14:39:27.072: [iter 20 : loss : 0.9195 = 0.4665 + 0.4520 + 0.0011, time: 23.346007]
2023-05-22 14:39:27.366: epoch 20:	0.02823555  	0.07571865  	0.06408583  
2023-05-22 14:39:27.366: Find a better model.
2023-05-22 14:39:50.720: [iter 21 : loss : 0.8708 = 0.4147 + 0.4548 + 0.0014, time: 23.349754]
2023-05-22 14:39:51.010: epoch 21:	0.02883520  	0.07728451  	0.06480492  
2023-05-22 14:39:51.010: Find a better model.
2023-05-22 14:40:14.340: [iter 22 : loss : 0.8264 = 0.3676 + 0.4572 + 0.0017, time: 23.325406]
2023-05-22 14:40:14.627: epoch 22:	0.02910912  	0.07863352  	0.06535601  
2023-05-22 14:40:14.627: Find a better model.
2023-05-22 14:40:38.583: [iter 23 : loss : 0.7886 = 0.3278 + 0.4588 + 0.0020, time: 23.949088]
2023-05-22 14:40:38.896: epoch 23:	0.02922758  	0.07901059  	0.06575266  
2023-05-22 14:40:38.896: Find a better model.
2023-05-22 14:41:02.936: [iter 24 : loss : 0.7569 = 0.2948 + 0.4598 + 0.0023, time: 24.034982]
2023-05-22 14:41:03.260: epoch 24:	0.02933122  	0.07943984  	0.06605271  
2023-05-22 14:41:03.261: Find a better model.
2023-05-22 14:41:24.192: [iter 25 : loss : 0.7296 = 0.2669 + 0.4601 + 0.0025, time: 20.927710]
2023-05-22 14:41:24.480: epoch 25:	0.02950150  	0.08036081  	0.06636203  
2023-05-22 14:41:24.480: Find a better model.
2023-05-22 14:41:45.215: [iter 26 : loss : 0.7073 = 0.2442 + 0.4603 + 0.0028, time: 20.732483]
2023-05-22 14:41:45.505: epoch 26:	0.02961256  	0.08041094  	0.06661248  
2023-05-22 14:41:45.506: Find a better model.
2023-05-22 14:42:06.352: [iter 27 : loss : 0.6875 = 0.2244 + 0.4600 + 0.0031, time: 20.842797]
2023-05-22 14:42:06.625: epoch 27:	0.02969400  	0.08065233  	0.06683163  
2023-05-22 14:42:06.626: Find a better model.
2023-05-22 14:42:27.384: [iter 28 : loss : 0.6712 = 0.2081 + 0.4597 + 0.0033, time: 20.753652]
2023-05-22 14:42:27.653: epoch 28:	0.02985688  	0.08123918  	0.06708503  
2023-05-22 14:42:27.653: Find a better model.
2023-05-22 14:42:48.339: [iter 29 : loss : 0.6567 = 0.1940 + 0.4592 + 0.0036, time: 20.682215]
2023-05-22 14:42:48.621: epoch 29:	0.02992350  	0.08188538  	0.06736077  
2023-05-22 14:42:48.622: Find a better model.
2023-05-22 14:43:09.568: [iter 30 : loss : 0.6436 = 0.1812 + 0.4586 + 0.0038, time: 20.943465]
2023-05-22 14:43:09.835: epoch 30:	0.02985686  	0.08168959  	0.06735913  
2023-05-22 14:43:30.562: [iter 31 : loss : 0.6321 = 0.1699 + 0.4581 + 0.0040, time: 20.723765]
2023-05-22 14:43:30.830: epoch 31:	0.02993090  	0.08193019  	0.06753709  
2023-05-22 14:43:30.830: Find a better model.
2023-05-22 14:43:51.809: [iter 32 : loss : 0.6225 = 0.1608 + 0.4575 + 0.0042, time: 20.973900]
2023-05-22 14:43:52.076: epoch 32:	0.02998272  	0.08210167  	0.06759927  
2023-05-22 14:43:52.076: Find a better model.
2023-05-22 14:44:13.123: [iter 33 : loss : 0.6142 = 0.1528 + 0.4570 + 0.0044, time: 21.042391]
2023-05-22 14:44:13.393: epoch 33:	0.03009376  	0.08211169  	0.06773461  
2023-05-22 14:44:13.393: Find a better model.
2023-05-22 14:44:34.299: [iter 34 : loss : 0.6057 = 0.1447 + 0.4564 + 0.0046, time: 20.901837]
2023-05-22 14:44:34.565: epoch 34:	0.03020480  	0.08224966  	0.06765063  
2023-05-22 14:44:34.565: Find a better model.
2023-05-22 14:44:55.362: [iter 35 : loss : 0.5978 = 0.1372 + 0.4558 + 0.0048, time: 20.793594]
2023-05-22 14:44:55.630: epoch 35:	0.03018260  	0.08236916  	0.06770048  
2023-05-22 14:44:55.630: Find a better model.
2023-05-22 14:45:16.338: [iter 36 : loss : 0.5917 = 0.1314 + 0.4553 + 0.0050, time: 20.704115]
2023-05-22 14:45:16.604: epoch 36:	0.03014559  	0.08238795  	0.06771276  
2023-05-22 14:45:16.604: Find a better model.
2023-05-22 14:45:37.699: [iter 37 : loss : 0.5859 = 0.1259 + 0.4548 + 0.0052, time: 21.090215]
2023-05-22 14:45:37.966: epoch 37:	0.03024923  	0.08254138  	0.06768364  
2023-05-22 14:45:37.966: Find a better model.
2023-05-22 14:45:59.087: [iter 38 : loss : 0.5801 = 0.1204 + 0.4543 + 0.0054, time: 21.117595]
2023-05-22 14:45:59.352: epoch 38:	0.03013078  	0.08259954  	0.06756034  
2023-05-22 14:45:59.352: Find a better model.
2023-05-22 14:46:20.093: [iter 39 : loss : 0.5752 = 0.1159 + 0.4539 + 0.0055, time: 20.737244]
2023-05-22 14:46:20.361: epoch 39:	0.03012338  	0.08255760  	0.06765303  
2023-05-22 14:46:41.271: [iter 40 : loss : 0.5702 = 0.1111 + 0.4534 + 0.0057, time: 20.906999]
2023-05-22 14:46:41.540: epoch 40:	0.03001233  	0.08202956  	0.06753939  
2023-05-22 14:47:02.272: [iter 41 : loss : 0.5656 = 0.1068 + 0.4530 + 0.0059, time: 20.728410]
2023-05-22 14:47:02.537: epoch 41:	0.02996050  	0.08202929  	0.06758261  
2023-05-22 14:47:23.076: [iter 42 : loss : 0.5619 = 0.1032 + 0.4527 + 0.0060, time: 20.536582]
2023-05-22 14:47:23.340: epoch 42:	0.02996050  	0.08171736  	0.06743117  
2023-05-22 14:47:44.051: [iter 43 : loss : 0.5578 = 0.0993 + 0.4523 + 0.0062, time: 20.705624]
2023-05-22 14:47:44.314: epoch 43:	0.02990868  	0.08148863  	0.06727026  
2023-05-22 14:48:05.263: [iter 44 : loss : 0.5555 = 0.0972 + 0.4520 + 0.0063, time: 20.945956]
2023-05-22 14:48:05.527: epoch 44:	0.02988646  	0.08118653  	0.06708565  
2023-05-22 14:48:26.419: [iter 45 : loss : 0.5515 = 0.0934 + 0.4516 + 0.0065, time: 20.888665]
2023-05-22 14:48:26.684: epoch 45:	0.02969397  	0.08042514  	0.06668869  
2023-05-22 14:48:47.502: [iter 46 : loss : 0.5484 = 0.0905 + 0.4513 + 0.0066, time: 20.815661]
2023-05-22 14:48:47.787: epoch 46:	0.02961993  	0.08019526  	0.06651185  
2023-05-22 14:49:09.753: [iter 47 : loss : 0.5456 = 0.0879 + 0.4510 + 0.0068, time: 21.962013]
2023-05-22 14:49:10.044: epoch 47:	0.02948667  	0.07996642  	0.06639758  
2023-05-22 14:49:47.042: my pid: 15088
2023-05-22 14:49:47.042: model: model.general_recommender.SGL
2023-05-22 14:49:47.042: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 14:49:47.042: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 14:49:51.285: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 14:50:14.694: [iter 1 : loss : 1.1327 = 0.6931 + 0.4397 + 0.0000, time: 23.408628]
2023-05-22 14:50:14.957: epoch 1:	0.00179894  	0.00384603  	0.00321888  
2023-05-22 14:50:14.957: Find a better model.
2023-05-22 14:50:38.401: [iter 2 : loss : 1.1314 = 0.6930 + 0.4384 + 0.0000, time: 23.440791]
2023-05-22 14:50:38.749: epoch 2:	0.00233936  	0.00513695  	0.00398679  
2023-05-22 14:50:38.749: Find a better model.
2023-05-22 14:51:02.962: [iter 3 : loss : 1.1312 = 0.6930 + 0.4382 + 0.0000, time: 24.206371]
2023-05-22 14:51:03.311: epoch 3:	0.00267249  	0.00544917  	0.00437463  
2023-05-22 14:51:03.311: Find a better model.
2023-05-22 14:51:27.468: [iter 4 : loss : 1.1311 = 0.6929 + 0.4382 + 0.0000, time: 24.152919]
2023-05-22 14:51:27.773: epoch 4:	0.00343500  	0.00748910  	0.00589906  
2023-05-22 14:51:27.773: Find a better model.
2023-05-22 14:51:51.845: [iter 5 : loss : 1.1310 = 0.6928 + 0.4382 + 0.0000, time: 24.068923]
2023-05-22 14:51:52.209: epoch 5:	0.00399763  	0.00967130  	0.00783063  
2023-05-22 14:51:52.209: Find a better model.
2023-05-22 14:52:16.398: [iter 6 : loss : 1.1308 = 0.6926 + 0.4382 + 0.0000, time: 24.183997]
2023-05-22 14:52:16.761: epoch 6:	0.00493780  	0.01147384  	0.00934229  
2023-05-22 14:52:16.762: Find a better model.
2023-05-22 14:52:40.782: [iter 7 : loss : 1.1307 = 0.6925 + 0.4382 + 0.0000, time: 24.015696]
2023-05-22 14:52:41.100: epoch 7:	0.00610007  	0.01564207  	0.01279167  
2023-05-22 14:52:41.100: Find a better model.
2023-05-22 14:53:05.240: [iter 8 : loss : 1.1305 = 0.6922 + 0.4383 + 0.0000, time: 24.135772]
2023-05-22 14:53:05.563: epoch 8:	0.00719570  	0.01891436  	0.01509874  
2023-05-22 14:53:05.564: Find a better model.
2023-05-22 14:53:29.876: [iter 9 : loss : 1.1302 = 0.6918 + 0.4384 + 0.0000, time: 24.307713]
2023-05-22 14:53:30.185: epoch 9:	0.00802483  	0.02180942  	0.01762518  
2023-05-22 14:53:30.185: Find a better model.
2023-05-22 14:53:54.318: [iter 10 : loss : 1.1297 = 0.6912 + 0.4385 + 0.0000, time: 24.129615]
2023-05-22 14:53:54.682: epoch 10:	0.00915008  	0.02432822  	0.02031213  
2023-05-22 14:53:54.682: Find a better model.
2023-05-22 14:54:18.765: [iter 11 : loss : 1.1287 = 0.6900 + 0.4386 + 0.0000, time: 24.076046]
2023-05-22 14:54:19.124: epoch 11:	0.00964608  	0.02482512  	0.02063289  
2023-05-22 14:54:19.124: Find a better model.
2023-05-22 14:54:43.598: [iter 12 : loss : 1.1268 = 0.6879 + 0.4389 + 0.0000, time: 24.464523]
2023-05-22 14:54:43.921: epoch 12:	0.01054187  	0.02676777  	0.02280468  
2023-05-22 14:54:43.921: Find a better model.
2023-05-22 14:55:07.912: [iter 13 : loss : 1.1235 = 0.6843 + 0.4392 + 0.0000, time: 23.986161]
2023-05-22 14:55:08.216: epoch 13:	0.01225198  	0.03204272  	0.02753767  
2023-05-22 14:55:08.216: Find a better model.
2023-05-22 14:55:32.091: [iter 14 : loss : 1.1176 = 0.6779 + 0.4396 + 0.0001, time: 23.869590]
2023-05-22 14:55:32.391: epoch 14:	0.01458398  	0.03936345  	0.03383039  
2023-05-22 14:55:32.391: Find a better model.
2023-05-22 14:55:56.173: [iter 15 : loss : 1.1067 = 0.6665 + 0.4401 + 0.0001, time: 23.777541]
2023-05-22 14:55:56.479: epoch 15:	0.01736017  	0.04591134  	0.04018374  
2023-05-22 14:55:56.479: Find a better model.
2023-05-22 14:56:20.414: [iter 16 : loss : 1.0876 = 0.6464 + 0.4410 + 0.0002, time: 23.930896]
2023-05-22 14:56:20.726: epoch 16:	0.02028446  	0.05393717  	0.04682277  
2023-05-22 14:56:20.726: Find a better model.
2023-05-22 14:56:44.517: [iter 17 : loss : 1.0579 = 0.6154 + 0.4422 + 0.0003, time: 23.782990]
2023-05-22 14:56:44.836: epoch 17:	0.02302366  	0.06183406  	0.05290376  
2023-05-22 14:56:44.836: Find a better model.
2023-05-22 14:57:08.680: [iter 18 : loss : 1.0170 = 0.5726 + 0.4439 + 0.0005, time: 23.839946]
2023-05-22 14:57:09.003: epoch 18:	0.02559998  	0.06809916  	0.05783519  
2023-05-22 14:57:09.003: Find a better model.
2023-05-22 14:57:32.834: [iter 19 : loss : 0.9692 = 0.5221 + 0.4463 + 0.0008, time: 23.826213]
2023-05-22 14:57:33.147: epoch 19:	0.02719168  	0.07307357  	0.06177382  
2023-05-22 14:57:33.147: Find a better model.
2023-05-22 14:57:56.914: [iter 20 : loss : 0.9189 = 0.4687 + 0.4492 + 0.0011, time: 23.762581]
2023-05-22 14:57:57.236: epoch 20:	0.02806527  	0.07546478  	0.06387043  
2023-05-22 14:57:57.236: Find a better model.
2023-05-22 14:58:18.704: [iter 21 : loss : 0.8710 = 0.4177 + 0.4519 + 0.0013, time: 21.464626]
2023-05-22 14:58:18.973: epoch 21:	0.02871675  	0.07666350  	0.06467196  
2023-05-22 14:58:18.973: Find a better model.
2023-05-22 14:58:40.538: [iter 22 : loss : 0.8267 = 0.3708 + 0.4542 + 0.0017, time: 21.562637]
2023-05-22 14:58:40.805: epoch 22:	0.02911653  	0.07814925  	0.06527685  
2023-05-22 14:58:40.805: Find a better model.
2023-05-22 14:59:02.067: [iter 23 : loss : 0.7890 = 0.3311 + 0.4559 + 0.0020, time: 21.258319]
2023-05-22 14:59:02.332: epoch 23:	0.02936822  	0.07909160  	0.06569652  
2023-05-22 14:59:02.332: Find a better model.
2023-05-22 14:59:23.687: [iter 24 : loss : 0.7573 = 0.2981 + 0.4570 + 0.0022, time: 21.351802]
2023-05-22 14:59:23.954: epoch 24:	0.02942007  	0.07915738  	0.06568560  
2023-05-22 14:59:23.954: Find a better model.
2023-05-22 14:59:45.264: [iter 25 : loss : 0.7298 = 0.2698 + 0.4575 + 0.0025, time: 21.304118]
2023-05-22 14:59:45.530: epoch 25:	0.02939785  	0.07957648  	0.06593202  
2023-05-22 14:59:45.530: Find a better model.
2023-05-22 15:00:07.006: [iter 26 : loss : 0.7072 = 0.2467 + 0.4577 + 0.0028, time: 21.471826]
2023-05-22 15:00:07.272: epoch 26:	0.02946449  	0.07976484  	0.06595492  
2023-05-22 15:00:07.272: Find a better model.
2023-05-22 15:00:29.000: [iter 27 : loss : 0.6875 = 0.2270 + 0.4575 + 0.0031, time: 21.723910]
2023-05-22 15:00:29.264: epoch 27:	0.02941265  	0.07968285  	0.06576796  
2023-05-22 15:00:50.628: [iter 28 : loss : 0.6709 = 0.2104 + 0.4573 + 0.0033, time: 21.361071]
2023-05-22 15:00:50.895: epoch 28:	0.02951629  	0.08031411  	0.06614769  
2023-05-22 15:00:50.895: Find a better model.
2023-05-22 15:01:12.402: [iter 29 : loss : 0.6564 = 0.1961 + 0.4568 + 0.0036, time: 21.503763]
2023-05-22 15:01:12.682: epoch 29:	0.02959774  	0.08062525  	0.06630480  
2023-05-22 15:01:12.682: Find a better model.
2023-05-22 15:01:34.198: [iter 30 : loss : 0.6432 = 0.1831 + 0.4564 + 0.0038, time: 21.511807]
2023-05-22 15:01:34.462: epoch 30:	0.02951630  	0.08058369  	0.06633899  
2023-05-22 15:01:55.803: [iter 31 : loss : 0.6315 = 0.1716 + 0.4559 + 0.0040, time: 21.337073]
2023-05-22 15:01:56.068: epoch 31:	0.02956073  	0.08054191  	0.06646875  
2023-05-22 15:02:17.648: [iter 32 : loss : 0.6216 = 0.1621 + 0.4553 + 0.0042, time: 21.576110]
2023-05-22 15:02:17.918: epoch 32:	0.02956073  	0.08074913  	0.06661718  
2023-05-22 15:02:17.918: Find a better model.
2023-05-22 15:02:39.561: [iter 33 : loss : 0.6135 = 0.1543 + 0.4548 + 0.0044, time: 21.638355]
2023-05-22 15:02:39.832: epoch 33:	0.02961996  	0.08070305  	0.06669762  
2023-05-22 15:03:01.178: [iter 34 : loss : 0.6050 = 0.1462 + 0.4542 + 0.0046, time: 21.341501]
2023-05-22 15:03:01.443: epoch 34:	0.02963476  	0.08084968  	0.06675065  
2023-05-22 15:03:01.443: Find a better model.
2023-05-22 15:03:22.776: [iter 35 : loss : 0.5968 = 0.1384 + 0.4536 + 0.0048, time: 21.329152]
2023-05-22 15:03:23.040: epoch 35:	0.02964216  	0.08080448  	0.06660666  
2023-05-22 15:03:44.544: [iter 36 : loss : 0.5909 = 0.1327 + 0.4531 + 0.0050, time: 21.499424]
2023-05-22 15:03:44.826: epoch 36:	0.02969399  	0.08079005  	0.06685094  
2023-05-22 15:04:06.147: [iter 37 : loss : 0.5848 = 0.1270 + 0.4526 + 0.0052, time: 21.318150]
2023-05-22 15:04:06.411: epoch 37:	0.02964217  	0.08053253  	0.06678009  
2023-05-22 15:04:27.774: [iter 38 : loss : 0.5790 = 0.1215 + 0.4522 + 0.0054, time: 21.359492]
2023-05-22 15:04:28.044: epoch 38:	0.02965698  	0.08071212  	0.06691422  
2023-05-22 15:04:49.549: [iter 39 : loss : 0.5743 = 0.1170 + 0.4517 + 0.0055, time: 21.500623]
2023-05-22 15:04:49.816: epoch 39:	0.02956073  	0.08044417  	0.06691482  
2023-05-22 15:05:11.326: [iter 40 : loss : 0.5688 = 0.1118 + 0.4513 + 0.0057, time: 21.506876]
2023-05-22 15:05:11.592: epoch 40:	0.02959035  	0.08041205  	0.06698319  
2023-05-22 15:05:33.128: [iter 41 : loss : 0.5642 = 0.1075 + 0.4508 + 0.0059, time: 21.531410]
2023-05-22 15:05:33.393: epoch 41:	0.02944229  	0.08038151  	0.06686859  
2023-05-22 15:05:54.698: [iter 42 : loss : 0.5602 = 0.1036 + 0.4506 + 0.0060, time: 21.300933]
2023-05-22 15:05:54.966: epoch 42:	0.02955333  	0.08024397  	0.06682736  
2023-05-22 15:06:16.557: [iter 43 : loss : 0.5566 = 0.1003 + 0.4501 + 0.0062, time: 21.584309]
2023-05-22 15:06:16.823: epoch 43:	0.02942007  	0.07970858  	0.06660442  
2023-05-22 15:06:38.322: [iter 44 : loss : 0.5541 = 0.0979 + 0.4498 + 0.0063, time: 21.494895]
2023-05-22 15:06:38.584: epoch 44:	0.02936824  	0.07992273  	0.06655027  
2023-05-22 15:07:00.081: [iter 45 : loss : 0.5502 = 0.0943 + 0.4495 + 0.0065, time: 21.493847]
2023-05-22 15:07:00.334: epoch 45:	0.02927200  	0.07958695  	0.06632597  
2023-05-22 15:07:21.695: [iter 46 : loss : 0.5470 = 0.0913 + 0.4491 + 0.0066, time: 21.356749]
2023-05-22 15:07:21.960: epoch 46:	0.02923498  	0.07964639  	0.06633021  
2023-05-22 15:07:43.447: [iter 47 : loss : 0.5440 = 0.0884 + 0.4488 + 0.0068, time: 21.483932]
2023-05-22 15:07:43.712: epoch 47:	0.02916835  	0.07922120  	0.06628081  
2023-05-22 15:08:05.240: [iter 48 : loss : 0.5417 = 0.0862 + 0.4486 + 0.0069, time: 21.524639]
2023-05-22 15:08:05.504: epoch 48:	0.02913876  	0.07925448  	0.06639535  
2023-05-22 15:08:27.184: [iter 49 : loss : 0.5389 = 0.0836 + 0.4482 + 0.0071, time: 21.675882]
2023-05-22 15:08:27.451: epoch 49:	0.02907212  	0.07906567  	0.06623720  
2023-05-22 15:08:50.075: [iter 50 : loss : 0.5368 = 0.0816 + 0.4480 + 0.0072, time: 22.620466]
2023-05-22 15:08:50.347: epoch 50:	0.02910174  	0.07899237  	0.06623748  
2023-05-22 15:09:36.411: my pid: 7780
2023-05-22 15:09:36.411: model: model.general_recommender.SGL
2023-05-22 15:09:36.411: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 15:09:36.412: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 15:09:40.755: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 15:10:04.958: [iter 1 : loss : 1.1318 = 0.6931 + 0.4387 + 0.0000, time: 24.202335]
2023-05-22 15:10:05.254: epoch 1:	0.00188777  	0.00389736  	0.00327477  
2023-05-22 15:10:05.254: Find a better model.
2023-05-22 15:10:28.272: [iter 2 : loss : 1.1305 = 0.6930 + 0.4374 + 0.0000, time: 23.014336]
2023-05-22 15:10:28.576: epoch 2:	0.00252443  	0.00579929  	0.00449588  
2023-05-22 15:10:28.576: Find a better model.
2023-05-22 15:10:51.274: [iter 3 : loss : 1.1303 = 0.6930 + 0.4373 + 0.0000, time: 22.693586]
2023-05-22 15:10:51.584: epoch 3:	0.00313148  	0.00625165  	0.00525474  
2023-05-22 15:10:51.584: Find a better model.
2023-05-22 15:11:14.291: [iter 4 : loss : 1.1301 = 0.6929 + 0.4372 + 0.0000, time: 22.703461]
2023-05-22 15:11:14.602: epoch 4:	0.00365709  	0.00778428  	0.00624991  
2023-05-22 15:11:14.602: Find a better model.
2023-05-22 15:11:40.039: [iter 5 : loss : 1.1300 = 0.6928 + 0.4372 + 0.0000, time: 25.433926]
2023-05-22 15:11:40.391: epoch 5:	0.00430115  	0.00948603  	0.00788905  
2023-05-22 15:11:40.391: Find a better model.
2023-05-22 15:12:05.488: [iter 6 : loss : 1.1299 = 0.6927 + 0.4371 + 0.0000, time: 25.087965]
2023-05-22 15:12:05.823: epoch 6:	0.00497482  	0.01167784  	0.00932330  
2023-05-22 15:12:05.823: Find a better model.
2023-05-22 15:12:30.867: [iter 7 : loss : 1.1297 = 0.6925 + 0.4371 + 0.0000, time: 25.034859]
2023-05-22 15:12:31.240: epoch 7:	0.00572992  	0.01442707  	0.01135103  
2023-05-22 15:12:31.240: Find a better model.
2023-05-22 15:12:56.299: [iter 8 : loss : 1.1294 = 0.6923 + 0.4371 + 0.0000, time: 25.055348]
2023-05-22 15:12:56.635: epoch 8:	0.00678854  	0.01752109  	0.01374396  
2023-05-22 15:12:56.635: Find a better model.
2023-05-22 15:13:21.861: [iter 9 : loss : 1.1291 = 0.6919 + 0.4372 + 0.0000, time: 25.219415]
2023-05-22 15:13:22.230: epoch 9:	0.00779533  	0.02066617  	0.01650730  
2023-05-22 15:13:22.230: Find a better model.
2023-05-22 15:13:47.215: [iter 10 : loss : 1.1286 = 0.6913 + 0.4372 + 0.0000, time: 24.977292]
2023-05-22 15:13:47.591: epoch 10:	0.00859486  	0.02270287  	0.01850111  
2023-05-22 15:13:47.591: Find a better model.
2023-05-22 15:14:12.408: [iter 11 : loss : 1.1277 = 0.6903 + 0.4373 + 0.0000, time: 24.811532]
2023-05-22 15:14:12.739: epoch 11:	0.00866889  	0.02302935  	0.01862565  
2023-05-22 15:14:12.739: Find a better model.
2023-05-22 15:14:37.562: [iter 12 : loss : 1.1259 = 0.6884 + 0.4375 + 0.0000, time: 24.817055]
2023-05-22 15:14:37.926: epoch 12:	0.00880955  	0.02341957  	0.01910925  
2023-05-22 15:14:37.926: Find a better model.
2023-05-22 15:15:02.880: [iter 13 : loss : 1.1226 = 0.6849 + 0.4377 + 0.0000, time: 24.950363]
2023-05-22 15:15:03.193: epoch 13:	0.01008288  	0.02706251  	0.02219324  
2023-05-22 15:15:03.193: Find a better model.
2023-05-22 15:15:27.798: [iter 14 : loss : 1.1168 = 0.6787 + 0.4380 + 0.0001, time: 24.600062]
2023-05-22 15:15:28.159: epoch 14:	0.01208911  	0.03323996  	0.02752641  
2023-05-22 15:15:28.159: Find a better model.
2023-05-22 15:15:53.075: [iter 15 : loss : 1.1062 = 0.6676 + 0.4384 + 0.0001, time: 24.911443]
2023-05-22 15:15:53.410: epoch 15:	0.01430267  	0.03961781  	0.03361575  
2023-05-22 15:15:53.411: Find a better model.
2023-05-22 15:16:18.145: [iter 16 : loss : 1.0876 = 0.6482 + 0.4392 + 0.0002, time: 24.729453]
2023-05-22 15:16:18.487: epoch 16:	0.01732316  	0.04837222  	0.04113755  
2023-05-22 15:16:18.488: Find a better model.
2023-05-22 15:16:43.255: [iter 17 : loss : 1.0588 = 0.6182 + 0.4402 + 0.0003, time: 24.757037]
2023-05-22 15:16:43.627: epoch 17:	0.02045475  	0.05689306  	0.04830632  
2023-05-22 15:16:43.627: Find a better model.
2023-05-22 15:17:08.370: [iter 18 : loss : 1.0191 = 0.5768 + 0.4418 + 0.0005, time: 24.738345]
2023-05-22 15:17:08.706: epoch 18:	0.02344567  	0.06433544  	0.05443977  
2023-05-22 15:17:08.706: Find a better model.
2023-05-22 15:17:33.341: [iter 19 : loss : 0.9726 = 0.5278 + 0.4440 + 0.0007, time: 24.632356]
2023-05-22 15:17:33.701: epoch 19:	0.02560001  	0.06935506  	0.05881392  
2023-05-22 15:17:33.701: Find a better model.
2023-05-22 15:17:58.302: [iter 20 : loss : 0.9234 = 0.4757 + 0.4467 + 0.0010, time: 24.592389]
2023-05-22 15:17:58.664: epoch 20:	0.02726575  	0.07416621  	0.06197450  
2023-05-22 15:17:58.664: Find a better model.
2023-05-22 15:18:23.237: [iter 21 : loss : 0.8758 = 0.4251 + 0.4494 + 0.0013, time: 24.570705]
2023-05-22 15:18:23.586: epoch 21:	0.02799127  	0.07634234  	0.06351940  
2023-05-22 15:18:23.586: Find a better model.
2023-05-22 15:18:48.102: [iter 22 : loss : 0.8313 = 0.3779 + 0.4517 + 0.0016, time: 24.511536]
2023-05-22 15:18:48.422: epoch 22:	0.02833182  	0.07725018  	0.06431477  
2023-05-22 15:18:48.422: Find a better model.
2023-05-22 15:19:12.842: [iter 23 : loss : 0.7936 = 0.3381 + 0.4536 + 0.0019, time: 24.417529]
2023-05-22 15:19:13.190: epoch 23:	0.02859092  	0.07788972  	0.06468749  
2023-05-22 15:19:13.190: Find a better model.
2023-05-22 15:19:37.816: [iter 24 : loss : 0.7614 = 0.3043 + 0.4549 + 0.0022, time: 24.616611]
2023-05-22 15:19:38.170: epoch 24:	0.02876859  	0.07853377  	0.06506658  
2023-05-22 15:19:38.170: Find a better model.
2023-05-22 15:20:02.541: [iter 25 : loss : 0.7333 = 0.2753 + 0.4555 + 0.0025, time: 24.365159]
2023-05-22 15:20:02.887: epoch 25:	0.02891666  	0.07879657  	0.06531503  
2023-05-22 15:20:02.887: Find a better model.
2023-05-22 15:20:27.443: [iter 26 : loss : 0.7102 = 0.2516 + 0.4559 + 0.0028, time: 24.552719]
2023-05-22 15:20:27.757: epoch 26:	0.02891667  	0.07898447  	0.06535684  
2023-05-22 15:20:27.757: Find a better model.
2023-05-22 15:20:52.621: [iter 27 : loss : 0.6903 = 0.2314 + 0.4559 + 0.0030, time: 24.860610]
2023-05-22 15:20:52.977: epoch 27:	0.02890185  	0.07945637  	0.06562430  
2023-05-22 15:20:52.977: Find a better model.
2023-05-22 15:21:15.548: [iter 28 : loss : 0.6731 = 0.2142 + 0.4557 + 0.0033, time: 22.568451]
2023-05-22 15:21:15.825: epoch 28:	0.02901290  	0.07953522  	0.06594058  
2023-05-22 15:21:15.825: Find a better model.
2023-05-22 15:21:38.282: [iter 29 : loss : 0.6587 = 0.1999 + 0.4552 + 0.0035, time: 22.452467]
2023-05-22 15:21:38.571: epoch 29:	0.02900550  	0.07925057  	0.06579160  
2023-05-22 15:22:00.854: [iter 30 : loss : 0.6449 = 0.1863 + 0.4548 + 0.0037, time: 22.277557]
2023-05-22 15:22:01.131: epoch 30:	0.02895368  	0.07895745  	0.06587932  
2023-05-22 15:22:23.471: [iter 31 : loss : 0.6334 = 0.1750 + 0.4544 + 0.0040, time: 22.335151]
2023-05-22 15:22:23.749: epoch 31:	0.02913875  	0.07969034  	0.06615040  
2023-05-22 15:22:23.749: Find a better model.
2023-05-22 15:22:46.077: [iter 32 : loss : 0.6230 = 0.1650 + 0.4538 + 0.0042, time: 22.324282]
2023-05-22 15:22:46.354: epoch 32:	0.02916837  	0.07980399  	0.06620229  
2023-05-22 15:22:46.354: Find a better model.
2023-05-22 15:23:08.791: [iter 33 : loss : 0.6145 = 0.1568 + 0.4533 + 0.0044, time: 22.433408]
2023-05-22 15:23:09.069: epoch 33:	0.02924982  	0.08009871  	0.06641152  
2023-05-22 15:23:09.069: Find a better model.
2023-05-22 15:23:31.397: [iter 34 : loss : 0.6062 = 0.1488 + 0.4528 + 0.0046, time: 22.325343]
2023-05-22 15:23:31.686: epoch 34:	0.02926462  	0.08049019  	0.06664120  
2023-05-22 15:23:31.686: Find a better model.
2023-05-22 15:23:54.006: [iter 35 : loss : 0.5978 = 0.1407 + 0.4522 + 0.0048, time: 22.316555]
2023-05-22 15:23:54.283: epoch 35:	0.02911656  	0.07996555  	0.06641694  
2023-05-22 15:24:16.780: [iter 36 : loss : 0.5916 = 0.1349 + 0.4517 + 0.0050, time: 22.493321]
2023-05-22 15:24:17.058: epoch 36:	0.02919058  	0.08046391  	0.06661049  
2023-05-22 15:24:39.396: [iter 37 : loss : 0.5857 = 0.1293 + 0.4513 + 0.0052, time: 22.334334]
2023-05-22 15:24:39.682: epoch 37:	0.02907953  	0.08007354  	0.06652858  
2023-05-22 15:25:02.163: [iter 38 : loss : 0.5797 = 0.1236 + 0.4508 + 0.0053, time: 22.471769]
2023-05-22 15:25:02.443: epoch 38:	0.02916837  	0.08020320  	0.06652336  
2023-05-22 15:25:25.335: [iter 39 : loss : 0.5747 = 0.1189 + 0.4503 + 0.0055, time: 22.887663]
2023-05-22 15:25:25.673: epoch 39:	0.02900551  	0.07958436  	0.06640207  
2023-05-22 15:25:50.373: [iter 40 : loss : 0.5695 = 0.1140 + 0.4499 + 0.0057, time: 24.693418]
2023-05-22 15:25:50.644: epoch 40:	0.02899070  	0.07974034  	0.06640115  
2023-05-22 15:26:13.615: [iter 41 : loss : 0.5648 = 0.1095 + 0.4495 + 0.0058, time: 22.965302]
2023-05-22 15:26:13.915: epoch 41:	0.02890185  	0.07933852  	0.06633721  
2023-05-22 15:26:37.553: [iter 42 : loss : 0.5606 = 0.1055 + 0.4492 + 0.0060, time: 23.634859]
2023-05-22 15:26:37.855: epoch 42:	0.02891666  	0.07907403  	0.06612671  
2023-05-22 15:27:00.847: [iter 43 : loss : 0.5570 = 0.1021 + 0.4488 + 0.0062, time: 22.988517]
2023-05-22 15:27:01.123: epoch 43:	0.02887224  	0.07901154  	0.06611782  
2023-05-22 15:27:23.736: [iter 44 : loss : 0.5544 = 0.0997 + 0.4484 + 0.0063, time: 22.608575]
2023-05-22 15:27:24.016: epoch 44:	0.02882783  	0.07901505  	0.06603274  
2023-05-22 15:27:46.529: [iter 45 : loss : 0.5502 = 0.0957 + 0.4480 + 0.0065, time: 22.509973]
2023-05-22 15:27:46.817: epoch 45:	0.02885003  	0.07888606  	0.06593822  
2023-05-22 15:28:09.309: [iter 46 : loss : 0.5471 = 0.0927 + 0.4477 + 0.0066, time: 22.489049]
2023-05-22 15:28:09.594: epoch 46:	0.02885744  	0.07870115  	0.06587975  
2023-05-22 15:28:31.922: [iter 47 : loss : 0.5441 = 0.0900 + 0.4474 + 0.0068, time: 22.323059]
2023-05-22 15:28:32.202: epoch 47:	0.02875379  	0.07824498  	0.06564398  
2023-05-22 15:28:54.701: [iter 48 : loss : 0.5416 = 0.0876 + 0.4471 + 0.0069, time: 22.494348]
2023-05-22 15:28:54.988: epoch 48:	0.02863534  	0.07818443  	0.06545903  
2023-05-22 15:29:17.511: [iter 49 : loss : 0.5387 = 0.0848 + 0.4468 + 0.0071, time: 22.519414]
2023-05-22 15:29:17.790: epoch 49:	0.02870197  	0.07843634  	0.06535960  
2023-05-22 15:29:40.279: [iter 50 : loss : 0.5366 = 0.0829 + 0.4465 + 0.0072, time: 22.485596]
2023-05-22 15:29:40.564: epoch 50:	0.02870197  	0.07817931  	0.06518734  
2023-05-22 15:30:03.088: [iter 51 : loss : 0.5342 = 0.0806 + 0.4463 + 0.0073, time: 22.518744]
2023-05-22 15:30:03.373: epoch 51:	0.02852430  	0.07783946  	0.06515929  
2023-05-22 15:30:25.699: [iter 52 : loss : 0.5316 = 0.0781 + 0.4460 + 0.0074, time: 22.321453]
2023-05-22 15:30:25.990: epoch 52:	0.02862054  	0.07791799  	0.06520843  
2023-05-22 15:30:48.452: [iter 53 : loss : 0.5306 = 0.0772 + 0.4459 + 0.0076, time: 22.458472]
2023-05-22 15:30:48.735: epoch 53:	0.02853169  	0.07745916  	0.06497233  
2023-05-22 15:31:11.050: [iter 54 : loss : 0.5283 = 0.0750 + 0.4456 + 0.0077, time: 22.311740]
2023-05-22 15:31:11.339: epoch 54:	0.02856130  	0.07733291  	0.06494993  
2023-05-22 15:31:33.831: [iter 55 : loss : 0.5263 = 0.0731 + 0.4454 + 0.0078, time: 22.487181]
2023-05-22 15:31:34.122: epoch 55:	0.02844285  	0.07696877  	0.06472799  
2023-05-22 15:31:56.457: [iter 56 : loss : 0.5242 = 0.0711 + 0.4451 + 0.0080, time: 22.330106]
2023-05-22 15:31:56.741: epoch 56:	0.02838362  	0.07665987  	0.06465780  
2023-05-22 15:32:19.039: [iter 57 : loss : 0.5225 = 0.0695 + 0.4449 + 0.0081, time: 22.292190]
2023-05-22 15:32:19.325: epoch 57:	0.02839843  	0.07651380  	0.06446529  
2023-05-22 15:32:41.819: [iter 58 : loss : 0.5212 = 0.0682 + 0.4448 + 0.0082, time: 22.490484]
2023-05-22 15:32:42.098: epoch 58:	0.02836141  	0.07639197  	0.06431403  
2023-05-22 15:33:04.259: [iter 59 : loss : 0.5195 = 0.0666 + 0.4446 + 0.0083, time: 22.156299]
2023-05-22 15:33:04.546: epoch 59:	0.02837622  	0.07624308  	0.06427572  
2023-05-22 15:33:04.546: Early stopping is trigger at epoch: 59
2023-05-22 15:33:04.546: best_result@epoch 34:

2023-05-22 15:33:04.546: 		0.0293      	0.0805      	0.0666      
2023-05-22 15:46:06.096: my pid: 13872
2023-05-22 15:46:06.096: model: model.general_recommender.SGL
2023-05-22 15:46:06.096: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 15:46:06.096: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 15:46:10.567: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 15:46:33.196: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 22.628565]
2023-05-22 15:46:33.532: epoch 1:	0.00130293  	0.00277180  	0.00231672  
2023-05-22 15:46:33.532: Find a better model.
2023-05-22 15:46:55.953: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 22.417367]
2023-05-22 15:46:56.309: epoch 2:	0.00205064  	0.00366879  	0.00313908  
2023-05-22 15:46:56.309: Find a better model.
2023-05-22 15:47:18.732: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 22.418038]
2023-05-22 15:47:19.098: epoch 3:	0.00221350  	0.00442601  	0.00354527  
2023-05-22 15:47:19.098: Find a better model.
2023-05-22 15:47:41.499: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 22.392461]
2023-05-22 15:47:41.875: epoch 4:	0.00267989  	0.00532399  	0.00431591  
2023-05-22 15:47:41.875: Find a better model.
2023-05-22 15:48:04.366: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 22.483590]
2023-05-22 15:48:04.724: epoch 5:	0.00349423  	0.00767634  	0.00589975  
2023-05-22 15:48:04.724: Find a better model.
2023-05-22 15:48:27.170: [iter 6 : loss : 1.1348 = 0.6925 + 0.4423 + 0.0000, time: 22.437823]
2023-05-22 15:48:27.527: epoch 6:	0.00405686  	0.00944557  	0.00761603  
2023-05-22 15:48:27.527: Find a better model.
2023-05-22 15:48:49.883: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 22.347403]
2023-05-22 15:48:50.241: epoch 7:	0.00459727  	0.01138364  	0.00953355  
2023-05-22 15:48:50.241: Find a better model.
2023-05-22 15:49:12.640: [iter 8 : loss : 1.1349 = 0.6918 + 0.4430 + 0.0000, time: 22.390407]
2023-05-22 15:49:12.995: epoch 8:	0.00535977  	0.01306709  	0.01089284  
2023-05-22 15:49:12.995: Find a better model.
2023-05-22 15:49:35.373: [iter 9 : loss : 1.1347 = 0.6913 + 0.4435 + 0.0000, time: 22.373971]
2023-05-22 15:49:35.723: epoch 9:	0.00626293  	0.01612316  	0.01323573  
2023-05-22 15:49:35.723: Find a better model.
2023-05-22 15:49:58.065: [iter 10 : loss : 1.1343 = 0.6903 + 0.4440 + 0.0000, time: 22.336756]
2023-05-22 15:49:58.423: epoch 10:	0.00684777  	0.01760513  	0.01482205  
2023-05-22 15:49:58.423: Find a better model.
2023-05-22 15:50:20.639: [iter 11 : loss : 1.1335 = 0.6888 + 0.4446 + 0.0000, time: 22.210632]
2023-05-22 15:50:20.989: epoch 11:	0.00835797  	0.02187231  	0.01818328  
2023-05-22 15:50:20.989: Find a better model.
2023-05-22 15:50:44.058: [iter 12 : loss : 1.1319 = 0.6867 + 0.4452 + 0.0000, time: 23.064698]
2023-05-22 15:50:44.415: epoch 12:	0.01019393  	0.02776113  	0.02329843  
2023-05-22 15:50:44.415: Find a better model.
2023-05-22 15:51:06.674: [iter 13 : loss : 1.1295 = 0.6835 + 0.4460 + 0.0001, time: 22.252687]
2023-05-22 15:51:06.975: epoch 13:	0.01322921  	0.03538750  	0.03054959  
2023-05-22 15:51:06.975: Find a better model.
2023-05-22 15:51:29.403: [iter 14 : loss : 1.1248 = 0.6780 + 0.4467 + 0.0001, time: 22.423077]
2023-05-22 15:51:29.712: epoch 14:	0.01670870  	0.04449056  	0.03784291  
2023-05-22 15:51:29.712: Find a better model.
2023-05-22 15:51:51.950: [iter 15 : loss : 1.1165 = 0.6687 + 0.4477 + 0.0001, time: 22.231200]
2023-05-22 15:51:52.292: epoch 15:	0.01998830  	0.05257311  	0.04526634  
2023-05-22 15:51:52.292: Find a better model.
2023-05-22 15:52:14.238: [iter 16 : loss : 1.1017 = 0.6527 + 0.4488 + 0.0002, time: 21.940335]
2023-05-22 15:52:14.577: epoch 16:	0.02293482  	0.06104353  	0.05212171  
2023-05-22 15:52:14.577: Find a better model.
2023-05-22 15:52:36.892: [iter 17 : loss : 1.0781 = 0.6275 + 0.4502 + 0.0003, time: 22.307944]
2023-05-22 15:52:37.236: epoch 17:	0.02552595  	0.06784860  	0.05811784  
2023-05-22 15:52:37.236: Find a better model.
2023-05-22 15:52:59.498: [iter 18 : loss : 1.0434 = 0.5907 + 0.4522 + 0.0005, time: 22.257145]
2023-05-22 15:52:59.809: epoch 18:	0.02743599  	0.07356296  	0.06229251  
2023-05-22 15:52:59.809: Find a better model.
2023-05-22 15:53:21.890: [iter 19 : loss : 0.9997 = 0.5441 + 0.4549 + 0.0007, time: 22.075005]
2023-05-22 15:53:22.213: epoch 19:	0.02847245  	0.07629310  	0.06457634  
2023-05-22 15:53:22.213: Find a better model.
2023-05-22 15:53:44.227: [iter 20 : loss : 0.9505 = 0.4915 + 0.4581 + 0.0009, time: 22.007609]
2023-05-22 15:53:44.564: epoch 20:	0.02918318  	0.07895642  	0.06587827  
2023-05-22 15:53:44.564: Find a better model.
2023-05-22 15:54:04.672: [iter 21 : loss : 0.9010 = 0.4385 + 0.4613 + 0.0012, time: 20.104853]
2023-05-22 15:54:04.953: epoch 21:	0.02956074  	0.08041882  	0.06672377  
2023-05-22 15:54:04.953: Find a better model.
2023-05-22 15:54:24.912: [iter 22 : loss : 0.8543 = 0.3888 + 0.4639 + 0.0015, time: 19.954741]
2023-05-22 15:54:25.188: epoch 22:	0.02970141  	0.08107162  	0.06670196  
2023-05-22 15:54:25.188: Find a better model.
2023-05-22 15:54:45.133: [iter 23 : loss : 0.8138 = 0.3462 + 0.4659 + 0.0018, time: 19.940436]
2023-05-22 15:54:45.409: epoch 23:	0.02980505  	0.08191403  	0.06696107  
2023-05-22 15:54:45.409: Find a better model.
2023-05-22 15:55:05.309: [iter 24 : loss : 0.7794 = 0.3103 + 0.4670 + 0.0021, time: 19.896754]
2023-05-22 15:55:05.582: epoch 24:	0.02990130  	0.08197689  	0.06720100  
2023-05-22 15:55:05.582: Find a better model.
2023-05-22 15:55:25.527: [iter 25 : loss : 0.7500 = 0.2801 + 0.4675 + 0.0024, time: 19.942047]
2023-05-22 15:55:25.800: epoch 25:	0.02994572  	0.08240176  	0.06735520  
2023-05-22 15:55:25.800: Find a better model.
2023-05-22 15:55:45.688: [iter 26 : loss : 0.7254 = 0.2552 + 0.4675 + 0.0027, time: 19.884014]
2023-05-22 15:55:45.964: epoch 26:	0.02989389  	0.08182578  	0.06725158  
2023-05-22 15:56:05.897: [iter 27 : loss : 0.7042 = 0.2339 + 0.4673 + 0.0029, time: 19.930276]
2023-05-22 15:56:06.164: epoch 27:	0.03012339  	0.08245464  	0.06753966  
2023-05-22 15:56:06.165: Find a better model.
2023-05-22 15:56:26.055: [iter 28 : loss : 0.6862 = 0.2160 + 0.4670 + 0.0032, time: 19.886597]
2023-05-22 15:56:26.320: epoch 28:	0.03012339  	0.08264744  	0.06774893  
2023-05-22 15:56:26.320: Find a better model.
2023-05-22 15:56:46.290: [iter 29 : loss : 0.6707 = 0.2010 + 0.4663 + 0.0034, time: 19.966280]
2023-05-22 15:56:46.556: epoch 29:	0.03019001  	0.08279388  	0.06780546  
2023-05-22 15:56:46.557: Find a better model.
2023-05-22 15:57:06.443: [iter 30 : loss : 0.6567 = 0.1873 + 0.4657 + 0.0037, time: 19.881464]
2023-05-22 15:57:06.711: epoch 30:	0.03020482  	0.08305984  	0.06793499  
2023-05-22 15:57:06.711: Find a better model.
2023-05-22 15:57:26.656: [iter 31 : loss : 0.6446 = 0.1754 + 0.4653 + 0.0039, time: 19.941637]
2023-05-22 15:57:26.921: epoch 31:	0.03039730  	0.08374599  	0.06824954  
2023-05-22 15:57:26.921: Find a better model.
2023-05-22 15:57:46.661: [iter 32 : loss : 0.6337 = 0.1651 + 0.4644 + 0.0041, time: 19.734936]
2023-05-22 15:57:46.924: epoch 32:	0.03045653  	0.08391989  	0.06839484  
2023-05-22 15:57:46.924: Find a better model.
2023-05-22 15:58:07.037: [iter 33 : loss : 0.6248 = 0.1567 + 0.4638 + 0.0043, time: 20.109472]
2023-05-22 15:58:07.306: epoch 33:	0.03047873  	0.08389813  	0.06853081  
2023-05-22 15:58:27.419: [iter 34 : loss : 0.6163 = 0.1485 + 0.4632 + 0.0045, time: 20.109856]
2023-05-22 15:58:27.683: epoch 34:	0.03059718  	0.08417238  	0.06874981  
2023-05-22 15:58:27.683: Find a better model.
2023-05-22 15:58:47.646: [iter 35 : loss : 0.6079 = 0.1406 + 0.4626 + 0.0047, time: 19.959126]
2023-05-22 15:58:47.912: epoch 35:	0.03052316  	0.08402669  	0.06876399  
2023-05-22 15:59:08.016: [iter 36 : loss : 0.6010 = 0.1341 + 0.4619 + 0.0049, time: 20.099727]
2023-05-22 15:59:08.284: epoch 36:	0.03052316  	0.08374418  	0.06874751  
2023-05-22 15:59:28.405: [iter 37 : loss : 0.5952 = 0.1285 + 0.4615 + 0.0051, time: 20.117734]
2023-05-22 15:59:28.671: epoch 37:	0.03052315  	0.08386928  	0.06893733  
2023-05-22 15:59:48.798: [iter 38 : loss : 0.5890 = 0.1228 + 0.4610 + 0.0053, time: 20.122096]
2023-05-22 15:59:49.062: epoch 38:	0.03043432  	0.08359607  	0.06887534  
2023-05-22 16:00:09.184: [iter 39 : loss : 0.5842 = 0.1184 + 0.4604 + 0.0054, time: 20.117978]
2023-05-22 16:00:09.449: epoch 39:	0.03038990  	0.08363398  	0.06880907  
2023-05-22 16:00:29.579: [iter 40 : loss : 0.5788 = 0.1133 + 0.4600 + 0.0056, time: 20.126740]
2023-05-22 16:00:29.844: epoch 40:	0.03030105  	0.08322509  	0.06870311  
2023-05-22 16:00:49.801: [iter 41 : loss : 0.5738 = 0.1084 + 0.4596 + 0.0058, time: 19.953180]
2023-05-22 16:00:50.063: epoch 41:	0.03024923  	0.08286888  	0.06854866  
2023-05-22 16:01:10.039: [iter 42 : loss : 0.5700 = 0.1049 + 0.4592 + 0.0059, time: 19.972147]
2023-05-22 16:01:10.330: epoch 42:	0.03023443  	0.08257917  	0.06854347  
2023-05-22 16:01:30.238: [iter 43 : loss : 0.5659 = 0.1010 + 0.4589 + 0.0061, time: 19.904068]
2023-05-22 16:01:30.530: epoch 43:	0.03024924  	0.08298346  	0.06866955  
2023-05-22 16:01:51.321: [iter 44 : loss : 0.5630 = 0.0983 + 0.4584 + 0.0062, time: 20.785980]
2023-05-22 16:01:51.631: epoch 44:	0.03030107  	0.08305402  	0.06868023  
2023-05-22 16:02:12.665: [iter 45 : loss : 0.5591 = 0.0947 + 0.4580 + 0.0064, time: 21.030128]
2023-05-22 16:02:12.914: epoch 45:	0.03015300  	0.08289133  	0.06851686  
2023-05-22 16:02:34.115: [iter 46 : loss : 0.5557 = 0.0915 + 0.4577 + 0.0065, time: 21.196082]
2023-05-22 16:02:34.403: epoch 46:	0.03016781  	0.08296115  	0.06855902  
2023-05-22 16:03:16.533: my pid: 14028
2023-05-22 16:03:16.533: model: model.general_recommender.SGL
2023-05-22 16:03:16.533: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 16:03:16.533: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 16:03:20.553: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 16:03:40.891: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.337750]
2023-05-22 16:03:41.161: epoch 1:	0.00137696  	0.00301423  	0.00236023  
2023-05-22 16:03:41.161: Find a better model.
2023-05-22 16:04:01.750: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.586258]
2023-05-22 16:04:02.038: epoch 2:	0.00197661  	0.00354131  	0.00305402  
2023-05-22 16:04:02.038: Find a better model.
2023-05-22 16:04:22.533: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 20.492037]
2023-05-22 16:04:22.831: epoch 3:	0.00224312  	0.00446284  	0.00374909  
2023-05-22 16:04:22.831: Find a better model.
2023-05-22 16:04:43.498: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.661594]
2023-05-22 16:04:43.803: epoch 4:	0.00285757  	0.00639969  	0.00511009  
2023-05-22 16:04:43.803: Find a better model.
2023-05-22 16:05:04.309: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 20.500769]
2023-05-22 16:05:04.600: epoch 5:	0.00324252  	0.00748883  	0.00579560  
2023-05-22 16:05:04.600: Find a better model.
2023-05-22 16:05:24.872: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 20.269179]
2023-05-22 16:05:25.166: epoch 6:	0.00368671  	0.00844335  	0.00694149  
2023-05-22 16:05:25.166: Find a better model.
2023-05-22 16:05:45.697: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.526537]
2023-05-22 16:05:46.000: epoch 7:	0.00448623  	0.01100327  	0.00861267  
2023-05-22 16:05:46.000: Find a better model.
2023-05-22 16:06:06.670: [iter 8 : loss : 1.1349 = 0.6919 + 0.4430 + 0.0000, time: 20.667296]
2023-05-22 16:06:06.974: epoch 8:	0.00551524  	0.01270826  	0.01067173  
2023-05-22 16:06:06.974: Find a better model.
2023-05-22 16:06:27.627: [iter 9 : loss : 1.1348 = 0.6913 + 0.4435 + 0.0000, time: 20.648746]
2023-05-22 16:06:27.925: epoch 9:	0.00624072  	0.01603412  	0.01309043  
2023-05-22 16:06:27.925: Find a better model.
2023-05-22 16:06:48.464: [iter 10 : loss : 1.1344 = 0.6905 + 0.4439 + 0.0000, time: 20.531194]
2023-05-22 16:06:48.750: epoch 10:	0.00732896  	0.01880916  	0.01538176  
2023-05-22 16:06:48.750: Find a better model.
2023-05-22 16:07:09.267: [iter 11 : loss : 1.1337 = 0.6891 + 0.4446 + 0.0000, time: 20.513999]
2023-05-22 16:07:09.554: epoch 11:	0.00865409  	0.02194888  	0.01794120  
2023-05-22 16:07:09.554: Find a better model.
2023-05-22 16:07:30.034: [iter 12 : loss : 1.1322 = 0.6869 + 0.4452 + 0.0000, time: 20.475512]
2023-05-22 16:07:30.323: epoch 12:	0.01043823  	0.02754639  	0.02238457  
2023-05-22 16:07:30.323: Find a better model.
2023-05-22 16:07:50.820: [iter 13 : loss : 1.1297 = 0.6838 + 0.4459 + 0.0001, time: 20.492626]
2023-05-22 16:07:51.110: epoch 13:	0.01281462  	0.03382917  	0.02848286  
2023-05-22 16:07:51.111: Find a better model.
2023-05-22 16:08:11.439: [iter 14 : loss : 1.1251 = 0.6784 + 0.4467 + 0.0001, time: 20.323225]
2023-05-22 16:08:11.718: epoch 14:	0.01599799  	0.04254531  	0.03652286  
2023-05-22 16:08:11.718: Find a better model.
2023-05-22 16:08:32.044: [iter 15 : loss : 1.1170 = 0.6691 + 0.4477 + 0.0001, time: 20.320329]
2023-05-22 16:08:32.323: epoch 15:	0.01890743  	0.05048135  	0.04329449  
2023-05-22 16:08:32.323: Find a better model.
2023-05-22 16:08:52.864: [iter 16 : loss : 1.1024 = 0.6535 + 0.4487 + 0.0002, time: 20.536284]
2023-05-22 16:08:53.151: epoch 16:	0.02223889  	0.05947533  	0.05061310  
2023-05-22 16:08:53.151: Find a better model.
2023-05-22 16:09:13.581: [iter 17 : loss : 1.0790 = 0.6285 + 0.4502 + 0.0003, time: 20.425205]
2023-05-22 16:09:13.860: epoch 17:	0.02460793  	0.06572041  	0.05602561  
2023-05-22 16:09:13.860: Find a better model.
2023-05-22 16:09:34.019: [iter 18 : loss : 1.0446 = 0.5919 + 0.4522 + 0.0005, time: 20.152912]
2023-05-22 16:09:34.299: epoch 18:	0.02634031  	0.07065939  	0.05965244  
2023-05-22 16:09:34.299: Find a better model.
2023-05-22 16:09:54.806: [iter 19 : loss : 1.0010 = 0.5455 + 0.4548 + 0.0007, time: 20.503097]
2023-05-22 16:09:55.082: epoch 19:	0.02768029  	0.07503517  	0.06286710  
2023-05-22 16:09:55.082: Find a better model.
2023-05-22 16:10:15.377: [iter 20 : loss : 0.9520 = 0.4931 + 0.4580 + 0.0009, time: 20.289808]
2023-05-22 16:10:15.655: epoch 20:	0.02835398  	0.07649673  	0.06427418  
2023-05-22 16:10:15.655: Find a better model.
2023-05-22 16:10:35.944: [iter 21 : loss : 0.9026 = 0.4401 + 0.4613 + 0.0012, time: 20.285238]
2023-05-22 16:10:36.226: epoch 21:	0.02879078  	0.07720533  	0.06508635  
2023-05-22 16:10:36.226: Find a better model.
2023-05-22 16:10:56.564: [iter 22 : loss : 0.8554 = 0.3900 + 0.4639 + 0.0015, time: 20.332591]
2023-05-22 16:10:56.838: epoch 22:	0.02918316  	0.07885058  	0.06571324  
2023-05-22 16:10:56.838: Find a better model.
2023-05-22 16:11:17.201: [iter 23 : loss : 0.8152 = 0.3474 + 0.4660 + 0.0018, time: 20.359523]
2023-05-22 16:11:17.479: epoch 23:	0.02922018  	0.07894418  	0.06570858  
2023-05-22 16:11:17.479: Find a better model.
2023-05-22 16:11:37.958: [iter 24 : loss : 0.7804 = 0.3112 + 0.4672 + 0.0021, time: 20.473164]
2023-05-22 16:11:38.231: epoch 24:	0.02944225  	0.07987260  	0.06627097  
2023-05-22 16:11:38.231: Find a better model.
2023-05-22 16:11:58.718: [iter 25 : loss : 0.7509 = 0.2809 + 0.4676 + 0.0024, time: 20.479077]
2023-05-22 16:11:58.992: epoch 25:	0.02956810  	0.08062947  	0.06689826  
2023-05-22 16:11:58.992: Find a better model.
2023-05-22 16:12:19.340: [iter 26 : loss : 0.7260 = 0.2558 + 0.4676 + 0.0027, time: 20.343595]
2023-05-22 16:12:19.608: epoch 26:	0.02962733  	0.08080803  	0.06707399  
2023-05-22 16:12:19.608: Find a better model.
2023-05-22 16:12:40.129: [iter 27 : loss : 0.7048 = 0.2344 + 0.4675 + 0.0029, time: 20.516907]
2023-05-22 16:12:40.408: epoch 27:	0.02978281  	0.08134393  	0.06734281  
2023-05-22 16:12:40.409: Find a better model.
2023-05-22 16:13:00.921: [iter 28 : loss : 0.6869 = 0.2166 + 0.4671 + 0.0032, time: 20.507468]
2023-05-22 16:13:01.195: epoch 28:	0.02983463  	0.08165020  	0.06749308  
2023-05-22 16:13:01.195: Find a better model.
2023-05-22 16:13:21.704: [iter 29 : loss : 0.6712 = 0.2014 + 0.4664 + 0.0034, time: 20.506228]
2023-05-22 16:13:21.976: epoch 29:	0.02988645  	0.08187908  	0.06760748  
2023-05-22 16:13:21.976: Find a better model.
2023-05-22 16:13:42.321: [iter 30 : loss : 0.6572 = 0.1876 + 0.4659 + 0.0037, time: 20.341412]
2023-05-22 16:13:42.597: epoch 30:	0.02983464  	0.08218723  	0.06777339  
2023-05-22 16:13:42.597: Find a better model.
2023-05-22 16:14:02.937: [iter 31 : loss : 0.6449 = 0.1758 + 0.4652 + 0.0039, time: 20.335384]
2023-05-22 16:14:03.215: epoch 31:	0.03006413  	0.08285562  	0.06805926  
2023-05-22 16:14:03.216: Find a better model.
2023-05-22 16:14:23.687: [iter 32 : loss : 0.6344 = 0.1658 + 0.4646 + 0.0041, time: 20.467581]
2023-05-22 16:14:23.964: epoch 32:	0.03001232  	0.08244715  	0.06807327  
2023-05-22 16:14:44.300: [iter 33 : loss : 0.6251 = 0.1569 + 0.4639 + 0.0043, time: 20.331578]
2023-05-22 16:14:44.574: epoch 33:	0.03005673  	0.08247221  	0.06823426  
2023-05-22 16:15:05.079: [iter 34 : loss : 0.6166 = 0.1489 + 0.4632 + 0.0045, time: 20.500943]
2023-05-22 16:15:05.353: epoch 34:	0.02990126  	0.08207866  	0.06794271  
2023-05-22 16:15:25.714: [iter 35 : loss : 0.6081 = 0.1407 + 0.4626 + 0.0047, time: 20.357572]
2023-05-22 16:15:25.991: epoch 35:	0.02984944  	0.08271563  	0.06796698  
2023-05-22 16:15:46.320: [iter 36 : loss : 0.6014 = 0.1344 + 0.4621 + 0.0049, time: 20.324149]
2023-05-22 16:15:46.596: epoch 36:	0.02984943  	0.08213128  	0.06788152  
2023-05-22 16:16:07.247: [iter 37 : loss : 0.5954 = 0.1289 + 0.4615 + 0.0051, time: 20.647189]
2023-05-22 16:16:07.529: epoch 37:	0.02979021  	0.08217961  	0.06782932  
2023-05-22 16:16:28.038: [iter 38 : loss : 0.5890 = 0.1228 + 0.4610 + 0.0052, time: 20.504466]
2023-05-22 16:16:28.303: epoch 38:	0.02990126  	0.08220918  	0.06792514  
2023-05-22 16:16:48.849: [iter 39 : loss : 0.5842 = 0.1183 + 0.4605 + 0.0054, time: 20.541646]
2023-05-22 16:16:49.116: epoch 39:	0.02986424  	0.08240319  	0.06808502  
2023-05-22 16:17:09.420: [iter 40 : loss : 0.5791 = 0.1134 + 0.4601 + 0.0056, time: 20.299123]
2023-05-22 16:17:09.691: epoch 40:	0.02988644  	0.08255811  	0.06817401  
2023-05-22 16:17:30.223: [iter 41 : loss : 0.5741 = 0.1088 + 0.4595 + 0.0057, time: 20.527311]
2023-05-22 16:17:30.492: epoch 41:	0.02975320  	0.08208539  	0.06780038  
2023-05-22 16:17:51.009: [iter 42 : loss : 0.5699 = 0.1049 + 0.4591 + 0.0059, time: 20.511708]
2023-05-22 16:17:51.285: epoch 42:	0.02973098  	0.08219262  	0.06774654  
2023-05-22 16:18:11.624: [iter 43 : loss : 0.5659 = 0.1010 + 0.4588 + 0.0061, time: 20.333852]
2023-05-22 16:18:11.895: epoch 43:	0.02967915  	0.08192988  	0.06775387  
2023-05-22 16:18:32.421: [iter 44 : loss : 0.5635 = 0.0988 + 0.4585 + 0.0062, time: 20.522485]
2023-05-22 16:18:32.696: epoch 44:	0.02959771  	0.08170468  	0.06767540  
2023-05-22 16:18:52.986: [iter 45 : loss : 0.5593 = 0.0948 + 0.4582 + 0.0064, time: 20.286307]
2023-05-22 16:18:53.255: epoch 45:	0.02953849  	0.08150589  	0.06751136  
2023-05-22 16:19:13.401: [iter 46 : loss : 0.5563 = 0.0921 + 0.4577 + 0.0065, time: 20.142400]
2023-05-22 16:19:13.684: epoch 46:	0.02955330  	0.08156957  	0.06753807  
2023-05-22 16:19:34.195: [iter 47 : loss : 0.5533 = 0.0891 + 0.4575 + 0.0067, time: 20.507321]
2023-05-22 16:19:34.468: epoch 47:	0.02965695  	0.08154669  	0.06742983  
2023-05-22 16:19:54.798: [iter 48 : loss : 0.5505 = 0.0866 + 0.4571 + 0.0068, time: 20.325474]
2023-05-22 16:19:55.069: epoch 48:	0.02972358  	0.08179981  	0.06746485  
2023-05-22 16:20:15.562: [iter 49 : loss : 0.5479 = 0.0842 + 0.4568 + 0.0070, time: 20.489882]
2023-05-22 16:20:15.842: epoch 49:	0.02958292  	0.08152084  	0.06730794  
2023-05-22 16:20:36.344: [iter 50 : loss : 0.5455 = 0.0820 + 0.4565 + 0.0071, time: 20.497946]
2023-05-22 16:20:36.615: epoch 50:	0.02950888  	0.08132208  	0.06708059  
2023-05-22 16:20:57.166: [iter 51 : loss : 0.5428 = 0.0794 + 0.4562 + 0.0072, time: 20.546030]
2023-05-22 16:20:57.435: epoch 51:	0.02950148  	0.08128449  	0.06714873  
2023-05-22 16:21:17.745: [iter 52 : loss : 0.5406 = 0.0773 + 0.4560 + 0.0073, time: 20.304342]
2023-05-22 16:21:18.017: epoch 52:	0.02961994  	0.08106843  	0.06702244  
2023-05-22 16:21:38.335: [iter 53 : loss : 0.5394 = 0.0761 + 0.4558 + 0.0075, time: 20.313760]
2023-05-22 16:21:38.606: epoch 53:	0.02945706  	0.08064003  	0.06668438  
2023-05-22 16:21:59.117: [iter 54 : loss : 0.5372 = 0.0741 + 0.4555 + 0.0076, time: 20.507709]
2023-05-22 16:21:59.390: epoch 54:	0.02944966  	0.08038160  	0.06648835  
2023-05-22 16:22:19.755: [iter 55 : loss : 0.5352 = 0.0722 + 0.4553 + 0.0077, time: 20.362664]
2023-05-22 16:22:20.029: epoch 55:	0.02939043  	0.08015540  	0.06628921  
2023-05-22 16:22:40.334: [iter 56 : loss : 0.5330 = 0.0701 + 0.4550 + 0.0078, time: 20.301194]
2023-05-22 16:22:40.605: epoch 56:	0.02941264  	0.07969644  	0.06603276  
2023-05-22 16:22:40.605: Early stopping is trigger at epoch: 56
2023-05-22 16:22:40.605: best_result@epoch 31:

2023-05-22 16:22:40.605: 		0.0301      	0.0829      	0.0681      
2023-05-22 16:24:03.798: my pid: 13416
2023-05-22 16:24:03.798: model: model.general_recommender.SGL
2023-05-22 16:24:03.799: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 16:24:03.799: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 16:24:08.030: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 16:24:29.162: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.131581]
2023-05-22 16:24:29.462: epoch 1:	0.00142138  	0.00276464  	0.00218209  
2023-05-22 16:24:29.462: Find a better model.
2023-05-22 16:24:50.777: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.310898]
2023-05-22 16:24:51.078: epoch 2:	0.00183595  	0.00360720  	0.00309096  
2023-05-22 16:24:51.078: Find a better model.
2023-05-22 16:25:12.379: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.298530]
2023-05-22 16:25:12.669: epoch 3:	0.00225792  	0.00516067  	0.00383786  
2023-05-22 16:25:12.669: Find a better model.
2023-05-22 16:25:33.811: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.139070]
2023-05-22 16:25:34.098: epoch 4:	0.00270210  	0.00574676  	0.00471295  
2023-05-22 16:25:34.099: Find a better model.
2023-05-22 16:25:55.543: [iter 5 : loss : 1.1345 = 0.6927 + 0.4419 + 0.0000, time: 21.440214]
2023-05-22 16:25:55.818: epoch 5:	0.00321291  	0.00725179  	0.00574933  
2023-05-22 16:25:55.818: Find a better model.
2023-05-22 16:26:17.292: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 21.469916]
2023-05-22 16:26:17.590: epoch 6:	0.00376814  	0.00807451  	0.00673138  
2023-05-22 16:26:17.590: Find a better model.
2023-05-22 16:26:38.917: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.323758]
2023-05-22 16:26:39.223: epoch 7:	0.00464909  	0.01124657  	0.00851223  
2023-05-22 16:26:39.223: Find a better model.
2023-05-22 16:26:59.655: [iter 8 : loss : 1.1349 = 0.6919 + 0.4430 + 0.0000, time: 20.426375]
2023-05-22 16:26:59.918: epoch 8:	0.00561147  	0.01358140  	0.01071314  
2023-05-22 16:26:59.918: Find a better model.
2023-05-22 16:27:21.326: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 21.404690]
2023-05-22 16:27:21.618: epoch 9:	0.00638138  	0.01670131  	0.01316789  
2023-05-22 16:27:21.618: Find a better model.
2023-05-22 16:27:42.543: [iter 10 : loss : 1.1345 = 0.6905 + 0.4439 + 0.0000, time: 20.921194]
2023-05-22 16:27:42.822: epoch 10:	0.00714389  	0.01885274  	0.01559706  
2023-05-22 16:27:42.822: Find a better model.
2023-05-22 16:28:03.767: [iter 11 : loss : 1.1336 = 0.6891 + 0.4445 + 0.0000, time: 20.941485]
2023-05-22 16:28:04.043: epoch 11:	0.00880214  	0.02299463  	0.01887926  
2023-05-22 16:28:04.043: Find a better model.
2023-05-22 16:28:24.863: [iter 12 : loss : 1.1321 = 0.6869 + 0.4452 + 0.0000, time: 20.815584]
2023-05-22 16:28:25.157: epoch 12:	0.01057889  	0.02770710  	0.02333697  
2023-05-22 16:28:25.157: Find a better model.
2023-05-22 16:28:45.844: [iter 13 : loss : 1.1297 = 0.6837 + 0.4459 + 0.0001, time: 20.682098]
2023-05-22 16:28:46.120: epoch 13:	0.01322179  	0.03533394  	0.02956555  
2023-05-22 16:28:46.120: Find a better model.
2023-05-22 16:29:06.643: [iter 14 : loss : 1.1251 = 0.6783 + 0.4467 + 0.0001, time: 20.520094]
2023-05-22 16:29:06.921: epoch 14:	0.01625709  	0.04265630  	0.03640723  
2023-05-22 16:29:06.921: Find a better model.
2023-05-22 16:29:27.868: [iter 15 : loss : 1.1168 = 0.6691 + 0.4475 + 0.0001, time: 20.941362]
2023-05-22 16:29:28.172: epoch 15:	0.01945527  	0.05135659  	0.04412464  
2023-05-22 16:29:28.172: Find a better model.
2023-05-22 16:29:52.037: [iter 16 : loss : 1.1021 = 0.6533 + 0.4486 + 0.0002, time: 23.855896]
2023-05-22 16:29:52.424: epoch 16:	0.02259427  	0.05909778  	0.05121559  
2023-05-22 16:29:52.424: Find a better model.
2023-05-22 16:30:15.078: [iter 17 : loss : 1.0787 = 0.6283 + 0.4502 + 0.0003, time: 22.650966]
2023-05-22 16:30:15.445: epoch 17:	0.02524463  	0.06712124  	0.05714067  
2023-05-22 16:30:15.445: Find a better model.
2023-05-22 16:30:38.162: [iter 18 : loss : 1.0441 = 0.5915 + 0.4522 + 0.0005, time: 22.708520]
2023-05-22 16:30:38.533: epoch 18:	0.02710285  	0.07245103  	0.06098174  
2023-05-22 16:30:38.533: Find a better model.
2023-05-22 16:31:01.562: [iter 19 : loss : 1.0004 = 0.5451 + 0.4547 + 0.0007, time: 23.025186]
2023-05-22 16:31:01.994: epoch 19:	0.02828736  	0.07529689  	0.06335083  
2023-05-22 16:31:01.994: Find a better model.
2023-05-22 16:31:25.495: [iter 20 : loss : 0.9512 = 0.4924 + 0.4579 + 0.0009, time: 23.497042]
2023-05-22 16:31:25.874: epoch 20:	0.02912392  	0.07766033  	0.06466311  
2023-05-22 16:31:25.874: Find a better model.
2023-05-22 16:31:49.107: [iter 21 : loss : 0.9017 = 0.4395 + 0.4610 + 0.0012, time: 23.223575]
2023-05-22 16:31:49.485: epoch 21:	0.02956812  	0.07923654  	0.06539713  
2023-05-22 16:31:49.485: Find a better model.
2023-05-22 16:32:12.908: [iter 22 : loss : 0.8549 = 0.3897 + 0.4636 + 0.0015, time: 23.418413]
2023-05-22 16:32:13.291: epoch 22:	0.02979764  	0.08050040  	0.06579652  
2023-05-22 16:32:13.291: Find a better model.
2023-05-22 16:32:36.017: [iter 23 : loss : 0.8143 = 0.3468 + 0.4657 + 0.0018, time: 22.723030]
2023-05-22 16:32:36.305: epoch 23:	0.02987169  	0.08094540  	0.06601110  
2023-05-22 16:32:36.306: Find a better model.
2023-05-22 16:32:59.042: [iter 24 : loss : 0.7797 = 0.3107 + 0.4669 + 0.0021, time: 22.729161]
2023-05-22 16:32:59.416: epoch 24:	0.02996053  	0.08145354  	0.06619605  
2023-05-22 16:32:59.416: Find a better model.
2023-05-22 16:33:22.449: [iter 25 : loss : 0.7498 = 0.2801 + 0.4673 + 0.0024, time: 23.027627]
2023-05-22 16:33:22.818: epoch 25:	0.03007157  	0.08240908  	0.06651581  
2023-05-22 16:33:22.818: Find a better model.
2023-05-22 16:33:46.142: [iter 26 : loss : 0.7253 = 0.2553 + 0.4673 + 0.0027, time: 23.320565]
2023-05-22 16:33:46.499: epoch 26:	0.03021223  	0.08301324  	0.06687830  
2023-05-22 16:33:46.499: Find a better model.
2023-05-22 16:34:09.977: [iter 27 : loss : 0.7042 = 0.2341 + 0.4672 + 0.0029, time: 23.468068]
2023-05-22 16:34:10.344: epoch 27:	0.03021222  	0.08329113  	0.06700034  
2023-05-22 16:34:10.344: Find a better model.
2023-05-22 16:34:33.264: [iter 28 : loss : 0.6861 = 0.2160 + 0.4669 + 0.0032, time: 22.916260]
2023-05-22 16:34:33.537: epoch 28:	0.03024925  	0.08384720  	0.06735460  
2023-05-22 16:34:33.537: Find a better model.
2023-05-22 16:34:56.356: [iter 29 : loss : 0.6706 = 0.2009 + 0.4663 + 0.0034, time: 22.812568]
2023-05-22 16:34:56.733: epoch 29:	0.03020483  	0.08387402  	0.06733965  
2023-05-22 16:34:56.733: Find a better model.
2023-05-22 16:35:19.386: [iter 30 : loss : 0.6564 = 0.1871 + 0.4656 + 0.0037, time: 22.643811]
2023-05-22 16:35:19.759: epoch 30:	0.03012340  	0.08358609  	0.06727618  
2023-05-22 16:35:43.027: [iter 31 : loss : 0.6441 = 0.1752 + 0.4650 + 0.0039, time: 23.264213]
2023-05-22 16:35:43.388: epoch 31:	0.03021224  	0.08362802  	0.06741031  
2023-05-22 16:36:06.517: [iter 32 : loss : 0.6340 = 0.1655 + 0.4644 + 0.0041, time: 23.122231]
2023-05-22 16:36:06.896: epoch 32:	0.03007899  	0.08327627  	0.06745369  
2023-05-22 16:36:30.237: [iter 33 : loss : 0.6247 = 0.1566 + 0.4638 + 0.0043, time: 23.335068]
2023-05-22 16:36:30.578: epoch 33:	0.03025665  	0.08371938  	0.06767098  
2023-05-22 16:36:53.303: [iter 34 : loss : 0.6164 = 0.1487 + 0.4632 + 0.0045, time: 22.719794]
2023-05-22 16:36:53.686: epoch 34:	0.03030108  	0.08382906  	0.06786621  
2023-05-22 16:37:16.374: [iter 35 : loss : 0.6078 = 0.1406 + 0.4626 + 0.0047, time: 22.677369]
2023-05-22 16:37:16.754: epoch 35:	0.03028626  	0.08390794  	0.06761495  
2023-05-22 16:37:16.754: Find a better model.
2023-05-22 16:37:39.618: [iter 36 : loss : 0.6012 = 0.1343 + 0.4620 + 0.0049, time: 22.859222]
2023-05-22 16:37:39.979: epoch 36:	0.03033069  	0.08396618  	0.06773032  
2023-05-22 16:37:39.979: Find a better model.
2023-05-22 16:38:03.925: [iter 37 : loss : 0.5948 = 0.1283 + 0.4614 + 0.0051, time: 23.941950]
2023-05-22 16:38:04.353: epoch 37:	0.03023444  	0.08367155  	0.06759788  
2023-05-22 16:38:27.962: [iter 38 : loss : 0.5889 = 0.1228 + 0.4609 + 0.0053, time: 23.604556]
2023-05-22 16:38:28.334: epoch 38:	0.03038990  	0.08398403  	0.06768300  
2023-05-22 16:38:28.334: Find a better model.
2023-05-22 16:38:51.213: [iter 39 : loss : 0.5840 = 0.1181 + 0.4605 + 0.0054, time: 22.875318]
2023-05-22 16:38:51.489: epoch 39:	0.03026404  	0.08371828  	0.06770679  
2023-05-22 16:39:14.211: [iter 40 : loss : 0.5786 = 0.1131 + 0.4600 + 0.0056, time: 22.718255]
2023-05-22 16:39:14.570: epoch 40:	0.03021963  	0.08343370  	0.06762083  
2023-05-22 16:39:37.225: [iter 41 : loss : 0.5738 = 0.1085 + 0.4596 + 0.0058, time: 22.645406]
2023-05-22 16:39:37.588: epoch 41:	0.03019742  	0.08313577  	0.06767531  
2023-05-22 16:40:00.299: [iter 42 : loss : 0.5698 = 0.1047 + 0.4592 + 0.0059, time: 22.706351]
2023-05-22 16:40:00.668: epoch 42:	0.02998274  	0.08266462  	0.06747611  
2023-05-22 16:40:20.950: [iter 43 : loss : 0.5658 = 0.1009 + 0.4588 + 0.0061, time: 20.263260]
2023-05-22 16:40:21.211: epoch 43:	0.02999755  	0.08264674  	0.06727777  
2023-05-22 16:40:41.343: [iter 44 : loss : 0.5629 = 0.0983 + 0.4584 + 0.0062, time: 20.128111]
2023-05-22 16:40:41.610: epoch 44:	0.03001234  	0.08268866  	0.06736071  
2023-05-22 16:41:01.539: [iter 45 : loss : 0.5594 = 0.0950 + 0.4581 + 0.0064, time: 19.923589]
2023-05-22 16:41:01.822: epoch 45:	0.02993091  	0.08227683  	0.06717113  
2023-05-22 16:41:21.911: [iter 46 : loss : 0.5560 = 0.0917 + 0.4577 + 0.0065, time: 20.085529]
2023-05-22 16:41:22.172: epoch 46:	0.02984947  	0.08207203  	0.06699202  
2023-05-22 16:41:42.307: [iter 47 : loss : 0.5533 = 0.0891 + 0.4575 + 0.0067, time: 20.130255]
2023-05-22 16:41:42.573: epoch 47:	0.02970141  	0.08153360  	0.06667410  
2023-05-22 16:42:02.513: [iter 48 : loss : 0.5503 = 0.0864 + 0.4571 + 0.0068, time: 19.934464]
2023-05-22 16:42:02.778: epoch 48:	0.02965699  	0.08094677  	0.06661762  
2023-05-22 16:42:22.730: [iter 49 : loss : 0.5479 = 0.0841 + 0.4568 + 0.0070, time: 19.947003]
2023-05-22 16:42:22.999: epoch 49:	0.02960516  	0.08071069  	0.06662454  
2023-05-22 16:42:42.888: [iter 50 : loss : 0.5454 = 0.0818 + 0.4565 + 0.0071, time: 19.884601]
2023-05-22 16:42:43.151: epoch 50:	0.02942007  	0.08043979  	0.06638499  
2023-05-22 16:43:03.272: [iter 51 : loss : 0.5432 = 0.0796 + 0.4563 + 0.0072, time: 20.116785]
2023-05-22 16:43:03.533: epoch 51:	0.02928681  	0.07988807  	0.06613886  
2023-05-22 16:43:23.678: [iter 52 : loss : 0.5403 = 0.0770 + 0.4559 + 0.0073, time: 20.141185]
2023-05-22 16:43:23.953: epoch 52:	0.02939786  	0.08022094  	0.06632245  
2023-05-22 16:43:43.877: [iter 53 : loss : 0.5395 = 0.0763 + 0.4557 + 0.0075, time: 19.919637]
2023-05-22 16:43:44.144: epoch 53:	0.02929422  	0.08009246  	0.06619501  
2023-05-22 16:44:04.267: [iter 54 : loss : 0.5372 = 0.0741 + 0.4555 + 0.0076, time: 20.118192]
2023-05-22 16:44:04.530: epoch 54:	0.02925720  	0.08002038  	0.06616309  
2023-05-22 16:44:24.656: [iter 55 : loss : 0.5355 = 0.0724 + 0.4553 + 0.0077, time: 20.122645]
2023-05-22 16:44:24.919: epoch 55:	0.02905731  	0.07920007  	0.06575920  
2023-05-22 16:44:44.851: [iter 56 : loss : 0.5332 = 0.0703 + 0.4551 + 0.0078, time: 19.927548]
2023-05-22 16:44:45.125: epoch 56:	0.02895366  	0.07862800  	0.06561870  
2023-05-22 16:45:05.038: [iter 57 : loss : 0.5315 = 0.0686 + 0.4549 + 0.0080, time: 19.909836]
2023-05-22 16:45:05.304: epoch 57:	0.02893886  	0.07854490  	0.06552820  
2023-05-22 16:45:25.429: [iter 58 : loss : 0.5301 = 0.0673 + 0.4548 + 0.0081, time: 20.119789]
2023-05-22 16:45:25.691: epoch 58:	0.02899808  	0.07847856  	0.06538339  
2023-05-22 16:45:45.647: [iter 59 : loss : 0.5283 = 0.0656 + 0.4546 + 0.0082, time: 19.952269]
2023-05-22 16:45:45.909: epoch 59:	0.02896846  	0.07843056  	0.06536433  
2023-05-22 16:46:05.826: [iter 60 : loss : 0.5273 = 0.0645 + 0.4545 + 0.0083, time: 19.913565]
2023-05-22 16:46:06.087: epoch 60:	0.02899067  	0.07789501  	0.06526714  
2023-05-22 16:46:26.024: [iter 61 : loss : 0.5261 = 0.0634 + 0.4543 + 0.0084, time: 19.932696]
2023-05-22 16:46:26.313: epoch 61:	0.02887224  	0.07795211  	0.06503805  
2023-05-22 16:46:46.198: [iter 62 : loss : 0.5249 = 0.0622 + 0.4542 + 0.0085, time: 19.881493]
2023-05-22 16:46:46.462: epoch 62:	0.02867975  	0.07730702  	0.06477579  
2023-05-22 16:47:06.752: [iter 63 : loss : 0.5236 = 0.0610 + 0.4540 + 0.0086, time: 20.287371]
2023-05-22 16:47:07.007: epoch 63:	0.02876859  	0.07752743  	0.06464224  
2023-05-22 16:47:07.007: Early stopping is trigger at epoch: 63
2023-05-22 16:47:07.007: best_result@epoch 38:

2023-05-22 16:47:07.007: 		0.0304      	0.0840      	0.0677      
2023-05-22 16:47:35.196: my pid: 12188
2023-05-22 16:47:35.196: model: model.general_recommender.SGL
2023-05-22 16:47:35.196: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 16:47:35.196: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 16:47:39.343: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 16:48:00.680: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.336648]
2023-05-22 16:48:00.962: epoch 1:	0.00119189  	0.00244229  	0.00197850  
2023-05-22 16:48:00.963: Find a better model.
2023-05-22 16:48:22.338: [iter 2 : loss : 1.1340 = 0.6930 + 0.4409 + 0.0000, time: 21.370584]
2023-05-22 16:48:22.635: epoch 2:	0.00153243  	0.00298823  	0.00274494  
2023-05-22 16:48:22.635: Find a better model.
2023-05-22 16:48:42.935: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 20.295429]
2023-05-22 16:48:43.223: epoch 3:	0.00182855  	0.00413398  	0.00312796  
2023-05-22 16:48:43.223: Find a better model.
2023-05-22 16:49:04.101: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.874074]
2023-05-22 16:49:04.390: epoch 4:	0.00223571  	0.00465868  	0.00377948  
2023-05-22 16:49:04.390: Find a better model.
2023-05-22 16:49:25.268: [iter 5 : loss : 1.1345 = 0.6927 + 0.4419 + 0.0000, time: 20.875087]
2023-05-22 16:49:25.564: epoch 5:	0.00293900  	0.00684103  	0.00531614  
2023-05-22 16:49:25.565: Find a better model.
2023-05-22 16:49:46.440: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.868799]
2023-05-22 16:49:46.733: epoch 6:	0.00350903  	0.00844058  	0.00678737  
2023-05-22 16:49:46.733: Find a better model.
2023-05-22 16:50:07.429: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 20.692584]
2023-05-22 16:50:07.718: epoch 7:	0.00438258  	0.01133210  	0.00862435  
2023-05-22 16:50:07.718: Find a better model.
2023-05-22 16:50:28.265: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 20.542119]
2023-05-22 16:50:28.542: epoch 8:	0.00513028  	0.01397752  	0.01074306  
2023-05-22 16:50:28.543: Find a better model.
2023-05-22 16:50:49.389: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 20.840237]
2023-05-22 16:50:49.676: epoch 9:	0.00639619  	0.01724472  	0.01378333  
2023-05-22 16:50:49.676: Find a better model.
2023-05-22 16:51:10.396: [iter 10 : loss : 1.1345 = 0.6906 + 0.4438 + 0.0000, time: 20.715991]
2023-05-22 16:51:10.684: epoch 10:	0.00717350  	0.01909034  	0.01572043  
2023-05-22 16:51:10.685: Find a better model.
2023-05-22 16:51:31.244: [iter 11 : loss : 1.1338 = 0.6894 + 0.4444 + 0.0000, time: 20.555637]
2023-05-22 16:51:31.547: epoch 11:	0.00789158  	0.02127771  	0.01783234  
2023-05-22 16:51:31.548: Find a better model.
2023-05-22 16:51:52.210: [iter 12 : loss : 1.1324 = 0.6873 + 0.4451 + 0.0000, time: 20.652037]
2023-05-22 16:51:52.487: epoch 12:	0.00971271  	0.02578145  	0.02238022  
2023-05-22 16:51:52.487: Find a better model.
2023-05-22 16:52:12.997: [iter 13 : loss : 1.1301 = 0.6843 + 0.4458 + 0.0001, time: 20.506248]
2023-05-22 16:52:13.270: epoch 13:	0.01193364  	0.03171735  	0.02798208  
2023-05-22 16:52:13.270: Find a better model.
2023-05-22 16:52:33.768: [iter 14 : loss : 1.1258 = 0.6793 + 0.4465 + 0.0001, time: 20.494067]
2023-05-22 16:52:34.042: epoch 14:	0.01518364  	0.03997502  	0.03532809  
2023-05-22 16:52:34.042: Find a better model.
2023-05-22 16:52:54.361: [iter 15 : loss : 1.1184 = 0.6708 + 0.4475 + 0.0001, time: 20.315027]
2023-05-22 16:52:54.646: epoch 15:	0.01833737  	0.04814797  	0.04228805  
2023-05-22 16:52:54.646: Find a better model.
2023-05-22 16:53:15.173: [iter 16 : loss : 1.1048 = 0.6561 + 0.4485 + 0.0002, time: 20.523396]
2023-05-22 16:53:15.444: epoch 16:	0.02149116  	0.05645892  	0.04924353  
2023-05-22 16:53:15.444: Find a better model.
2023-05-22 16:53:35.946: [iter 17 : loss : 1.0830 = 0.6327 + 0.4499 + 0.0003, time: 20.497766]
2023-05-22 16:53:36.219: epoch 17:	0.02404527  	0.06355753  	0.05483885  
2023-05-22 16:53:36.219: Find a better model.
2023-05-22 16:53:56.543: [iter 18 : loss : 1.0500 = 0.5977 + 0.4519 + 0.0004, time: 20.320233]
2023-05-22 16:53:56.814: epoch 18:	0.02617742  	0.06992521  	0.05892001  
2023-05-22 16:53:56.814: Find a better model.
2023-05-22 16:54:17.169: [iter 19 : loss : 1.0076 = 0.5526 + 0.4544 + 0.0006, time: 20.351969]
2023-05-22 16:54:17.442: epoch 19:	0.02756923  	0.07406253  	0.06207763  
2023-05-22 16:54:17.442: Find a better model.
2023-05-22 16:54:37.554: [iter 20 : loss : 0.9590 = 0.5005 + 0.4576 + 0.0009, time: 20.107331]
2023-05-22 16:54:37.830: epoch 20:	0.02854648  	0.07658795  	0.06405208  
2023-05-22 16:54:37.830: Find a better model.
2023-05-22 16:54:58.106: [iter 21 : loss : 0.9092 = 0.4473 + 0.4608 + 0.0012, time: 20.271575]
2023-05-22 16:54:58.372: epoch 21:	0.02900550  	0.07824694  	0.06460551  
2023-05-22 16:54:58.372: Find a better model.
2023-05-22 16:55:18.707: [iter 22 : loss : 0.8615 = 0.3965 + 0.4635 + 0.0015, time: 20.330777]
2023-05-22 16:55:18.974: epoch 22:	0.02939047  	0.07987264  	0.06530986  
2023-05-22 16:55:18.974: Find a better model.
2023-05-22 16:55:39.308: [iter 23 : loss : 0.8201 = 0.3529 + 0.4655 + 0.0018, time: 20.329463]
2023-05-22 16:55:39.576: epoch 23:	0.02946451  	0.08059206  	0.06558864  
2023-05-22 16:55:39.576: Find a better model.
2023-05-22 16:55:59.922: [iter 24 : loss : 0.7847 = 0.3158 + 0.4669 + 0.0021, time: 20.338540]
2023-05-22 16:56:00.188: epoch 24:	0.02969401  	0.08137459  	0.06587014  
2023-05-22 16:56:00.189: Find a better model.
2023-05-22 16:56:20.691: [iter 25 : loss : 0.7546 = 0.2847 + 0.4676 + 0.0024, time: 20.498571]
2023-05-22 16:56:20.953: epoch 25:	0.02984947  	0.08161385  	0.06619477  
2023-05-22 16:56:20.953: Find a better model.
2023-05-22 16:56:41.284: [iter 26 : loss : 0.7291 = 0.2587 + 0.4677 + 0.0026, time: 20.327570]
2023-05-22 16:56:41.548: epoch 26:	0.02993091  	0.08210851  	0.06644499  
2023-05-22 16:56:41.548: Find a better model.
2023-05-22 16:57:02.084: [iter 27 : loss : 0.7074 = 0.2370 + 0.4675 + 0.0029, time: 20.532066]
2023-05-22 16:57:02.345: epoch 27:	0.02988649  	0.08230193  	0.06644973  
2023-05-22 16:57:02.346: Find a better model.
2023-05-22 16:57:22.685: [iter 28 : loss : 0.6894 = 0.2191 + 0.4671 + 0.0032, time: 20.334799]
2023-05-22 16:57:22.953: epoch 28:	0.02995312  	0.08227000  	0.06649805  
2023-05-22 16:57:44.265: [iter 29 : loss : 0.6730 = 0.2031 + 0.4665 + 0.0034, time: 21.308582]
2023-05-22 16:57:44.627: epoch 29:	0.02990130  	0.08228952  	0.06655955  
2023-05-22 16:58:05.463: [iter 30 : loss : 0.6588 = 0.1892 + 0.4659 + 0.0036, time: 20.831303]
2023-05-22 16:58:05.772: epoch 30:	0.02984948  	0.08220751  	0.06655795  
2023-05-22 16:58:26.505: [iter 31 : loss : 0.6465 = 0.1773 + 0.4654 + 0.0039, time: 20.728173]
2023-05-22 16:58:26.789: epoch 31:	0.02979025  	0.08174885  	0.06670747  
2023-05-22 16:58:47.793: [iter 32 : loss : 0.6355 = 0.1669 + 0.4646 + 0.0041, time: 21.000726]
2023-05-22 16:58:48.085: epoch 32:	0.02988649  	0.08203548  	0.06695685  
2023-05-22 16:59:11.216: [iter 33 : loss : 0.6265 = 0.1582 + 0.4641 + 0.0043, time: 23.120169]
2023-05-22 16:59:11.606: epoch 33:	0.02996792  	0.08201784  	0.06718589  
2023-05-22 16:59:35.290: [iter 34 : loss : 0.6178 = 0.1500 + 0.4634 + 0.0045, time: 23.676114]
2023-05-22 16:59:35.686: epoch 34:	0.03011598  	0.08257426  	0.06748012  
2023-05-22 16:59:35.686: Find a better model.
2023-05-22 16:59:59.507: [iter 35 : loss : 0.6091 = 0.1417 + 0.4628 + 0.0047, time: 23.813063]
2023-05-22 16:59:59.902: epoch 35:	0.03001975  	0.08228792  	0.06730980  
2023-05-22 17:00:22.979: [iter 36 : loss : 0.6025 = 0.1354 + 0.4622 + 0.0049, time: 23.073147]
2023-05-22 17:00:23.381: epoch 36:	0.02985686  	0.08175641  	0.06699344  
2023-05-22 17:00:46.522: [iter 37 : loss : 0.5962 = 0.1295 + 0.4617 + 0.0050, time: 23.136854]
2023-05-22 17:00:46.900: epoch 37:	0.02996050  	0.08151187  	0.06688681  
2023-05-22 17:01:10.103: [iter 38 : loss : 0.5902 = 0.1237 + 0.4612 + 0.0052, time: 23.199062]
2023-05-22 17:01:10.493: epoch 38:	0.02993830  	0.08162756  	0.06709102  
2023-05-22 17:01:34.037: [iter 39 : loss : 0.5850 = 0.1190 + 0.4607 + 0.0054, time: 23.535213]
2023-05-22 17:01:34.416: epoch 39:	0.02994571  	0.08177527  	0.06718539  
2023-05-22 17:01:58.137: [iter 40 : loss : 0.5799 = 0.1142 + 0.4602 + 0.0056, time: 23.713805]
2023-05-22 17:01:58.524: epoch 40:	0.02990868  	0.08119853  	0.06696399  
2023-05-22 17:02:22.149: [iter 41 : loss : 0.5747 = 0.1093 + 0.4597 + 0.0057, time: 23.621421]
2023-05-22 17:02:22.454: epoch 41:	0.03001233  	0.08092683  	0.06689688  
2023-05-22 17:02:45.437: [iter 42 : loss : 0.5705 = 0.1053 + 0.4593 + 0.0059, time: 22.977501]
2023-05-22 17:02:45.824: epoch 42:	0.02996790  	0.08095569  	0.06683932  
2023-05-22 17:03:08.814: [iter 43 : loss : 0.5666 = 0.1016 + 0.4589 + 0.0060, time: 22.982370]
2023-05-22 17:03:09.182: epoch 43:	0.02991608  	0.08080567  	0.06683388  
2023-05-22 17:03:32.983: [iter 44 : loss : 0.5636 = 0.0989 + 0.4585 + 0.0062, time: 23.797847]
2023-05-22 17:03:33.379: epoch 44:	0.02992348  	0.08047128  	0.06676339  
2023-05-22 17:03:56.902: [iter 45 : loss : 0.5599 = 0.0954 + 0.4582 + 0.0063, time: 23.518551]
2023-05-22 17:03:57.302: epoch 45:	0.02989387  	0.08037296  	0.06664790  
2023-05-22 17:04:21.056: [iter 46 : loss : 0.5569 = 0.0925 + 0.4579 + 0.0065, time: 23.745417]
2023-05-22 17:04:21.448: epoch 46:	0.02974581  	0.07991068  	0.06642748  
2023-05-22 17:04:44.554: [iter 47 : loss : 0.5536 = 0.0894 + 0.4576 + 0.0066, time: 23.096771]
2023-05-22 17:04:44.962: epoch 47:	0.02969397  	0.07952905  	0.06627519  
2023-05-22 17:05:07.796: [iter 48 : loss : 0.5506 = 0.0866 + 0.4572 + 0.0068, time: 22.828508]
2023-05-22 17:05:08.193: epoch 48:	0.02956071  	0.07941993  	0.06617942  
2023-05-22 17:05:31.094: [iter 49 : loss : 0.5481 = 0.0843 + 0.4568 + 0.0069, time: 22.894826]
2023-05-22 17:05:31.495: epoch 49:	0.02952370  	0.07913010  	0.06602297  
2023-05-22 17:05:55.073: [iter 50 : loss : 0.5460 = 0.0824 + 0.4566 + 0.0071, time: 23.569286]
2023-05-22 17:05:55.478: epoch 50:	0.02950150  	0.07926997  	0.06596548  
2023-05-22 17:06:18.871: [iter 51 : loss : 0.5435 = 0.0800 + 0.4564 + 0.0072, time: 23.386090]
2023-05-22 17:06:19.273: epoch 51:	0.02947188  	0.07904354  	0.06587692  
2023-05-22 17:06:42.958: [iter 52 : loss : 0.5408 = 0.0774 + 0.4561 + 0.0073, time: 23.674676]
2023-05-22 17:06:43.358: epoch 52:	0.02944966  	0.07877196  	0.06583628  
2023-05-22 17:07:07.062: [iter 53 : loss : 0.5399 = 0.0765 + 0.4559 + 0.0074, time: 23.699158]
2023-05-22 17:07:07.440: epoch 53:	0.02948668  	0.07887014  	0.06575814  
2023-05-22 17:07:30.455: [iter 54 : loss : 0.5376 = 0.0744 + 0.4556 + 0.0076, time: 23.011352]
2023-05-22 17:07:30.834: epoch 54:	0.02939044  	0.07869924  	0.06572768  
2023-05-22 17:07:53.696: [iter 55 : loss : 0.5354 = 0.0723 + 0.4554 + 0.0077, time: 22.857086]
2023-05-22 17:07:54.089: epoch 55:	0.02929419  	0.07828365  	0.06560668  
2023-05-22 17:08:16.977: [iter 56 : loss : 0.5334 = 0.0704 + 0.4552 + 0.0078, time: 22.878443]
2023-05-22 17:08:17.374: epoch 56:	0.02919056  	0.07782826  	0.06537951  
2023-05-22 17:08:40.304: [iter 57 : loss : 0.5316 = 0.0686 + 0.4550 + 0.0079, time: 22.925838]
2023-05-22 17:08:40.701: epoch 57:	0.02919056  	0.07769379  	0.06560460  
2023-05-22 17:09:04.331: [iter 58 : loss : 0.5303 = 0.0672 + 0.4550 + 0.0081, time: 23.624043]
2023-05-22 17:09:04.728: epoch 58:	0.02919796  	0.07725330  	0.06541606  
2023-05-22 17:09:28.299: [iter 59 : loss : 0.5286 = 0.0658 + 0.4546 + 0.0082, time: 23.566042]
2023-05-22 17:09:28.696: epoch 59:	0.02903509  	0.07709257  	0.06508727  
2023-05-22 17:09:28.696: Early stopping is trigger at epoch: 59
2023-05-22 17:09:28.696: best_result@epoch 34:

2023-05-22 17:09:28.696: 		0.0301      	0.0826      	0.0675      
2023-05-22 17:19:30.823: my pid: 2536
2023-05-22 17:19:30.823: model: model.general_recommender.SGL
2023-05-22 17:19:30.823: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 17:19:30.823: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 17:19:34.926: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 17:19:55.811: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.883819]
2023-05-22 17:19:56.076: epoch 1:	0.00114747  	0.00261888  	0.00203733  
2023-05-22 17:19:56.076: Find a better model.
2023-05-22 17:20:17.010: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.931271]
2023-05-22 17:20:17.296: epoch 2:	0.00157684  	0.00291813  	0.00254354  
2023-05-22 17:20:17.296: Find a better model.
2023-05-22 17:20:38.204: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 20.904188]
2023-05-22 17:20:38.508: epoch 3:	0.00191738  	0.00428468  	0.00332997  
2023-05-22 17:20:38.508: Find a better model.
2023-05-22 17:20:59.409: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.897338]
2023-05-22 17:20:59.703: epoch 4:	0.00225052  	0.00416313  	0.00356479  
2023-05-22 17:21:20.410: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.701329]
2023-05-22 17:21:20.698: epoch 5:	0.00257625  	0.00523798  	0.00468369  
2023-05-22 17:21:20.698: Find a better model.
2023-05-22 17:21:41.380: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.679039]
2023-05-22 17:21:41.675: epoch 6:	0.00342760  	0.00738915  	0.00586198  
2023-05-22 17:21:41.675: Find a better model.
2023-05-22 17:22:02.386: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 20.708064]
2023-05-22 17:22:02.678: epoch 7:	0.00429375  	0.00958344  	0.00782728  
2023-05-22 17:22:02.678: Find a better model.
2023-05-22 17:22:23.429: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.747470]
2023-05-22 17:22:23.715: epoch 8:	0.00482676  	0.01133234  	0.00958691  
2023-05-22 17:22:23.715: Find a better model.
2023-05-22 17:22:44.140: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 20.421465]
2023-05-22 17:22:44.421: epoch 9:	0.00592980  	0.01489955  	0.01257560  
2023-05-22 17:22:44.421: Find a better model.
2023-05-22 17:23:05.132: [iter 10 : loss : 1.1345 = 0.6907 + 0.4438 + 0.0000, time: 20.706525]
2023-05-22 17:23:05.410: epoch 10:	0.00652204  	0.01616059  	0.01417885  
2023-05-22 17:23:05.410: Find a better model.
2023-05-22 17:23:25.963: [iter 11 : loss : 1.1338 = 0.6894 + 0.4444 + 0.0000, time: 20.548334]
2023-05-22 17:23:26.239: epoch 11:	0.00748442  	0.01940199  	0.01642267  
2023-05-22 17:23:26.239: Find a better model.
2023-05-22 17:23:46.733: [iter 12 : loss : 1.1324 = 0.6874 + 0.4450 + 0.0000, time: 20.488663]
2023-05-22 17:23:47.009: epoch 12:	0.00913528  	0.02450224  	0.02045116  
2023-05-22 17:23:47.009: Find a better model.
2023-05-22 17:24:07.374: [iter 13 : loss : 1.1303 = 0.6845 + 0.4457 + 0.0001, time: 20.361917]
2023-05-22 17:24:07.657: epoch 13:	0.01214833  	0.03273169  	0.02708971  
2023-05-22 17:24:07.657: Find a better model.
2023-05-22 17:24:28.321: [iter 14 : loss : 1.1261 = 0.6796 + 0.4464 + 0.0001, time: 20.658109]
2023-05-22 17:24:28.610: epoch 14:	0.01516883  	0.04036612  	0.03467852  
2023-05-22 17:24:28.610: Find a better model.
2023-05-22 17:24:48.959: [iter 15 : loss : 1.1189 = 0.6714 + 0.4473 + 0.0001, time: 20.344216]
2023-05-22 17:24:49.231: epoch 15:	0.01813750  	0.04825244  	0.04183117  
2023-05-22 17:24:49.231: Find a better model.
2023-05-22 17:25:09.556: [iter 16 : loss : 1.1059 = 0.6572 + 0.4485 + 0.0002, time: 20.320756]
2023-05-22 17:25:09.834: epoch 16:	0.02139493  	0.05665177  	0.04922924  
2023-05-22 17:25:09.834: Find a better model.
2023-05-22 17:25:30.169: [iter 17 : loss : 1.0847 = 0.6346 + 0.4498 + 0.0003, time: 20.331364]
2023-05-22 17:25:30.441: epoch 17:	0.02382320  	0.06321881  	0.05460348  
2023-05-22 17:25:30.441: Find a better model.
2023-05-22 17:25:51.089: [iter 18 : loss : 1.0526 = 0.6006 + 0.4516 + 0.0004, time: 20.644002]
2023-05-22 17:25:51.360: epoch 18:	0.02615524  	0.07023914  	0.05952354  
2023-05-22 17:25:51.361: Find a better model.
2023-05-22 17:26:11.720: [iter 19 : loss : 1.0111 = 0.5565 + 0.4540 + 0.0006, time: 20.354743]
2023-05-22 17:26:11.986: epoch 19:	0.02757667  	0.07320645  	0.06186277  
2023-05-22 17:26:11.986: Find a better model.
2023-05-22 17:26:32.301: [iter 20 : loss : 0.9629 = 0.5050 + 0.4571 + 0.0009, time: 20.310120]
2023-05-22 17:26:32.586: epoch 20:	0.02861311  	0.07650817  	0.06402283  
2023-05-22 17:26:32.586: Find a better model.
2023-05-22 17:26:52.891: [iter 21 : loss : 0.9133 = 0.4519 + 0.4602 + 0.0012, time: 20.301737]
2023-05-22 17:26:53.157: epoch 21:	0.02919055  	0.07865790  	0.06520234  
2023-05-22 17:26:53.157: Find a better model.
2023-05-22 17:27:13.456: [iter 22 : loss : 0.8653 = 0.4010 + 0.4629 + 0.0014, time: 20.294041]
2023-05-22 17:27:13.730: epoch 22:	0.02942006  	0.07926524  	0.06555603  
2023-05-22 17:27:13.730: Find a better model.
2023-05-22 17:27:34.046: [iter 23 : loss : 0.8237 = 0.3567 + 0.4652 + 0.0018, time: 20.310723]
2023-05-22 17:27:34.308: epoch 23:	0.02951632  	0.07979045  	0.06590540  
2023-05-22 17:27:34.309: Find a better model.
2023-05-22 17:27:54.678: [iter 24 : loss : 0.7877 = 0.3191 + 0.4665 + 0.0020, time: 20.365347]
2023-05-22 17:27:54.944: epoch 24:	0.02962737  	0.08039964  	0.06635806  
2023-05-22 17:27:54.944: Find a better model.
2023-05-22 17:28:15.457: [iter 25 : loss : 0.7568 = 0.2873 + 0.4672 + 0.0023, time: 20.509834]
2023-05-22 17:28:15.734: epoch 25:	0.02982725  	0.08115647  	0.06653624  
2023-05-22 17:28:15.734: Find a better model.
2023-05-22 17:28:36.062: [iter 26 : loss : 0.7315 = 0.2614 + 0.4676 + 0.0026, time: 20.324181]
2023-05-22 17:28:36.343: epoch 26:	0.02990130  	0.08139795  	0.06675610  
2023-05-22 17:28:36.343: Find a better model.
2023-05-22 17:28:56.866: [iter 27 : loss : 0.7094 = 0.2391 + 0.4674 + 0.0029, time: 20.519073]
2023-05-22 17:28:57.134: epoch 27:	0.02993091  	0.08152129  	0.06674718  
2023-05-22 17:28:57.134: Find a better model.
2023-05-22 17:29:17.627: [iter 28 : loss : 0.6906 = 0.2204 + 0.4671 + 0.0031, time: 20.489451]
2023-05-22 17:29:17.892: epoch 28:	0.03001233  	0.08184065  	0.06683266  
2023-05-22 17:29:17.892: Find a better model.
2023-05-22 17:29:39.061: [iter 29 : loss : 0.6746 = 0.2047 + 0.4665 + 0.0034, time: 21.165851]
2023-05-22 17:29:39.335: epoch 29:	0.03013078  	0.08239178  	0.06714753  
2023-05-22 17:29:39.335: Find a better model.
2023-05-22 17:30:03.406: [iter 30 : loss : 0.6599 = 0.1904 + 0.4659 + 0.0036, time: 24.066556]
2023-05-22 17:30:03.826: epoch 30:	0.03013078  	0.08236621  	0.06724928  
2023-05-22 17:30:27.673: [iter 31 : loss : 0.6474 = 0.1782 + 0.4654 + 0.0038, time: 23.842480]
2023-05-22 17:30:28.076: epoch 31:	0.03015299  	0.08258554  	0.06748430  
2023-05-22 17:30:28.076: Find a better model.
2023-05-22 17:30:52.194: [iter 32 : loss : 0.6367 = 0.1679 + 0.4647 + 0.0041, time: 24.112226]
2023-05-22 17:30:52.597: epoch 32:	0.03022702  	0.08257266  	0.06757720  
2023-05-22 17:31:16.590: [iter 33 : loss : 0.6275 = 0.1591 + 0.4641 + 0.0043, time: 23.988538]
2023-05-22 17:31:16.976: epoch 33:	0.03029365  	0.08255878  	0.06763260  
2023-05-22 17:31:40.832: [iter 34 : loss : 0.6184 = 0.1505 + 0.4635 + 0.0045, time: 23.850425]
2023-05-22 17:31:41.216: epoch 34:	0.03028626  	0.08255306  	0.06776504  
2023-05-22 17:32:05.239: [iter 35 : loss : 0.6095 = 0.1420 + 0.4628 + 0.0047, time: 24.017854]
2023-05-22 17:32:05.650: epoch 35:	0.03028625  	0.08254092  	0.06771987  
2023-05-22 17:32:29.185: [iter 36 : loss : 0.6030 = 0.1360 + 0.4622 + 0.0048, time: 23.528503]
2023-05-22 17:32:29.577: epoch 36:	0.03024925  	0.08276804  	0.06778946  
2023-05-22 17:32:29.577: Find a better model.
2023-05-22 17:32:53.119: [iter 37 : loss : 0.5968 = 0.1300 + 0.4618 + 0.0050, time: 23.537188]
2023-05-22 17:32:53.516: epoch 37:	0.03033067  	0.08256045  	0.06785094  
2023-05-22 17:33:17.085: [iter 38 : loss : 0.5906 = 0.1243 + 0.4611 + 0.0052, time: 23.558430]
2023-05-22 17:33:17.477: epoch 38:	0.03030847  	0.08272178  	0.06792329  
2023-05-22 17:33:40.984: [iter 39 : loss : 0.5855 = 0.1194 + 0.4607 + 0.0054, time: 23.502066]
2023-05-22 17:33:41.382: epoch 39:	0.03029366  	0.08255494  	0.06782763  
2023-05-22 17:34:04.467: [iter 40 : loss : 0.5800 = 0.1142 + 0.4602 + 0.0055, time: 23.080177]
2023-05-22 17:34:04.858: epoch 40:	0.03033808  	0.08284651  	0.06796427  
2023-05-22 17:34:04.858: Find a better model.
2023-05-22 17:34:28.024: [iter 41 : loss : 0.5751 = 0.1096 + 0.4598 + 0.0057, time: 23.163412]
2023-05-22 17:34:28.413: epoch 41:	0.03030107  	0.08288892  	0.06809408  
2023-05-22 17:34:28.413: Find a better model.
2023-05-22 17:34:51.539: [iter 42 : loss : 0.5712 = 0.1060 + 0.4593 + 0.0059, time: 23.121137]
2023-05-22 17:34:51.939: epoch 42:	0.03019001  	0.08252110  	0.06792684  
2023-05-22 17:35:15.108: [iter 43 : loss : 0.5671 = 0.1020 + 0.4591 + 0.0060, time: 23.164150]
2023-05-22 17:35:15.498: epoch 43:	0.03012338  	0.08221298  	0.06774053  
2023-05-22 17:35:38.835: [iter 44 : loss : 0.5644 = 0.0996 + 0.4586 + 0.0062, time: 23.325066]
2023-05-22 17:35:39.219: epoch 44:	0.03002715  	0.08201904  	0.06757838  
2023-05-22 17:36:02.374: [iter 45 : loss : 0.5603 = 0.0957 + 0.4582 + 0.0063, time: 23.150065]
2023-05-22 17:36:02.788: epoch 45:	0.03003455  	0.08223686  	0.06750230  
2023-05-22 17:36:25.982: [iter 46 : loss : 0.5570 = 0.0926 + 0.4579 + 0.0065, time: 23.186758]
2023-05-22 17:36:26.380: epoch 46:	0.03001234  	0.08197517  	0.06739035  
2023-05-22 17:36:49.714: [iter 47 : loss : 0.5540 = 0.0898 + 0.4575 + 0.0066, time: 23.328839]
2023-05-22 17:36:50.106: epoch 47:	0.03008638  	0.08186895  	0.06746758  
2023-05-22 17:37:13.095: [iter 48 : loss : 0.5514 = 0.0873 + 0.4573 + 0.0068, time: 22.983041]
2023-05-22 17:37:13.480: epoch 48:	0.03004935  	0.08200023  	0.06747767  
2023-05-22 17:37:36.775: [iter 49 : loss : 0.5484 = 0.0846 + 0.4569 + 0.0069, time: 23.291630]
2023-05-22 17:37:37.162: epoch 49:	0.03001235  	0.08176530  	0.06732590  
2023-05-22 17:37:59.942: [iter 50 : loss : 0.5464 = 0.0827 + 0.4567 + 0.0070, time: 22.773551]
2023-05-22 17:38:00.331: epoch 50:	0.02999753  	0.08136119  	0.06725724  
2023-05-22 17:38:23.375: [iter 51 : loss : 0.5435 = 0.0800 + 0.4563 + 0.0072, time: 23.035336]
2023-05-22 17:38:23.783: epoch 51:	0.03002714  	0.08144038  	0.06733524  
2023-05-22 17:38:46.891: [iter 52 : loss : 0.5411 = 0.0777 + 0.4561 + 0.0073, time: 23.104169]
2023-05-22 17:38:47.283: epoch 52:	0.02992350  	0.08119628  	0.06713281  
2023-05-22 17:39:10.575: [iter 53 : loss : 0.5400 = 0.0766 + 0.4559 + 0.0074, time: 23.284230]
2023-05-22 17:39:10.980: epoch 53:	0.02985687  	0.08097564  	0.06713170  
2023-05-22 17:39:34.108: [iter 54 : loss : 0.5379 = 0.0747 + 0.4556 + 0.0076, time: 23.120994]
2023-05-22 17:39:34.395: epoch 54:	0.02979764  	0.08069412  	0.06693926  
2023-05-22 17:39:58.165: [iter 55 : loss : 0.5359 = 0.0727 + 0.4555 + 0.0077, time: 23.766051]
2023-05-22 17:39:58.480: epoch 55:	0.02970881  	0.08056942  	0.06696930  
2023-05-22 17:40:22.504: [iter 56 : loss : 0.5338 = 0.0707 + 0.4552 + 0.0078, time: 24.019033]
2023-05-22 17:40:22.831: epoch 56:	0.02971622  	0.08046862  	0.06694304  
2023-05-22 17:40:43.343: [iter 57 : loss : 0.5320 = 0.0690 + 0.4551 + 0.0079, time: 20.506641]
2023-05-22 17:40:43.603: epoch 57:	0.02967180  	0.08026803  	0.06685792  
2023-05-22 17:41:03.808: [iter 58 : loss : 0.5304 = 0.0675 + 0.4549 + 0.0081, time: 20.200107]
2023-05-22 17:41:04.067: epoch 58:	0.02951633  	0.08006179  	0.06665417  
2023-05-22 17:41:24.581: [iter 59 : loss : 0.5289 = 0.0661 + 0.4547 + 0.0082, time: 20.510263]
2023-05-22 17:41:24.852: epoch 59:	0.02947190  	0.07946214  	0.06647178  
2023-05-22 17:41:45.193: [iter 60 : loss : 0.5283 = 0.0655 + 0.4546 + 0.0083, time: 20.337013]
2023-05-22 17:41:45.456: epoch 60:	0.02938307  	0.07887482  	0.06613772  
2023-05-22 17:42:05.776: [iter 61 : loss : 0.5265 = 0.0638 + 0.4544 + 0.0084, time: 20.316613]
2023-05-22 17:42:06.037: epoch 61:	0.02936084  	0.07854486  	0.06603049  
2023-05-22 17:42:26.581: [iter 62 : loss : 0.5256 = 0.0628 + 0.4543 + 0.0085, time: 20.539343]
2023-05-22 17:42:26.850: epoch 62:	0.02936085  	0.07859836  	0.06601236  
2023-05-22 17:42:47.133: [iter 63 : loss : 0.5241 = 0.0614 + 0.4541 + 0.0086, time: 20.279986]
2023-05-22 17:42:47.396: epoch 63:	0.02930902  	0.07822920  	0.06580695  
2023-05-22 17:43:07.773: [iter 64 : loss : 0.5229 = 0.0603 + 0.4540 + 0.0087, time: 20.371936]
2023-05-22 17:43:08.038: epoch 64:	0.02929421  	0.07792266  	0.06554445  
2023-05-22 17:43:28.557: [iter 65 : loss : 0.5215 = 0.0588 + 0.4539 + 0.0088, time: 20.515153]
2023-05-22 17:43:28.827: epoch 65:	0.02913874  	0.07739557  	0.06531546  
2023-05-22 17:43:49.155: [iter 66 : loss : 0.5205 = 0.0579 + 0.4537 + 0.0089, time: 20.322516]
2023-05-22 17:43:49.414: epoch 66:	0.02916835  	0.07733727  	0.06531730  
2023-05-22 17:43:49.415: Early stopping is trigger at epoch: 66
2023-05-22 17:43:49.415: best_result@epoch 41:

2023-05-22 17:43:49.415: 		0.0303      	0.0829      	0.0681      
2023-05-22 18:34:30.226: my pid: 7720
2023-05-22 18:34:30.226: model: model.general_recommender.SGL
2023-05-22 18:34:30.226: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 18:34:30.226: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 18:34:34.459: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 18:34:55.711: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.252170]
2023-05-22 18:34:55.970: epoch 1:	0.00128072  	0.00289054  	0.00228682  
2023-05-22 18:34:55.970: Find a better model.
2023-05-22 18:35:17.081: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.107644]
2023-05-22 18:35:17.364: epoch 2:	0.00158425  	0.00347572  	0.00250149  
2023-05-22 18:35:17.364: Find a better model.
2023-05-22 18:35:38.300: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.931453]
2023-05-22 18:35:38.589: epoch 3:	0.00194700  	0.00435662  	0.00362908  
2023-05-22 18:35:38.589: Find a better model.
2023-05-22 18:35:59.455: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.862696]
2023-05-22 18:35:59.758: epoch 4:	0.00253924  	0.00619276  	0.00471678  
2023-05-22 18:35:59.758: Find a better model.
2023-05-22 18:36:20.641: [iter 5 : loss : 1.1344 = 0.6927 + 0.4418 + 0.0000, time: 20.879697]
2023-05-22 18:36:20.929: epoch 5:	0.00269470  	0.00578408  	0.00482926  
2023-05-22 18:36:41.896: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.962230]
2023-05-22 18:36:42.184: epoch 6:	0.00362748  	0.00822267  	0.00676815  
2023-05-22 18:36:42.184: Find a better model.
2023-05-22 18:37:03.019: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 20.830062]
2023-05-22 18:37:03.299: epoch 7:	0.00409387  	0.00921650  	0.00784972  
2023-05-22 18:37:03.299: Find a better model.
2023-05-22 18:37:24.201: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 20.898743]
2023-05-22 18:37:24.482: epoch 8:	0.00494521  	0.01208747  	0.01003482  
2023-05-22 18:37:24.482: Find a better model.
2023-05-22 18:37:45.299: [iter 9 : loss : 1.1346 = 0.6915 + 0.4432 + 0.0000, time: 20.812694]
2023-05-22 18:37:45.574: epoch 9:	0.00573733  	0.01487699  	0.01146406  
2023-05-22 18:37:45.574: Find a better model.
2023-05-22 18:38:06.217: [iter 10 : loss : 1.1344 = 0.6906 + 0.4437 + 0.0000, time: 20.638863]
2023-05-22 18:38:06.491: epoch 10:	0.00615189  	0.01656648  	0.01310546  
2023-05-22 18:38:06.491: Find a better model.
2023-05-22 18:38:27.003: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 20.507507]
2023-05-22 18:38:27.278: epoch 11:	0.00758066  	0.02093280  	0.01674979  
2023-05-22 18:38:27.278: Find a better model.
2023-05-22 18:38:47.838: [iter 12 : loss : 1.1324 = 0.6874 + 0.4450 + 0.0000, time: 20.556227]
2023-05-22 18:38:48.112: epoch 12:	0.00932036  	0.02552892  	0.02142761  
2023-05-22 18:38:48.112: Find a better model.
2023-05-22 18:39:08.587: [iter 13 : loss : 1.1302 = 0.6845 + 0.4456 + 0.0001, time: 20.470184]
2023-05-22 18:39:08.866: epoch 13:	0.01192624  	0.03308223  	0.02792958  
2023-05-22 18:39:08.866: Find a better model.
2023-05-22 18:39:29.368: [iter 14 : loss : 1.1262 = 0.6797 + 0.4464 + 0.0001, time: 20.498921]
2023-05-22 18:39:29.641: epoch 14:	0.01468022  	0.03999740  	0.03441701  
2023-05-22 18:39:29.641: Find a better model.
2023-05-22 18:39:50.003: [iter 15 : loss : 1.1190 = 0.6715 + 0.4474 + 0.0001, time: 20.357781]
2023-05-22 18:39:50.292: epoch 15:	0.01774514  	0.04845857  	0.04179503  
2023-05-22 18:39:50.292: Find a better model.
2023-05-22 18:40:10.748: [iter 16 : loss : 1.1060 = 0.6574 + 0.4485 + 0.0002, time: 20.452412]
2023-05-22 18:40:11.026: epoch 16:	0.02103218  	0.05766275  	0.04923591  
2023-05-22 18:40:11.026: Find a better model.
2023-05-22 18:40:31.377: [iter 17 : loss : 1.0846 = 0.6347 + 0.4497 + 0.0003, time: 20.347383]
2023-05-22 18:40:31.648: epoch 17:	0.02368993  	0.06417938  	0.05483842  
2023-05-22 18:40:31.649: Find a better model.
2023-05-22 18:40:52.000: [iter 18 : loss : 1.0524 = 0.6005 + 0.4515 + 0.0004, time: 20.347161]
2023-05-22 18:40:52.275: epoch 18:	0.02577026  	0.06989986  	0.05900017  
2023-05-22 18:40:52.276: Find a better model.
2023-05-22 18:41:12.747: [iter 19 : loss : 1.0107 = 0.5562 + 0.4539 + 0.0006, time: 20.466356]
2023-05-22 18:41:13.021: epoch 19:	0.02726572  	0.07418284  	0.06200228  
2023-05-22 18:41:13.021: Find a better model.
2023-05-22 18:41:33.522: [iter 20 : loss : 0.9626 = 0.5048 + 0.4570 + 0.0009, time: 20.497240]
2023-05-22 18:41:33.791: epoch 20:	0.02835400  	0.07699513  	0.06449464  
2023-05-22 18:41:33.791: Find a better model.
2023-05-22 18:41:54.184: [iter 21 : loss : 0.9128 = 0.4515 + 0.4601 + 0.0012, time: 20.389291]
2023-05-22 18:41:54.449: epoch 21:	0.02879820  	0.07841710  	0.06539435  
2023-05-22 18:41:54.449: Find a better model.
2023-05-22 18:42:14.759: [iter 22 : loss : 0.8648 = 0.4006 + 0.4627 + 0.0015, time: 20.306554]
2023-05-22 18:42:15.033: epoch 22:	0.02937566  	0.08028334  	0.06637571  
2023-05-22 18:42:15.034: Find a better model.
2023-05-22 18:42:35.366: [iter 23 : loss : 0.8230 = 0.3563 + 0.4649 + 0.0018, time: 20.327674]
2023-05-22 18:42:35.646: epoch 23:	0.02945709  	0.08030394  	0.06635118  
2023-05-22 18:42:35.646: Find a better model.
2023-05-22 18:42:56.120: [iter 24 : loss : 0.7868 = 0.3183 + 0.4664 + 0.0021, time: 20.470808]
2023-05-22 18:42:56.384: epoch 24:	0.02961997  	0.08103471  	0.06654520  
2023-05-22 18:42:56.384: Find a better model.
2023-05-22 18:43:16.749: [iter 25 : loss : 0.7559 = 0.2866 + 0.4670 + 0.0023, time: 20.361091]
2023-05-22 18:43:17.017: epoch 25:	0.02979024  	0.08173171  	0.06676880  
2023-05-22 18:43:17.017: Find a better model.
2023-05-22 18:43:37.306: [iter 26 : loss : 0.7304 = 0.2605 + 0.4673 + 0.0026, time: 20.286315]
2023-05-22 18:43:37.570: epoch 26:	0.02990870  	0.08204764  	0.06697288  
2023-05-22 18:43:37.570: Find a better model.
2023-05-22 18:43:57.910: [iter 27 : loss : 0.7085 = 0.2384 + 0.4673 + 0.0029, time: 20.335534]
2023-05-22 18:43:58.171: epoch 27:	0.02993091  	0.08222289  	0.06695709  
2023-05-22 18:43:58.171: Find a better model.
2023-05-22 18:44:18.677: [iter 28 : loss : 0.6899 = 0.2200 + 0.4668 + 0.0031, time: 20.502607]
2023-05-22 18:44:18.951: epoch 28:	0.03007157  	0.08267339  	0.06747018  
2023-05-22 18:44:18.951: Find a better model.
2023-05-22 18:44:39.459: [iter 29 : loss : 0.6739 = 0.2041 + 0.4664 + 0.0034, time: 20.504808]
2023-05-22 18:44:39.723: epoch 29:	0.03022704  	0.08329694  	0.06788622  
2023-05-22 18:44:39.723: Find a better model.
2023-05-22 18:45:00.081: [iter 30 : loss : 0.6593 = 0.1899 + 0.4658 + 0.0036, time: 20.353965]
2023-05-22 18:45:00.342: epoch 30:	0.03016042  	0.08319164  	0.06801821  
2023-05-22 18:45:20.842: [iter 31 : loss : 0.6466 = 0.1776 + 0.4652 + 0.0038, time: 20.496151]
2023-05-22 18:45:21.108: epoch 31:	0.03018262  	0.08355542  	0.06820843  
2023-05-22 18:45:21.108: Find a better model.
2023-05-22 18:45:41.448: [iter 32 : loss : 0.6359 = 0.1673 + 0.4646 + 0.0041, time: 20.336453]
2023-05-22 18:45:41.707: epoch 32:	0.03015301  	0.08312201  	0.06829642  
2023-05-22 18:46:02.082: [iter 33 : loss : 0.6266 = 0.1584 + 0.4639 + 0.0043, time: 20.368315]
2023-05-22 18:46:02.342: epoch 33:	0.03027146  	0.08344139  	0.06842113  
2023-05-22 18:46:22.668: [iter 34 : loss : 0.6176 = 0.1499 + 0.4632 + 0.0045, time: 20.322464]
2023-05-22 18:46:22.948: epoch 34:	0.03033068  	0.08313917  	0.06829190  
2023-05-22 18:46:43.430: [iter 35 : loss : 0.6091 = 0.1418 + 0.4627 + 0.0047, time: 20.478613]
2023-05-22 18:46:43.707: epoch 35:	0.03019742  	0.08275252  	0.06824332  
2023-05-22 18:47:04.091: [iter 36 : loss : 0.6028 = 0.1358 + 0.4621 + 0.0048, time: 20.379835]
2023-05-22 18:47:04.350: epoch 36:	0.03027886  	0.08308887  	0.06824610  
2023-05-22 18:47:24.828: [iter 37 : loss : 0.5964 = 0.1298 + 0.4615 + 0.0050, time: 20.474887]
2023-05-22 18:47:25.095: epoch 37:	0.03028626  	0.08294551  	0.06826629  
2023-05-22 18:47:45.606: [iter 38 : loss : 0.5899 = 0.1236 + 0.4611 + 0.0052, time: 20.507580]
2023-05-22 18:47:45.871: epoch 38:	0.03024183  	0.08285189  	0.06826842  
2023-05-22 18:48:06.223: [iter 39 : loss : 0.5851 = 0.1191 + 0.4606 + 0.0054, time: 20.348325]
2023-05-22 18:48:06.485: epoch 39:	0.03036029  	0.08268733  	0.06833267  
2023-05-22 18:48:26.995: [iter 40 : loss : 0.5798 = 0.1141 + 0.4602 + 0.0056, time: 20.506075]
2023-05-22 18:48:27.258: epoch 40:	0.03030847  	0.08273040  	0.06825819  
2023-05-22 18:48:47.770: [iter 41 : loss : 0.5748 = 0.1094 + 0.4597 + 0.0057, time: 20.508488]
2023-05-22 18:48:48.044: epoch 41:	0.03037509  	0.08282728  	0.06835565  
2023-05-22 18:49:08.620: [iter 42 : loss : 0.5705 = 0.1055 + 0.4592 + 0.0059, time: 20.572258]
2023-05-22 18:49:08.880: epoch 42:	0.03041951  	0.08275174  	0.06832429  
2023-05-22 18:49:29.373: [iter 43 : loss : 0.5664 = 0.1016 + 0.4588 + 0.0060, time: 20.488170]
2023-05-22 18:49:29.651: epoch 43:	0.03037508  	0.08240026  	0.06818889  
2023-05-22 18:49:49.986: [iter 44 : loss : 0.5638 = 0.0990 + 0.4586 + 0.0062, time: 20.330558]
2023-05-22 18:49:50.249: epoch 44:	0.03032327  	0.08217897  	0.06816522  
2023-05-22 18:50:10.634: [iter 45 : loss : 0.5599 = 0.0954 + 0.4581 + 0.0063, time: 20.381729]
2023-05-22 18:50:10.913: epoch 45:	0.03027144  	0.08184055  	0.06801625  
2023-05-22 18:50:31.522: [iter 46 : loss : 0.5570 = 0.0928 + 0.4577 + 0.0065, time: 20.603853]
2023-05-22 18:50:31.779: epoch 46:	0.03021962  	0.08174939  	0.06782119  
2023-05-22 18:50:52.145: [iter 47 : loss : 0.5536 = 0.0895 + 0.4575 + 0.0066, time: 20.361579]
2023-05-22 18:50:52.402: epoch 47:	0.03005676  	0.08140051  	0.06776851  
2023-05-22 18:51:12.749: [iter 48 : loss : 0.5510 = 0.0871 + 0.4570 + 0.0068, time: 20.343365]
2023-05-22 18:51:13.028: epoch 48:	0.02996792  	0.08107550  	0.06773736  
2023-05-22 18:51:33.554: [iter 49 : loss : 0.5485 = 0.0847 + 0.4568 + 0.0069, time: 20.521454]
2023-05-22 18:51:33.813: epoch 49:	0.02996051  	0.08107994  	0.06755472  
2023-05-22 18:51:54.116: [iter 50 : loss : 0.5461 = 0.0825 + 0.4566 + 0.0071, time: 20.299983]
2023-05-22 18:51:54.378: epoch 50:	0.02988647  	0.08094084  	0.06762026  
2023-05-22 18:52:14.737: [iter 51 : loss : 0.5432 = 0.0797 + 0.4563 + 0.0072, time: 20.356056]
2023-05-22 18:52:15.012: epoch 51:	0.02981984  	0.08084208  	0.06748581  
2023-05-22 18:52:35.545: [iter 52 : loss : 0.5410 = 0.0776 + 0.4561 + 0.0073, time: 20.528889]
2023-05-22 18:52:35.806: epoch 52:	0.02973101  	0.08062650  	0.06735831  
2023-05-22 18:52:56.163: [iter 53 : loss : 0.5399 = 0.0766 + 0.4558 + 0.0074, time: 20.353156]
2023-05-22 18:52:56.440: epoch 53:	0.02976061  	0.08076701  	0.06735836  
2023-05-22 18:53:16.730: [iter 54 : loss : 0.5376 = 0.0745 + 0.4555 + 0.0076, time: 20.285860]
2023-05-22 18:53:17.005: epoch 54:	0.02960515  	0.08033745  	0.06708297  
2023-05-22 18:53:37.482: [iter 55 : loss : 0.5354 = 0.0724 + 0.4554 + 0.0077, time: 20.472030]
2023-05-22 18:53:37.745: epoch 55:	0.02959773  	0.08031347  	0.06704162  
2023-05-22 18:53:58.090: [iter 56 : loss : 0.5337 = 0.0707 + 0.4552 + 0.0078, time: 20.340286]
2023-05-22 18:53:58.349: epoch 56:	0.02945706  	0.07973708  	0.06671730  
2023-05-22 18:53:58.349: Early stopping is trigger at epoch: 56
2023-05-22 18:53:58.349: best_result@epoch 31:

2023-05-22 18:53:58.349: 		0.0302      	0.0836      	0.0682      
2023-05-22 18:59:01.954: my pid: 13620
2023-05-22 18:59:01.955: model: model.general_recommender.SGL
2023-05-22 18:59:01.955: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 18:59:01.955: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 18:59:06.120: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 18:59:27.557: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.436750]
2023-05-22 18:59:27.838: epoch 1:	0.00102162  	0.00231901  	0.00178538  
2023-05-22 18:59:27.838: Find a better model.
2023-05-22 18:59:49.006: [iter 2 : loss : 1.1339 = 0.6930 + 0.4408 + 0.0000, time: 21.164171]
2023-05-22 18:59:49.294: epoch 2:	0.00151762  	0.00321859  	0.00253503  
2023-05-22 18:59:49.294: Find a better model.
2023-05-22 19:00:10.417: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 21.119237]
2023-05-22 19:00:10.699: epoch 3:	0.00188777  	0.00443879  	0.00322273  
2023-05-22 19:00:10.699: Find a better model.
2023-05-22 19:00:31.979: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.276459]
2023-05-22 19:00:32.265: epoch 4:	0.00205064  	0.00462040  	0.00364919  
2023-05-22 19:00:32.265: Find a better model.
2023-05-22 19:00:53.302: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.033819]
2023-05-22 19:00:53.586: epoch 5:	0.00257625  	0.00509310  	0.00446339  
2023-05-22 19:00:53.586: Find a better model.
2023-05-22 19:01:14.754: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 21.163775]
2023-05-22 19:01:15.039: epoch 6:	0.00350903  	0.00752765  	0.00642831  
2023-05-22 19:01:15.039: Find a better model.
2023-05-22 19:01:36.409: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 21.366214]
2023-05-22 19:01:36.693: epoch 7:	0.00414569  	0.01022655  	0.00800395  
2023-05-22 19:01:36.693: Find a better model.
2023-05-22 19:01:57.814: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 21.116720]
2023-05-22 19:01:58.095: epoch 8:	0.00498222  	0.01242446  	0.00995689  
2023-05-22 19:01:58.095: Find a better model.
2023-05-22 19:02:19.164: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 21.065611]
2023-05-22 19:02:19.444: epoch 9:	0.00637398  	0.01562657  	0.01284321  
2023-05-22 19:02:19.444: Find a better model.
2023-05-22 19:02:40.360: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.911038]
2023-05-22 19:02:40.642: epoch 10:	0.00747702  	0.01957041  	0.01597151  
2023-05-22 19:02:40.642: Find a better model.
2023-05-22 19:03:01.733: [iter 11 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 21.086690]
2023-05-22 19:03:02.029: epoch 11:	0.00842459  	0.02240590  	0.01863003  
2023-05-22 19:03:02.029: Find a better model.
2023-05-22 19:03:22.903: [iter 12 : loss : 1.1327 = 0.6879 + 0.4448 + 0.0000, time: 20.871216]
2023-05-22 19:03:23.181: epoch 12:	0.01001625  	0.02713406  	0.02244931  
2023-05-22 19:03:23.181: Find a better model.
2023-05-22 19:03:43.953: [iter 13 : loss : 1.1306 = 0.6850 + 0.4456 + 0.0001, time: 20.768708]
2023-05-22 19:03:44.230: epoch 13:	0.01257032  	0.03373154  	0.02878994  
2023-05-22 19:03:44.230: Find a better model.
2023-05-22 19:04:04.969: [iter 14 : loss : 1.1268 = 0.6803 + 0.4464 + 0.0001, time: 20.736608]
2023-05-22 19:04:05.240: epoch 14:	0.01522066  	0.04071428  	0.03550785  
2023-05-22 19:04:05.240: Find a better model.
2023-05-22 19:04:26.079: [iter 15 : loss : 1.1196 = 0.6724 + 0.4472 + 0.0001, time: 20.834769]
2023-05-22 19:04:26.352: epoch 15:	0.01821154  	0.04823592  	0.04242425  
2023-05-22 19:04:26.352: Find a better model.
2023-05-22 19:04:46.964: [iter 16 : loss : 1.1071 = 0.6587 + 0.4482 + 0.0002, time: 20.607358]
2023-05-22 19:04:47.237: epoch 16:	0.02099516  	0.05483300  	0.04847653  
2023-05-22 19:04:47.237: Find a better model.
2023-05-22 19:05:08.084: [iter 17 : loss : 1.0866 = 0.6368 + 0.4496 + 0.0003, time: 20.843622]
2023-05-22 19:05:08.354: epoch 17:	0.02371954  	0.06165236  	0.05418566  
2023-05-22 19:05:08.354: Find a better model.
2023-05-22 19:05:29.097: [iter 18 : loss : 1.0550 = 0.6033 + 0.4513 + 0.0004, time: 20.740002]
2023-05-22 19:05:29.368: epoch 18:	0.02599236  	0.06886949  	0.05881149  
2023-05-22 19:05:29.369: Find a better model.
2023-05-22 19:05:50.092: [iter 19 : loss : 1.0138 = 0.5595 + 0.4537 + 0.0006, time: 20.718640]
2023-05-22 19:05:50.363: epoch 19:	0.02731014  	0.07301018  	0.06167676  
2023-05-22 19:05:50.363: Find a better model.
2023-05-22 19:06:11.133: [iter 20 : loss : 0.9656 = 0.5081 + 0.4566 + 0.0009, time: 20.767226]
2023-05-22 19:06:11.407: epoch 20:	0.02838361  	0.07649570  	0.06359236  
2023-05-22 19:06:11.407: Find a better model.
2023-05-22 19:06:32.235: [iter 21 : loss : 0.9158 = 0.4548 + 0.4598 + 0.0011, time: 20.824096]
2023-05-22 19:06:32.525: epoch 21:	0.02892406  	0.07817025  	0.06447552  
2023-05-22 19:06:32.525: Find a better model.
2023-05-22 19:06:53.101: [iter 22 : loss : 0.8676 = 0.4035 + 0.4627 + 0.0014, time: 20.571501]
2023-05-22 19:06:53.389: epoch 22:	0.02919058  	0.07946064  	0.06508414  
2023-05-22 19:06:53.389: Find a better model.
2023-05-22 19:07:14.029: [iter 23 : loss : 0.8253 = 0.3587 + 0.4648 + 0.0017, time: 20.637541]
2023-05-22 19:07:14.296: epoch 23:	0.02924981  	0.07965461  	0.06509067  
2023-05-22 19:07:14.296: Find a better model.
2023-05-22 19:07:35.106: [iter 24 : loss : 0.7891 = 0.3207 + 0.4663 + 0.0020, time: 20.806366]
2023-05-22 19:07:35.371: epoch 24:	0.02940528  	0.08048344  	0.06556751  
2023-05-22 19:07:35.371: Find a better model.
2023-05-22 19:07:56.058: [iter 25 : loss : 0.7577 = 0.2885 + 0.4669 + 0.0023, time: 20.682480]
2023-05-22 19:07:56.324: epoch 25:	0.02953113  	0.08107959  	0.06581663  
2023-05-22 19:07:56.324: Find a better model.
2023-05-22 19:08:17.044: [iter 26 : loss : 0.7321 = 0.2623 + 0.4672 + 0.0026, time: 20.714842]
2023-05-22 19:08:17.311: epoch 26:	0.02962738  	0.08168115  	0.06607199  
2023-05-22 19:08:17.311: Find a better model.
2023-05-22 19:08:38.187: [iter 27 : loss : 0.7098 = 0.2399 + 0.4670 + 0.0029, time: 20.872459]
2023-05-22 19:08:38.453: epoch 27:	0.02972362  	0.08196265  	0.06639896  
2023-05-22 19:08:38.453: Find a better model.
2023-05-22 19:08:59.052: [iter 28 : loss : 0.6912 = 0.2213 + 0.4667 + 0.0031, time: 20.594452]
2023-05-22 19:08:59.313: epoch 28:	0.02973844  	0.08241890  	0.06656014  
2023-05-22 19:08:59.313: Find a better model.
2023-05-22 19:09:20.027: [iter 29 : loss : 0.6750 = 0.2055 + 0.4662 + 0.0034, time: 20.709539]
2023-05-22 19:09:20.292: epoch 29:	0.02984947  	0.08234117  	0.06682529  
2023-05-22 19:09:41.046: [iter 30 : loss : 0.6601 = 0.1910 + 0.4655 + 0.0036, time: 20.749716]
2023-05-22 19:09:41.328: epoch 30:	0.02984207  	0.08265861  	0.06703799  
2023-05-22 19:09:41.328: Find a better model.
2023-05-22 19:10:02.186: [iter 31 : loss : 0.6476 = 0.1788 + 0.4650 + 0.0038, time: 20.854585]
2023-05-22 19:10:02.448: epoch 31:	0.02967921  	0.08210858  	0.06678285  
2023-05-22 19:10:23.057: [iter 32 : loss : 0.6366 = 0.1682 + 0.4644 + 0.0040, time: 20.606943]
2023-05-22 19:10:23.319: epoch 32:	0.02974583  	0.08233789  	0.06706719  
2023-05-22 19:10:44.150: [iter 33 : loss : 0.6272 = 0.1593 + 0.4636 + 0.0042, time: 20.827163]
2023-05-22 19:10:44.412: epoch 33:	0.02986428  	0.08281584  	0.06720251  
2023-05-22 19:10:44.412: Find a better model.
2023-05-22 19:11:05.175: [iter 34 : loss : 0.6187 = 0.1511 + 0.4631 + 0.0045, time: 20.758938]
2023-05-22 19:11:05.436: epoch 34:	0.03000494  	0.08315729  	0.06745356  
2023-05-22 19:11:05.436: Find a better model.
2023-05-22 19:11:26.142: [iter 35 : loss : 0.6096 = 0.1424 + 0.4626 + 0.0046, time: 20.701808]
2023-05-22 19:11:26.402: epoch 35:	0.02993091  	0.08261050  	0.06717009  
2023-05-22 19:11:47.347: [iter 36 : loss : 0.6031 = 0.1364 + 0.4619 + 0.0048, time: 20.940591]
2023-05-22 19:11:47.608: epoch 36:	0.02987168  	0.08241373  	0.06707234  
2023-05-22 19:12:08.508: [iter 37 : loss : 0.5967 = 0.1303 + 0.4614 + 0.0050, time: 20.896704]
2023-05-22 19:12:08.769: epoch 37:	0.02987167  	0.08225813  	0.06718735  
2023-05-22 19:12:29.546: [iter 38 : loss : 0.5905 = 0.1245 + 0.4608 + 0.0052, time: 20.773406]
2023-05-22 19:12:29.809: epoch 38:	0.02981986  	0.08261222  	0.06731851  
2023-05-22 19:12:50.534: [iter 39 : loss : 0.5853 = 0.1196 + 0.4603 + 0.0054, time: 20.722668]
2023-05-22 19:12:50.795: epoch 39:	0.02984948  	0.08208048  	0.06707529  
2023-05-22 19:13:11.741: [iter 40 : loss : 0.5801 = 0.1146 + 0.4600 + 0.0055, time: 20.942196]
2023-05-22 19:13:11.999: epoch 40:	0.02975322  	0.08179570  	0.06688939  
2023-05-22 19:13:32.689: [iter 41 : loss : 0.5751 = 0.1099 + 0.4595 + 0.0057, time: 20.685768]
2023-05-22 19:13:32.955: epoch 41:	0.02976802  	0.08180209  	0.06691170  
2023-05-22 19:13:53.722: [iter 42 : loss : 0.5706 = 0.1058 + 0.4589 + 0.0059, time: 20.763377]
2023-05-22 19:13:53.986: epoch 42:	0.02977543  	0.08143772  	0.06695177  
2023-05-22 19:14:14.882: [iter 43 : loss : 0.5668 = 0.1021 + 0.4587 + 0.0060, time: 20.891550]
2023-05-22 19:14:15.181: epoch 43:	0.02969399  	0.08147128  	0.06698012  
2023-05-22 19:14:35.906: [iter 44 : loss : 0.5640 = 0.0996 + 0.4583 + 0.0062, time: 20.720227]
2023-05-22 19:14:36.201: epoch 44:	0.02969399  	0.08107439  	0.06678932  
2023-05-22 19:14:57.112: [iter 45 : loss : 0.5601 = 0.0959 + 0.4579 + 0.0063, time: 20.908629]
2023-05-22 19:14:57.383: epoch 45:	0.02965698  	0.08115599  	0.06670995  
2023-05-22 19:15:18.280: [iter 46 : loss : 0.5569 = 0.0929 + 0.4575 + 0.0065, time: 20.891541]
2023-05-22 19:15:18.537: epoch 46:	0.02967918  	0.08114971  	0.06668798  
2023-05-22 19:15:39.249: [iter 47 : loss : 0.5538 = 0.0899 + 0.4573 + 0.0066, time: 20.707407]
2023-05-22 19:15:39.510: epoch 47:	0.02961996  	0.08126539  	0.06661574  
2023-05-22 19:16:00.302: [iter 48 : loss : 0.5514 = 0.0876 + 0.4570 + 0.0068, time: 20.788839]
2023-05-22 19:16:00.561: epoch 48:	0.02950891  	0.08066241  	0.06644448  
2023-05-22 19:16:21.231: [iter 49 : loss : 0.5483 = 0.0847 + 0.4566 + 0.0069, time: 20.664430]
2023-05-22 19:16:21.494: epoch 49:	0.02942007  	0.08023576  	0.06641157  
2023-05-22 19:16:42.268: [iter 50 : loss : 0.5464 = 0.0830 + 0.4564 + 0.0070, time: 20.769250]
2023-05-22 19:16:42.531: epoch 50:	0.02920538  	0.07960319  	0.06601568  
2023-05-22 19:17:03.224: [iter 51 : loss : 0.5436 = 0.0804 + 0.4560 + 0.0072, time: 20.689015]
2023-05-22 19:17:03.487: epoch 51:	0.02912395  	0.07949225  	0.06573544  
2023-05-22 19:17:24.265: [iter 52 : loss : 0.5412 = 0.0779 + 0.4559 + 0.0073, time: 20.773493]
2023-05-22 19:17:24.525: epoch 52:	0.02911654  	0.07909834  	0.06561490  
2023-05-22 19:17:45.388: [iter 53 : loss : 0.5404 = 0.0772 + 0.4558 + 0.0074, time: 20.860211]
2023-05-22 19:17:45.649: epoch 53:	0.02904990  	0.07892870  	0.06547673  
2023-05-22 19:18:06.409: [iter 54 : loss : 0.5378 = 0.0748 + 0.4554 + 0.0076, time: 20.757135]
2023-05-22 19:18:06.669: epoch 54:	0.02899808  	0.07850967  	0.06527111  
2023-05-22 19:18:27.436: [iter 55 : loss : 0.5355 = 0.0727 + 0.4552 + 0.0077, time: 20.762937]
2023-05-22 19:18:27.699: epoch 55:	0.02900548  	0.07827022  	0.06515688  
2023-05-22 19:18:48.269: [iter 56 : loss : 0.5335 = 0.0707 + 0.4550 + 0.0078, time: 20.567106]
2023-05-22 19:18:48.529: epoch 56:	0.02890924  	0.07811892  	0.06516096  
2023-05-22 19:19:09.377: [iter 57 : loss : 0.5318 = 0.0690 + 0.4548 + 0.0079, time: 20.844374]
2023-05-22 19:19:09.641: epoch 57:	0.02893884  	0.07779890  	0.06508907  
2023-05-22 19:19:30.392: [iter 58 : loss : 0.5305 = 0.0678 + 0.4547 + 0.0080, time: 20.746063]
2023-05-22 19:19:30.657: epoch 58:	0.02889443  	0.07776563  	0.06506718  
2023-05-22 19:19:51.204: [iter 59 : loss : 0.5289 = 0.0663 + 0.4545 + 0.0082, time: 20.543998]
2023-05-22 19:19:51.471: epoch 59:	0.02887222  	0.07769167  	0.06504513  
2023-05-22 19:19:51.471: Early stopping is trigger at epoch: 59
2023-05-22 19:19:51.471: best_result@epoch 34:

2023-05-22 19:19:51.471: 		0.0300      	0.0832      	0.0675      
2023-05-22 19:25:05.337: my pid: 1180
2023-05-22 19:25:05.337: model: model.general_recommender.SGL
2023-05-22 19:25:05.337: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 19:25:05.337: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 19:25:09.493: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 19:25:30.268: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.774408]
2023-05-22 19:25:30.530: epoch 1:	0.00119189  	0.00217219  	0.00184798  
2023-05-22 19:25:30.530: Find a better model.
2023-05-22 19:25:51.434: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.899339]
2023-05-22 19:25:51.727: epoch 2:	0.00180634  	0.00383052  	0.00314793  
2023-05-22 19:25:51.727: Find a better model.
2023-05-22 19:26:12.667: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.935532]
2023-05-22 19:26:12.952: epoch 3:	0.00208025  	0.00441977  	0.00373603  
2023-05-22 19:26:12.952: Find a better model.
2023-05-22 19:26:33.996: [iter 4 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 21.040477]
2023-05-22 19:26:34.294: epoch 4:	0.00241339  	0.00508745  	0.00408693  
2023-05-22 19:26:34.294: Find a better model.
2023-05-22 19:26:55.027: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.729417]
2023-05-22 19:26:55.323: epoch 5:	0.00299082  	0.00726249  	0.00582652  
2023-05-22 19:26:55.323: Find a better model.
2023-05-22 19:27:16.204: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.877999]
2023-05-22 19:27:16.488: epoch 6:	0.00360527  	0.00866985  	0.00694137  
2023-05-22 19:27:16.488: Find a better model.
2023-05-22 19:27:37.188: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.696228]
2023-05-22 19:27:37.470: epoch 7:	0.00399763  	0.00910891  	0.00746109  
2023-05-22 19:27:37.470: Find a better model.
2023-05-22 19:27:58.194: [iter 8 : loss : 1.1348 = 0.6920 + 0.4428 + 0.0000, time: 20.720691]
2023-05-22 19:27:58.478: epoch 8:	0.00474533  	0.01205161  	0.00994894  
2023-05-22 19:27:58.479: Find a better model.
2023-05-22 19:28:19.186: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.704202]
2023-05-22 19:28:19.470: epoch 9:	0.00578174  	0.01523714  	0.01214505  
2023-05-22 19:28:19.470: Find a better model.
2023-05-22 19:28:40.208: [iter 10 : loss : 1.1345 = 0.6908 + 0.4437 + 0.0000, time: 20.733513]
2023-05-22 19:28:40.496: epoch 10:	0.00652204  	0.01742100  	0.01378160  
2023-05-22 19:28:40.496: Find a better model.
2023-05-22 19:29:01.221: [iter 11 : loss : 1.1339 = 0.6897 + 0.4442 + 0.0000, time: 20.719592]
2023-05-22 19:29:01.502: epoch 11:	0.00769170  	0.02049655  	0.01684774  
2023-05-22 19:29:01.502: Find a better model.
2023-05-22 19:29:22.127: [iter 12 : loss : 1.1326 = 0.6877 + 0.4449 + 0.0000, time: 20.621121]
2023-05-22 19:29:22.415: epoch 12:	0.00889098  	0.02425293  	0.02009249  
2023-05-22 19:29:22.415: Find a better model.
2023-05-22 19:29:42.952: [iter 13 : loss : 1.1306 = 0.6849 + 0.4456 + 0.0001, time: 20.532666]
2023-05-22 19:29:43.225: epoch 13:	0.01126737  	0.03151380  	0.02626256  
2023-05-22 19:29:43.225: Find a better model.
2023-05-22 19:30:03.721: [iter 14 : loss : 1.1269 = 0.6804 + 0.4464 + 0.0001, time: 20.492153]
2023-05-22 19:30:03.994: epoch 14:	0.01398432  	0.03771261  	0.03286085  
2023-05-22 19:30:03.995: Find a better model.
2023-05-22 19:30:24.584: [iter 15 : loss : 1.1200 = 0.6727 + 0.4472 + 0.0001, time: 20.586153]
2023-05-22 19:30:24.855: epoch 15:	0.01699742  	0.04625419  	0.03967194  
2023-05-22 19:30:24.855: Find a better model.
2023-05-22 19:30:45.198: [iter 16 : loss : 1.1079 = 0.6593 + 0.4483 + 0.0002, time: 20.337726]
2023-05-22 19:30:45.484: epoch 16:	0.02060280  	0.05547864  	0.04702040  
2023-05-22 19:30:45.484: Find a better model.
2023-05-22 19:31:05.937: [iter 17 : loss : 1.0877 = 0.6378 + 0.4496 + 0.0003, time: 20.449288]
2023-05-22 19:31:06.212: epoch 17:	0.02320874  	0.06239629  	0.05257006  
2023-05-22 19:31:06.212: Find a better model.
2023-05-22 19:31:26.568: [iter 18 : loss : 1.0567 = 0.6050 + 0.4514 + 0.0004, time: 20.352098]
2023-05-22 19:31:26.859: epoch 18:	0.02567403  	0.06936780  	0.05765370  
2023-05-22 19:31:26.859: Find a better model.
2023-05-22 19:31:47.190: [iter 19 : loss : 1.0160 = 0.5616 + 0.4538 + 0.0006, time: 20.327204]
2023-05-22 19:31:47.467: epoch 19:	0.02737677  	0.07365511  	0.06110712  
2023-05-22 19:31:47.467: Find a better model.
2023-05-22 19:32:08.094: [iter 20 : loss : 0.9683 = 0.5107 + 0.4568 + 0.0008, time: 20.623277]
2023-05-22 19:32:08.377: epoch 20:	0.02837624  	0.07647571  	0.06313876  
2023-05-22 19:32:08.377: Find a better model.
2023-05-22 19:32:28.881: [iter 21 : loss : 0.9188 = 0.4577 + 0.4599 + 0.0011, time: 20.499941]
2023-05-22 19:32:29.169: epoch 21:	0.02893887  	0.07763550  	0.06416548  
2023-05-22 19:32:29.169: Find a better model.
2023-05-22 19:32:49.518: [iter 22 : loss : 0.8705 = 0.4063 + 0.4627 + 0.0014, time: 20.344645]
2023-05-22 19:32:49.779: epoch 22:	0.02907215  	0.07857538  	0.06469098  
2023-05-22 19:32:49.779: Find a better model.
2023-05-22 19:33:10.282: [iter 23 : loss : 0.8280 = 0.3613 + 0.4650 + 0.0017, time: 20.499451]
2023-05-22 19:33:10.549: epoch 23:	0.02949413  	0.08004578  	0.06524350  
2023-05-22 19:33:10.549: Find a better model.
2023-05-22 19:33:30.900: [iter 24 : loss : 0.7913 = 0.3227 + 0.4665 + 0.0020, time: 20.347431]
2023-05-22 19:33:31.165: epoch 24:	0.02970143  	0.08072203  	0.06543304  
2023-05-22 19:33:31.165: Find a better model.
2023-05-22 19:33:51.649: [iter 25 : loss : 0.7599 = 0.2904 + 0.4671 + 0.0023, time: 20.479211]
2023-05-22 19:33:51.915: epoch 25:	0.02981989  	0.08117908  	0.06577230  
2023-05-22 19:33:51.915: Find a better model.
2023-05-22 19:34:12.293: [iter 26 : loss : 0.7341 = 0.2639 + 0.4676 + 0.0026, time: 20.374206]
2023-05-22 19:34:12.562: epoch 26:	0.02990132  	0.08118743  	0.06582128  
2023-05-22 19:34:12.562: Find a better model.
2023-05-22 19:34:33.101: [iter 27 : loss : 0.7117 = 0.2414 + 0.4674 + 0.0029, time: 20.536447]
2023-05-22 19:34:33.397: epoch 27:	0.02994573  	0.08157251  	0.06597508  
2023-05-22 19:34:33.397: Find a better model.
2023-05-22 19:34:53.842: [iter 28 : loss : 0.6929 = 0.2227 + 0.4671 + 0.0031, time: 20.441011]
2023-05-22 19:34:54.103: epoch 28:	0.03007158  	0.08192377  	0.06624452  
2023-05-22 19:34:54.103: Find a better model.
2023-05-22 19:35:14.646: [iter 29 : loss : 0.6765 = 0.2066 + 0.4666 + 0.0033, time: 20.538446]
2023-05-22 19:35:14.908: epoch 29:	0.03007159  	0.08222862  	0.06644748  
2023-05-22 19:35:14.909: Find a better model.
2023-05-22 19:35:35.623: [iter 30 : loss : 0.6617 = 0.1921 + 0.4660 + 0.0036, time: 20.711449]
2023-05-22 19:35:35.884: epoch 30:	0.03009380  	0.08223826  	0.06654714  
2023-05-22 19:35:35.884: Find a better model.
2023-05-22 19:35:56.608: [iter 31 : loss : 0.6493 = 0.1801 + 0.4654 + 0.0038, time: 20.720954]
2023-05-22 19:35:56.870: epoch 31:	0.03010859  	0.08222604  	0.06682393  
2023-05-22 19:36:17.472: [iter 32 : loss : 0.6381 = 0.1693 + 0.4648 + 0.0040, time: 20.598220]
2023-05-22 19:36:17.734: epoch 32:	0.03002716  	0.08172069  	0.06693700  
2023-05-22 19:36:38.424: [iter 33 : loss : 0.6286 = 0.1602 + 0.4642 + 0.0042, time: 20.686118]
2023-05-22 19:36:38.683: epoch 33:	0.02997534  	0.08203968  	0.06699423  
2023-05-22 19:36:59.216: [iter 34 : loss : 0.6200 = 0.1519 + 0.4636 + 0.0044, time: 20.529387]
2023-05-22 19:36:59.490: epoch 34:	0.03001235  	0.08186916  	0.06710468  
2023-05-22 19:37:20.017: [iter 35 : loss : 0.6110 = 0.1435 + 0.4628 + 0.0046, time: 20.523350]
2023-05-22 19:37:20.277: epoch 35:	0.02996793  	0.08150618  	0.06691610  
2023-05-22 19:37:40.968: [iter 36 : loss : 0.6041 = 0.1370 + 0.4623 + 0.0048, time: 20.686843]
2023-05-22 19:37:41.230: epoch 36:	0.02991612  	0.08164256  	0.06708068  
2023-05-22 19:38:01.805: [iter 37 : loss : 0.5979 = 0.1312 + 0.4618 + 0.0050, time: 20.572167]
2023-05-22 19:38:02.067: epoch 37:	0.02988650  	0.08152261  	0.06706960  
2023-05-22 19:38:22.571: [iter 38 : loss : 0.5913 = 0.1249 + 0.4612 + 0.0052, time: 20.501209]
2023-05-22 19:38:22.836: epoch 38:	0.02984947  	0.08103997  	0.06691658  
2023-05-22 19:38:43.375: [iter 39 : loss : 0.5864 = 0.1203 + 0.4608 + 0.0054, time: 20.534672]
2023-05-22 19:38:43.643: epoch 39:	0.02988647  	0.08100177  	0.06700721  
2023-05-22 19:39:04.190: [iter 40 : loss : 0.5811 = 0.1152 + 0.4603 + 0.0055, time: 20.541374]
2023-05-22 19:39:04.466: epoch 40:	0.02996792  	0.08131319  	0.06707507  
2023-05-22 19:39:24.963: [iter 41 : loss : 0.5758 = 0.1103 + 0.4598 + 0.0057, time: 20.493677]
2023-05-22 19:39:25.245: epoch 41:	0.02989388  	0.08094934  	0.06696233  
2023-05-22 19:39:45.768: [iter 42 : loss : 0.5719 = 0.1065 + 0.4595 + 0.0059, time: 20.518880]
2023-05-22 19:39:46.029: epoch 42:	0.02992349  	0.08109771  	0.06713178  
2023-05-22 19:40:06.393: [iter 43 : loss : 0.5676 = 0.1025 + 0.4591 + 0.0060, time: 20.359514]
2023-05-22 19:40:06.658: epoch 43:	0.02992349  	0.08115801  	0.06711945  
2023-05-22 19:40:27.137: [iter 44 : loss : 0.5648 = 0.0999 + 0.4588 + 0.0062, time: 20.476196]
2023-05-22 19:40:27.401: epoch 44:	0.02970880  	0.08036923  	0.06683318  
2023-05-22 19:40:47.911: [iter 45 : loss : 0.5609 = 0.0962 + 0.4583 + 0.0063, time: 20.505867]
2023-05-22 19:40:48.171: epoch 45:	0.02974581  	0.08042392  	0.06693749  
2023-05-22 19:41:08.535: [iter 46 : loss : 0.5577 = 0.0933 + 0.4579 + 0.0065, time: 20.360333]
2023-05-22 19:41:08.792: epoch 46:	0.02971620  	0.08009467  	0.06669461  
2023-05-22 19:41:29.311: [iter 47 : loss : 0.5547 = 0.0904 + 0.4577 + 0.0066, time: 20.513469]
2023-05-22 19:41:29.582: epoch 47:	0.02964217  	0.08006347  	0.06662461  
2023-05-22 19:41:50.147: [iter 48 : loss : 0.5520 = 0.0879 + 0.4573 + 0.0068, time: 20.561217]
2023-05-22 19:41:50.429: epoch 48:	0.02953852  	0.07993123  	0.06646430  
2023-05-22 19:42:10.873: [iter 49 : loss : 0.5492 = 0.0854 + 0.4569 + 0.0069, time: 20.440819]
2023-05-22 19:42:11.133: epoch 49:	0.02952372  	0.07948607  	0.06632369  
2023-05-22 19:42:31.794: [iter 50 : loss : 0.5469 = 0.0832 + 0.4567 + 0.0070, time: 20.656018]
2023-05-22 19:42:32.063: epoch 50:	0.02946450  	0.07952984  	0.06635293  
2023-05-22 19:42:52.913: [iter 51 : loss : 0.5441 = 0.0805 + 0.4564 + 0.0072, time: 20.844835]
2023-05-22 19:42:53.178: epoch 51:	0.02937566  	0.07895420  	0.06627368  
2023-05-22 19:43:22.333: my pid: 11596
2023-05-22 19:43:22.333: model: model.general_recommender.SGL
2023-05-22 19:43:22.333: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 19:43:22.333: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 19:43:26.522: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 19:43:47.952: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.430045]
2023-05-22 19:43:48.221: epoch 1:	0.00131774  	0.00242795  	0.00233262  
2023-05-22 19:43:48.221: Find a better model.
2023-05-22 19:44:08.937: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.713152]
2023-05-22 19:44:09.237: epoch 2:	0.00156204  	0.00333450  	0.00264277  
2023-05-22 19:44:09.237: Find a better model.
2023-05-22 19:44:30.146: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.903850]
2023-05-22 19:44:30.454: epoch 3:	0.00180634  	0.00386957  	0.00293777  
2023-05-22 19:44:30.454: Find a better model.
2023-05-22 19:44:51.293: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.836057]
2023-05-22 19:44:51.580: epoch 4:	0.00240598  	0.00513847  	0.00395316  
2023-05-22 19:44:51.580: Find a better model.
2023-05-22 19:45:12.456: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.872854]
2023-05-22 19:45:12.742: epoch 5:	0.00293900  	0.00687267  	0.00518459  
2023-05-22 19:45:12.742: Find a better model.
2023-05-22 19:45:33.483: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.737208]
2023-05-22 19:45:33.767: epoch 6:	0.00351644  	0.00856239  	0.00663031  
2023-05-22 19:45:33.767: Find a better model.
2023-05-22 19:45:54.454: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.683791]
2023-05-22 19:45:54.742: epoch 7:	0.00410868  	0.01029785  	0.00838286  
2023-05-22 19:45:54.742: Find a better model.
2023-05-22 19:46:15.490: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.744361]
2023-05-22 19:46:15.771: epoch 8:	0.00481196  	0.01237584  	0.00971302  
2023-05-22 19:46:15.771: Find a better model.
2023-05-22 19:46:36.445: [iter 9 : loss : 1.1346 = 0.6915 + 0.4432 + 0.0000, time: 20.669111]
2023-05-22 19:46:36.726: epoch 9:	0.00572992  	0.01453757  	0.01238015  
2023-05-22 19:46:36.726: Find a better model.
2023-05-22 19:46:57.265: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.535027]
2023-05-22 19:46:57.546: epoch 10:	0.00636657  	0.01692201  	0.01371782  
2023-05-22 19:46:57.546: Find a better model.
2023-05-22 19:47:18.175: [iter 11 : loss : 1.1338 = 0.6895 + 0.4443 + 0.0000, time: 20.625471]
2023-05-22 19:47:18.466: epoch 11:	0.00732155  	0.02074320  	0.01663111  
2023-05-22 19:47:18.467: Find a better model.
2023-05-22 19:47:39.200: [iter 12 : loss : 1.1325 = 0.6876 + 0.4449 + 0.0000, time: 20.730066]
2023-05-22 19:47:39.486: epoch 12:	0.00906124  	0.02541818  	0.02083098  
2023-05-22 19:47:39.486: Find a better model.
2023-05-22 19:48:00.000: [iter 13 : loss : 1.1304 = 0.6847 + 0.4456 + 0.0001, time: 20.509531]
2023-05-22 19:48:00.276: epoch 13:	0.01145245  	0.03252050  	0.02715228  
2023-05-22 19:48:00.276: Find a better model.
2023-05-22 19:48:20.800: [iter 14 : loss : 1.1264 = 0.6800 + 0.4463 + 0.0001, time: 20.521308]
2023-05-22 19:48:21.070: epoch 14:	0.01420642  	0.03924128  	0.03377682  
2023-05-22 19:48:21.070: Find a better model.
2023-05-22 19:48:41.771: [iter 15 : loss : 1.1195 = 0.6721 + 0.4473 + 0.0001, time: 20.695745]
2023-05-22 19:48:42.042: epoch 15:	0.01736017  	0.04736241  	0.04097360  
2023-05-22 19:48:42.042: Find a better model.
2023-05-22 19:49:02.590: [iter 16 : loss : 1.1070 = 0.6584 + 0.4484 + 0.0002, time: 20.542430]
2023-05-22 19:49:02.879: epoch 16:	0.02054356  	0.05539437  	0.04767757  
2023-05-22 19:49:02.879: Find a better model.
2023-05-22 19:49:23.403: [iter 17 : loss : 1.0863 = 0.6364 + 0.4497 + 0.0003, time: 20.520226]
2023-05-22 19:49:23.696: epoch 17:	0.02385283  	0.06437860  	0.05427709  
2023-05-22 19:49:23.696: Find a better model.
2023-05-22 19:49:44.170: [iter 18 : loss : 1.0549 = 0.6030 + 0.4515 + 0.0004, time: 20.467968]
2023-05-22 19:49:44.448: epoch 18:	0.02582953  	0.06972733  	0.05860988  
2023-05-22 19:49:44.448: Find a better model.
2023-05-22 19:50:04.997: [iter 19 : loss : 1.0139 = 0.5596 + 0.4538 + 0.0006, time: 20.541687]
2023-05-22 19:50:05.289: epoch 19:	0.02734719  	0.07364976  	0.06165273  
2023-05-22 19:50:05.289: Find a better model.
2023-05-22 19:50:25.797: [iter 20 : loss : 0.9664 = 0.5089 + 0.4567 + 0.0009, time: 20.503505]
2023-05-22 19:50:26.085: epoch 20:	0.02829478  	0.07582516  	0.06371582  
2023-05-22 19:50:26.085: Find a better model.
2023-05-22 19:50:46.732: [iter 21 : loss : 0.9167 = 0.4559 + 0.4597 + 0.0011, time: 20.642732]
2023-05-22 19:50:47.005: epoch 21:	0.02886482  	0.07743813  	0.06477242  
2023-05-22 19:50:47.005: Find a better model.
2023-05-22 19:51:07.573: [iter 22 : loss : 0.8690 = 0.4051 + 0.4625 + 0.0014, time: 20.564633]
2023-05-22 19:51:07.842: epoch 22:	0.02899808  	0.07808793  	0.06524369  
2023-05-22 19:51:07.842: Find a better model.
2023-05-22 19:51:28.343: [iter 23 : loss : 0.8269 = 0.3607 + 0.4645 + 0.0017, time: 20.496397]
2023-05-22 19:51:28.628: epoch 23:	0.02913133  	0.07883255  	0.06593737  
2023-05-22 19:51:28.628: Find a better model.
2023-05-22 19:51:49.161: [iter 24 : loss : 0.7907 = 0.3226 + 0.4660 + 0.0020, time: 20.529143]
2023-05-22 19:51:49.446: epoch 24:	0.02933122  	0.07987746  	0.06632210  
2023-05-22 19:51:49.446: Find a better model.
2023-05-22 19:52:09.711: [iter 25 : loss : 0.7593 = 0.2902 + 0.4668 + 0.0023, time: 20.260309]
2023-05-22 19:52:09.976: epoch 25:	0.02944228  	0.08031399  	0.06665850  
2023-05-22 19:52:09.976: Find a better model.
2023-05-22 19:52:30.317: [iter 26 : loss : 0.7333 = 0.2637 + 0.4670 + 0.0026, time: 20.337379]
2023-05-22 19:52:30.587: epoch 26:	0.02961257  	0.08100446  	0.06697118  
2023-05-22 19:52:30.587: Find a better model.
2023-05-22 19:52:51.128: [iter 27 : loss : 0.7112 = 0.2414 + 0.4670 + 0.0029, time: 20.532411]
2023-05-22 19:52:51.391: epoch 27:	0.02978284  	0.08134631  	0.06724942  
2023-05-22 19:52:51.392: Find a better model.
2023-05-22 19:53:11.713: [iter 28 : loss : 0.6924 = 0.2226 + 0.4668 + 0.0031, time: 20.317720]
2023-05-22 19:53:11.975: epoch 28:	0.02987170  	0.08188587  	0.06737516  
2023-05-22 19:53:11.975: Find a better model.
2023-05-22 19:53:32.458: [iter 29 : loss : 0.6760 = 0.2064 + 0.4662 + 0.0034, time: 20.478704]
2023-05-22 19:53:32.730: epoch 29:	0.03002715  	0.08190802  	0.06747998  
2023-05-22 19:53:32.730: Find a better model.
2023-05-22 19:53:53.497: [iter 30 : loss : 0.6610 = 0.1917 + 0.4657 + 0.0036, time: 20.762582]
2023-05-22 19:53:53.768: epoch 30:	0.02993091  	0.08219971  	0.06733464  
2023-05-22 19:53:53.768: Find a better model.
2023-05-22 19:54:14.654: [iter 31 : loss : 0.6484 = 0.1796 + 0.4651 + 0.0038, time: 20.881958]
2023-05-22 19:54:14.916: epoch 31:	0.02990130  	0.08229441  	0.06748623  
2023-05-22 19:54:14.917: Find a better model.
2023-05-22 19:54:35.470: [iter 32 : loss : 0.6373 = 0.1688 + 0.4645 + 0.0040, time: 20.549004]
2023-05-22 19:54:35.738: epoch 32:	0.02996053  	0.08212574  	0.06729329  
2023-05-22 19:54:56.347: [iter 33 : loss : 0.6279 = 0.1598 + 0.4638 + 0.0042, time: 20.604557]
2023-05-22 19:54:56.612: epoch 33:	0.03007898  	0.08261141  	0.06744651  
2023-05-22 19:54:56.612: Find a better model.
2023-05-22 19:55:17.292: [iter 34 : loss : 0.6187 = 0.1510 + 0.4633 + 0.0044, time: 20.667526]
2023-05-22 19:55:17.564: epoch 34:	0.03004195  	0.08228570  	0.06738868  
2023-05-22 19:55:38.267: [iter 35 : loss : 0.6102 = 0.1430 + 0.4625 + 0.0046, time: 20.699455]
2023-05-22 19:55:38.529: epoch 35:	0.03005677  	0.08202709  	0.06731859  
2023-05-22 19:55:59.041: [iter 36 : loss : 0.6034 = 0.1364 + 0.4621 + 0.0048, time: 20.508460]
2023-05-22 19:55:59.305: epoch 36:	0.03001975  	0.08185342  	0.06735771  
2023-05-22 19:56:19.817: [iter 37 : loss : 0.5972 = 0.1305 + 0.4617 + 0.0050, time: 20.508928]
2023-05-22 19:56:20.081: epoch 37:	0.02997532  	0.08179463  	0.06714788  
2023-05-22 19:56:40.500: [iter 38 : loss : 0.5911 = 0.1248 + 0.4611 + 0.0052, time: 20.414847]
2023-05-22 19:56:40.778: epoch 38:	0.02986428  	0.08121216  	0.06679854  
2023-05-22 19:57:01.594: [iter 39 : loss : 0.5856 = 0.1197 + 0.4605 + 0.0054, time: 20.812025]
2023-05-22 19:57:01.860: epoch 39:	0.03000494  	0.08184735  	0.06715021  
2023-05-22 19:57:22.522: [iter 40 : loss : 0.5801 = 0.1145 + 0.4601 + 0.0055, time: 20.657569]
2023-05-22 19:57:22.791: epoch 40:	0.02986428  	0.08148023  	0.06682666  
2023-05-22 19:57:43.534: [iter 41 : loss : 0.5754 = 0.1100 + 0.4597 + 0.0057, time: 20.740552]
2023-05-22 19:57:43.807: epoch 41:	0.02993831  	0.08162501  	0.06668968  
2023-05-22 19:58:05.130: [iter 42 : loss : 0.5712 = 0.1060 + 0.4593 + 0.0059, time: 21.318323]
2023-05-22 19:58:05.433: epoch 42:	0.02992350  	0.08120761  	0.06670229  
2023-05-22 19:58:26.264: [iter 43 : loss : 0.5674 = 0.1025 + 0.4589 + 0.0060, time: 20.828150]
2023-05-22 19:58:26.525: epoch 43:	0.02980505  	0.08043619  	0.06639665  
2023-05-22 19:58:46.830: [iter 44 : loss : 0.5644 = 0.0996 + 0.4585 + 0.0062, time: 20.299636]
2023-05-22 19:58:47.094: epoch 44:	0.02988648  	0.08066327  	0.06667725  
2023-05-22 19:59:07.612: [iter 45 : loss : 0.5601 = 0.0956 + 0.4581 + 0.0063, time: 20.513960]
2023-05-22 19:59:07.879: epoch 45:	0.02991609  	0.08074880  	0.06671857  
2023-05-22 19:59:28.582: [iter 46 : loss : 0.5570 = 0.0927 + 0.4578 + 0.0065, time: 20.699907]
2023-05-22 19:59:28.866: epoch 46:	0.02984947  	0.08043936  	0.06653030  
2023-05-22 19:59:49.592: [iter 47 : loss : 0.5540 = 0.0900 + 0.4575 + 0.0066, time: 20.721877]
2023-05-22 19:59:49.881: epoch 47:	0.02987909  	0.08050106  	0.06656010  
2023-05-22 20:00:11.275: [iter 48 : loss : 0.5512 = 0.0872 + 0.4572 + 0.0068, time: 21.389793]
2023-05-22 20:00:11.551: epoch 48:	0.02984207  	0.08037228  	0.06640504  
2023-05-22 20:00:32.722: [iter 49 : loss : 0.5489 = 0.0850 + 0.4569 + 0.0069, time: 21.168500]
2023-05-22 20:00:32.986: epoch 49:	0.02976063  	0.08001801  	0.06629030  
2023-05-22 20:00:53.684: [iter 50 : loss : 0.5463 = 0.0827 + 0.4566 + 0.0070, time: 20.695076]
2023-05-22 20:00:53.953: epoch 50:	0.02971620  	0.07968374  	0.06613656  
2023-05-22 20:01:16.882: [iter 51 : loss : 0.5434 = 0.0799 + 0.4563 + 0.0072, time: 22.923306]
2023-05-22 20:01:17.224: epoch 51:	0.02959775  	0.07936587  	0.06585976  
2023-05-22 20:02:30.770: my pid: 9536
2023-05-22 20:02:30.770: model: model.general_recommender.SGL
2023-05-22 20:02:30.770: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 20:02:30.770: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 20:02:35.181: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 20:02:58.378: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 23.194011]
2023-05-22 20:02:58.726: epoch 1:	0.00133995  	0.00249983  	0.00205213  
2023-05-22 20:02:58.726: Find a better model.
2023-05-22 20:03:21.861: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 23.118940]
2023-05-22 20:03:22.223: epoch 2:	0.00156944  	0.00332863  	0.00271238  
2023-05-22 20:03:22.223: Find a better model.
2023-05-22 20:03:45.488: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 23.262196]
2023-05-22 20:03:45.821: epoch 3:	0.00198401  	0.00380747  	0.00318770  
2023-05-22 20:03:45.821: Find a better model.
2023-05-22 20:04:08.953: [iter 4 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 23.125451]
2023-05-22 20:04:09.327: epoch 4:	0.00212467  	0.00454685  	0.00347636  
2023-05-22 20:04:09.327: Find a better model.
2023-05-22 20:04:32.322: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 22.990643]
2023-05-22 20:04:32.693: epoch 5:	0.00270951  	0.00660625  	0.00476484  
2023-05-22 20:04:32.693: Find a better model.
2023-05-22 20:04:55.971: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 23.274426]
2023-05-22 20:04:56.281: epoch 6:	0.00327954  	0.00789092  	0.00620870  
2023-05-22 20:04:56.281: Find a better model.
2023-05-22 20:05:19.413: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 23.124699]
2023-05-22 20:05:19.780: epoch 7:	0.00403465  	0.01053092  	0.00753083  
2023-05-22 20:05:19.780: Find a better model.
2023-05-22 20:05:42.821: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 23.036916]
2023-05-22 20:05:43.161: epoch 8:	0.00464169  	0.01236612  	0.00964598  
2023-05-22 20:05:43.161: Find a better model.
2023-05-22 20:06:06.426: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 23.260546]
2023-05-22 20:06:06.751: epoch 9:	0.00558186  	0.01595034  	0.01257916  
2023-05-22 20:06:06.751: Find a better model.
2023-05-22 20:06:29.769: [iter 10 : loss : 1.1345 = 0.6908 + 0.4436 + 0.0000, time: 23.012298]
2023-05-22 20:06:30.134: epoch 10:	0.00638878  	0.01751433  	0.01371764  
2023-05-22 20:06:30.134: Find a better model.
2023-05-22 20:06:53.080: [iter 11 : loss : 1.1339 = 0.6896 + 0.4443 + 0.0000, time: 22.939400]
2023-05-22 20:06:53.435: epoch 11:	0.00712168  	0.01992528  	0.01615268  
2023-05-22 20:06:53.435: Find a better model.
2023-05-22 20:07:16.365: [iter 12 : loss : 1.1327 = 0.6877 + 0.4449 + 0.0000, time: 22.921782]
2023-05-22 20:07:16.694: epoch 12:	0.00865408  	0.02450178  	0.02047178  
2023-05-22 20:07:16.694: Find a better model.
2023-05-22 20:07:39.713: [iter 13 : loss : 1.1306 = 0.6849 + 0.4456 + 0.0001, time: 23.014166]
2023-05-22 20:07:40.026: epoch 13:	0.01125997  	0.03137514  	0.02630108  
2023-05-22 20:07:40.026: Find a better model.
2023-05-22 20:08:02.965: [iter 14 : loss : 1.1268 = 0.6804 + 0.4464 + 0.0001, time: 22.934056]
2023-05-22 20:08:03.269: epoch 14:	0.01409537  	0.03858224  	0.03362911  
2023-05-22 20:08:03.270: Find a better model.
2023-05-22 20:08:26.260: [iter 15 : loss : 1.1199 = 0.6726 + 0.4472 + 0.0001, time: 22.985483]
2023-05-22 20:08:26.550: epoch 15:	0.01753044  	0.04668846  	0.04129845  
2023-05-22 20:08:26.550: Find a better model.
2023-05-22 20:08:49.656: [iter 16 : loss : 1.1075 = 0.6591 + 0.4482 + 0.0002, time: 23.102862]
2023-05-22 20:08:49.956: epoch 16:	0.02077305  	0.05507767  	0.04850052  
2023-05-22 20:08:49.956: Find a better model.
2023-05-22 20:09:12.968: [iter 17 : loss : 1.0873 = 0.6374 + 0.4496 + 0.0003, time: 23.006188]
2023-05-22 20:09:13.259: epoch 17:	0.02371215  	0.06294990  	0.05477708  
2023-05-22 20:09:13.259: Find a better model.
2023-05-22 20:09:36.348: [iter 18 : loss : 1.0561 = 0.6042 + 0.4515 + 0.0004, time: 23.085299]
2023-05-22 20:09:36.650: epoch 18:	0.02545932  	0.06779835  	0.05823893  
2023-05-22 20:09:36.650: Find a better model.
2023-05-22 20:09:59.489: [iter 19 : loss : 1.0153 = 0.5608 + 0.4539 + 0.0006, time: 22.832458]
2023-05-22 20:09:59.826: epoch 19:	0.02725090  	0.07236109  	0.06178594  
2023-05-22 20:09:59.826: Find a better model.
2023-05-22 20:10:22.709: [iter 20 : loss : 0.9674 = 0.5097 + 0.4568 + 0.0009, time: 22.872984]
2023-05-22 20:10:23.009: epoch 20:	0.02838361  	0.07620484  	0.06384674  
2023-05-22 20:10:23.009: Find a better model.
2023-05-22 20:10:43.742: [iter 21 : loss : 0.9176 = 0.4563 + 0.4601 + 0.0011, time: 20.727020]
2023-05-22 20:10:44.012: epoch 21:	0.02889444  	0.07820103  	0.06477755  
2023-05-22 20:10:44.013: Find a better model.
2023-05-22 20:11:04.583: [iter 22 : loss : 0.8695 = 0.4052 + 0.4629 + 0.0014, time: 20.566863]
2023-05-22 20:11:04.856: epoch 22:	0.02912394  	0.07929838  	0.06519090  
2023-05-22 20:11:04.856: Find a better model.
2023-05-22 20:11:25.535: [iter 23 : loss : 0.8273 = 0.3605 + 0.4651 + 0.0017, time: 20.674272]
2023-05-22 20:11:25.811: epoch 23:	0.02941266  	0.08097769  	0.06564715  
2023-05-22 20:11:25.811: Find a better model.
2023-05-22 20:11:46.693: [iter 24 : loss : 0.7908 = 0.3222 + 0.4666 + 0.0020, time: 20.878825]
2023-05-22 20:11:46.960: epoch 24:	0.02959034  	0.08139867  	0.06599478  
2023-05-22 20:11:46.960: Find a better model.
2023-05-22 20:12:07.514: [iter 25 : loss : 0.7596 = 0.2901 + 0.4672 + 0.0023, time: 20.550241]
2023-05-22 20:12:07.805: epoch 25:	0.02958293  	0.08146406  	0.06613944  
2023-05-22 20:12:07.805: Find a better model.
2023-05-22 20:12:28.505: [iter 26 : loss : 0.7337 = 0.2637 + 0.4674 + 0.0026, time: 20.697239]
2023-05-22 20:12:28.798: epoch 26:	0.02962736  	0.08161125  	0.06633583  
2023-05-22 20:12:28.798: Find a better model.
2023-05-22 20:12:49.500: [iter 27 : loss : 0.7112 = 0.2411 + 0.4673 + 0.0029, time: 20.697671]
2023-05-22 20:12:49.785: epoch 27:	0.02968658  	0.08157439  	0.06647003  
2023-05-22 20:13:10.453: [iter 28 : loss : 0.6924 = 0.2224 + 0.4669 + 0.0031, time: 20.663794]
2023-05-22 20:13:10.715: epoch 28:	0.02979763  	0.08198274  	0.06660625  
2023-05-22 20:13:10.715: Find a better model.
2023-05-22 20:13:31.437: [iter 29 : loss : 0.6763 = 0.2065 + 0.4664 + 0.0034, time: 20.717570]
2023-05-22 20:13:31.699: epoch 29:	0.02987907  	0.08206608  	0.06673861  
2023-05-22 20:13:31.699: Find a better model.
2023-05-22 20:13:52.674: [iter 30 : loss : 0.6613 = 0.1919 + 0.4658 + 0.0036, time: 20.972546]
2023-05-22 20:13:52.941: epoch 30:	0.03001232  	0.08261039  	0.06695572  
2023-05-22 20:13:52.941: Find a better model.
2023-05-22 20:14:13.705: [iter 31 : loss : 0.6485 = 0.1795 + 0.4652 + 0.0038, time: 20.760329]
2023-05-22 20:14:13.971: epoch 31:	0.03001973  	0.08254951  	0.06707975  
2023-05-22 20:14:34.843: [iter 32 : loss : 0.6378 = 0.1692 + 0.4646 + 0.0040, time: 20.868217]
2023-05-22 20:14:35.104: epoch 32:	0.03003454  	0.08285133  	0.06730366  
2023-05-22 20:14:35.104: Find a better model.
2023-05-22 20:14:55.863: [iter 33 : loss : 0.6283 = 0.1602 + 0.4639 + 0.0042, time: 20.755037]
2023-05-22 20:14:56.140: epoch 33:	0.03001233  	0.08267712  	0.06730264  
2023-05-22 20:15:16.893: [iter 34 : loss : 0.6194 = 0.1516 + 0.4633 + 0.0044, time: 20.749154]
2023-05-22 20:15:17.154: epoch 34:	0.03011598  	0.08288706  	0.06742891  
2023-05-22 20:15:17.154: Find a better model.
2023-05-22 20:15:37.864: [iter 35 : loss : 0.6105 = 0.1433 + 0.4626 + 0.0046, time: 20.707408]
2023-05-22 20:15:38.129: epoch 35:	0.03015299  	0.08309163  	0.06766775  
2023-05-22 20:15:38.129: Find a better model.
2023-05-22 20:15:59.008: [iter 36 : loss : 0.6037 = 0.1368 + 0.4620 + 0.0048, time: 20.875984]
2023-05-22 20:15:59.269: epoch 36:	0.03024182  	0.08321867  	0.06779909  
2023-05-22 20:15:59.269: Find a better model.
2023-05-22 20:16:19.822: [iter 37 : loss : 0.5974 = 0.1308 + 0.4616 + 0.0050, time: 20.549255]
2023-05-22 20:16:20.085: epoch 37:	0.03022702  	0.08290069  	0.06767976  
2023-05-22 20:16:40.863: [iter 38 : loss : 0.5914 = 0.1252 + 0.4610 + 0.0052, time: 20.773592]
2023-05-22 20:16:41.124: epoch 38:	0.03027143  	0.08258621  	0.06777747  
2023-05-22 20:17:01.836: [iter 39 : loss : 0.5862 = 0.1204 + 0.4604 + 0.0054, time: 20.708308]
2023-05-22 20:17:02.096: epoch 39:	0.03022702  	0.08268244  	0.06774643  
2023-05-22 20:17:22.768: [iter 40 : loss : 0.5804 = 0.1149 + 0.4599 + 0.0055, time: 20.668211]
2023-05-22 20:17:23.033: epoch 40:	0.03027884  	0.08290530  	0.06775016  
2023-05-22 20:17:43.774: [iter 41 : loss : 0.5756 = 0.1103 + 0.4596 + 0.0057, time: 20.735872]
2023-05-22 20:17:44.042: epoch 41:	0.03017520  	0.08245602  	0.06747602  
2023-05-22 20:18:04.852: [iter 42 : loss : 0.5713 = 0.1062 + 0.4592 + 0.0059, time: 20.805483]
2023-05-22 20:18:05.113: epoch 42:	0.03021963  	0.08273402  	0.06759481  
2023-05-22 20:18:25.611: [iter 43 : loss : 0.5673 = 0.1024 + 0.4588 + 0.0060, time: 20.494116]
2023-05-22 20:18:25.891: epoch 43:	0.03019001  	0.08280439  	0.06760264  
2023-05-22 20:18:46.534: [iter 44 : loss : 0.5645 = 0.0998 + 0.4585 + 0.0062, time: 20.638382]
2023-05-22 20:18:46.811: epoch 44:	0.03008636  	0.08223964  	0.06739238  
2023-05-22 20:19:07.552: [iter 45 : loss : 0.5606 = 0.0961 + 0.4581 + 0.0063, time: 20.736716]
2023-05-22 20:19:07.815: epoch 45:	0.02996790  	0.08217018  	0.06728571  
2023-05-22 20:19:28.392: [iter 46 : loss : 0.5573 = 0.0930 + 0.4578 + 0.0065, time: 20.572179]
2023-05-22 20:19:28.669: epoch 46:	0.02988645  	0.08176097  	0.06712652  
2023-05-22 20:19:49.520: [iter 47 : loss : 0.5539 = 0.0899 + 0.4574 + 0.0066, time: 20.847070]
2023-05-22 20:19:49.782: epoch 47:	0.02981983  	0.08171494  	0.06705363  
2023-05-22 20:20:10.507: [iter 48 : loss : 0.5517 = 0.0878 + 0.4571 + 0.0068, time: 20.720476]
2023-05-22 20:20:10.770: epoch 48:	0.02981983  	0.08197374  	0.06695955  
2023-05-22 20:20:31.347: [iter 49 : loss : 0.5488 = 0.0851 + 0.4568 + 0.0069, time: 20.571908]
2023-05-22 20:20:31.605: epoch 49:	0.02977542  	0.08173377  	0.06693019  
2023-05-22 20:20:52.157: [iter 50 : loss : 0.5468 = 0.0832 + 0.4566 + 0.0070, time: 20.548036]
2023-05-22 20:20:52.417: epoch 50:	0.02958294  	0.08137570  	0.06670432  
2023-05-22 20:21:13.153: [iter 51 : loss : 0.5438 = 0.0804 + 0.4562 + 0.0072, time: 20.732524]
2023-05-22 20:21:13.430: epoch 51:	0.02956072  	0.08126689  	0.06671219  
2023-05-22 20:21:33.918: [iter 52 : loss : 0.5412 = 0.0779 + 0.4560 + 0.0073, time: 20.484259]
2023-05-22 20:21:34.188: epoch 52:	0.02950891  	0.08099043  	0.06665856  
2023-05-22 20:21:54.708: [iter 53 : loss : 0.5404 = 0.0771 + 0.4559 + 0.0074, time: 20.517442]
2023-05-22 20:21:54.971: epoch 53:	0.02942747  	0.08059842  	0.06640368  
2023-05-22 20:22:15.737: [iter 54 : loss : 0.5381 = 0.0749 + 0.4556 + 0.0076, time: 20.762120]
2023-05-22 20:22:15.996: epoch 54:	0.02933122  	0.08032009  	0.06622035  
2023-05-22 20:22:36.544: [iter 55 : loss : 0.5360 = 0.0729 + 0.4554 + 0.0077, time: 20.539243]
2023-05-22 20:22:36.807: epoch 55:	0.02924978  	0.07983817  	0.06614538  
2023-05-22 20:22:57.441: [iter 56 : loss : 0.5337 = 0.0708 + 0.4551 + 0.0078, time: 20.630895]
2023-05-22 20:22:57.699: epoch 56:	0.02927939  	0.07968164  	0.06599390  
2023-05-22 20:23:18.462: [iter 57 : loss : 0.5319 = 0.0690 + 0.4549 + 0.0079, time: 20.758655]
2023-05-22 20:23:18.724: epoch 57:	0.02916094  	0.07920083  	0.06576557  
2023-05-22 20:23:39.279: [iter 58 : loss : 0.5307 = 0.0679 + 0.4548 + 0.0080, time: 20.550956]
2023-05-22 20:23:39.542: epoch 58:	0.02910171  	0.07893556  	0.06553197  
2023-05-22 20:24:00.270: [iter 59 : loss : 0.5292 = 0.0664 + 0.4546 + 0.0082, time: 20.723069]
2023-05-22 20:24:00.530: epoch 59:	0.02903509  	0.07897240  	0.06545459  
2023-05-22 20:24:21.267: [iter 60 : loss : 0.5275 = 0.0648 + 0.4544 + 0.0083, time: 20.733257]
2023-05-22 20:24:21.543: epoch 60:	0.02903509  	0.07880296  	0.06538606  
2023-05-22 20:24:42.029: [iter 61 : loss : 0.5262 = 0.0636 + 0.4542 + 0.0084, time: 20.482083]
2023-05-22 20:24:42.295: epoch 61:	0.02902768  	0.07845451  	0.06527838  
2023-05-22 20:24:42.296: Early stopping is trigger at epoch: 61
2023-05-22 20:24:42.296: best_result@epoch 36:

2023-05-22 20:24:42.296: 		0.0302      	0.0832      	0.0678      
2023-05-22 20:25:26.138: my pid: 3544
2023-05-22 20:25:26.138: model: model.general_recommender.SGL
2023-05-22 20:25:26.138: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 20:25:26.138: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 20:25:30.275: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 20:25:51.668: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.392419]
2023-05-22 20:25:51.946: epoch 1:	0.00126592  	0.00242933  	0.00193951  
2023-05-22 20:25:51.947: Find a better model.
2023-05-22 20:26:13.235: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.284999]
2023-05-22 20:26:13.523: epoch 2:	0.00162867  	0.00376295  	0.00284299  
2023-05-22 20:26:13.523: Find a better model.
2023-05-22 20:26:34.658: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.132550]
2023-05-22 20:26:34.960: epoch 3:	0.00196180  	0.00448745  	0.00355784  
2023-05-22 20:26:34.960: Find a better model.
2023-05-22 20:26:56.214: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.251136]
2023-05-22 20:26:56.527: epoch 4:	0.00234676  	0.00535741  	0.00410073  
2023-05-22 20:26:56.527: Find a better model.
2023-05-22 20:27:17.631: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.100799]
2023-05-22 20:27:17.937: epoch 5:	0.00273912  	0.00565906  	0.00465561  
2023-05-22 20:27:17.937: Find a better model.
2023-05-22 20:27:39.248: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 21.307868]
2023-05-22 20:27:39.555: epoch 6:	0.00350163  	0.00863840  	0.00635388  
2023-05-22 20:27:39.555: Find a better model.
2023-05-22 20:28:00.798: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.238708]
2023-05-22 20:28:01.093: epoch 7:	0.00416790  	0.01100472  	0.00856816  
2023-05-22 20:28:01.093: Find a better model.
2023-05-22 20:28:22.232: [iter 8 : loss : 1.1347 = 0.6920 + 0.4426 + 0.0000, time: 21.135660]
2023-05-22 20:28:22.520: epoch 8:	0.00476013  	0.01211858  	0.00979107  
2023-05-22 20:28:22.520: Find a better model.
2023-05-22 20:28:43.777: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 21.250549]
2023-05-22 20:28:44.077: epoch 9:	0.00570771  	0.01576132  	0.01237991  
2023-05-22 20:28:44.078: Find a better model.
2023-05-22 20:29:05.190: [iter 10 : loss : 1.1345 = 0.6909 + 0.4435 + 0.0000, time: 21.108273]
2023-05-22 20:29:05.476: epoch 10:	0.00670711  	0.01905427  	0.01466141  
2023-05-22 20:29:05.476: Find a better model.
2023-05-22 20:29:26.602: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 21.121798]
2023-05-22 20:29:26.892: epoch 11:	0.00822471  	0.02334908  	0.01879322  
2023-05-22 20:29:26.892: Find a better model.
2023-05-22 20:29:47.993: [iter 12 : loss : 1.1328 = 0.6881 + 0.4447 + 0.0000, time: 21.096982]
2023-05-22 20:29:48.278: epoch 12:	0.00957945  	0.02768793  	0.02267268  
2023-05-22 20:29:48.278: Find a better model.
2023-05-22 20:30:09.353: [iter 13 : loss : 1.1308 = 0.6853 + 0.4454 + 0.0001, time: 21.071502]
2023-05-22 20:30:09.637: epoch 13:	0.01177818  	0.03299827  	0.02865282  
2023-05-22 20:30:09.638: Find a better model.
2023-05-22 20:30:30.746: [iter 14 : loss : 1.1272 = 0.6809 + 0.4462 + 0.0001, time: 21.104424]
2023-05-22 20:30:31.054: epoch 14:	0.01446552  	0.03951278  	0.03493802  
2023-05-22 20:30:31.054: Find a better model.
2023-05-22 20:30:52.129: [iter 15 : loss : 1.1207 = 0.6736 + 0.4470 + 0.0001, time: 21.071540]
2023-05-22 20:30:52.407: epoch 15:	0.01796722  	0.04793265  	0.04212954  
2023-05-22 20:30:52.408: Find a better model.
2023-05-22 20:31:13.318: [iter 16 : loss : 1.1091 = 0.6608 + 0.4481 + 0.0002, time: 20.906805]
2023-05-22 20:31:13.600: epoch 16:	0.02109881  	0.05673258  	0.04936575  
2023-05-22 20:31:13.600: Find a better model.
2023-05-22 20:31:34.549: [iter 17 : loss : 1.0898 = 0.6401 + 0.4494 + 0.0003, time: 20.945286]
2023-05-22 20:31:34.827: epoch 17:	0.02425259  	0.06381196  	0.05547591  
2023-05-22 20:31:34.827: Find a better model.
2023-05-22 20:31:55.913: [iter 18 : loss : 1.0598 = 0.6083 + 0.4512 + 0.0004, time: 21.082487]
2023-05-22 20:31:56.196: epoch 18:	0.02648098  	0.06941108  	0.05993005  
2023-05-22 20:31:56.196: Find a better model.
2023-05-22 20:32:17.123: [iter 19 : loss : 1.0198 = 0.5660 + 0.4533 + 0.0006, time: 20.922395]
2023-05-22 20:32:17.400: epoch 19:	0.02797643  	0.07446957  	0.06333368  
2023-05-22 20:32:17.400: Find a better model.
2023-05-22 20:32:38.294: [iter 20 : loss : 0.9727 = 0.5158 + 0.4561 + 0.0008, time: 20.888611]
2023-05-22 20:32:38.569: epoch 20:	0.02870196  	0.07649041  	0.06474438  
2023-05-22 20:32:38.569: Find a better model.
2023-05-22 20:32:59.480: [iter 21 : loss : 0.9231 = 0.4627 + 0.4593 + 0.0011, time: 20.906629]
2023-05-22 20:32:59.754: epoch 21:	0.02920537  	0.07802638  	0.06536470  
2023-05-22 20:32:59.754: Find a better model.
2023-05-22 20:33:20.722: [iter 22 : loss : 0.8748 = 0.4112 + 0.4622 + 0.0014, time: 20.963353]
2023-05-22 20:33:21.011: epoch 22:	0.02942007  	0.07892337  	0.06553536  
2023-05-22 20:33:21.011: Find a better model.
2023-05-22 20:33:41.878: [iter 23 : loss : 0.8317 = 0.3656 + 0.4645 + 0.0017, time: 20.862764]
2023-05-22 20:33:42.151: epoch 23:	0.02978282  	0.08005997  	0.06576282  
2023-05-22 20:33:42.152: Find a better model.
2023-05-22 20:34:02.889: [iter 24 : loss : 0.7945 = 0.3266 + 0.4659 + 0.0020, time: 20.732538]
2023-05-22 20:34:03.158: epoch 24:	0.02999752  	0.08094948  	0.06627631  
2023-05-22 20:34:03.158: Find a better model.
2023-05-22 20:34:23.890: [iter 25 : loss : 0.7627 = 0.2937 + 0.4668 + 0.0023, time: 20.727869]
2023-05-22 20:34:24.161: epoch 25:	0.02997532  	0.08143483  	0.06652429  
2023-05-22 20:34:24.161: Find a better model.
2023-05-22 20:34:45.058: [iter 26 : loss : 0.7362 = 0.2666 + 0.4671 + 0.0026, time: 20.893648]
2023-05-22 20:34:45.324: epoch 26:	0.03005675  	0.08217093  	0.06655820  
2023-05-22 20:34:45.324: Find a better model.
2023-05-22 20:35:06.037: [iter 27 : loss : 0.7133 = 0.2434 + 0.4670 + 0.0028, time: 20.709608]
2023-05-22 20:35:06.304: epoch 27:	0.02999752  	0.08205234  	0.06638386  
2023-05-22 20:35:27.237: [iter 28 : loss : 0.6940 = 0.2242 + 0.4667 + 0.0031, time: 20.928275]
2023-05-22 20:35:27.502: epoch 28:	0.02999013  	0.08214220  	0.06646425  
2023-05-22 20:35:48.270: [iter 29 : loss : 0.6775 = 0.2080 + 0.4661 + 0.0033, time: 20.764874]
2023-05-22 20:35:48.545: epoch 29:	0.03002714  	0.08192627  	0.06643667  
2023-05-22 20:36:09.229: [iter 30 : loss : 0.6625 = 0.1933 + 0.4656 + 0.0036, time: 20.679991]
2023-05-22 20:36:09.494: epoch 30:	0.02989388  	0.08177399  	0.06646057  
2023-05-22 20:36:30.257: [iter 31 : loss : 0.6495 = 0.1807 + 0.4650 + 0.0038, time: 20.759223]
2023-05-22 20:36:30.521: epoch 31:	0.02990129  	0.08177520  	0.06651523  
2023-05-22 20:36:51.442: [iter 32 : loss : 0.6385 = 0.1702 + 0.4644 + 0.0040, time: 20.917160]
2023-05-22 20:36:51.729: epoch 32:	0.02998273  	0.08219513  	0.06676714  
2023-05-22 20:36:51.729: Find a better model.
2023-05-22 20:37:12.789: [iter 33 : loss : 0.6290 = 0.1611 + 0.4636 + 0.0042, time: 21.057187]
2023-05-22 20:37:13.066: epoch 33:	0.03004936  	0.08199038  	0.06689913  
2023-05-22 20:37:34.017: [iter 34 : loss : 0.6198 = 0.1522 + 0.4632 + 0.0044, time: 20.946933]
2023-05-22 20:37:34.279: epoch 34:	0.03009378  	0.08215985  	0.06695711  
2023-05-22 20:37:55.201: [iter 35 : loss : 0.6112 = 0.1441 + 0.4625 + 0.0046, time: 20.917746]
2023-05-22 20:37:55.465: epoch 35:	0.02991611  	0.08170157  	0.06687630  
2023-05-22 20:38:16.191: [iter 36 : loss : 0.6038 = 0.1371 + 0.4619 + 0.0048, time: 20.722350]
2023-05-22 20:38:16.453: epoch 36:	0.02993832  	0.08183453  	0.06698912  
2023-05-22 20:38:37.206: [iter 37 : loss : 0.5977 = 0.1313 + 0.4614 + 0.0050, time: 20.748626]
2023-05-22 20:38:37.470: epoch 37:	0.03001975  	0.08173795  	0.06712662  
2023-05-22 20:38:58.570: [iter 38 : loss : 0.5916 = 0.1255 + 0.4609 + 0.0052, time: 21.096991]
2023-05-22 20:38:58.832: epoch 38:	0.02990870  	0.08160684  	0.06712771  
2023-05-22 20:39:19.796: [iter 39 : loss : 0.5861 = 0.1204 + 0.4604 + 0.0053, time: 20.959802]
2023-05-22 20:39:20.076: epoch 39:	0.02990869  	0.08141138  	0.06719579  
2023-05-22 20:39:40.984: [iter 40 : loss : 0.5810 = 0.1155 + 0.4599 + 0.0055, time: 20.905008]
2023-05-22 20:39:41.254: epoch 40:	0.02998273  	0.08166006  	0.06746594  
2023-05-22 20:40:02.190: [iter 41 : loss : 0.5758 = 0.1106 + 0.4595 + 0.0057, time: 20.932517]
2023-05-22 20:40:02.458: epoch 41:	0.02993089  	0.08140480  	0.06743825  
2023-05-22 20:43:16.271: my pid: 736
2023-05-22 20:43:16.271: model: model.general_recommender.SGL
2023-05-22 20:43:16.271: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 20:43:16.272: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 20:43:20.423: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 20:43:41.287: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.863154]
2023-05-22 20:43:41.554: epoch 1:	0.00171010  	0.00312880  	0.00282151  
2023-05-22 20:43:41.554: Find a better model.
2023-05-22 20:44:02.506: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.948818]
2023-05-22 20:44:02.800: epoch 2:	0.00214688  	0.00362482  	0.00319392  
2023-05-22 20:44:02.800: Find a better model.
2023-05-22 20:44:23.627: [iter 3 : loss : 1.1343 = 0.6929 + 0.4414 + 0.0000, time: 20.824385]
2023-05-22 20:44:23.913: epoch 3:	0.00240598  	0.00466314  	0.00393167  
2023-05-22 20:44:23.913: Find a better model.
2023-05-22 20:44:44.786: [iter 4 : loss : 1.1345 = 0.6928 + 0.4417 + 0.0000, time: 20.868809]
2023-05-22 20:44:45.084: epoch 4:	0.00302043  	0.00655564  	0.00544582  
2023-05-22 20:44:45.084: Find a better model.
2023-05-22 20:45:05.616: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.528104]
2023-05-22 20:45:05.901: epoch 5:	0.00389399  	0.00857605  	0.00729151  
2023-05-22 20:45:05.901: Find a better model.
2023-05-22 20:45:26.552: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.647629]
2023-05-22 20:45:26.837: epoch 6:	0.00434557  	0.01010339  	0.00794818  
2023-05-22 20:45:26.837: Find a better model.
2023-05-22 20:45:47.020: [iter 7 : loss : 1.1348 = 0.6921 + 0.4427 + 0.0000, time: 20.179418]
2023-05-22 20:45:47.300: epoch 7:	0.00495261  	0.01209625  	0.00996008  
2023-05-22 20:45:47.300: Find a better model.
2023-05-22 20:46:07.579: [iter 8 : loss : 1.1348 = 0.6917 + 0.4431 + 0.0000, time: 20.275570]
2023-05-22 20:46:07.857: epoch 8:	0.00575213  	0.01463003  	0.01205241  
2023-05-22 20:46:07.857: Find a better model.
2023-05-22 20:46:28.150: [iter 9 : loss : 1.1346 = 0.6911 + 0.4435 + 0.0000, time: 20.288887]
2023-05-22 20:46:28.424: epoch 9:	0.00663308  	0.01719261  	0.01462771  
2023-05-22 20:46:28.424: Find a better model.
2023-05-22 20:46:48.807: [iter 10 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 20.378327]
2023-05-22 20:46:49.099: epoch 10:	0.00729935  	0.01860256  	0.01568677  
2023-05-22 20:46:49.099: Find a better model.
2023-05-22 20:47:09.209: [iter 11 : loss : 1.1331 = 0.6883 + 0.4447 + 0.0000, time: 20.105644]
2023-05-22 20:47:09.487: epoch 11:	0.00902424  	0.02287667  	0.01939091  
2023-05-22 20:47:09.487: Find a better model.
2023-05-22 20:47:29.548: [iter 12 : loss : 1.1314 = 0.6860 + 0.4454 + 0.0000, time: 20.056563]
2023-05-22 20:47:29.820: epoch 12:	0.01148946  	0.03011250  	0.02540468  
2023-05-22 20:47:29.820: Find a better model.
2023-05-22 20:47:49.747: [iter 13 : loss : 1.1285 = 0.6823 + 0.4461 + 0.0001, time: 19.923269]
2023-05-22 20:47:50.017: epoch 13:	0.01437669  	0.03818003  	0.03283093  
2023-05-22 20:47:50.017: Find a better model.
2023-05-22 20:48:09.985: [iter 14 : loss : 1.1230 = 0.6760 + 0.4469 + 0.0001, time: 19.964300]
2023-05-22 20:48:10.262: epoch 14:	0.01731576  	0.04537597  	0.03992269  
2023-05-22 20:48:10.262: Find a better model.
2023-05-22 20:48:30.329: [iter 15 : loss : 1.1130 = 0.6650 + 0.4479 + 0.0001, time: 20.062757]
2023-05-22 20:48:30.601: epoch 15:	0.02018080  	0.05377185  	0.04647345  
2023-05-22 20:48:30.601: Find a better model.
2023-05-22 20:48:50.770: [iter 16 : loss : 1.0957 = 0.6465 + 0.4491 + 0.0002, time: 20.165462]
2023-05-22 20:48:51.039: epoch 16:	0.02298663  	0.06131544  	0.05313241  
2023-05-22 20:48:51.039: Find a better model.
2023-05-22 20:49:10.906: [iter 17 : loss : 1.0690 = 0.6179 + 0.4507 + 0.0003, time: 19.862873]
2023-05-22 20:49:11.200: epoch 17:	0.02554077  	0.06796683  	0.05823699  
2023-05-22 20:49:11.200: Find a better model.
2023-05-22 20:49:31.504: [iter 18 : loss : 1.0310 = 0.5776 + 0.4528 + 0.0005, time: 20.300873]
2023-05-22 20:49:31.775: epoch 18:	0.02725832  	0.07266431  	0.06150838  
2023-05-22 20:49:31.775: Find a better model.
2023-05-22 20:49:51.927: [iter 19 : loss : 0.9850 = 0.5286 + 0.4556 + 0.0008, time: 20.147514]
2023-05-22 20:49:52.220: epoch 19:	0.02839842  	0.07553479  	0.06376372  
2023-05-22 20:49:52.220: Find a better model.
2023-05-22 20:50:12.641: [iter 20 : loss : 0.9352 = 0.4754 + 0.4587 + 0.0010, time: 20.416095]
2023-05-22 20:50:12.932: epoch 20:	0.02912393  	0.07824759  	0.06510340  
2023-05-22 20:50:12.932: Find a better model.
2023-05-22 20:50:32.902: [iter 21 : loss : 0.8865 = 0.4234 + 0.4618 + 0.0013, time: 19.966581]
2023-05-22 20:50:33.187: epoch 21:	0.02939786  	0.07928873  	0.06577312  
2023-05-22 20:50:33.187: Find a better model.
2023-05-22 20:50:53.109: [iter 22 : loss : 0.8414 = 0.3755 + 0.4642 + 0.0016, time: 19.917058]
2023-05-22 20:50:53.376: epoch 22:	0.02960516  	0.08062328  	0.06628369  
2023-05-22 20:50:53.376: Find a better model.
2023-05-22 20:51:13.495: [iter 23 : loss : 0.8028 = 0.3349 + 0.4660 + 0.0019, time: 20.115436]
2023-05-22 20:51:13.760: epoch 23:	0.02962738  	0.08134548  	0.06646219  
2023-05-22 20:51:13.760: Find a better model.
2023-05-22 20:51:34.049: [iter 24 : loss : 0.7702 = 0.3011 + 0.4670 + 0.0022, time: 20.283984]
2023-05-22 20:51:34.315: epoch 24:	0.02983467  	0.08192564  	0.06663640  
2023-05-22 20:51:34.316: Find a better model.
2023-05-22 20:51:54.314: [iter 25 : loss : 0.7420 = 0.2723 + 0.4672 + 0.0025, time: 19.993557]
2023-05-22 20:51:54.575: epoch 25:	0.03001976  	0.08230049  	0.06696900  
2023-05-22 20:51:54.576: Find a better model.
2023-05-22 20:52:14.476: [iter 26 : loss : 0.7188 = 0.2486 + 0.4674 + 0.0028, time: 19.896082]
2023-05-22 20:52:14.758: epoch 26:	0.03005678  	0.08264361  	0.06732874  
2023-05-22 20:52:14.758: Find a better model.
2023-05-22 20:52:34.830: [iter 27 : loss : 0.6979 = 0.2280 + 0.4669 + 0.0030, time: 20.068707]
2023-05-22 20:52:35.092: epoch 27:	0.02994573  	0.08231793  	0.06706677  
2023-05-22 20:52:55.040: [iter 28 : loss : 0.6811 = 0.2113 + 0.4666 + 0.0033, time: 19.944477]
2023-05-22 20:52:55.318: epoch 28:	0.03002715  	0.08278523  	0.06727465  
2023-05-22 20:52:55.318: Find a better model.
2023-05-22 20:53:15.642: [iter 29 : loss : 0.6662 = 0.1968 + 0.4659 + 0.0035, time: 20.320108]
2023-05-22 20:53:15.907: epoch 29:	0.03004937  	0.08287038  	0.06739828  
2023-05-22 20:53:15.907: Find a better model.
2023-05-22 20:53:36.061: [iter 30 : loss : 0.6526 = 0.1835 + 0.4654 + 0.0037, time: 20.150005]
2023-05-22 20:53:36.330: epoch 30:	0.03013821  	0.08326272  	0.06750294  
2023-05-22 20:53:36.330: Find a better model.
2023-05-22 20:53:56.399: [iter 31 : loss : 0.6407 = 0.1720 + 0.4647 + 0.0040, time: 20.064577]
2023-05-22 20:53:56.659: epoch 31:	0.03010118  	0.08306246  	0.06756840  
2023-05-22 20:54:16.820: [iter 32 : loss : 0.6308 = 0.1626 + 0.4640 + 0.0042, time: 20.157400]
2023-05-22 20:54:17.078: epoch 32:	0.03010119  	0.08271986  	0.06752933  
2023-05-22 20:54:37.231: [iter 33 : loss : 0.6223 = 0.1544 + 0.4635 + 0.0044, time: 20.149074]
2023-05-22 20:54:37.491: epoch 33:	0.03008639  	0.08306167  	0.06769330  
2023-05-22 20:54:57.409: [iter 34 : loss : 0.6135 = 0.1460 + 0.4629 + 0.0046, time: 19.912855]
2023-05-22 20:54:57.665: epoch 34:	0.03003456  	0.08328222  	0.06786095  
2023-05-22 20:54:57.665: Find a better model.
2023-05-22 20:55:17.799: [iter 35 : loss : 0.6056 = 0.1386 + 0.4622 + 0.0048, time: 20.130148]
2023-05-22 20:55:18.056: epoch 35:	0.03023443  	0.08350382  	0.06816532  
2023-05-22 20:55:18.056: Find a better model.
2023-05-22 20:55:38.230: [iter 36 : loss : 0.5991 = 0.1324 + 0.4617 + 0.0050, time: 20.168337]
2023-05-22 20:55:38.513: epoch 36:	0.03007897  	0.08334220  	0.06809209  
2023-05-22 20:55:58.374: [iter 37 : loss : 0.5933 = 0.1269 + 0.4612 + 0.0051, time: 19.856762]
2023-05-22 20:55:58.636: epoch 37:	0.03005676  	0.08325927  	0.06812659  
2023-05-22 20:56:18.818: [iter 38 : loss : 0.5870 = 0.1210 + 0.4607 + 0.0053, time: 20.178344]
2023-05-22 20:56:19.081: epoch 38:	0.03009378  	0.08346311  	0.06812579  
2023-05-22 20:56:39.229: [iter 39 : loss : 0.5824 = 0.1168 + 0.4601 + 0.0055, time: 20.143407]
2023-05-22 20:56:39.511: epoch 39:	0.03012339  	0.08335725  	0.06816240  
2023-05-22 20:56:59.367: [iter 40 : loss : 0.5773 = 0.1118 + 0.4598 + 0.0057, time: 19.850383]
2023-05-22 20:56:59.642: epoch 40:	0.03004936  	0.08286974  	0.06811439  
2023-05-22 20:57:19.613: [iter 41 : loss : 0.5725 = 0.1073 + 0.4593 + 0.0058, time: 19.967497]
2023-05-22 20:57:19.873: epoch 41:	0.02993091  	0.08251626  	0.06785909  
2023-05-22 20:57:40.268: [iter 42 : loss : 0.5687 = 0.1038 + 0.4589 + 0.0060, time: 20.390430]
2023-05-22 20:57:40.531: epoch 42:	0.02984947  	0.08234352  	0.06773180  
2023-05-22 20:58:01.004: [iter 43 : loss : 0.5648 = 0.1000 + 0.4586 + 0.0061, time: 20.469067]
2023-05-22 20:58:01.270: epoch 43:	0.02990128  	0.08236581  	0.06788199  
2023-05-22 20:59:07.878: my pid: 9376
2023-05-22 20:59:07.878: model: model.general_recommender.SGL
2023-05-22 20:59:07.878: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 20:59:07.878: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 20:59:12.138: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 20:59:33.603: [iter 1 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 21.464063]
2023-05-22 20:59:33.866: epoch 1:	0.00175452  	0.00317001  	0.00272148  
2023-05-22 20:59:33.866: Find a better model.
2023-05-22 20:59:55.185: [iter 2 : loss : 1.1330 = 0.6930 + 0.4400 + 0.0000, time: 21.314525]
2023-05-22 20:59:55.474: epoch 2:	0.00210986  	0.00347976  	0.00293910  
2023-05-22 20:59:55.474: Find a better model.
2023-05-22 21:00:16.698: [iter 3 : loss : 1.1330 = 0.6929 + 0.4401 + 0.0000, time: 21.218218]
2023-05-22 21:00:17.007: epoch 3:	0.00260586  	0.00506644  	0.00418690  
2023-05-22 21:00:17.007: Find a better model.
2023-05-22 21:00:38.116: [iter 4 : loss : 1.1330 = 0.6928 + 0.4402 + 0.0000, time: 21.104761]
2023-05-22 21:00:38.423: epoch 4:	0.00290199  	0.00631561  	0.00512954  
2023-05-22 21:00:38.424: Find a better model.
2023-05-22 21:00:59.885: [iter 5 : loss : 1.1331 = 0.6927 + 0.4404 + 0.0000, time: 21.457593]
2023-05-22 21:01:00.181: epoch 5:	0.00350163  	0.00728927  	0.00639274  
2023-05-22 21:01:00.181: Find a better model.
2023-05-22 21:01:21.263: [iter 6 : loss : 1.1331 = 0.6925 + 0.4406 + 0.0000, time: 21.078326]
2023-05-22 21:01:21.559: epoch 6:	0.00474533  	0.01060843  	0.00888134  
2023-05-22 21:01:21.559: Find a better model.
2023-05-22 21:01:42.699: [iter 7 : loss : 1.1331 = 0.6923 + 0.4408 + 0.0000, time: 21.137409]
2023-05-22 21:01:43.026: epoch 7:	0.00542640  	0.01297154  	0.01093096  
2023-05-22 21:01:43.026: Find a better model.
2023-05-22 21:02:04.344: [iter 8 : loss : 1.1331 = 0.6920 + 0.4411 + 0.0000, time: 21.313660]
2023-05-22 21:02:04.649: epoch 8:	0.00640359  	0.01573626  	0.01315203  
2023-05-22 21:02:04.649: Find a better model.
2023-05-22 21:02:25.876: [iter 9 : loss : 1.1328 = 0.6915 + 0.4414 + 0.0000, time: 21.223502]
2023-05-22 21:02:26.169: epoch 9:	0.00749182  	0.01907090  	0.01564321  
2023-05-22 21:02:26.169: Find a better model.
2023-05-22 21:02:47.261: [iter 10 : loss : 1.1323 = 0.6905 + 0.4418 + 0.0000, time: 21.088735]
2023-05-22 21:02:47.546: epoch 10:	0.00770651  	0.02010431  	0.01688659  
2023-05-22 21:02:47.546: Find a better model.
2023-05-22 21:03:08.655: [iter 11 : loss : 1.1312 = 0.6890 + 0.4422 + 0.0000, time: 21.105831]
2023-05-22 21:03:08.937: epoch 11:	0.00923152  	0.02357617  	0.01971319  
2023-05-22 21:03:08.937: Find a better model.
2023-05-22 21:03:30.067: [iter 12 : loss : 1.1294 = 0.6866 + 0.4428 + 0.0000, time: 21.127135]
2023-05-22 21:03:30.370: epoch 12:	0.01117114  	0.02837961  	0.02471817  
2023-05-22 21:03:30.370: Find a better model.
2023-05-22 21:03:51.267: [iter 13 : loss : 1.1263 = 0.6829 + 0.4433 + 0.0001, time: 20.892720]
2023-05-22 21:03:51.545: epoch 13:	0.01374743  	0.03481825  	0.03131257  
2023-05-22 21:03:51.545: Find a better model.
2023-05-22 21:04:12.460: [iter 14 : loss : 1.1204 = 0.6763 + 0.4440 + 0.0001, time: 20.911820]
2023-05-22 21:04:12.737: epoch 14:	0.01671611  	0.04336845  	0.03829406  
2023-05-22 21:04:12.738: Find a better model.
2023-05-22 21:04:33.637: [iter 15 : loss : 1.1094 = 0.6645 + 0.4447 + 0.0001, time: 20.895453]
2023-05-22 21:04:33.915: epoch 15:	0.01992909  	0.05192187  	0.04540236  
2023-05-22 21:04:33.916: Find a better model.
2023-05-22 21:04:55.011: [iter 16 : loss : 1.0905 = 0.6444 + 0.4458 + 0.0002, time: 21.091321]
2023-05-22 21:04:55.288: epoch 16:	0.02292741  	0.06062699  	0.05240293  
2023-05-22 21:04:55.288: Find a better model.
2023-05-22 21:05:16.211: [iter 17 : loss : 1.0611 = 0.6134 + 0.4473 + 0.0004, time: 20.918425]
2023-05-22 21:05:16.486: epoch 17:	0.02513358  	0.06693874  	0.05759210  
2023-05-22 21:05:16.486: Find a better model.
2023-05-22 21:05:37.378: [iter 18 : loss : 1.0203 = 0.5703 + 0.4494 + 0.0006, time: 20.888298]
2023-05-22 21:05:37.652: epoch 18:	0.02699920  	0.07125603  	0.06107689  
2023-05-22 21:05:37.652: Find a better model.
2023-05-22 21:05:58.613: [iter 19 : loss : 0.9721 = 0.5192 + 0.4522 + 0.0008, time: 20.956697]
2023-05-22 21:05:58.886: epoch 19:	0.02827995  	0.07519469  	0.06359500  
2023-05-22 21:05:58.886: Find a better model.
2023-05-22 21:06:19.775: [iter 20 : loss : 0.9210 = 0.4647 + 0.4553 + 0.0011, time: 20.885473]
2023-05-22 21:06:20.060: epoch 20:	0.02887961  	0.07721736  	0.06485623  
2023-05-22 21:06:20.060: Find a better model.
2023-05-22 21:06:40.979: [iter 21 : loss : 0.8725 = 0.4129 + 0.4582 + 0.0014, time: 20.913595]
2023-05-22 21:06:41.254: epoch 21:	0.02941266  	0.07911391  	0.06577603  
2023-05-22 21:06:41.255: Find a better model.
2023-05-22 21:07:02.208: [iter 22 : loss : 0.8279 = 0.3656 + 0.4606 + 0.0017, time: 20.950615]
2023-05-22 21:07:02.486: epoch 22:	0.02947930  	0.07989206  	0.06590328  
2023-05-22 21:07:02.486: Find a better model.
2023-05-22 21:07:23.557: [iter 23 : loss : 0.7904 = 0.3262 + 0.4622 + 0.0020, time: 21.066142]
2023-05-22 21:07:23.833: epoch 23:	0.02961996  	0.08075652  	0.06629049  
2023-05-22 21:07:23.833: Find a better model.
2023-05-22 21:07:44.734: [iter 24 : loss : 0.7585 = 0.2932 + 0.4631 + 0.0023, time: 20.897936]
2023-05-22 21:07:45.004: epoch 24:	0.02978284  	0.08152459  	0.06650674  
2023-05-22 21:07:45.004: Find a better model.
2023-05-22 21:08:05.967: [iter 25 : loss : 0.7314 = 0.2655 + 0.4633 + 0.0026, time: 20.958100]
2023-05-22 21:08:06.252: epoch 25:	0.02984946  	0.08167467  	0.06655525  
2023-05-22 21:08:06.252: Find a better model.
2023-05-22 21:08:27.148: [iter 26 : loss : 0.7090 = 0.2428 + 0.4634 + 0.0028, time: 20.892742]
2023-05-22 21:08:27.420: epoch 26:	0.02985686  	0.08175515  	0.06664119  
2023-05-22 21:08:27.420: Find a better model.
2023-05-22 21:08:48.160: [iter 27 : loss : 0.6892 = 0.2232 + 0.4630 + 0.0031, time: 20.736474]
2023-05-22 21:08:48.429: epoch 27:	0.02989388  	0.08159907  	0.06657672  
2023-05-22 21:09:09.360: [iter 28 : loss : 0.6728 = 0.2068 + 0.4627 + 0.0033, time: 20.925368]
2023-05-22 21:09:09.649: epoch 28:	0.02979023  	0.08163346  	0.06658059  
2023-05-22 21:09:30.328: [iter 29 : loss : 0.6586 = 0.1930 + 0.4620 + 0.0036, time: 20.673836]
2023-05-22 21:09:30.594: epoch 29:	0.02970879  	0.08113233  	0.06656855  
2023-05-22 21:09:51.489: [iter 30 : loss : 0.6454 = 0.1801 + 0.4615 + 0.0038, time: 20.891845]
2023-05-22 21:09:51.754: epoch 30:	0.02966438  	0.08130994  	0.06668332  
2023-05-22 21:10:12.498: [iter 31 : loss : 0.6338 = 0.1689 + 0.4609 + 0.0040, time: 20.740471]
2023-05-22 21:10:12.764: epoch 31:	0.02979765  	0.08165931  	0.06686273  
2023-05-22 21:10:33.691: [iter 32 : loss : 0.6241 = 0.1597 + 0.4602 + 0.0042, time: 20.923886]
2023-05-22 21:10:33.955: epoch 32:	0.02973102  	0.08134858  	0.06683565  
2023-05-22 21:10:54.680: [iter 33 : loss : 0.6160 = 0.1519 + 0.4597 + 0.0044, time: 20.721704]
2023-05-22 21:10:54.940: epoch 33:	0.02987908  	0.08208957  	0.06723032  
2023-05-22 21:10:54.941: Find a better model.
2023-05-22 21:11:15.862: [iter 34 : loss : 0.6077 = 0.1440 + 0.4591 + 0.0046, time: 20.918215]
2023-05-22 21:11:16.143: epoch 34:	0.02986426  	0.08190495  	0.06730908  
2023-05-22 21:11:37.093: [iter 35 : loss : 0.5996 = 0.1363 + 0.4585 + 0.0048, time: 20.945196]
2023-05-22 21:11:37.356: epoch 35:	0.02978283  	0.08149670  	0.06729387  
2023-05-22 21:11:58.105: [iter 36 : loss : 0.5933 = 0.1303 + 0.4580 + 0.0050, time: 20.743447]
2023-05-22 21:11:58.368: epoch 36:	0.02979022  	0.08138842  	0.06741584  
2023-05-22 21:12:19.260: [iter 37 : loss : 0.5880 = 0.1253 + 0.4575 + 0.0052, time: 20.888525]
2023-05-22 21:12:19.520: epoch 37:	0.02984945  	0.08159850  	0.06755086  
2023-05-22 21:12:40.474: [iter 38 : loss : 0.5820 = 0.1196 + 0.4570 + 0.0054, time: 20.951160]
2023-05-22 21:12:40.737: epoch 38:	0.02984205  	0.08159965  	0.06758022  
2023-05-22 21:13:01.638: [iter 39 : loss : 0.5776 = 0.1155 + 0.4565 + 0.0055, time: 20.896506]
2023-05-22 21:13:01.901: epoch 39:	0.02982725  	0.08181882  	0.06758817  
2023-05-22 21:13:22.863: [iter 40 : loss : 0.5720 = 0.1102 + 0.4561 + 0.0057, time: 20.957902]
2023-05-22 21:13:23.160: epoch 40:	0.02980504  	0.08167984  	0.06760302  
2023-05-22 21:13:44.277: [iter 41 : loss : 0.5677 = 0.1062 + 0.4556 + 0.0059, time: 21.113549]
2023-05-22 21:13:44.540: epoch 41:	0.02977542  	0.08137722  	0.06758034  
2023-05-22 21:14:05.670: [iter 42 : loss : 0.5637 = 0.1023 + 0.4554 + 0.0060, time: 21.125137]
2023-05-22 21:14:05.937: epoch 42:	0.02965698  	0.08097961  	0.06740364  
2023-05-22 21:14:27.658: [iter 43 : loss : 0.5598 = 0.0987 + 0.4549 + 0.0062, time: 21.716001]
2023-05-22 21:14:27.928: epoch 43:	0.02956813  	0.08080116  	0.06729557  
2023-05-22 21:15:11.282: my pid: 10600
2023-05-22 21:15:11.283: model: model.general_recommender.SGL
2023-05-22 21:15:11.283: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-22 21:15:11.283: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-22 21:15:15.374: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-22 21:15:36.986: [iter 1 : loss : 1.1334 = 0.6931 + 0.4403 + 0.0000, time: 21.611395]
2023-05-22 21:15:37.267: epoch 1:	0.00165828  	0.00374678  	0.00292385  
2023-05-22 21:15:37.267: Find a better model.
2023-05-22 21:15:58.953: [iter 2 : loss : 1.1321 = 0.6930 + 0.4391 + 0.0000, time: 21.682526]
2023-05-22 21:15:59.236: epoch 2:	0.00217649  	0.00492594  	0.00373136  
2023-05-22 21:15:59.236: Find a better model.
2023-05-22 21:16:20.775: [iter 3 : loss : 1.1320 = 0.6930 + 0.4391 + 0.0000, time: 21.534820]
2023-05-22 21:16:21.067: epoch 3:	0.00262807  	0.00495450  	0.00417947  
2023-05-22 21:16:21.067: Find a better model.
2023-05-22 21:16:42.747: [iter 4 : loss : 1.1320 = 0.6929 + 0.4391 + 0.0000, time: 21.676092]
2023-05-22 21:16:43.048: epoch 4:	0.00324993  	0.00693434  	0.00582721  
2023-05-22 21:16:43.049: Find a better model.
2023-05-22 21:17:04.757: [iter 5 : loss : 1.1319 = 0.6928 + 0.4392 + 0.0000, time: 21.703489]
2023-05-22 21:17:05.049: epoch 5:	0.00359047  	0.00828885  	0.00725416  
2023-05-22 21:17:05.049: Find a better model.
2023-05-22 21:17:26.358: [iter 6 : loss : 1.1319 = 0.6926 + 0.4393 + 0.0000, time: 21.305661]
2023-05-22 21:17:26.639: epoch 6:	0.00472312  	0.01122468  	0.00892885  
2023-05-22 21:17:26.639: Find a better model.
2023-05-22 21:17:48.457: [iter 7 : loss : 1.1318 = 0.6924 + 0.4394 + 0.0000, time: 21.813598]
2023-05-22 21:17:48.740: epoch 7:	0.00538198  	0.01267983  	0.01053452  
2023-05-22 21:17:48.740: Find a better model.
2023-05-22 21:18:10.321: [iter 8 : loss : 1.1317 = 0.6921 + 0.4395 + 0.0000, time: 21.577878]
2023-05-22 21:18:10.606: epoch 8:	0.00637397  	0.01519017  	0.01301140  
2023-05-22 21:18:10.606: Find a better model.
2023-05-22 21:18:32.367: [iter 9 : loss : 1.1314 = 0.6917 + 0.4397 + 0.0000, time: 21.755651]
2023-05-22 21:18:32.647: epoch 9:	0.00786937  	0.01984491  	0.01667939  
2023-05-22 21:18:32.647: Find a better model.
2023-05-22 21:18:54.192: [iter 10 : loss : 1.1310 = 0.6910 + 0.4400 + 0.0000, time: 21.540848]
2023-05-22 21:18:54.470: epoch 10:	0.00894280  	0.02251779  	0.01873462  
2023-05-22 21:18:54.470: Find a better model.
2023-05-22 21:19:16.199: [iter 11 : loss : 1.1300 = 0.6898 + 0.4402 + 0.0000, time: 21.722826]
2023-05-22 21:19:16.476: epoch 11:	0.00956465  	0.02393003  	0.02054160  
2023-05-22 21:19:16.476: Find a better model.
2023-05-22 21:19:38.216: [iter 12 : loss : 1.1281 = 0.6875 + 0.4406 + 0.0000, time: 21.736915]
2023-05-22 21:19:38.494: epoch 12:	0.01074917  	0.02738573  	0.02344704  
2023-05-22 21:19:38.495: Find a better model.
2023-05-22 21:20:00.346: [iter 13 : loss : 1.1249 = 0.6839 + 0.4410 + 0.0000, time: 21.846512]
2023-05-22 21:20:00.661: epoch 13:	0.01319219  	0.03419945  	0.02932835  
2023-05-22 21:20:00.661: Find a better model.
2023-05-22 21:20:24.519: [iter 14 : loss : 1.1191 = 0.6775 + 0.4416 + 0.0001, time: 23.852075]
2023-05-22 21:20:24.879: epoch 14:	0.01591656  	0.04174288  	0.03601886  
2023-05-22 21:20:24.879: Find a better model.
2023-05-22 21:20:48.591: [iter 15 : loss : 1.1082 = 0.6659 + 0.4422 + 0.0001, time: 23.705915]
2023-05-22 21:20:48.973: epoch 15:	0.01907032  	0.05124897  	0.04377694  
2023-05-22 21:20:48.973: Find a better model.
2023-05-22 21:21:12.379: [iter 16 : loss : 1.0891 = 0.6458 + 0.4431 + 0.0002, time: 23.402145]
2023-05-22 21:21:12.743: epoch 16:	0.02208347  	0.05960136  	0.05101917  
2023-05-22 21:21:12.743: Find a better model.
2023-05-22 21:21:36.697: [iter 17 : loss : 1.0595 = 0.6146 + 0.4445 + 0.0003, time: 23.948940]
2023-05-22 21:21:37.021: epoch 17:	0.02480787  	0.06655905  	0.05738962  
2023-05-22 21:21:37.021: Find a better model.
2023-05-22 21:22:00.822: [iter 18 : loss : 1.0181 = 0.5712 + 0.4464 + 0.0005, time: 23.796237]
2023-05-22 21:22:01.193: epoch 18:	0.02676973  	0.07184207  	0.06143470  
2023-05-22 21:22:01.193: Find a better model.
2023-05-22 21:22:25.021: [iter 19 : loss : 0.9696 = 0.5198 + 0.4490 + 0.0008, time: 23.823179]
2023-05-22 21:22:25.380: epoch 19:	0.02823555  	0.07588875  	0.06419910  
2023-05-22 21:22:25.380: Find a better model.
2023-05-22 21:22:49.159: [iter 20 : loss : 0.9185 = 0.4654 + 0.4520 + 0.0011, time: 23.775299]
2023-05-22 21:22:49.530: epoch 20:	0.02886482  	0.07721759  	0.06542756  
2023-05-22 21:22:49.530: Find a better model.
2023-05-22 21:23:13.200: [iter 21 : loss : 0.8700 = 0.4137 + 0.4549 + 0.0014, time: 23.665498]
2023-05-22 21:23:13.574: epoch 21:	0.02933123  	0.07928408  	0.06631889  
2023-05-22 21:23:13.574: Find a better model.
2023-05-22 21:23:37.477: [iter 22 : loss : 0.8254 = 0.3666 + 0.4571 + 0.0017, time: 23.893069]
2023-05-22 21:23:37.797: epoch 22:	0.02944228  	0.07993197  	0.06655154  
2023-05-22 21:23:37.797: Find a better model.
2023-05-22 21:24:01.490: [iter 23 : loss : 0.7880 = 0.3272 + 0.4588 + 0.0020, time: 23.686871]
2023-05-22 21:24:01.859: epoch 23:	0.02950891  	0.08041548  	0.06680179  
2023-05-22 21:24:01.859: Find a better model.
2023-05-22 21:24:25.524: [iter 24 : loss : 0.7562 = 0.2943 + 0.4597 + 0.0023, time: 23.653152]
2023-05-22 21:24:25.884: epoch 24:	0.02953112  	0.08074278  	0.06674333  
2023-05-22 21:24:25.884: Find a better model.
2023-05-22 21:24:49.944: [iter 25 : loss : 0.7290 = 0.2664 + 0.4601 + 0.0026, time: 24.056026]
2023-05-22 21:24:50.227: epoch 25:	0.02964958  	0.08109576  	0.06694664  
2023-05-22 21:24:50.228: Find a better model.
2023-05-22 21:25:14.002: [iter 26 : loss : 0.7064 = 0.2434 + 0.4602 + 0.0028, time: 23.768583]
2023-05-22 21:25:14.367: epoch 26:	0.02982726  	0.08123677  	0.06721590  
2023-05-22 21:25:14.367: Find a better model.
2023-05-22 21:25:38.487: [iter 27 : loss : 0.6867 = 0.2237 + 0.4599 + 0.0031, time: 24.115531]
2023-05-22 21:25:38.792: epoch 27:	0.02976804  	0.08137782  	0.06726619  
2023-05-22 21:25:38.792: Find a better model.
2023-05-22 21:26:02.883: [iter 28 : loss : 0.6705 = 0.2076 + 0.4596 + 0.0033, time: 24.086072]
2023-05-22 21:26:03.247: epoch 28:	0.02987909  	0.08204532  	0.06745133  
2023-05-22 21:26:03.247: Find a better model.
2023-05-22 21:26:27.440: [iter 29 : loss : 0.6561 = 0.1935 + 0.4590 + 0.0036, time: 24.188344]
2023-05-22 21:26:27.756: epoch 29:	0.02985688  	0.08168549  	0.06739512  
2023-05-22 21:26:52.065: [iter 30 : loss : 0.6430 = 0.1807 + 0.4585 + 0.0038, time: 24.304833]
2023-05-22 21:26:52.420: epoch 30:	0.03002716  	0.08215939  	0.06773365  
2023-05-22 21:26:52.420: Find a better model.
2023-05-22 21:27:16.386: [iter 31 : loss : 0.6315 = 0.1695 + 0.4580 + 0.0040, time: 23.962734]
2023-05-22 21:27:16.728: epoch 31:	0.02994572  	0.08196665  	0.06768210  
2023-05-22 21:27:40.581: [iter 32 : loss : 0.6219 = 0.1603 + 0.4574 + 0.0042, time: 23.842674]
2023-05-22 21:27:40.969: epoch 32:	0.03013821  	0.08254293  	0.06781323  
2023-05-22 21:27:40.969: Find a better model.
2023-05-22 21:28:04.820: [iter 33 : loss : 0.6135 = 0.1522 + 0.4568 + 0.0044, time: 23.847834]
2023-05-22 21:28:05.167: epoch 33:	0.03013079  	0.08224348  	0.06796487  
2023-05-22 21:28:28.588: [iter 34 : loss : 0.6053 = 0.1444 + 0.4563 + 0.0046, time: 23.418447]
2023-05-22 21:28:28.947: epoch 34:	0.03008638  	0.08189608  	0.06785832  
2023-05-22 21:28:52.415: [iter 35 : loss : 0.5970 = 0.1364 + 0.4557 + 0.0048, time: 23.463317]
2023-05-22 21:28:52.692: epoch 35:	0.03012340  	0.08214428  	0.06794099  
2023-05-22 21:29:16.034: [iter 36 : loss : 0.5910 = 0.1308 + 0.4552 + 0.0050, time: 23.338748]
2023-05-22 21:29:16.385: epoch 36:	0.03007158  	0.08223383  	0.06791259  
2023-05-22 21:29:39.757: [iter 37 : loss : 0.5854 = 0.1255 + 0.4547 + 0.0052, time: 23.367669]
2023-05-22 21:29:40.101: epoch 37:	0.03003455  	0.08228710  	0.06780924  
2023-05-22 21:30:03.806: [iter 38 : loss : 0.5794 = 0.1197 + 0.4543 + 0.0054, time: 23.697006]
2023-05-22 21:30:04.113: epoch 38:	0.02996792  	0.08181118  	0.06781807  
2023-05-22 21:30:27.458: [iter 39 : loss : 0.5748 = 0.1154 + 0.4538 + 0.0055, time: 23.340091]
2023-05-22 21:30:27.805: epoch 39:	0.02993831  	0.08182065  	0.06779679  
2023-05-22 21:30:51.216: [iter 40 : loss : 0.5697 = 0.1106 + 0.4534 + 0.0057, time: 23.405873]
2023-05-22 21:30:51.571: epoch 40:	0.03006415  	0.08215933  	0.06795392  
2023-05-22 21:31:14.891: [iter 41 : loss : 0.5652 = 0.1064 + 0.4529 + 0.0059, time: 23.317009]
2023-05-22 21:31:15.240: epoch 41:	0.03001233  	0.08169101  	0.06791548  
2023-05-22 21:31:38.930: [iter 42 : loss : 0.5612 = 0.1026 + 0.4526 + 0.0060, time: 23.686574]
2023-05-22 21:31:39.216: epoch 42:	0.03000492  	0.08135767  	0.06788474  
2023-05-22 21:32:02.695: [iter 43 : loss : 0.5573 = 0.0989 + 0.4522 + 0.0062, time: 23.473731]
2023-05-22 21:32:03.046: epoch 43:	0.02990128  	0.08091882  	0.06771410  
2023-05-22 21:32:26.650: [iter 44 : loss : 0.5549 = 0.0966 + 0.4519 + 0.0063, time: 23.600195]
2023-05-22 21:32:27.004: epoch 44:	0.02985687  	0.08116792  	0.06783127  
2023-05-22 21:32:50.555: [iter 45 : loss : 0.5511 = 0.0931 + 0.4515 + 0.0065, time: 23.547143]
2023-05-22 21:32:50.848: epoch 45:	0.02990128  	0.08107782  	0.06773899  
2023-05-22 21:33:14.228: [iter 46 : loss : 0.5480 = 0.0901 + 0.4512 + 0.0066, time: 23.376132]
2023-05-22 21:33:14.574: epoch 46:	0.02987166  	0.08108064  	0.06763446  
2023-05-22 21:33:38.068: [iter 47 : loss : 0.5451 = 0.0874 + 0.4509 + 0.0068, time: 23.485780]
2023-05-22 21:33:38.421: epoch 47:	0.02988647  	0.08093583  	0.06753343  
2023-05-22 21:34:01.969: [iter 48 : loss : 0.5425 = 0.0850 + 0.4506 + 0.0069, time: 23.545371]
2023-05-22 21:34:02.265: epoch 48:	0.02978283  	0.08059027  	0.06746256  
2023-05-22 21:34:25.608: [iter 49 : loss : 0.5403 = 0.0830 + 0.4503 + 0.0071, time: 23.338512]
2023-05-22 21:34:25.961: epoch 49:	0.02972361  	0.08021004  	0.06708955  
2023-05-22 21:34:49.395: [iter 50 : loss : 0.5378 = 0.0806 + 0.4501 + 0.0072, time: 23.423216]
2023-05-22 21:34:49.740: epoch 50:	0.02961255  	0.07961956  	0.06679201  
2023-05-22 21:35:13.331: [iter 51 : loss : 0.5356 = 0.0785 + 0.4498 + 0.0073, time: 23.585964]
2023-05-22 21:35:13.623: epoch 51:	0.02952372  	0.07926676  	0.06679009  
2023-05-22 21:35:36.957: [iter 52 : loss : 0.5327 = 0.0757 + 0.4496 + 0.0075, time: 23.328968]
2023-05-22 21:35:37.293: epoch 52:	0.02963476  	0.07932603  	0.06680662  
2023-05-22 21:36:00.828: [iter 53 : loss : 0.5323 = 0.0753 + 0.4494 + 0.0076, time: 23.531144]
2023-05-22 21:36:01.178: epoch 53:	0.02947929  	0.07869416  	0.06647351  
2023-05-22 21:36:24.823: [iter 54 : loss : 0.5300 = 0.0732 + 0.4491 + 0.0077, time: 23.639632]
2023-05-22 21:36:25.118: epoch 54:	0.02944228  	0.07867638  	0.06642387  
2023-05-22 21:36:48.577: [iter 55 : loss : 0.5280 = 0.0712 + 0.4489 + 0.0078, time: 23.454186]
2023-05-22 21:36:48.924: epoch 55:	0.02939785  	0.07830123  	0.06641749  
2023-05-22 21:37:12.303: [iter 56 : loss : 0.5259 = 0.0693 + 0.4487 + 0.0080, time: 23.376044]
2023-05-22 21:37:12.649: epoch 56:	0.02932381  	0.07809101  	0.06640172  
2023-05-22 21:37:35.828: [iter 57 : loss : 0.5244 = 0.0678 + 0.4485 + 0.0081, time: 23.175133]
2023-05-22 21:37:36.173: epoch 57:	0.02922016  	0.07772336  	0.06626251  
2023-05-22 21:37:36.173: Early stopping is trigger at epoch: 57
2023-05-22 21:37:36.173: best_result@epoch 32:

2023-05-22 21:37:36.173: 		0.0301      	0.0825      	0.0678      
2023-05-23 09:27:26.269: my pid: 11468
2023-05-23 09:27:26.269: model: model.general_recommender.SGL
2023-05-23 09:27:26.269: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-23 09:27:26.269: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-23 09:27:30.543: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-23 09:27:52.497: [iter 1 : loss : 1.1327 = 0.6931 + 0.4397 + 0.0000, time: 21.954073]
2023-05-23 09:27:52.770: epoch 1:	0.00180634  	0.00388100  	0.00308223  
2023-05-23 09:27:52.770: Find a better model.
2023-05-23 09:28:14.849: [iter 2 : loss : 1.1314 = 0.6930 + 0.4384 + 0.0000, time: 22.075112]
2023-05-23 09:28:15.141: epoch 2:	0.00228754  	0.00462154  	0.00381012  
2023-05-23 09:28:15.141: Find a better model.
2023-05-23 09:28:37.048: [iter 3 : loss : 1.1312 = 0.6930 + 0.4382 + 0.0000, time: 21.903607]
2023-05-23 09:28:37.340: epoch 3:	0.00263548  	0.00526703  	0.00447037  
2023-05-23 09:28:37.340: Find a better model.
2023-05-23 09:28:59.021: [iter 4 : loss : 1.1311 = 0.6929 + 0.4382 + 0.0000, time: 21.677270]
2023-05-23 09:28:59.315: epoch 4:	0.00329435  	0.00721477  	0.00565059  
2023-05-23 09:28:59.315: Find a better model.
2023-05-23 09:29:21.217: [iter 5 : loss : 1.1310 = 0.6928 + 0.4382 + 0.0000, time: 21.898771]
2023-05-23 09:29:21.512: epoch 5:	0.00385697  	0.00873843  	0.00722329  
2023-05-23 09:29:21.512: Find a better model.
2023-05-23 09:29:43.236: [iter 6 : loss : 1.1308 = 0.6926 + 0.4382 + 0.0000, time: 21.720567]
2023-05-23 09:29:43.525: epoch 6:	0.00484156  	0.01142658  	0.00929416  
2023-05-23 09:29:43.525: Find a better model.
2023-05-23 09:30:05.400: [iter 7 : loss : 1.1307 = 0.6925 + 0.4382 + 0.0000, time: 21.870556]
2023-05-23 09:30:05.699: epoch 7:	0.00631475  	0.01612708  	0.01285595  
2023-05-23 09:30:05.699: Find a better model.
2023-05-23 09:30:27.397: [iter 8 : loss : 1.1305 = 0.6922 + 0.4383 + 0.0000, time: 21.693912]
2023-05-23 09:30:27.704: epoch 8:	0.00725492  	0.01882813  	0.01551605  
2023-05-23 09:30:27.704: Find a better model.
2023-05-23 09:30:49.391: [iter 9 : loss : 1.1301 = 0.6918 + 0.4384 + 0.0000, time: 21.681565]
2023-05-23 09:30:49.676: epoch 9:	0.00828393  	0.02187075  	0.01839804  
2023-05-23 09:30:49.677: Find a better model.
2023-05-23 09:31:11.242: [iter 10 : loss : 1.1296 = 0.6911 + 0.4385 + 0.0000, time: 21.557268]
2023-05-23 09:31:11.526: epoch 10:	0.00960166  	0.02549601  	0.02094666  
2023-05-23 09:31:11.526: Find a better model.
2023-05-23 09:31:33.032: [iter 11 : loss : 1.1286 = 0.6900 + 0.4387 + 0.0000, time: 21.501697]
2023-05-23 09:31:33.317: epoch 11:	0.01003106  	0.02586593  	0.02199069  
2023-05-23 09:31:33.318: Find a better model.
2023-05-23 09:31:54.989: [iter 12 : loss : 1.1267 = 0.6878 + 0.4389 + 0.0000, time: 21.667505]
2023-05-23 09:31:55.267: epoch 12:	0.01059370  	0.02748995  	0.02362084  
2023-05-23 09:31:55.267: Find a better model.
2023-05-23 09:32:16.758: [iter 13 : loss : 1.1232 = 0.6840 + 0.4392 + 0.0000, time: 21.488231]
2023-05-23 09:32:17.034: epoch 13:	0.01222236  	0.03247189  	0.02792704  
2023-05-23 09:32:17.034: Find a better model.
2023-05-23 09:32:38.760: [iter 14 : loss : 1.1171 = 0.6774 + 0.4396 + 0.0001, time: 21.722146]
2023-05-23 09:32:39.042: epoch 14:	0.01505778  	0.04071801  	0.03474877  
2023-05-23 09:32:39.042: Find a better model.
2023-05-23 09:33:00.555: [iter 15 : loss : 1.1058 = 0.6655 + 0.4401 + 0.0001, time: 21.509770]
2023-05-23 09:33:00.852: epoch 15:	0.01761928  	0.04755910  	0.04119496  
2023-05-23 09:33:00.852: Find a better model.
2023-05-23 09:33:22.147: [iter 16 : loss : 1.0862 = 0.6449 + 0.4410 + 0.0002, time: 21.291039]
2023-05-23 09:33:22.421: epoch 16:	0.02074346  	0.05617223  	0.04813997  
2023-05-23 09:33:22.421: Find a better model.
2023-05-23 09:33:43.921: [iter 17 : loss : 1.0559 = 0.6133 + 0.4422 + 0.0004, time: 21.494706]
2023-05-23 09:33:44.193: epoch 17:	0.02371216  	0.06380367  	0.05441128  
2023-05-23 09:33:44.193: Find a better model.
2023-05-23 09:34:05.725: [iter 18 : loss : 1.0146 = 0.5700 + 0.4441 + 0.0005, time: 21.528912]
2023-05-23 09:34:06.010: epoch 18:	0.02566662  	0.06887067  	0.05894856  
2023-05-23 09:34:06.010: Find a better model.
2023-05-23 09:34:27.501: [iter 19 : loss : 0.9666 = 0.5194 + 0.4465 + 0.0008, time: 21.485524]
2023-05-23 09:34:27.774: epoch 19:	0.02731754  	0.07292018  	0.06220189  
2023-05-23 09:34:27.774: Find a better model.
2023-05-23 09:34:49.122: [iter 20 : loss : 0.9166 = 0.4662 + 0.4493 + 0.0011, time: 21.343394]
2023-05-23 09:34:49.394: epoch 20:	0.02808008  	0.07526062  	0.06382605  
2023-05-23 09:34:49.394: Find a better model.
2023-05-23 09:35:10.725: [iter 21 : loss : 0.8688 = 0.4155 + 0.4520 + 0.0014, time: 21.326769]
2023-05-23 09:35:10.997: epoch 21:	0.02868714  	0.07701037  	0.06497177  
2023-05-23 09:35:10.997: Find a better model.
2023-05-23 09:35:32.491: [iter 22 : loss : 0.8250 = 0.3691 + 0.4542 + 0.0017, time: 21.490090]
2023-05-23 09:35:32.763: epoch 22:	0.02904249  	0.07838534  	0.06552488  
2023-05-23 09:35:32.763: Find a better model.
2023-05-23 09:35:54.096: [iter 23 : loss : 0.7877 = 0.3298 + 0.4559 + 0.0020, time: 21.327786]
2023-05-23 09:35:54.366: epoch 23:	0.02927200  	0.07897889  	0.06579071  
2023-05-23 09:35:54.366: Find a better model.
2023-05-23 09:36:15.877: [iter 24 : loss : 0.7563 = 0.2971 + 0.4569 + 0.0023, time: 21.507189]
2023-05-23 09:36:16.148: epoch 24:	0.02913133  	0.07866611  	0.06555805  
2023-05-23 09:36:37.468: [iter 25 : loss : 0.7289 = 0.2690 + 0.4574 + 0.0025, time: 21.316855]
2023-05-23 09:36:37.740: epoch 25:	0.02938306  	0.07955456  	0.06578209  
2023-05-23 09:36:37.740: Find a better model.
2023-05-23 09:36:59.457: [iter 26 : loss : 0.7064 = 0.2460 + 0.4576 + 0.0028, time: 21.712921]
2023-05-23 09:36:59.727: epoch 26:	0.02954593  	0.07999476  	0.06592359  
2023-05-23 09:36:59.727: Find a better model.
2023-05-23 09:37:21.452: [iter 27 : loss : 0.6871 = 0.2266 + 0.4574 + 0.0031, time: 21.721443]
2023-05-23 09:37:21.725: epoch 27:	0.02953853  	0.08014482  	0.06577416  
2023-05-23 09:37:21.726: Find a better model.
2023-05-23 09:37:43.455: [iter 28 : loss : 0.6703 = 0.2099 + 0.4571 + 0.0033, time: 21.725096]
2023-05-23 09:37:43.722: epoch 28:	0.02952372  	0.08018030  	0.06589988  
2023-05-23 09:37:43.722: Find a better model.
2023-05-23 09:38:05.237: [iter 29 : loss : 0.6561 = 0.1958 + 0.4567 + 0.0036, time: 21.510593]
2023-05-23 09:38:05.501: epoch 29:	0.02968659  	0.08016036  	0.06623205  
2023-05-23 09:38:27.061: [iter 30 : loss : 0.6427 = 0.1827 + 0.4562 + 0.0038, time: 21.556392]
2023-05-23 09:38:27.332: epoch 30:	0.02971620  	0.08048716  	0.06634996  
2023-05-23 09:38:27.332: Find a better model.
2023-05-23 09:38:48.829: [iter 31 : loss : 0.6314 = 0.1716 + 0.4557 + 0.0040, time: 21.494085]
2023-05-23 09:38:49.097: epoch 31:	0.02955333  	0.07981744  	0.06613691  
2023-05-23 09:39:10.658: [iter 32 : loss : 0.6215 = 0.1621 + 0.4551 + 0.0042, time: 21.557445]
2023-05-23 09:39:10.928: epoch 32:	0.02967918  	0.08009637  	0.06629741  
2023-05-23 09:39:32.608: [iter 33 : loss : 0.6132 = 0.1542 + 0.4546 + 0.0044, time: 21.676238]
2023-05-23 09:39:32.873: epoch 33:	0.02961995  	0.08044227  	0.06645868  
2023-05-23 09:39:54.391: [iter 34 : loss : 0.6048 = 0.1461 + 0.4541 + 0.0046, time: 21.514311]
2023-05-23 09:39:54.660: epoch 34:	0.02973840  	0.08060360  	0.06650188  
2023-05-23 09:39:54.660: Find a better model.
2023-05-23 09:40:16.002: [iter 35 : loss : 0.5966 = 0.1383 + 0.4535 + 0.0048, time: 21.338268]
2023-05-23 09:40:16.285: epoch 35:	0.02961255  	0.08034002  	0.06642958  
2023-05-23 09:40:38.015: [iter 36 : loss : 0.5905 = 0.1325 + 0.4530 + 0.0050, time: 21.725556]
2023-05-23 09:40:38.287: epoch 36:	0.02974581  	0.08031386  	0.06655337  
2023-05-23 09:40:59.977: [iter 37 : loss : 0.5846 = 0.1269 + 0.4525 + 0.0052, time: 21.687445]
2023-05-23 09:41:00.245: epoch 37:	0.02970880  	0.08052211  	0.06664221  
2023-05-23 09:41:21.819: [iter 38 : loss : 0.5789 = 0.1215 + 0.4521 + 0.0054, time: 21.571158]
2023-05-23 09:41:22.087: epoch 38:	0.02967179  	0.08084224  	0.06674908  
2023-05-23 09:41:22.087: Find a better model.
2023-05-23 09:41:43.765: [iter 39 : loss : 0.5741 = 0.1170 + 0.4516 + 0.0055, time: 21.673497]
2023-05-23 09:41:44.030: epoch 39:	0.02973102  	0.08049995  	0.06682508  
2023-05-23 09:42:05.759: [iter 40 : loss : 0.5688 = 0.1119 + 0.4512 + 0.0057, time: 21.724846]
2023-05-23 09:42:06.026: epoch 40:	0.02956815  	0.08017276  	0.06659976  
2023-05-23 09:42:27.380: [iter 41 : loss : 0.5642 = 0.1076 + 0.4508 + 0.0059, time: 21.348949]
2023-05-23 09:42:27.649: epoch 41:	0.02959776  	0.08049025  	0.06668525  
2023-05-23 09:42:49.148: [iter 42 : loss : 0.5602 = 0.1037 + 0.4505 + 0.0060, time: 21.496095]
2023-05-23 09:42:49.428: epoch 42:	0.02951633  	0.08017202  	0.06657892  
2023-05-23 09:43:10.960: [iter 43 : loss : 0.5564 = 0.1002 + 0.4501 + 0.0062, time: 21.528056]
2023-05-23 09:43:11.231: epoch 43:	0.02950152  	0.07989471  	0.06639853  
2023-05-23 09:43:32.550: [iter 44 : loss : 0.5538 = 0.0977 + 0.4497 + 0.0063, time: 21.313985]
2023-05-23 09:43:32.817: epoch 44:	0.02943489  	0.07958950  	0.06632448  
2023-05-23 09:43:54.317: [iter 45 : loss : 0.5501 = 0.0942 + 0.4494 + 0.0065, time: 21.496382]
2023-05-23 09:43:54.582: epoch 45:	0.02929422  	0.07889233  	0.06594268  
2023-05-23 09:44:16.356: [iter 46 : loss : 0.5470 = 0.0913 + 0.4491 + 0.0066, time: 21.771040]
2023-05-23 09:44:16.628: epoch 46:	0.02921278  	0.07863543  	0.06581873  
2023-05-23 09:44:38.127: [iter 47 : loss : 0.5440 = 0.0885 + 0.4488 + 0.0068, time: 21.495591]
2023-05-23 09:44:38.393: epoch 47:	0.02917577  	0.07846438  	0.06568556  
2023-05-23 09:44:59.912: [iter 48 : loss : 0.5415 = 0.0861 + 0.4485 + 0.0069, time: 21.514977]
2023-05-23 09:45:00.183: epoch 48:	0.02907212  	0.07829636  	0.06559085  
2023-05-23 09:45:21.880: [iter 49 : loss : 0.5389 = 0.0837 + 0.4481 + 0.0071, time: 21.692959]
2023-05-23 09:45:22.144: epoch 49:	0.02910173  	0.07826931  	0.06563452  
2023-05-23 09:45:43.683: [iter 50 : loss : 0.5367 = 0.0816 + 0.4479 + 0.0072, time: 21.534898]
2023-05-23 09:45:43.946: epoch 50:	0.02887223  	0.07801391  	0.06544017  
2023-05-23 09:46:05.657: [iter 51 : loss : 0.5342 = 0.0792 + 0.4476 + 0.0074, time: 21.706921]
2023-05-23 09:46:05.924: epoch 51:	0.02885743  	0.07801515  	0.06536761  
2023-05-23 09:46:27.483: [iter 52 : loss : 0.5317 = 0.0768 + 0.4474 + 0.0075, time: 21.556206]
2023-05-23 09:46:27.751: epoch 52:	0.02884262  	0.07768820  	0.06524917  
2023-05-23 09:46:49.080: [iter 53 : loss : 0.5309 = 0.0761 + 0.4472 + 0.0076, time: 21.323537]
2023-05-23 09:46:49.342: epoch 53:	0.02876119  	0.07725558  	0.06510050  
2023-05-23 09:47:11.044: [iter 54 : loss : 0.5286 = 0.0739 + 0.4470 + 0.0077, time: 21.697596]
2023-05-23 09:47:11.308: epoch 54:	0.02864273  	0.07686792  	0.06495751  
2023-05-23 09:47:33.036: [iter 55 : loss : 0.5266 = 0.0720 + 0.4468 + 0.0079, time: 21.724082]
2023-05-23 09:47:33.300: epoch 55:	0.02859832  	0.07707084  	0.06494118  
2023-05-23 09:47:54.846: [iter 56 : loss : 0.5244 = 0.0699 + 0.4465 + 0.0080, time: 21.541564]
2023-05-23 09:47:55.109: epoch 56:	0.02854649  	0.07671004  	0.06473901  
2023-05-23 09:48:16.657: [iter 57 : loss : 0.5228 = 0.0683 + 0.4464 + 0.0081, time: 21.542681]
2023-05-23 09:48:16.928: epoch 57:	0.02847246  	0.07644732  	0.06462878  
2023-05-23 09:48:38.809: [iter 58 : loss : 0.5215 = 0.0670 + 0.4462 + 0.0082, time: 21.878706]
2023-05-23 09:48:39.071: epoch 58:	0.02844284  	0.07586266  	0.06441376  
2023-05-23 09:49:00.444: [iter 59 : loss : 0.5198 = 0.0654 + 0.4461 + 0.0083, time: 21.368006]
2023-05-23 09:49:00.713: epoch 59:	0.02836141  	0.07568884  	0.06432313  
2023-05-23 09:49:22.050: [iter 60 : loss : 0.5186 = 0.0642 + 0.4459 + 0.0084, time: 21.332669]
2023-05-23 09:49:22.316: epoch 60:	0.02836881  	0.07552843  	0.06413924  
2023-05-23 09:49:43.812: [iter 61 : loss : 0.5174 = 0.0631 + 0.4457 + 0.0086, time: 21.492623]
2023-05-23 09:49:44.077: epoch 61:	0.02828738  	0.07510784  	0.06397056  
2023-05-23 09:50:05.435: [iter 62 : loss : 0.5163 = 0.0620 + 0.4456 + 0.0087, time: 21.354526]
2023-05-23 09:50:05.699: epoch 62:	0.02825776  	0.07514953  	0.06389862  
2023-05-23 09:50:27.001: [iter 63 : loss : 0.5151 = 0.0609 + 0.4455 + 0.0088, time: 21.297051]
2023-05-23 09:50:27.266: epoch 63:	0.02819113  	0.07493383  	0.06389611  
2023-05-23 09:50:27.266: Early stopping is trigger at epoch: 63
2023-05-23 09:50:27.266: best_result@epoch 38:

2023-05-23 09:50:27.266: 		0.0297      	0.0808      	0.0667      
2023-05-23 09:50:56.850: my pid: 8820
2023-05-23 09:50:56.850: model: model.general_recommender.SGL
2023-05-23 09:50:56.850: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-23 09:50:56.850: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-23 09:51:00.860: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-23 09:51:23.309: [iter 1 : loss : 1.1319 = 0.6931 + 0.4388 + 0.0000, time: 22.449085]
2023-05-23 09:51:23.572: epoch 1:	0.00182855  	0.00348994  	0.00310760  
2023-05-23 09:51:23.572: Find a better model.
2023-05-23 09:51:46.086: [iter 2 : loss : 1.1305 = 0.6930 + 0.4375 + 0.0000, time: 22.511688]
2023-05-23 09:51:46.399: epoch 2:	0.00249482  	0.00523164  	0.00431707  
2023-05-23 09:51:46.399: Find a better model.
2023-05-23 09:52:08.801: [iter 3 : loss : 1.1303 = 0.6930 + 0.4373 + 0.0000, time: 22.398711]
2023-05-23 09:52:09.098: epoch 3:	0.00319070  	0.00688619  	0.00545971  
2023-05-23 09:52:09.098: Find a better model.
2023-05-23 09:52:31.620: [iter 4 : loss : 1.1301 = 0.6929 + 0.4372 + 0.0000, time: 22.517236]
2023-05-23 09:52:31.917: epoch 4:	0.00362008  	0.00775678  	0.00622629  
2023-05-23 09:52:31.917: Find a better model.
2023-05-23 09:52:54.235: [iter 5 : loss : 1.1300 = 0.6928 + 0.4372 + 0.0000, time: 22.314866]
2023-05-23 09:52:54.535: epoch 5:	0.00433076  	0.00912444  	0.00786088  
2023-05-23 09:52:54.536: Find a better model.
2023-05-23 09:53:16.830: [iter 6 : loss : 1.1298 = 0.6927 + 0.4372 + 0.0000, time: 22.291033]
2023-05-23 09:53:17.126: epoch 6:	0.00509327  	0.01099684  	0.00948311  
2023-05-23 09:53:17.126: Find a better model.
2023-05-23 09:53:39.392: [iter 7 : loss : 1.1297 = 0.6925 + 0.4372 + 0.0000, time: 22.262532]
2023-05-23 09:53:39.694: epoch 7:	0.00612968  	0.01452651  	0.01165587  
2023-05-23 09:53:39.694: Find a better model.
2023-05-23 09:54:02.032: [iter 8 : loss : 1.1294 = 0.6922 + 0.4371 + 0.0000, time: 22.333632]
2023-05-23 09:54:02.325: epoch 8:	0.00687737  	0.01691936  	0.01387081  
2023-05-23 09:54:02.325: Find a better model.
2023-05-23 09:54:24.557: [iter 9 : loss : 1.1290 = 0.6919 + 0.4372 + 0.0000, time: 22.229005]
2023-05-23 09:54:24.850: epoch 9:	0.00785456  	0.02017034  	0.01649421  
2023-05-23 09:54:24.850: Find a better model.
2023-05-23 09:54:46.956: [iter 10 : loss : 1.1285 = 0.6912 + 0.4372 + 0.0000, time: 22.103768]
2023-05-23 09:54:47.248: epoch 10:	0.00897241  	0.02372838  	0.01939534  
2023-05-23 09:54:47.248: Find a better model.
2023-05-23 09:55:09.350: [iter 11 : loss : 1.1276 = 0.6902 + 0.4373 + 0.0000, time: 22.098142]
2023-05-23 09:55:09.652: epoch 11:	0.00980155  	0.02642752  	0.02111850  
2023-05-23 09:55:09.652: Find a better model.
2023-05-23 09:55:31.940: [iter 12 : loss : 1.1258 = 0.6883 + 0.4374 + 0.0000, time: 22.283551]
2023-05-23 09:55:32.225: epoch 12:	0.01011989  	0.02770312  	0.02243238  
2023-05-23 09:55:32.225: Find a better model.
2023-05-23 09:55:54.334: [iter 13 : loss : 1.1224 = 0.6846 + 0.4377 + 0.0000, time: 22.106039]
2023-05-23 09:55:54.639: epoch 13:	0.01111931  	0.03018707  	0.02463679  
2023-05-23 09:55:54.639: Find a better model.
2023-05-23 09:56:16.707: [iter 14 : loss : 1.1161 = 0.6780 + 0.4380 + 0.0001, time: 22.064807]
2023-05-23 09:56:16.992: epoch 14:	0.01280722  	0.03530290  	0.02960674  
2023-05-23 09:56:16.992: Find a better model.
2023-05-23 09:56:39.130: [iter 15 : loss : 1.1049 = 0.6663 + 0.4385 + 0.0001, time: 22.133788]
2023-05-23 09:56:39.413: epoch 15:	0.01521325  	0.04227474  	0.03535509  
2023-05-23 09:56:39.413: Find a better model.
2023-05-23 09:57:01.355: [iter 16 : loss : 1.0856 = 0.6462 + 0.4392 + 0.0002, time: 21.938928]
2023-05-23 09:57:01.638: epoch 16:	0.01798204  	0.05000814  	0.04216740  
2023-05-23 09:57:01.638: Find a better model.
2023-05-23 09:57:23.540: [iter 17 : loss : 1.0561 = 0.6154 + 0.4403 + 0.0003, time: 21.898525]
2023-05-23 09:57:23.826: epoch 17:	0.02129871  	0.05834195  	0.04964576  
2023-05-23 09:57:23.827: Find a better model.
2023-05-23 09:57:45.727: [iter 18 : loss : 1.0160 = 0.5735 + 0.4420 + 0.0005, time: 21.897655]
2023-05-23 09:57:46.010: epoch 18:	0.02377140  	0.06520482  	0.05545130  
2023-05-23 09:57:46.010: Find a better model.
2023-05-23 09:58:07.896: [iter 19 : loss : 0.9693 = 0.5243 + 0.4442 + 0.0008, time: 21.882391]
2023-05-23 09:58:08.179: epoch 19:	0.02555558  	0.06874596  	0.05903621  
2023-05-23 09:58:08.179: Find a better model.
2023-05-23 09:58:30.286: [iter 20 : loss : 0.9202 = 0.4721 + 0.4470 + 0.0010, time: 22.103376]
2023-05-23 09:58:30.572: epoch 20:	0.02700662  	0.07315027  	0.06191901  
2023-05-23 09:58:30.572: Find a better model.
2023-05-23 09:58:52.498: [iter 21 : loss : 0.8730 = 0.4219 + 0.4498 + 0.0013, time: 21.921745]
2023-05-23 09:58:52.795: epoch 21:	0.02788760  	0.07559296  	0.06337382  
2023-05-23 09:58:52.795: Find a better model.
2023-05-23 09:59:14.676: [iter 22 : loss : 0.8289 = 0.3752 + 0.4521 + 0.0016, time: 21.876953]
2023-05-23 09:59:14.963: epoch 22:	0.02836141  	0.07704054  	0.06418281  
2023-05-23 09:59:14.963: Find a better model.
2023-05-23 09:59:36.859: [iter 23 : loss : 0.7917 = 0.3358 + 0.4540 + 0.0019, time: 21.893000]
2023-05-23 09:59:37.140: epoch 23:	0.02842064  	0.07770929  	0.06430294  
2023-05-23 09:59:37.140: Find a better model.
2023-05-23 09:59:59.096: [iter 24 : loss : 0.7599 = 0.3024 + 0.4552 + 0.0022, time: 21.951248]
2023-05-23 09:59:59.375: epoch 24:	0.02849467  	0.07826107  	0.06452478  
2023-05-23 09:59:59.376: Find a better model.
2023-05-23 10:00:21.492: [iter 25 : loss : 0.7321 = 0.2738 + 0.4558 + 0.0025, time: 22.112982]
2023-05-23 10:00:21.759: epoch 25:	0.02875379  	0.07879530  	0.06493892  
2023-05-23 10:00:21.759: Find a better model.
2023-05-23 10:00:43.641: [iter 26 : loss : 0.7094 = 0.2505 + 0.4561 + 0.0028, time: 21.878499]
2023-05-23 10:00:43.926: epoch 26:	0.02893147  	0.07922485  	0.06529580  
2023-05-23 10:00:43.927: Find a better model.
2023-05-23 10:01:05.836: [iter 27 : loss : 0.6897 = 0.2306 + 0.4561 + 0.0030, time: 21.905493]
2023-05-23 10:01:06.112: epoch 27:	0.02894628  	0.07897657  	0.06538238  
2023-05-23 10:01:28.267: [iter 28 : loss : 0.6728 = 0.2137 + 0.4558 + 0.0033, time: 22.152204]
2023-05-23 10:01:28.544: epoch 28:	0.02912394  	0.07941110  	0.06577630  
2023-05-23 10:01:28.544: Find a better model.
2023-05-23 10:01:50.619: [iter 29 : loss : 0.6583 = 0.1994 + 0.4554 + 0.0035, time: 22.072070]
2023-05-23 10:01:50.898: epoch 29:	0.02912395  	0.07928688  	0.06584295  
2023-05-23 10:02:13.028: [iter 30 : loss : 0.6448 = 0.1861 + 0.4549 + 0.0038, time: 22.127005]
2023-05-23 10:02:13.303: epoch 30:	0.02921279  	0.07958364  	0.06582265  
2023-05-23 10:02:13.303: Find a better model.
2023-05-23 10:02:35.422: [iter 31 : loss : 0.6333 = 0.1748 + 0.4545 + 0.0040, time: 22.114357]
2023-05-23 10:02:35.699: epoch 31:	0.02925722  	0.08001896  	0.06607717  
2023-05-23 10:02:35.699: Find a better model.
2023-05-23 10:02:58.228: [iter 32 : loss : 0.6229 = 0.1648 + 0.4539 + 0.0042, time: 22.524991]
2023-05-23 10:02:58.501: epoch 32:	0.02930905  	0.08001143  	0.06634738  
2023-05-23 10:03:20.783: [iter 33 : loss : 0.6145 = 0.1568 + 0.4534 + 0.0044, time: 22.277208]
2023-05-23 10:03:21.068: epoch 33:	0.02920539  	0.07985596  	0.06634658  
2023-05-23 10:03:43.170: [iter 34 : loss : 0.6061 = 0.1487 + 0.4528 + 0.0046, time: 22.097159]
2023-05-23 10:03:43.447: epoch 34:	0.02916838  	0.07976557  	0.06631754  
2023-05-23 10:04:05.572: [iter 35 : loss : 0.5977 = 0.1407 + 0.4522 + 0.0048, time: 22.121488]
2023-05-23 10:04:05.850: epoch 35:	0.02919799  	0.07983383  	0.06627920  
2023-05-23 10:04:28.157: [iter 36 : loss : 0.5916 = 0.1349 + 0.4517 + 0.0050, time: 22.304365]
2023-05-23 10:04:28.432: epoch 36:	0.02911656  	0.07955452  	0.06639993  
2023-05-23 10:04:50.563: [iter 37 : loss : 0.5856 = 0.1292 + 0.4512 + 0.0052, time: 22.125440]
2023-05-23 10:04:50.839: epoch 37:	0.02888706  	0.07916639  	0.06613546  
2023-05-23 10:05:12.781: [iter 38 : loss : 0.5798 = 0.1236 + 0.4508 + 0.0053, time: 21.939847]
2023-05-23 10:05:13.059: epoch 38:	0.02899070  	0.07907070  	0.06636181  
2023-05-23 10:05:35.131: [iter 39 : loss : 0.5747 = 0.1189 + 0.4503 + 0.0055, time: 22.067216]
2023-05-23 10:05:35.406: epoch 39:	0.02896850  	0.07853343  	0.06625648  
2023-05-23 10:05:57.538: [iter 40 : loss : 0.5695 = 0.1139 + 0.4499 + 0.0057, time: 22.129436]
2023-05-23 10:05:57.814: epoch 40:	0.02879082  	0.07808536  	0.06614169  
2023-05-23 10:06:19.910: [iter 41 : loss : 0.5649 = 0.1096 + 0.4494 + 0.0058, time: 22.091816]
2023-05-23 10:06:20.200: epoch 41:	0.02876120  	0.07794233  	0.06620146  
2023-05-23 10:06:42.326: [iter 42 : loss : 0.5608 = 0.1057 + 0.4491 + 0.0060, time: 22.122594]
2023-05-23 10:06:42.598: epoch 42:	0.02886485  	0.07809041  	0.06632975  
2023-05-23 10:07:04.904: [iter 43 : loss : 0.5569 = 0.1020 + 0.4487 + 0.0062, time: 22.301880]
2023-05-23 10:07:05.182: epoch 43:	0.02887224  	0.07808271  	0.06636728  
2023-05-23 10:07:27.512: [iter 44 : loss : 0.5544 = 0.0997 + 0.4484 + 0.0063, time: 22.325838]
2023-05-23 10:07:27.786: epoch 44:	0.02882042  	0.07748441  	0.06620603  
2023-05-23 10:07:50.294: [iter 45 : loss : 0.5502 = 0.0957 + 0.4480 + 0.0065, time: 22.504360]
2023-05-23 10:07:50.564: epoch 45:	0.02883523  	0.07731830  	0.06604931  
2023-05-23 10:08:12.912: [iter 46 : loss : 0.5470 = 0.0927 + 0.4477 + 0.0066, time: 22.344452]
2023-05-23 10:08:13.183: epoch 46:	0.02875379  	0.07727806  	0.06595615  
2023-05-23 10:08:35.480: [iter 47 : loss : 0.5443 = 0.0902 + 0.4474 + 0.0068, time: 22.292949]
2023-05-23 10:08:35.749: epoch 47:	0.02870197  	0.07741985  	0.06594353  
2023-05-23 10:08:58.071: [iter 48 : loss : 0.5417 = 0.0877 + 0.4471 + 0.0069, time: 22.317708]
2023-05-23 10:08:58.354: epoch 48:	0.02871677  	0.07725866  	0.06591015  
2023-05-23 10:09:20.869: [iter 49 : loss : 0.5388 = 0.0850 + 0.4468 + 0.0071, time: 22.511709]
2023-05-23 10:09:21.138: epoch 49:	0.02869457  	0.07712467  	0.06584509  
2023-05-23 10:09:43.647: [iter 50 : loss : 0.5366 = 0.0830 + 0.4465 + 0.0072, time: 22.503941]
2023-05-23 10:09:43.913: epoch 50:	0.02863534  	0.07682972  	0.06562891  
2023-05-23 10:10:06.254: [iter 51 : loss : 0.5343 = 0.0807 + 0.4462 + 0.0073, time: 22.335089]
2023-05-23 10:10:06.527: epoch 51:	0.02858353  	0.07677104  	0.06565465  
2023-05-23 10:10:28.822: [iter 52 : loss : 0.5315 = 0.0781 + 0.4460 + 0.0074, time: 22.291771]
2023-05-23 10:10:29.096: epoch 52:	0.02848727  	0.07626428  	0.06542812  
2023-05-23 10:10:51.625: [iter 53 : loss : 0.5306 = 0.0772 + 0.4458 + 0.0076, time: 22.524668]
2023-05-23 10:10:51.895: epoch 53:	0.02837624  	0.07585467  	0.06521933  
2023-05-23 10:11:14.226: [iter 54 : loss : 0.5284 = 0.0752 + 0.4455 + 0.0077, time: 22.327494]
2023-05-23 10:11:14.502: epoch 54:	0.02828739  	0.07573266  	0.06502839  
2023-05-23 10:11:36.819: [iter 55 : loss : 0.5264 = 0.0733 + 0.4453 + 0.0078, time: 22.313337]
2023-05-23 10:11:37.093: epoch 55:	0.02835401  	0.07600287  	0.06512810  
2023-05-23 10:11:59.436: [iter 56 : loss : 0.5242 = 0.0711 + 0.4451 + 0.0080, time: 22.340265]
2023-05-23 10:11:59.708: epoch 56:	0.02839102  	0.07606299  	0.06506881  
2023-05-23 10:11:59.708: Early stopping is trigger at epoch: 56
2023-05-23 10:11:59.708: best_result@epoch 31:

2023-05-23 10:11:59.708: 		0.0293      	0.0800      	0.0661      
2023-05-23 10:21:48.806: my pid: 2008
2023-05-23 10:21:48.806: model: model.general_recommender.SGL
2023-05-23 10:21:48.806: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-23 10:21:48.806: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 16:24:41.719: my pid: 3400
2023-05-29 16:24:41.719: model: model.general_recommender.SGL
2023-05-29 16:24:41.719: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 16:24:41.719: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 16:24:45.784: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 16:25:08.108: [iter 1 : loss : 1.1319 = 0.6931 + 0.4388 + 0.0000, time: 22.323891]
2023-05-29 16:25:08.375: epoch 1:	0.00182855  	0.00348994  	0.00310760  
2023-05-29 16:25:08.375: Find a better model.
2023-05-29 16:25:30.861: [iter 2 : loss : 1.1305 = 0.6930 + 0.4375 + 0.0000, time: 22.483188]
2023-05-29 16:25:31.149: epoch 2:	0.00249482  	0.00523164  	0.00431707  
2023-05-29 16:25:31.149: Find a better model.
2023-05-29 16:25:53.499: [iter 3 : loss : 1.1303 = 0.6930 + 0.4373 + 0.0000, time: 22.345415]
2023-05-29 16:25:53.792: epoch 3:	0.00319070  	0.00688619  	0.00545971  
2023-05-29 16:25:53.792: Find a better model.
2023-05-29 16:26:16.218: [iter 4 : loss : 1.1301 = 0.6929 + 0.4372 + 0.0000, time: 22.422229]
2023-05-29 16:26:16.509: epoch 4:	0.00362008  	0.00775678  	0.00622629  
2023-05-29 16:26:16.509: Find a better model.
2023-05-29 16:26:38.674: [iter 5 : loss : 1.1300 = 0.6928 + 0.4372 + 0.0000, time: 22.162163]
2023-05-29 16:26:38.965: epoch 5:	0.00433076  	0.00912444  	0.00786088  
2023-05-29 16:26:38.965: Find a better model.
2023-05-29 16:27:01.287: [iter 6 : loss : 1.1298 = 0.6927 + 0.4372 + 0.0000, time: 22.319216]
2023-05-29 16:27:01.574: epoch 6:	0.00509327  	0.01099684  	0.00948311  
2023-05-29 16:27:01.574: Find a better model.
2023-05-29 16:27:23.839: [iter 7 : loss : 1.1297 = 0.6925 + 0.4372 + 0.0000, time: 22.260303]
2023-05-29 16:27:24.124: epoch 7:	0.00612968  	0.01452651  	0.01165587  
2023-05-29 16:27:24.124: Find a better model.
2023-05-29 16:27:46.426: [iter 8 : loss : 1.1294 = 0.6922 + 0.4371 + 0.0000, time: 22.298627]
2023-05-29 16:27:46.712: epoch 8:	0.00687737  	0.01691936  	0.01387081  
2023-05-29 16:27:46.712: Find a better model.
2023-05-29 16:28:09.014: [iter 9 : loss : 1.1290 = 0.6919 + 0.4372 + 0.0000, time: 22.298895]
2023-05-29 16:28:09.297: epoch 9:	0.00785456  	0.02017034  	0.01649421  
2023-05-29 16:28:09.297: Find a better model.
2023-05-29 16:28:31.390: [iter 10 : loss : 1.1285 = 0.6912 + 0.4372 + 0.0000, time: 22.090158]
2023-05-29 16:28:31.683: epoch 10:	0.00897241  	0.02372838  	0.01939534  
2023-05-29 16:28:31.683: Find a better model.
2023-05-29 16:28:54.370: [iter 11 : loss : 1.1275 = 0.6902 + 0.4372 + 0.0000, time: 22.683058]
2023-05-29 16:28:54.659: epoch 11:	0.00992740  	0.02664746  	0.02153187  
2023-05-29 16:28:54.659: Find a better model.
2023-05-29 16:29:17.404: [iter 12 : loss : 1.1257 = 0.6883 + 0.4374 + 0.0000, time: 22.740500]
2023-05-29 16:29:17.691: epoch 12:	0.01037160  	0.02793598  	0.02295421  
2023-05-29 16:29:17.691: Find a better model.
2023-05-29 16:29:40.160: [iter 13 : loss : 1.1222 = 0.6846 + 0.4376 + 0.0000, time: 22.466249]
2023-05-29 16:29:40.437: epoch 13:	0.01134140  	0.03109615  	0.02522458  
2023-05-29 16:29:40.437: Find a better model.
2023-05-29 16:30:02.991: [iter 14 : loss : 1.1159 = 0.6779 + 0.4379 + 0.0001, time: 22.550177]
2023-05-29 16:30:03.268: epoch 14:	0.01315517  	0.03620530  	0.03019635  
2023-05-29 16:30:03.268: Find a better model.
2023-05-29 16:30:25.942: [iter 15 : loss : 1.1046 = 0.6661 + 0.4384 + 0.0001, time: 22.671204]
2023-05-29 16:30:26.217: epoch 15:	0.01533911  	0.04244620  	0.03573320  
2023-05-29 16:30:26.217: Find a better model.
2023-05-29 16:30:48.781: [iter 16 : loss : 1.0851 = 0.6458 + 0.4391 + 0.0002, time: 22.560553]
2023-05-29 16:30:49.057: epoch 16:	0.01826337  	0.05083506  	0.04293715  
2023-05-29 16:30:49.057: Find a better model.
2023-05-29 16:31:11.553: [iter 17 : loss : 1.0554 = 0.6148 + 0.4402 + 0.0003, time: 22.492605]
2023-05-29 16:31:11.834: epoch 17:	0.02151341  	0.05923179  	0.05026772  
2023-05-29 16:31:11.834: Find a better model.
2023-05-29 16:31:34.149: [iter 18 : loss : 1.0150 = 0.5725 + 0.4419 + 0.0005, time: 22.311500]
2023-05-29 16:31:34.426: epoch 18:	0.02405272  	0.06597859  	0.05593808  
2023-05-29 16:31:34.426: Find a better model.
2023-05-29 16:31:56.928: [iter 19 : loss : 0.9681 = 0.5231 + 0.4442 + 0.0008, time: 22.497994]
2023-05-29 16:31:57.204: epoch 19:	0.02592575  	0.06984095  	0.05967985  
2023-05-29 16:31:57.204: Find a better model.
2023-05-29 16:32:19.735: [iter 20 : loss : 0.9189 = 0.4708 + 0.4471 + 0.0010, time: 22.527302]
2023-05-29 16:32:20.012: epoch 20:	0.02711767  	0.07330117  	0.06228671  
2023-05-29 16:32:20.012: Find a better model.
2023-05-29 16:32:42.502: [iter 21 : loss : 0.8718 = 0.4206 + 0.4498 + 0.0013, time: 22.486597]
2023-05-29 16:32:42.780: epoch 21:	0.02798385  	0.07577834  	0.06354813  
2023-05-29 16:32:42.780: Find a better model.
2023-05-29 16:33:05.109: [iter 22 : loss : 0.8278 = 0.3739 + 0.4522 + 0.0016, time: 22.326026]
2023-05-29 16:33:05.380: epoch 22:	0.02860572  	0.07739609  	0.06447066  
2023-05-29 16:33:05.380: Find a better model.
2023-05-29 16:33:27.871: [iter 23 : loss : 0.7907 = 0.3347 + 0.4541 + 0.0019, time: 22.485724]
2023-05-29 16:33:28.141: epoch 23:	0.02865755  	0.07789989  	0.06445702  
2023-05-29 16:33:28.141: Find a better model.
2023-05-29 16:33:50.876: [iter 24 : loss : 0.7590 = 0.3015 + 0.4553 + 0.0022, time: 22.730891]
2023-05-29 16:33:51.146: epoch 24:	0.02859092  	0.07823383  	0.06471170  
2023-05-29 16:33:51.146: Find a better model.
2023-05-29 16:34:13.478: [iter 25 : loss : 0.7314 = 0.2730 + 0.4558 + 0.0025, time: 22.328044]
2023-05-29 16:34:13.748: epoch 25:	0.02886483  	0.07900462  	0.06516122  
2023-05-29 16:34:13.748: Find a better model.
2023-05-29 16:34:36.270: [iter 26 : loss : 0.7087 = 0.2498 + 0.4561 + 0.0028, time: 22.517534]
2023-05-29 16:34:36.538: epoch 26:	0.02902030  	0.07959957  	0.06543422  
2023-05-29 16:34:36.538: Find a better model.
2023-05-29 16:34:59.022: [iter 27 : loss : 0.6891 = 0.2300 + 0.4560 + 0.0030, time: 22.481178]
2023-05-29 16:34:59.295: epoch 27:	0.02910913  	0.07957847  	0.06559149  
2023-05-29 16:35:21.668: [iter 28 : loss : 0.6722 = 0.2132 + 0.4558 + 0.0033, time: 22.370610]
2023-05-29 16:35:21.934: epoch 28:	0.02904991  	0.07926123  	0.06571659  
2023-05-29 16:35:44.276: [iter 29 : loss : 0.6578 = 0.1989 + 0.4553 + 0.0035, time: 22.338298]
2023-05-29 16:35:44.545: epoch 29:	0.02922019  	0.07939553  	0.06594537  
2023-05-29 16:36:07.046: [iter 30 : loss : 0.6443 = 0.1857 + 0.4549 + 0.0038, time: 22.497318]
2023-05-29 16:36:07.314: epoch 30:	0.02926461  	0.07974985  	0.06599995  
2023-05-29 16:36:07.314: Find a better model.
2023-05-29 16:36:30.034: [iter 31 : loss : 0.6328 = 0.1744 + 0.4544 + 0.0040, time: 22.717203]
2023-05-29 16:36:30.302: epoch 31:	0.02932385  	0.08006988  	0.06615913  
2023-05-29 16:36:30.302: Find a better model.
2023-05-29 16:36:52.852: [iter 32 : loss : 0.6225 = 0.1645 + 0.4538 + 0.0042, time: 22.547004]
2023-05-29 16:36:53.124: epoch 32:	0.02937567  	0.08051696  	0.06642798  
2023-05-29 16:36:53.124: Find a better model.
2023-05-29 16:37:16.004: [iter 33 : loss : 0.6141 = 0.1564 + 0.4532 + 0.0044, time: 22.876818]
2023-05-29 16:37:16.270: epoch 33:	0.02941268  	0.08027423  	0.06661166  
2023-05-29 16:37:38.647: [iter 34 : loss : 0.6057 = 0.1484 + 0.4527 + 0.0046, time: 22.373149]
2023-05-29 16:37:38.915: epoch 34:	0.02933865  	0.08041153  	0.06684229  
2023-05-29 16:38:01.431: [iter 35 : loss : 0.5973 = 0.1404 + 0.4521 + 0.0048, time: 22.510793]
2023-05-29 16:38:01.700: epoch 35:	0.02915357  	0.07966395  	0.06634710  
2023-05-29 16:38:24.426: [iter 36 : loss : 0.5912 = 0.1346 + 0.4516 + 0.0050, time: 22.720617]
2023-05-29 16:38:24.694: epoch 36:	0.02903511  	0.07922933  	0.06640464  
2023-05-29 16:38:47.221: [iter 37 : loss : 0.5852 = 0.1289 + 0.4511 + 0.0052, time: 22.524119]
2023-05-29 16:38:47.487: epoch 37:	0.02903511  	0.07901251  	0.06626046  
2023-05-29 16:39:10.009: [iter 38 : loss : 0.5794 = 0.1234 + 0.4506 + 0.0054, time: 22.518196]
2023-05-29 16:39:10.273: epoch 38:	0.02895368  	0.07855540  	0.06611366  
2023-05-29 16:39:33.170: [iter 39 : loss : 0.5744 = 0.1187 + 0.4501 + 0.0055, time: 22.892608]
2023-05-29 16:39:33.438: epoch 39:	0.02887965  	0.07804461  	0.06611282  
2023-05-29 16:39:56.165: [iter 40 : loss : 0.5691 = 0.1137 + 0.4497 + 0.0057, time: 22.723158]
2023-05-29 16:39:56.434: epoch 40:	0.02888704  	0.07803418  	0.06624875  
2023-05-29 16:40:19.164: [iter 41 : loss : 0.5645 = 0.1094 + 0.4493 + 0.0059, time: 22.727054]
2023-05-29 16:40:19.433: epoch 41:	0.02886483  	0.07805035  	0.06630178  
2023-05-29 16:40:42.342: [iter 42 : loss : 0.5605 = 0.1055 + 0.4490 + 0.0060, time: 22.906101]
2023-05-29 16:40:42.621: epoch 42:	0.02883522  	0.07796775  	0.06629212  
2023-05-29 16:41:05.375: [iter 43 : loss : 0.5565 = 0.1018 + 0.4486 + 0.0062, time: 22.749485]
2023-05-29 16:41:05.670: epoch 43:	0.02880562  	0.07751367  	0.06627509  
2023-05-29 16:41:29.835: [iter 44 : loss : 0.5540 = 0.0995 + 0.4482 + 0.0063, time: 24.159989]
2023-05-29 16:41:30.113: epoch 44:	0.02876118  	0.07702670  	0.06598144  
2023-05-29 16:41:59.746: my pid: 6776
2023-05-29 16:41:59.746: model: model.general_recommender.SGL
2023-05-29 16:41:59.747: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 16:41:59.747: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 16:42:04.183: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 16:42:26.761: [iter 1 : loss : 1.1327 = 0.6931 + 0.4397 + 0.0000, time: 22.576624]
2023-05-29 16:42:27.030: epoch 1:	0.00180634  	0.00388100  	0.00308223  
2023-05-29 16:42:27.030: Find a better model.
2023-05-29 16:42:48.889: [iter 2 : loss : 1.1314 = 0.6930 + 0.4384 + 0.0000, time: 21.855969]
2023-05-29 16:42:49.182: epoch 2:	0.00228754  	0.00462154  	0.00381012  
2023-05-29 16:42:49.182: Find a better model.
2023-05-29 16:43:10.859: [iter 3 : loss : 1.1312 = 0.6930 + 0.4382 + 0.0000, time: 21.674083]
2023-05-29 16:43:11.134: epoch 3:	0.00263548  	0.00526703  	0.00447037  
2023-05-29 16:43:11.134: Find a better model.
2023-05-29 16:43:32.852: [iter 4 : loss : 1.1311 = 0.6929 + 0.4382 + 0.0000, time: 21.715459]
2023-05-29 16:43:33.143: epoch 4:	0.00329435  	0.00721477  	0.00565059  
2023-05-29 16:43:33.143: Find a better model.
2023-05-29 16:43:54.824: [iter 5 : loss : 1.1310 = 0.6928 + 0.4382 + 0.0000, time: 21.677710]
2023-05-29 16:43:55.115: epoch 5:	0.00385697  	0.00873843  	0.00722329  
2023-05-29 16:43:55.115: Find a better model.
2023-05-29 16:44:16.818: [iter 6 : loss : 1.1308 = 0.6926 + 0.4382 + 0.0000, time: 21.700154]
2023-05-29 16:44:17.107: epoch 6:	0.00484156  	0.01142658  	0.00929416  
2023-05-29 16:44:17.107: Find a better model.
2023-05-29 16:44:38.626: [iter 7 : loss : 1.1307 = 0.6925 + 0.4382 + 0.0000, time: 21.514964]
2023-05-29 16:44:38.912: epoch 7:	0.00631475  	0.01612708  	0.01285595  
2023-05-29 16:44:38.912: Find a better model.
2023-05-29 16:45:00.401: [iter 8 : loss : 1.1305 = 0.6922 + 0.4383 + 0.0000, time: 21.485566]
2023-05-29 16:45:00.699: epoch 8:	0.00725492  	0.01882813  	0.01551605  
2023-05-29 16:45:00.700: Find a better model.
2023-05-29 16:45:22.195: [iter 9 : loss : 1.1301 = 0.6918 + 0.4384 + 0.0000, time: 21.491710]
2023-05-29 16:45:22.490: epoch 9:	0.00828393  	0.02187075  	0.01839804  
2023-05-29 16:45:22.490: Find a better model.
2023-05-29 16:45:43.981: [iter 10 : loss : 1.1296 = 0.6911 + 0.4385 + 0.0000, time: 21.488136]
2023-05-29 16:45:44.265: epoch 10:	0.00960166  	0.02549601  	0.02094666  
2023-05-29 16:45:44.265: Find a better model.
2023-05-29 16:46:06.361: [iter 11 : loss : 1.1285 = 0.6900 + 0.4386 + 0.0000, time: 22.093023]
2023-05-29 16:46:06.660: epoch 11:	0.01016431  	0.02649861  	0.02236087  
2023-05-29 16:46:06.660: Find a better model.
2023-05-29 16:46:28.595: [iter 12 : loss : 1.1265 = 0.6878 + 0.4387 + 0.0000, time: 21.931774]
2023-05-29 16:46:28.874: epoch 12:	0.01091203  	0.02864447  	0.02452139  
2023-05-29 16:46:28.874: Find a better model.
2023-05-29 16:46:50.756: [iter 13 : loss : 1.1230 = 0.6839 + 0.4390 + 0.0000, time: 21.879169]
2023-05-29 16:46:51.033: epoch 13:	0.01268876  	0.03349352  	0.02876110  
2023-05-29 16:46:51.033: Find a better model.
2023-05-29 16:47:12.932: [iter 14 : loss : 1.1168 = 0.6773 + 0.4394 + 0.0001, time: 21.893820]
2023-05-29 16:47:13.209: epoch 14:	0.01540573  	0.04091883  	0.03556265  
2023-05-29 16:47:13.209: Find a better model.
2023-05-29 16:47:35.174: [iter 15 : loss : 1.1053 = 0.6652 + 0.4400 + 0.0001, time: 21.961110]
2023-05-29 16:47:35.454: epoch 15:	0.01811529  	0.04822712  	0.04195771  
2023-05-29 16:47:35.454: Find a better model.
2023-05-29 16:47:57.309: [iter 16 : loss : 1.0854 = 0.6443 + 0.4409 + 0.0002, time: 21.851056]
2023-05-29 16:47:57.588: epoch 16:	0.02109142  	0.05669863  	0.04881008  
2023-05-29 16:47:57.588: Find a better model.
2023-05-29 16:48:19.496: [iter 17 : loss : 1.0547 = 0.6122 + 0.4421 + 0.0004, time: 21.904068]
2023-05-29 16:48:19.780: epoch 17:	0.02385282  	0.06400325  	0.05496828  
2023-05-29 16:48:19.780: Find a better model.
2023-05-29 16:48:41.691: [iter 18 : loss : 1.0131 = 0.5685 + 0.4440 + 0.0006, time: 21.908118]
2023-05-29 16:48:41.965: epoch 18:	0.02597756  	0.06973382  	0.05953461  
2023-05-29 16:48:41.965: Find a better model.
2023-05-29 16:49:03.713: [iter 19 : loss : 0.9649 = 0.5175 + 0.4465 + 0.0008, time: 21.743610]
2023-05-29 16:49:03.985: epoch 19:	0.02740639  	0.07250769  	0.06254064  
2023-05-29 16:49:03.985: Find a better model.
2023-05-29 16:49:25.923: [iter 20 : loss : 0.9147 = 0.4642 + 0.4494 + 0.0011, time: 21.932684]
2023-05-29 16:49:26.199: epoch 20:	0.02823555  	0.07555609  	0.06449410  
2023-05-29 16:49:26.199: Find a better model.
2023-05-29 16:49:48.131: [iter 21 : loss : 0.8670 = 0.4135 + 0.4521 + 0.0014, time: 21.928999]
2023-05-29 16:49:48.406: epoch 21:	0.02885741  	0.07721410  	0.06552425  
2023-05-29 16:49:48.406: Find a better model.
2023-05-29 16:50:10.511: [iter 22 : loss : 0.8234 = 0.3673 + 0.4544 + 0.0017, time: 22.101871]
2023-05-29 16:50:10.790: epoch 22:	0.02933123  	0.07886501  	0.06619767  
2023-05-29 16:50:10.790: Find a better model.
2023-05-29 16:50:32.692: [iter 23 : loss : 0.7862 = 0.3282 + 0.4560 + 0.0020, time: 21.898391]
2023-05-29 16:50:32.972: epoch 23:	0.02926460  	0.07903571  	0.06596656  
2023-05-29 16:50:32.972: Find a better model.
2023-05-29 16:50:55.048: [iter 24 : loss : 0.7550 = 0.2958 + 0.4570 + 0.0023, time: 22.072079]
2023-05-29 16:50:55.319: epoch 24:	0.02910172  	0.07853556  	0.06568179  
2023-05-29 16:51:17.098: [iter 25 : loss : 0.7278 = 0.2678 + 0.4574 + 0.0026, time: 21.776045]
2023-05-29 16:51:17.368: epoch 25:	0.02933863  	0.07967379  	0.06586909  
2023-05-29 16:51:17.368: Find a better model.
2023-05-29 16:51:39.463: [iter 26 : loss : 0.7054 = 0.2450 + 0.4576 + 0.0028, time: 22.091037]
2023-05-29 16:51:39.737: epoch 26:	0.02957555  	0.08025983  	0.06625018  
2023-05-29 16:51:39.737: Find a better model.
2023-05-29 16:52:01.660: [iter 27 : loss : 0.6862 = 0.2257 + 0.4574 + 0.0031, time: 21.918280]
2023-05-29 16:52:01.930: epoch 27:	0.02956074  	0.08011916  	0.06604868  
2023-05-29 16:52:24.018: [iter 28 : loss : 0.6695 = 0.2091 + 0.4571 + 0.0033, time: 22.085411]
2023-05-29 16:52:24.287: epoch 28:	0.02966438  	0.07996592  	0.06600970  
2023-05-29 16:52:46.236: [iter 29 : loss : 0.6553 = 0.1952 + 0.4566 + 0.0036, time: 21.944041]
2023-05-29 16:52:46.506: epoch 29:	0.02956073  	0.07947788  	0.06612891  
2023-05-29 16:53:08.610: [iter 30 : loss : 0.6421 = 0.1821 + 0.4561 + 0.0038, time: 22.101079]
2023-05-29 16:53:08.881: epoch 30:	0.02966437  	0.08020703  	0.06636138  
2023-05-29 16:53:30.780: [iter 31 : loss : 0.6307 = 0.1711 + 0.4556 + 0.0040, time: 21.893166]
2023-05-29 16:53:31.050: epoch 31:	0.02958294  	0.07967988  	0.06618296  
2023-05-29 16:53:53.207: [iter 32 : loss : 0.6209 = 0.1616 + 0.4550 + 0.0042, time: 22.153135]
2023-05-29 16:53:53.476: epoch 32:	0.02947929  	0.07943450  	0.06607483  
2023-05-29 16:54:15.584: [iter 33 : loss : 0.6126 = 0.1537 + 0.4545 + 0.0044, time: 22.105031]
2023-05-29 16:54:15.854: epoch 33:	0.02953111  	0.07957629  	0.06626420  
2023-05-29 16:54:37.960: [iter 34 : loss : 0.6042 = 0.1457 + 0.4539 + 0.0046, time: 22.102823]
2023-05-29 16:54:38.227: epoch 34:	0.02956072  	0.07979950  	0.06628428  
2023-05-29 16:55:00.569: [iter 35 : loss : 0.5961 = 0.1379 + 0.4533 + 0.0048, time: 22.338380]
2023-05-29 16:55:00.840: epoch 35:	0.02946448  	0.07939185  	0.06619977  
2023-05-29 16:55:23.165: [iter 36 : loss : 0.5900 = 0.1322 + 0.4528 + 0.0050, time: 22.320819]
2023-05-29 16:55:23.435: epoch 36:	0.02959035  	0.07959516  	0.06647028  
2023-05-29 16:55:45.537: [iter 37 : loss : 0.5841 = 0.1265 + 0.4523 + 0.0052, time: 22.098629]
2023-05-29 16:55:45.807: epoch 37:	0.02964217  	0.08011384  	0.06654134  
2023-05-29 16:56:07.943: [iter 38 : loss : 0.5784 = 0.1211 + 0.4519 + 0.0054, time: 22.131254]
2023-05-29 16:56:08.213: epoch 38:	0.02970140  	0.08030380  	0.06673133  
2023-05-29 16:56:08.213: Find a better model.
2023-05-29 16:56:30.324: [iter 39 : loss : 0.5736 = 0.1167 + 0.4514 + 0.0056, time: 22.107965]
2023-05-29 16:56:30.593: epoch 39:	0.02970139  	0.08033772  	0.06673573  
2023-05-29 16:56:30.593: Find a better model.
2023-05-29 16:56:52.549: [iter 40 : loss : 0.5683 = 0.1116 + 0.4510 + 0.0057, time: 21.952483]
2023-05-29 16:56:52.825: epoch 40:	0.02959035  	0.08000921  	0.06673918  
2023-05-29 16:57:14.975: [iter 41 : loss : 0.5638 = 0.1073 + 0.4505 + 0.0059, time: 22.145418]
2023-05-29 16:57:15.244: epoch 41:	0.02964218  	0.08020101  	0.06679406  
2023-05-29 16:57:37.533: [iter 42 : loss : 0.5597 = 0.1034 + 0.4502 + 0.0061, time: 22.285050]
2023-05-29 16:57:37.807: epoch 42:	0.02961256  	0.07997973  	0.06679978  
2023-05-29 16:57:59.696: [iter 43 : loss : 0.5560 = 0.0999 + 0.4498 + 0.0062, time: 21.885008]
2023-05-29 16:57:59.970: epoch 43:	0.02954592  	0.07952464  	0.06659053  
2023-05-29 16:58:21.961: [iter 44 : loss : 0.5534 = 0.0975 + 0.4495 + 0.0064, time: 21.987222]
2023-05-29 16:58:22.231: epoch 44:	0.02947191  	0.07937907  	0.06646158  
2023-05-29 16:58:44.293: [iter 45 : loss : 0.5497 = 0.0940 + 0.4491 + 0.0065, time: 22.059237]
2023-05-29 16:58:44.563: epoch 45:	0.02950152  	0.07928218  	0.06622607  
2023-05-29 16:59:06.701: [iter 46 : loss : 0.5466 = 0.0911 + 0.4488 + 0.0067, time: 22.135088]
2023-05-29 16:59:06.973: epoch 46:	0.02947930  	0.07919992  	0.06630214  
2023-05-29 16:59:29.091: [iter 47 : loss : 0.5436 = 0.0883 + 0.4485 + 0.0068, time: 22.114019]
2023-05-29 16:59:29.361: epoch 47:	0.02924981  	0.07878765  	0.06585533  
2023-05-29 16:59:52.681: [iter 48 : loss : 0.5411 = 0.0859 + 0.4483 + 0.0069, time: 23.314784]
2023-05-29 16:59:52.986: epoch 48:	0.02930162  	0.07891147  	0.06597377  
2023-05-29 17:00:17.149: my pid: 8208
2023-05-29 17:00:17.149: model: model.general_recommender.SGL
2023-05-29 17:00:17.149: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 17:00:17.149: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 17:00:21.523: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 17:00:44.135: [iter 1 : loss : 1.1334 = 0.6931 + 0.4403 + 0.0000, time: 22.610914]
2023-05-29 17:00:44.392: epoch 1:	0.00165828  	0.00374678  	0.00292385  
2023-05-29 17:00:44.392: Find a better model.
2023-05-29 17:01:05.671: [iter 2 : loss : 1.1321 = 0.6930 + 0.4391 + 0.0000, time: 21.275628]
2023-05-29 17:01:05.964: epoch 2:	0.00217649  	0.00492594  	0.00373136  
2023-05-29 17:01:05.964: Find a better model.
2023-05-29 17:01:27.404: [iter 3 : loss : 1.1320 = 0.6930 + 0.4391 + 0.0000, time: 21.435354]
2023-05-29 17:01:27.706: epoch 3:	0.00262807  	0.00495450  	0.00417947  
2023-05-29 17:01:27.707: Find a better model.
2023-05-29 17:01:48.799: [iter 4 : loss : 1.1320 = 0.6929 + 0.4391 + 0.0000, time: 21.088315]
2023-05-29 17:01:49.088: epoch 4:	0.00324993  	0.00693434  	0.00582721  
2023-05-29 17:01:49.088: Find a better model.
2023-05-29 17:02:10.383: [iter 5 : loss : 1.1319 = 0.6928 + 0.4392 + 0.0000, time: 21.292032]
2023-05-29 17:02:10.684: epoch 5:	0.00359047  	0.00828885  	0.00725416  
2023-05-29 17:02:10.684: Find a better model.
2023-05-29 17:02:31.990: [iter 6 : loss : 1.1319 = 0.6926 + 0.4393 + 0.0000, time: 21.302500]
2023-05-29 17:02:32.280: epoch 6:	0.00472312  	0.01122468  	0.00892885  
2023-05-29 17:02:32.280: Find a better model.
2023-05-29 17:02:53.410: [iter 7 : loss : 1.1318 = 0.6924 + 0.4394 + 0.0000, time: 21.125376]
2023-05-29 17:02:53.707: epoch 7:	0.00538198  	0.01267983  	0.01053452  
2023-05-29 17:02:53.707: Find a better model.
2023-05-29 17:03:15.037: [iter 8 : loss : 1.1317 = 0.6921 + 0.4395 + 0.0000, time: 21.327266]
2023-05-29 17:03:15.308: epoch 8:	0.00637397  	0.01519017  	0.01301140  
2023-05-29 17:03:15.308: Find a better model.
2023-05-29 17:03:36.554: [iter 9 : loss : 1.1314 = 0.6917 + 0.4397 + 0.0000, time: 21.242335]
2023-05-29 17:03:36.841: epoch 9:	0.00786937  	0.01984491  	0.01667939  
2023-05-29 17:03:36.841: Find a better model.
2023-05-29 17:03:58.011: [iter 10 : loss : 1.1310 = 0.6910 + 0.4400 + 0.0000, time: 21.166045]
2023-05-29 17:03:58.292: epoch 10:	0.00894280  	0.02251779  	0.01873462  
2023-05-29 17:03:58.292: Find a better model.
2023-05-29 17:04:19.984: [iter 11 : loss : 1.1299 = 0.6898 + 0.4402 + 0.0000, time: 21.689242]
2023-05-29 17:04:20.267: epoch 11:	0.00991260  	0.02513206  	0.02093346  
2023-05-29 17:04:20.267: Find a better model.
2023-05-29 17:04:41.993: [iter 12 : loss : 1.1280 = 0.6875 + 0.4405 + 0.0000, time: 21.722404]
2023-05-29 17:04:42.272: epoch 12:	0.01121555  	0.02846376  	0.02424657  
2023-05-29 17:04:42.272: Find a better model.
2023-05-29 17:05:04.161: [iter 13 : loss : 1.1247 = 0.6838 + 0.4409 + 0.0000, time: 21.885065]
2023-05-29 17:05:04.439: epoch 13:	0.01339948  	0.03495486  	0.02972111  
2023-05-29 17:05:04.440: Find a better model.
2023-05-29 17:05:26.154: [iter 14 : loss : 1.1188 = 0.6773 + 0.4414 + 0.0001, time: 21.711015]
2023-05-29 17:05:26.431: epoch 14:	0.01639036  	0.04308960  	0.03676537  
2023-05-29 17:05:26.431: Find a better model.
2023-05-29 17:05:48.121: [iter 15 : loss : 1.1076 = 0.6655 + 0.4420 + 0.0001, time: 21.686176]
2023-05-29 17:05:48.400: epoch 15:	0.01928501  	0.05160896  	0.04449694  
2023-05-29 17:05:48.400: Find a better model.
2023-05-29 17:06:10.093: [iter 16 : loss : 1.0882 = 0.6451 + 0.4429 + 0.0002, time: 21.690067]
2023-05-29 17:06:10.369: epoch 16:	0.02235738  	0.06002024  	0.05146226  
2023-05-29 17:06:10.369: Find a better model.
2023-05-29 17:06:31.954: [iter 17 : loss : 1.0581 = 0.6134 + 0.4444 + 0.0004, time: 21.580213]
2023-05-29 17:06:32.226: epoch 17:	0.02487450  	0.06641022  	0.05725437  
2023-05-29 17:06:32.226: Find a better model.
2023-05-29 17:06:53.883: [iter 18 : loss : 1.0163 = 0.5694 + 0.4464 + 0.0005, time: 21.653057]
2023-05-29 17:06:54.158: epoch 18:	0.02702884  	0.07228066  	0.06180957  
2023-05-29 17:06:54.158: Find a better model.
2023-05-29 17:07:15.910: [iter 19 : loss : 0.9674 = 0.5176 + 0.4490 + 0.0008, time: 21.749443]
2023-05-29 17:07:16.188: epoch 19:	0.02835400  	0.07586884  	0.06431804  
2023-05-29 17:07:16.188: Find a better model.
2023-05-29 17:07:37.921: [iter 20 : loss : 0.9162 = 0.4631 + 0.4521 + 0.0011, time: 21.728446]
2023-05-29 17:07:38.194: epoch 20:	0.02898326  	0.07761479  	0.06580423  
2023-05-29 17:07:38.194: Find a better model.
2023-05-29 17:08:00.097: [iter 21 : loss : 0.8678 = 0.4114 + 0.4550 + 0.0014, time: 21.898823]
2023-05-29 17:08:00.360: epoch 21:	0.02929420  	0.07907656  	0.06643661  
2023-05-29 17:08:00.360: Find a better model.
2023-05-29 17:08:22.248: [iter 22 : loss : 0.8235 = 0.3645 + 0.4573 + 0.0017, time: 21.883271]
2023-05-29 17:08:22.525: epoch 22:	0.02952371  	0.08036760  	0.06684005  
2023-05-29 17:08:22.525: Find a better model.
2023-05-29 17:08:44.286: [iter 23 : loss : 0.7863 = 0.3253 + 0.4589 + 0.0020, time: 21.757055]
2023-05-29 17:08:44.557: epoch 23:	0.02949411  	0.08077561  	0.06685136  
2023-05-29 17:08:44.557: Find a better model.
2023-05-29 17:09:06.247: [iter 24 : loss : 0.7548 = 0.2927 + 0.4598 + 0.0023, time: 21.686067]
2023-05-29 17:09:06.517: epoch 24:	0.02957554  	0.08048881  	0.06683913  
2023-05-29 17:09:28.029: [iter 25 : loss : 0.7277 = 0.2650 + 0.4601 + 0.0026, time: 21.507774]
2023-05-29 17:09:28.299: epoch 25:	0.02964217  	0.08107360  	0.06714068  
2023-05-29 17:09:28.299: Find a better model.
2023-05-29 17:09:50.055: [iter 26 : loss : 0.7053 = 0.2422 + 0.4602 + 0.0028, time: 21.752010]
2023-05-29 17:09:50.326: epoch 26:	0.02976063  	0.08155038  	0.06721690  
2023-05-29 17:09:50.326: Find a better model.
2023-05-29 17:10:12.211: [iter 27 : loss : 0.6857 = 0.2227 + 0.4599 + 0.0031, time: 21.881299]
2023-05-29 17:10:12.480: epoch 27:	0.02979765  	0.08155401  	0.06722143  
2023-05-29 17:10:12.480: Find a better model.
2023-05-29 17:10:34.043: [iter 28 : loss : 0.6695 = 0.2067 + 0.4595 + 0.0033, time: 21.558980]
2023-05-29 17:10:34.309: epoch 28:	0.02985688  	0.08170141  	0.06744993  
2023-05-29 17:10:34.309: Find a better model.
2023-05-29 17:10:56.072: [iter 29 : loss : 0.6553 = 0.1927 + 0.4590 + 0.0036, time: 21.759023]
2023-05-29 17:10:56.340: epoch 29:	0.02988648  	0.08172064  	0.06761175  
2023-05-29 17:10:56.340: Find a better model.
2023-05-29 17:11:18.191: [iter 30 : loss : 0.6422 = 0.1800 + 0.4584 + 0.0038, time: 21.845597]
2023-05-29 17:11:18.457: epoch 30:	0.02983467  	0.08164717  	0.06750036  
2023-05-29 17:11:40.399: [iter 31 : loss : 0.6307 = 0.1688 + 0.4579 + 0.0040, time: 21.938279]
2023-05-29 17:11:40.676: epoch 31:	0.02995313  	0.08211583  	0.06782822  
2023-05-29 17:11:40.677: Find a better model.
2023-05-29 17:12:02.838: [iter 32 : loss : 0.6212 = 0.1597 + 0.4572 + 0.0042, time: 22.157004]
2023-05-29 17:12:03.104: epoch 32:	0.03007158  	0.08191333  	0.06794687  
2023-05-29 17:12:25.191: [iter 33 : loss : 0.6128 = 0.1516 + 0.4567 + 0.0044, time: 22.081590]
2023-05-29 17:12:25.459: epoch 33:	0.03000494  	0.08158516  	0.06791301  
2023-05-29 17:12:47.370: [iter 34 : loss : 0.6046 = 0.1438 + 0.4561 + 0.0046, time: 21.907175]
2023-05-29 17:12:47.659: epoch 34:	0.03002716  	0.08162710  	0.06791686  
2023-05-29 17:13:09.413: [iter 35 : loss : 0.5964 = 0.1360 + 0.4555 + 0.0048, time: 21.750090]
2023-05-29 17:13:09.691: epoch 35:	0.02999755  	0.08158576  	0.06791193  
2023-05-29 17:13:31.576: [iter 36 : loss : 0.5904 = 0.1304 + 0.4550 + 0.0050, time: 21.880597]
2023-05-29 17:13:31.847: epoch 36:	0.03005677  	0.08171556  	0.06776831  
2023-05-29 17:13:53.734: [iter 37 : loss : 0.5848 = 0.1251 + 0.4545 + 0.0052, time: 21.884140]
2023-05-29 17:13:54.004: epoch 37:	0.03008638  	0.08179431  	0.06793052  
2023-05-29 17:14:15.790: [iter 38 : loss : 0.5788 = 0.1194 + 0.4540 + 0.0054, time: 21.782525]
2023-05-29 17:14:16.056: epoch 38:	0.03010118  	0.08203257  	0.06796812  
2023-05-29 17:14:37.925: [iter 39 : loss : 0.5742 = 0.1151 + 0.4535 + 0.0056, time: 21.865110]
2023-05-29 17:14:38.191: epoch 39:	0.03010118  	0.08177260  	0.06789839  
2023-05-29 17:15:00.172: [iter 40 : loss : 0.5692 = 0.1103 + 0.4531 + 0.0057, time: 21.977116]
2023-05-29 17:15:00.426: epoch 40:	0.03005675  	0.08172116  	0.06804059  
2023-05-29 17:15:22.392: [iter 41 : loss : 0.5646 = 0.1061 + 0.4527 + 0.0059, time: 21.962516]
2023-05-29 17:15:22.673: epoch 41:	0.02999753  	0.08146122  	0.06783135  
2023-05-29 17:15:44.722: [iter 42 : loss : 0.5607 = 0.1023 + 0.4524 + 0.0060, time: 22.045433]
2023-05-29 17:15:44.989: epoch 42:	0.03000494  	0.08136779  	0.06783390  
2023-05-29 17:16:07.317: [iter 43 : loss : 0.5568 = 0.0986 + 0.4519 + 0.0062, time: 22.324887]
2023-05-29 17:16:07.582: epoch 43:	0.02985687  	0.08105121  	0.06768885  
2023-05-29 17:16:29.759: [iter 44 : loss : 0.5544 = 0.0963 + 0.4517 + 0.0064, time: 22.172878]
2023-05-29 17:16:30.027: epoch 44:	0.02989388  	0.08111919  	0.06772178  
2023-05-29 17:16:52.294: [iter 45 : loss : 0.5506 = 0.0928 + 0.4513 + 0.0065, time: 22.264052]
2023-05-29 17:16:52.562: epoch 45:	0.02993830  	0.08100453  	0.06776615  
2023-05-29 17:17:14.871: [iter 46 : loss : 0.5475 = 0.0899 + 0.4509 + 0.0067, time: 22.306057]
2023-05-29 17:17:15.136: epoch 46:	0.02993830  	0.08081011  	0.06782102  
2023-05-29 17:17:37.514: [iter 47 : loss : 0.5446 = 0.0872 + 0.4506 + 0.0068, time: 22.374246]
2023-05-29 17:17:37.787: epoch 47:	0.02994571  	0.08092978  	0.06781472  
2023-05-29 17:17:59.858: [iter 48 : loss : 0.5421 = 0.0848 + 0.4503 + 0.0069, time: 22.067073]
2023-05-29 17:18:00.130: epoch 48:	0.02973103  	0.08030487  	0.06752838  
2023-05-29 17:18:22.309: [iter 49 : loss : 0.5399 = 0.0828 + 0.4500 + 0.0071, time: 22.174998]
2023-05-29 17:18:22.575: epoch 49:	0.02981246  	0.08024630  	0.06754420  
2023-05-29 17:18:44.906: [iter 50 : loss : 0.5374 = 0.0804 + 0.4498 + 0.0072, time: 22.327584]
2023-05-29 17:18:45.173: epoch 50:	0.02973842  	0.08000717  	0.06736792  
2023-05-29 17:19:07.671: [iter 51 : loss : 0.5352 = 0.0783 + 0.4495 + 0.0074, time: 22.494015]
2023-05-29 17:19:07.936: epoch 51:	0.02968660  	0.07970153  	0.06720463  
2023-05-29 17:19:30.256: [iter 52 : loss : 0.5324 = 0.0756 + 0.4493 + 0.0075, time: 22.317017]
2023-05-29 17:19:30.520: epoch 52:	0.02965699  	0.07958287  	0.06706290  
2023-05-29 17:19:52.706: [iter 53 : loss : 0.5319 = 0.0752 + 0.4491 + 0.0076, time: 22.183019]
2023-05-29 17:19:52.973: epoch 53:	0.02956074  	0.07914012  	0.06679807  
2023-05-29 17:20:15.249: [iter 54 : loss : 0.5296 = 0.0731 + 0.4488 + 0.0077, time: 22.272168]
2023-05-29 17:20:15.517: epoch 54:	0.02954593  	0.07919843  	0.06666890  
2023-05-29 17:20:37.819: [iter 55 : loss : 0.5276 = 0.0711 + 0.4487 + 0.0079, time: 22.299038]
2023-05-29 17:20:38.088: epoch 55:	0.02953111  	0.07917460  	0.06649476  
2023-05-29 17:21:00.288: [iter 56 : loss : 0.5255 = 0.0691 + 0.4484 + 0.0080, time: 22.197076]
2023-05-29 17:21:00.560: epoch 56:	0.02950150  	0.07896388  	0.06663825  
2023-05-29 17:21:00.560: Early stopping is trigger at epoch: 56
2023-05-29 17:21:00.560: best_result@epoch 31:

2023-05-29 17:21:00.560: 		0.0300      	0.0821      	0.0678      
2023-05-29 18:39:42.565: my pid: 5224
2023-05-29 18:39:42.565: model: model.general_recommender.SGL
2023-05-29 18:39:42.565: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 18:39:42.565: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 18:39:46.549: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 18:40:07.213: [iter 1 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 20.662505]
2023-05-29 18:40:07.478: epoch 1:	0.00175452  	0.00317001  	0.00272148  
2023-05-29 18:40:07.479: Find a better model.
2023-05-29 18:40:28.221: [iter 2 : loss : 1.1330 = 0.6930 + 0.4400 + 0.0000, time: 20.738739]
2023-05-29 18:40:28.509: epoch 2:	0.00210986  	0.00347976  	0.00293910  
2023-05-29 18:40:28.509: Find a better model.
2023-05-29 18:40:49.213: [iter 3 : loss : 1.1330 = 0.6929 + 0.4401 + 0.0000, time: 20.700653]
2023-05-29 18:40:49.503: epoch 3:	0.00260586  	0.00506644  	0.00418690  
2023-05-29 18:40:49.503: Find a better model.
2023-05-29 18:41:10.204: [iter 4 : loss : 1.1330 = 0.6928 + 0.4402 + 0.0000, time: 20.697425]
2023-05-29 18:41:10.495: epoch 4:	0.00290199  	0.00631561  	0.00512954  
2023-05-29 18:41:10.495: Find a better model.
2023-05-29 18:41:30.809: [iter 5 : loss : 1.1331 = 0.6927 + 0.4404 + 0.0000, time: 20.310142]
2023-05-29 18:41:31.097: epoch 5:	0.00350163  	0.00728927  	0.00639274  
2023-05-29 18:41:31.097: Find a better model.
2023-05-29 18:41:51.577: [iter 6 : loss : 1.1331 = 0.6925 + 0.4406 + 0.0000, time: 20.477027]
2023-05-29 18:41:51.864: epoch 6:	0.00474533  	0.01060843  	0.00888134  
2023-05-29 18:41:51.864: Find a better model.
2023-05-29 18:42:12.382: [iter 7 : loss : 1.1331 = 0.6923 + 0.4408 + 0.0000, time: 20.514148]
2023-05-29 18:42:12.676: epoch 7:	0.00542640  	0.01297154  	0.01093096  
2023-05-29 18:42:12.676: Find a better model.
2023-05-29 18:42:33.360: [iter 8 : loss : 1.1331 = 0.6920 + 0.4411 + 0.0000, time: 20.680056]
2023-05-29 18:42:33.654: epoch 8:	0.00640359  	0.01573626  	0.01315203  
2023-05-29 18:42:33.654: Find a better model.
2023-05-29 18:42:54.182: [iter 9 : loss : 1.1328 = 0.6915 + 0.4414 + 0.0000, time: 20.524296]
2023-05-29 18:42:54.467: epoch 9:	0.00749182  	0.01907090  	0.01564321  
2023-05-29 18:42:54.467: Find a better model.
2023-05-29 18:43:14.811: [iter 10 : loss : 1.1323 = 0.6905 + 0.4418 + 0.0000, time: 20.338995]
2023-05-29 18:43:15.095: epoch 10:	0.00770651  	0.02010431  	0.01688659  
2023-05-29 18:43:15.095: Find a better model.
2023-05-29 18:43:36.196: [iter 11 : loss : 1.1312 = 0.6890 + 0.4422 + 0.0000, time: 21.098349]
2023-05-29 18:43:36.475: epoch 11:	0.00952763  	0.02401458  	0.02012954  
2023-05-29 18:43:36.475: Find a better model.
2023-05-29 18:43:57.378: [iter 12 : loss : 1.1292 = 0.6866 + 0.4426 + 0.0000, time: 20.898422]
2023-05-29 18:43:57.662: epoch 12:	0.01126737  	0.02859672  	0.02526100  
2023-05-29 18:43:57.662: Find a better model.
2023-05-29 18:44:18.538: [iter 13 : loss : 1.1260 = 0.6828 + 0.4431 + 0.0001, time: 20.873022]
2023-05-29 18:44:18.809: epoch 13:	0.01398433  	0.03541042  	0.03165667  
2023-05-29 18:44:18.809: Find a better model.
2023-05-29 18:44:39.728: [iter 14 : loss : 1.1200 = 0.6761 + 0.4438 + 0.0001, time: 20.915061]
2023-05-29 18:44:40.001: epoch 14:	0.01698262  	0.04396143  	0.03903154  
2023-05-29 18:44:40.001: Find a better model.
2023-05-29 18:45:00.877: [iter 15 : loss : 1.1088 = 0.6641 + 0.4445 + 0.0001, time: 20.872059]
2023-05-29 18:45:01.158: epoch 15:	0.02023263  	0.05275286  	0.04604249  
2023-05-29 18:45:01.158: Find a better model.
2023-05-29 18:45:22.091: [iter 16 : loss : 1.0895 = 0.6436 + 0.4456 + 0.0002, time: 20.930235]
2023-05-29 18:45:22.369: epoch 16:	0.02315693  	0.06119641  	0.05291833  
2023-05-29 18:45:22.369: Find a better model.
2023-05-29 18:45:43.260: [iter 17 : loss : 1.0596 = 0.6120 + 0.4472 + 0.0004, time: 20.888058]
2023-05-29 18:45:43.536: epoch 17:	0.02540011  	0.06702173  	0.05789011  
2023-05-29 18:45:43.536: Find a better model.
2023-05-29 18:46:04.451: [iter 18 : loss : 1.0181 = 0.5682 + 0.4494 + 0.0006, time: 20.910946]
2023-05-29 18:46:04.734: epoch 18:	0.02715466  	0.07205292  	0.06143517  
2023-05-29 18:46:04.734: Find a better model.
2023-05-29 18:46:25.458: [iter 19 : loss : 0.9696 = 0.5166 + 0.4522 + 0.0008, time: 20.721373]
2023-05-29 18:46:25.736: epoch 19:	0.02831697  	0.07550148  	0.06366231  
2023-05-29 18:46:25.736: Find a better model.
2023-05-29 18:46:46.457: [iter 20 : loss : 0.9183 = 0.4618 + 0.4555 + 0.0011, time: 20.716017]
2023-05-29 18:46:46.733: epoch 20:	0.02908691  	0.07813779  	0.06514037  
2023-05-29 18:46:46.733: Find a better model.
2023-05-29 18:47:07.466: [iter 21 : loss : 0.8698 = 0.4100 + 0.4585 + 0.0014, time: 20.730121]
2023-05-29 18:47:07.743: epoch 21:	0.02936084  	0.07914566  	0.06559279  
2023-05-29 18:47:07.743: Find a better model.
2023-05-29 18:47:28.475: [iter 22 : loss : 0.8255 = 0.3630 + 0.4608 + 0.0017, time: 20.727942]
2023-05-29 18:47:28.749: epoch 22:	0.02956814  	0.08018866  	0.06609742  
2023-05-29 18:47:28.749: Find a better model.
2023-05-29 18:47:49.445: [iter 23 : loss : 0.7883 = 0.3238 + 0.4624 + 0.0020, time: 20.693020]
2023-05-29 18:47:49.719: epoch 23:	0.02970139  	0.08105583  	0.06632625  
2023-05-29 18:47:49.719: Find a better model.
2023-05-29 18:48:10.432: [iter 24 : loss : 0.7567 = 0.2912 + 0.4632 + 0.0023, time: 20.708952]
2023-05-29 18:48:10.710: epoch 24:	0.02972361  	0.08141716  	0.06656212  
2023-05-29 18:48:10.710: Find a better model.
2023-05-29 18:48:31.448: [iter 25 : loss : 0.7298 = 0.2638 + 0.4634 + 0.0026, time: 20.732994]
2023-05-29 18:48:31.723: epoch 25:	0.02977543  	0.08179679  	0.06657077  
2023-05-29 18:48:31.723: Find a better model.
2023-05-29 18:48:52.412: [iter 26 : loss : 0.7076 = 0.2413 + 0.4634 + 0.0028, time: 20.686047]
2023-05-29 18:48:52.686: epoch 26:	0.02978284  	0.08161730  	0.06673383  
2023-05-29 18:49:13.597: [iter 27 : loss : 0.6879 = 0.2219 + 0.4630 + 0.0031, time: 20.907110]
2023-05-29 18:49:13.863: epoch 27:	0.02983466  	0.08188499  	0.06677829  
2023-05-29 18:49:13.863: Find a better model.
2023-05-29 18:49:34.835: [iter 28 : loss : 0.6716 = 0.2056 + 0.4626 + 0.0033, time: 20.967156]
2023-05-29 18:49:35.101: epoch 28:	0.02989388  	0.08236779  	0.06702669  
2023-05-29 18:49:35.101: Find a better model.
2023-05-29 18:49:56.022: [iter 29 : loss : 0.6575 = 0.1920 + 0.4620 + 0.0036, time: 20.918461]
2023-05-29 18:49:56.286: epoch 29:	0.02986428  	0.08239549  	0.06709950  
2023-05-29 18:49:56.286: Find a better model.
2023-05-29 18:50:17.420: [iter 30 : loss : 0.6444 = 0.1792 + 0.4614 + 0.0038, time: 21.129528]
2023-05-29 18:50:17.691: epoch 30:	0.02972361  	0.08200257  	0.06711427  
2023-05-29 18:50:39.044: [iter 31 : loss : 0.6328 = 0.1680 + 0.4608 + 0.0040, time: 21.349089]
2023-05-29 18:50:39.308: epoch 31:	0.02974582  	0.08220293  	0.06725005  
2023-05-29 18:51:00.237: [iter 32 : loss : 0.6232 = 0.1589 + 0.4601 + 0.0042, time: 20.925799]
2023-05-29 18:51:00.501: epoch 32:	0.02979764  	0.08218946  	0.06722834  
2023-05-29 18:51:21.598: [iter 33 : loss : 0.6151 = 0.1512 + 0.4595 + 0.0044, time: 21.094094]
2023-05-29 18:51:21.862: epoch 33:	0.02998271  	0.08275908  	0.06749416  
2023-05-29 18:51:21.862: Find a better model.
2023-05-29 18:51:42.795: [iter 34 : loss : 0.6069 = 0.1433 + 0.4589 + 0.0046, time: 20.929769]
2023-05-29 18:51:43.061: epoch 34:	0.03010117  	0.08277842  	0.06754074  
2023-05-29 18:51:43.061: Find a better model.
2023-05-29 18:52:03.996: [iter 35 : loss : 0.5988 = 0.1357 + 0.4583 + 0.0048, time: 20.931548]
2023-05-29 18:52:04.260: epoch 35:	0.02999012  	0.08246026  	0.06741347  
2023-05-29 18:52:25.554: [iter 36 : loss : 0.5925 = 0.1297 + 0.4578 + 0.0050, time: 21.288668]
2023-05-29 18:52:25.815: epoch 36:	0.02992349  	0.08195619  	0.06720868  
2023-05-29 18:52:46.956: [iter 37 : loss : 0.5872 = 0.1247 + 0.4573 + 0.0052, time: 21.138392]
2023-05-29 18:52:47.220: epoch 37:	0.02990868  	0.08208765  	0.06738447  
2023-05-29 18:53:08.555: [iter 38 : loss : 0.5812 = 0.1191 + 0.4567 + 0.0054, time: 21.330394]
2023-05-29 18:53:08.819: epoch 38:	0.02987907  	0.08196350  	0.06740768  
2023-05-29 18:53:29.766: [iter 39 : loss : 0.5769 = 0.1150 + 0.4563 + 0.0056, time: 20.943796]
2023-05-29 18:53:30.028: epoch 39:	0.02987167  	0.08166645  	0.06732771  
2023-05-29 18:53:51.163: [iter 40 : loss : 0.5714 = 0.1098 + 0.4559 + 0.0057, time: 21.131594]
2023-05-29 18:53:51.430: epoch 40:	0.02991609  	0.08142722  	0.06733876  
2023-05-29 18:54:12.557: [iter 41 : loss : 0.5670 = 0.1057 + 0.4554 + 0.0059, time: 21.123244]
2023-05-29 18:54:12.824: epoch 41:	0.02984207  	0.08123082  	0.06732454  
2023-05-29 18:54:33.903: [iter 42 : loss : 0.5631 = 0.1020 + 0.4551 + 0.0060, time: 21.075361]
2023-05-29 18:54:34.166: epoch 42:	0.02971621  	0.08090850  	0.06728958  
2023-05-29 18:54:55.154: [iter 43 : loss : 0.5591 = 0.0983 + 0.4546 + 0.0062, time: 20.984607]
2023-05-29 18:54:55.416: epoch 43:	0.02971621  	0.08071958  	0.06720378  
2023-05-29 18:55:16.351: [iter 44 : loss : 0.5569 = 0.0961 + 0.4544 + 0.0064, time: 20.930295]
2023-05-29 18:55:16.629: epoch 44:	0.02958294  	0.08030699  	0.06704997  
2023-05-29 18:55:37.480: [iter 45 : loss : 0.5529 = 0.0924 + 0.4540 + 0.0065, time: 20.848045]
2023-05-29 18:55:37.748: epoch 45:	0.02956815  	0.08015532  	0.06688119  
2023-05-29 18:55:58.698: [iter 46 : loss : 0.5500 = 0.0897 + 0.4536 + 0.0066, time: 20.945926]
2023-05-29 18:55:58.961: epoch 46:	0.02968658  	0.08056477  	0.06714816  
2023-05-29 18:56:19.674: [iter 47 : loss : 0.5472 = 0.0870 + 0.4533 + 0.0068, time: 20.710025]
2023-05-29 18:56:19.936: epoch 47:	0.02967918  	0.08030453  	0.06714597  
2023-05-29 18:56:40.833: [iter 48 : loss : 0.5446 = 0.0847 + 0.4530 + 0.0069, time: 20.892029]
2023-05-29 18:56:41.097: epoch 48:	0.02950890  	0.08011129  	0.06683151  
2023-05-29 18:57:02.059: [iter 49 : loss : 0.5421 = 0.0823 + 0.4527 + 0.0071, time: 20.957027]
2023-05-29 18:57:02.321: epoch 49:	0.02942006  	0.07964122  	0.06681114  
2023-05-29 18:57:23.476: [iter 50 : loss : 0.5397 = 0.0800 + 0.4525 + 0.0072, time: 21.151138]
2023-05-29 18:57:23.738: epoch 50:	0.02939785  	0.07954188  	0.06680494  
2023-05-29 18:57:44.652: [iter 51 : loss : 0.5374 = 0.0778 + 0.4522 + 0.0073, time: 20.909513]
2023-05-29 18:57:44.916: epoch 51:	0.02935343  	0.07940113  	0.06670421  
2023-05-29 18:58:05.844: [iter 52 : loss : 0.5351 = 0.0756 + 0.4520 + 0.0075, time: 20.925028]
2023-05-29 18:58:06.107: epoch 52:	0.02933124  	0.07958879  	0.06659273  
2023-05-29 18:58:26.840: [iter 53 : loss : 0.5340 = 0.0746 + 0.4518 + 0.0076, time: 20.730092]
2023-05-29 18:58:27.102: epoch 53:	0.02917577  	0.07912659  	0.06631408  
2023-05-29 18:58:48.027: [iter 54 : loss : 0.5321 = 0.0728 + 0.4515 + 0.0077, time: 20.921295]
2023-05-29 18:58:48.290: epoch 54:	0.02916096  	0.07872999  	0.06616150  
2023-05-29 18:59:09.448: [iter 55 : loss : 0.5304 = 0.0712 + 0.4514 + 0.0078, time: 21.155040]
2023-05-29 18:59:09.713: epoch 55:	0.02910914  	0.07836492  	0.06591409  
2023-05-29 18:59:30.430: [iter 56 : loss : 0.5280 = 0.0689 + 0.4511 + 0.0080, time: 20.713669]
2023-05-29 18:59:30.695: epoch 56:	0.02908692  	0.07818600  	0.06586885  
2023-05-29 18:59:51.612: [iter 57 : loss : 0.5265 = 0.0675 + 0.4509 + 0.0081, time: 20.912488]
2023-05-29 18:59:51.875: epoch 57:	0.02906470  	0.07806747  	0.06566869  
2023-05-29 19:00:12.809: [iter 58 : loss : 0.5251 = 0.0661 + 0.4508 + 0.0082, time: 20.930814]
2023-05-29 19:00:13.071: epoch 58:	0.02910173  	0.07786640  	0.06564324  
2023-05-29 19:00:33.817: [iter 59 : loss : 0.5235 = 0.0645 + 0.4507 + 0.0083, time: 20.743049]
2023-05-29 19:00:34.081: epoch 59:	0.02909432  	0.07773860  	0.06550568  
2023-05-29 19:00:34.081: Early stopping is trigger at epoch: 59
2023-05-29 19:00:34.081: best_result@epoch 34:

2023-05-29 19:00:34.081: 		0.0301      	0.0828      	0.0675      
2023-05-29 19:06:16.556: my pid: 9532
2023-05-29 19:06:16.556: model: model.general_recommender.SGL
2023-05-29 19:06:16.556: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 19:06:16.557: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 19:06:20.787: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 19:06:41.098: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.310390]
2023-05-29 19:06:41.376: epoch 1:	0.00171010  	0.00312880  	0.00282151  
2023-05-29 19:06:41.376: Find a better model.
2023-05-29 19:07:01.878: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.498143]
2023-05-29 19:07:02.187: epoch 2:	0.00214688  	0.00362482  	0.00319392  
2023-05-29 19:07:02.187: Find a better model.
2023-05-29 19:07:22.480: [iter 3 : loss : 1.1343 = 0.6929 + 0.4414 + 0.0000, time: 20.289603]
2023-05-29 19:07:22.778: epoch 3:	0.00240598  	0.00466314  	0.00393167  
2023-05-29 19:07:22.778: Find a better model.
2023-05-29 19:07:43.046: [iter 4 : loss : 1.1345 = 0.6928 + 0.4417 + 0.0000, time: 20.264402]
2023-05-29 19:07:43.344: epoch 4:	0.00302043  	0.00655564  	0.00544582  
2023-05-29 19:07:43.344: Find a better model.
2023-05-29 19:08:03.490: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.142173]
2023-05-29 19:08:03.785: epoch 5:	0.00389399  	0.00857605  	0.00729151  
2023-05-29 19:08:03.785: Find a better model.
2023-05-29 19:08:23.837: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.048036]
2023-05-29 19:08:24.128: epoch 6:	0.00434557  	0.01010339  	0.00794818  
2023-05-29 19:08:24.128: Find a better model.
2023-05-29 19:08:44.060: [iter 7 : loss : 1.1348 = 0.6921 + 0.4427 + 0.0000, time: 19.928106]
2023-05-29 19:08:44.355: epoch 7:	0.00495261  	0.01209625  	0.00996008  
2023-05-29 19:08:44.355: Find a better model.
2023-05-29 19:09:04.447: [iter 8 : loss : 1.1348 = 0.6917 + 0.4431 + 0.0000, time: 20.088063]
2023-05-29 19:09:04.734: epoch 8:	0.00575213  	0.01463003  	0.01205241  
2023-05-29 19:09:04.734: Find a better model.
2023-05-29 19:09:24.796: [iter 9 : loss : 1.1346 = 0.6911 + 0.4435 + 0.0000, time: 20.058124]
2023-05-29 19:09:25.087: epoch 9:	0.00663308  	0.01719261  	0.01462771  
2023-05-29 19:09:25.087: Find a better model.
2023-05-29 19:09:44.999: [iter 10 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 19.909300]
2023-05-29 19:09:45.290: epoch 10:	0.00729935  	0.01860256  	0.01568677  
2023-05-29 19:09:45.290: Find a better model.
2023-05-29 19:10:05.843: [iter 11 : loss : 1.1330 = 0.6883 + 0.4447 + 0.0000, time: 20.549010]
2023-05-29 19:10:06.131: epoch 11:	0.00935737  	0.02335586  	0.01995692  
2023-05-29 19:10:06.131: Find a better model.
2023-05-29 19:10:26.826: [iter 12 : loss : 1.1313 = 0.6859 + 0.4453 + 0.0000, time: 20.690038]
2023-05-29 19:10:27.117: epoch 12:	0.01165973  	0.03058323  	0.02626975  
2023-05-29 19:10:27.117: Find a better model.
2023-05-29 19:10:47.764: [iter 13 : loss : 1.1283 = 0.6822 + 0.4460 + 0.0001, time: 20.644009]
2023-05-29 19:10:48.052: epoch 13:	0.01446553  	0.03769556  	0.03343841  
2023-05-29 19:10:48.052: Find a better model.
2023-05-29 19:11:08.765: [iter 14 : loss : 1.1226 = 0.6758 + 0.4468 + 0.0001, time: 20.708078]
2023-05-29 19:11:09.048: epoch 14:	0.01781177  	0.04661297  	0.04075652  
2023-05-29 19:11:09.049: Find a better model.
2023-05-29 19:11:29.737: [iter 15 : loss : 1.1123 = 0.6645 + 0.4477 + 0.0001, time: 20.683229]
2023-05-29 19:11:30.019: epoch 15:	0.02054355  	0.05448554  	0.04738254  
2023-05-29 19:11:30.019: Find a better model.
2023-05-29 19:11:50.550: [iter 16 : loss : 1.0947 = 0.6456 + 0.4489 + 0.0002, time: 20.526798]
2023-05-29 19:11:50.832: epoch 16:	0.02334200  	0.06225503  	0.05356172  
2023-05-29 19:11:50.833: Find a better model.
2023-05-29 19:12:11.323: [iter 17 : loss : 1.0674 = 0.6165 + 0.4505 + 0.0004, time: 20.487099]
2023-05-29 19:12:11.606: epoch 17:	0.02571847  	0.06839842  	0.05869487  
2023-05-29 19:12:11.607: Find a better model.
2023-05-29 19:12:32.326: [iter 18 : loss : 1.0288 = 0.5755 + 0.4528 + 0.0005, time: 20.715065]
2023-05-29 19:12:32.609: epoch 18:	0.02722131  	0.07258265  	0.06162309  
2023-05-29 19:12:32.609: Find a better model.
2023-05-29 19:12:53.151: [iter 19 : loss : 0.9824 = 0.5259 + 0.4557 + 0.0008, time: 20.538009]
2023-05-29 19:12:53.431: epoch 19:	0.02839842  	0.07594755  	0.06390905  
2023-05-29 19:12:53.431: Find a better model.
2023-05-29 19:13:13.927: [iter 20 : loss : 0.9323 = 0.4723 + 0.4589 + 0.0010, time: 20.493075]
2023-05-29 19:13:14.217: epoch 20:	0.02913134  	0.07872309  	0.06523854  
2023-05-29 19:13:14.217: Find a better model.
2023-05-29 19:13:34.528: [iter 21 : loss : 0.8837 = 0.4203 + 0.4621 + 0.0013, time: 20.308035]
2023-05-29 19:13:34.806: epoch 21:	0.02934603  	0.07948980  	0.06574582  
2023-05-29 19:13:34.806: Find a better model.
2023-05-29 19:13:55.273: [iter 22 : loss : 0.8388 = 0.3726 + 0.4646 + 0.0016, time: 20.464232]
2023-05-29 19:13:55.554: epoch 22:	0.02947189  	0.08022034  	0.06617113  
2023-05-29 19:13:55.554: Find a better model.
2023-05-29 19:14:15.906: [iter 23 : loss : 0.8005 = 0.3323 + 0.4663 + 0.0019, time: 20.346692]
2023-05-29 19:14:16.199: epoch 23:	0.02959035  	0.08103783  	0.06629402  
2023-05-29 19:14:16.199: Find a better model.
2023-05-29 19:14:36.674: [iter 24 : loss : 0.7682 = 0.2988 + 0.4672 + 0.0022, time: 20.470761]
2023-05-29 19:14:36.950: epoch 24:	0.02968660  	0.08149949  	0.06647935  
2023-05-29 19:14:36.950: Find a better model.
2023-05-29 19:14:57.282: [iter 25 : loss : 0.7402 = 0.2703 + 0.4674 + 0.0025, time: 20.329495]
2023-05-29 19:14:57.556: epoch 25:	0.02988650  	0.08211050  	0.06681545  
2023-05-29 19:14:57.556: Find a better model.
2023-05-29 19:15:17.855: [iter 26 : loss : 0.7172 = 0.2469 + 0.4675 + 0.0028, time: 20.294499]
2023-05-29 19:15:18.131: epoch 26:	0.03004197  	0.08288858  	0.06710673  
2023-05-29 19:15:18.131: Find a better model.
2023-05-29 19:15:38.668: [iter 27 : loss : 0.6966 = 0.2266 + 0.4670 + 0.0030, time: 20.532058]
2023-05-29 19:15:38.941: epoch 27:	0.02999016  	0.08260373  	0.06699616  
2023-05-29 19:15:59.652: [iter 28 : loss : 0.6798 = 0.2100 + 0.4666 + 0.0033, time: 20.708124]
2023-05-29 19:15:59.924: epoch 28:	0.02999014  	0.08268800  	0.06713183  
2023-05-29 19:16:20.473: [iter 29 : loss : 0.6651 = 0.1956 + 0.4659 + 0.0035, time: 20.545696]
2023-05-29 19:16:20.748: epoch 29:	0.03010860  	0.08309361  	0.06752338  
2023-05-29 19:16:20.748: Find a better model.
2023-05-29 19:16:41.255: [iter 30 : loss : 0.6515 = 0.1824 + 0.4653 + 0.0038, time: 20.503362]
2023-05-29 19:16:41.527: epoch 30:	0.03023445  	0.08370574  	0.06775055  
2023-05-29 19:16:41.527: Find a better model.
2023-05-29 19:17:02.029: [iter 31 : loss : 0.6398 = 0.1711 + 0.4647 + 0.0040, time: 20.499038]
2023-05-29 19:17:02.307: epoch 31:	0.03022704  	0.08381720  	0.06776918  
2023-05-29 19:17:02.307: Find a better model.
2023-05-29 19:17:22.857: [iter 32 : loss : 0.6298 = 0.1617 + 0.4639 + 0.0042, time: 20.546270]
2023-05-29 19:17:23.133: epoch 32:	0.03010860  	0.08310858  	0.06744685  
2023-05-29 19:17:43.648: [iter 33 : loss : 0.6214 = 0.1537 + 0.4633 + 0.0044, time: 20.509072]
2023-05-29 19:17:43.923: epoch 33:	0.03004937  	0.08254954  	0.06749701  
2023-05-29 19:18:04.822: [iter 34 : loss : 0.6127 = 0.1453 + 0.4627 + 0.0046, time: 20.896008]
2023-05-29 19:18:05.097: epoch 34:	0.02998274  	0.08261265  	0.06763284  
2023-05-29 19:18:25.625: [iter 35 : loss : 0.6048 = 0.1379 + 0.4620 + 0.0048, time: 20.523121]
2023-05-29 19:18:25.897: epoch 35:	0.02996053  	0.08271141  	0.06758576  
2023-05-29 19:18:46.367: [iter 36 : loss : 0.5983 = 0.1318 + 0.4615 + 0.0050, time: 20.467288]
2023-05-29 19:18:46.640: epoch 36:	0.02992352  	0.08268383  	0.06769551  
2023-05-29 19:19:07.162: [iter 37 : loss : 0.5925 = 0.1264 + 0.4610 + 0.0052, time: 20.516818]
2023-05-29 19:19:07.434: epoch 37:	0.02999754  	0.08276712  	0.06774014  
2023-05-29 19:19:27.821: [iter 38 : loss : 0.5863 = 0.1206 + 0.4604 + 0.0053, time: 20.382042]
2023-05-29 19:19:28.092: epoch 38:	0.03001975  	0.08275539  	0.06771915  
2023-05-29 19:19:48.771: [iter 39 : loss : 0.5818 = 0.1164 + 0.4599 + 0.0055, time: 20.674724]
2023-05-29 19:19:49.039: epoch 39:	0.03007156  	0.08281372  	0.06790909  
2023-05-29 19:20:09.548: [iter 40 : loss : 0.5766 = 0.1114 + 0.4595 + 0.0057, time: 20.505051]
2023-05-29 19:20:09.820: epoch 40:	0.02999012  	0.08219189  	0.06781245  
2023-05-29 19:20:30.155: [iter 41 : loss : 0.5718 = 0.1069 + 0.4591 + 0.0058, time: 20.332522]
2023-05-29 19:20:30.427: epoch 41:	0.02990128  	0.08207142  	0.06774386  
2023-05-29 19:20:50.931: [iter 42 : loss : 0.5680 = 0.1034 + 0.4586 + 0.0060, time: 20.500901]
2023-05-29 19:20:51.209: epoch 42:	0.02992349  	0.08216962  	0.06770192  
2023-05-29 19:21:11.747: [iter 43 : loss : 0.5642 = 0.0997 + 0.4583 + 0.0061, time: 20.533170]
2023-05-29 19:21:12.019: epoch 43:	0.02984946  	0.08178508  	0.06764516  
2023-05-29 19:21:32.511: [iter 44 : loss : 0.5615 = 0.0971 + 0.4581 + 0.0063, time: 20.487095]
2023-05-29 19:21:32.784: epoch 44:	0.02974582  	0.08186851  	0.06750774  
2023-05-29 19:21:53.149: [iter 45 : loss : 0.5576 = 0.0936 + 0.4576 + 0.0064, time: 20.361133]
2023-05-29 19:21:53.421: epoch 45:	0.02969400  	0.08153201  	0.06727125  
2023-05-29 19:22:13.908: [iter 46 : loss : 0.5549 = 0.0910 + 0.4573 + 0.0066, time: 20.484053]
2023-05-29 19:22:14.192: epoch 46:	0.02967179  	0.08145902  	0.06722869  
2023-05-29 19:22:34.532: [iter 47 : loss : 0.5516 = 0.0878 + 0.4570 + 0.0067, time: 20.337072]
2023-05-29 19:22:34.806: epoch 47:	0.02977543  	0.08140550  	0.06730876  
2023-05-29 19:22:55.498: [iter 48 : loss : 0.5492 = 0.0857 + 0.4566 + 0.0069, time: 20.687620]
2023-05-29 19:22:55.767: epoch 48:	0.02973101  	0.08148646  	0.06724641  
2023-05-29 19:23:16.276: [iter 49 : loss : 0.5467 = 0.0834 + 0.4563 + 0.0070, time: 20.506294]
2023-05-29 19:23:16.546: epoch 49:	0.02959034  	0.08082497  	0.06712680  
2023-05-29 19:23:36.902: [iter 50 : loss : 0.5444 = 0.0811 + 0.4561 + 0.0071, time: 20.352064]
2023-05-29 19:23:37.188: epoch 50:	0.02956072  	0.08057733  	0.06684041  
2023-05-29 19:23:57.897: [iter 51 : loss : 0.5419 = 0.0789 + 0.4558 + 0.0073, time: 20.706328]
2023-05-29 19:23:58.185: epoch 51:	0.02955332  	0.08062431  	0.06695828  
2023-05-29 19:24:18.693: [iter 52 : loss : 0.5391 = 0.0761 + 0.4556 + 0.0074, time: 20.504031]
2023-05-29 19:24:18.968: epoch 52:	0.02950150  	0.08019476  	0.06665106  
2023-05-29 19:24:39.665: [iter 53 : loss : 0.5384 = 0.0755 + 0.4554 + 0.0075, time: 20.693018]
2023-05-29 19:24:39.937: epoch 53:	0.02947189  	0.08002254  	0.06654518  
2023-05-29 19:25:00.691: [iter 54 : loss : 0.5364 = 0.0736 + 0.4552 + 0.0077, time: 20.751366]
2023-05-29 19:25:00.964: epoch 54:	0.02934603  	0.07957434  	0.06628742  
2023-05-29 19:25:21.695: [iter 55 : loss : 0.5346 = 0.0718 + 0.4550 + 0.0078, time: 20.720048]
2023-05-29 19:25:21.967: epoch 55:	0.02933863  	0.07931454  	0.06626880  
2023-05-29 19:25:42.685: [iter 56 : loss : 0.5322 = 0.0696 + 0.4547 + 0.0079, time: 20.714001]
2023-05-29 19:25:42.958: epoch 56:	0.02946448  	0.07937935  	0.06618733  
2023-05-29 19:25:42.958: Early stopping is trigger at epoch: 56
2023-05-29 19:25:42.958: best_result@epoch 31:

2023-05-29 19:25:42.958: 		0.0302      	0.0838      	0.0678      
2023-05-29 19:31:40.142: my pid: 5712
2023-05-29 19:31:40.143: model: model.general_recommender.SGL
2023-05-29 19:31:40.143: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 19:31:40.143: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 19:31:44.098: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 19:32:03.596: [iter 1 : loss : 1.1360 = 0.6931 + 0.4429 + 0.0000, time: 19.496342]
2023-05-29 19:32:03.863: epoch 1:	0.00171750  	0.00340853  	0.00276330  
2023-05-29 19:32:03.863: Find a better model.
2023-05-29 19:32:23.538: [iter 2 : loss : 1.1362 = 0.6930 + 0.4432 + 0.0000, time: 19.670477]
2023-05-29 19:32:23.825: epoch 2:	0.00220610  	0.00360176  	0.00329479  
2023-05-29 19:32:23.825: Find a better model.
2023-05-29 19:32:43.712: [iter 3 : loss : 1.1366 = 0.6929 + 0.4437 + 0.0000, time: 19.883030]
2023-05-29 19:32:43.998: epoch 3:	0.00225792  	0.00402329  	0.00348002  
2023-05-29 19:32:43.998: Find a better model.
2023-05-29 19:33:03.730: [iter 4 : loss : 1.1370 = 0.6927 + 0.4443 + 0.0000, time: 19.728784]
2023-05-29 19:33:04.017: epoch 4:	0.00309446  	0.00689642  	0.00522526  
2023-05-29 19:33:04.017: Find a better model.
2023-05-29 19:33:23.732: [iter 5 : loss : 1.1373 = 0.6925 + 0.4448 + 0.0000, time: 19.710293]
2023-05-29 19:33:24.018: epoch 5:	0.00368671  	0.00824480  	0.00679670  
2023-05-29 19:33:24.018: Find a better model.
2023-05-29 19:33:43.738: [iter 6 : loss : 1.1377 = 0.6923 + 0.4454 + 0.0000, time: 19.717097]
2023-05-29 19:33:44.023: epoch 6:	0.00453064  	0.01096300  	0.00864665  
2023-05-29 19:33:44.023: Find a better model.
2023-05-29 19:34:03.692: [iter 7 : loss : 1.1379 = 0.6920 + 0.4459 + 0.0000, time: 19.665194]
2023-05-29 19:34:03.978: epoch 7:	0.00514509  	0.01312741  	0.01042816  
2023-05-29 19:34:03.978: Find a better model.
2023-05-29 19:34:23.657: [iter 8 : loss : 1.1380 = 0.6915 + 0.4465 + 0.0000, time: 19.675025]
2023-05-29 19:34:23.943: epoch 8:	0.00583357  	0.01432780  	0.01200892  
2023-05-29 19:34:23.943: Find a better model.
2023-05-29 19:34:43.689: [iter 9 : loss : 1.1378 = 0.6907 + 0.4471 + 0.0000, time: 19.742139]
2023-05-29 19:34:43.972: epoch 9:	0.00618150  	0.01489354  	0.01221266  
2023-05-29 19:34:43.972: Find a better model.
2023-05-29 19:35:03.514: [iter 10 : loss : 1.1373 = 0.6895 + 0.4478 + 0.0000, time: 19.538646]
2023-05-29 19:35:03.796: epoch 10:	0.00735117  	0.01819280  	0.01496571  
2023-05-29 19:35:03.796: Find a better model.
2023-05-29 19:35:23.909: [iter 11 : loss : 1.1368 = 0.6882 + 0.4486 + 0.0000, time: 20.109022]
2023-05-29 19:35:24.190: epoch 11:	0.00925373  	0.02262143  	0.01957173  
2023-05-29 19:35:24.190: Find a better model.
2023-05-29 19:35:44.430: [iter 12 : loss : 1.1356 = 0.6862 + 0.4494 + 0.0000, time: 20.236105]
2023-05-29 19:35:44.710: epoch 12:	0.01157830  	0.02851860  	0.02497879  
2023-05-29 19:35:44.710: Find a better model.
2023-05-29 19:36:04.853: [iter 13 : loss : 1.1336 = 0.6832 + 0.4503 + 0.0001, time: 20.138014]
2023-05-29 19:36:05.130: epoch 13:	0.01438409  	0.03532553  	0.03096742  
2023-05-29 19:36:05.130: Find a better model.
2023-05-29 19:36:25.296: [iter 14 : loss : 1.1297 = 0.6784 + 0.4511 + 0.0001, time: 20.163041]
2023-05-29 19:36:25.574: epoch 14:	0.01727873  	0.04309327  	0.03801269  
2023-05-29 19:36:25.574: Find a better model.
2023-05-29 19:36:45.625: [iter 15 : loss : 1.1229 = 0.6707 + 0.4521 + 0.0001, time: 20.045914]
2023-05-29 19:36:45.898: epoch 15:	0.02072865  	0.05236606  	0.04558267  
2023-05-29 19:36:45.898: Find a better model.
2023-05-29 19:37:06.033: [iter 16 : loss : 1.1113 = 0.6579 + 0.4533 + 0.0002, time: 20.130108]
2023-05-29 19:37:06.306: epoch 16:	0.02333458  	0.05899357  	0.05172097  
2023-05-29 19:37:06.306: Find a better model.
2023-05-29 19:37:26.359: [iter 17 : loss : 1.0929 = 0.6377 + 0.4549 + 0.0003, time: 20.045116]
2023-05-29 19:37:26.640: epoch 17:	0.02576286  	0.06564655  	0.05727258  
2023-05-29 19:37:26.640: Find a better model.
2023-05-29 19:37:46.737: [iter 18 : loss : 1.0648 = 0.6075 + 0.4569 + 0.0004, time: 20.094036]
2023-05-29 19:37:47.011: epoch 18:	0.02770994  	0.07212001  	0.06113800  
2023-05-29 19:37:47.011: Find a better model.
2023-05-29 19:38:06.994: [iter 19 : loss : 1.0271 = 0.5669 + 0.4597 + 0.0006, time: 19.979120]
2023-05-29 19:38:07.269: epoch 19:	0.02883520  	0.07520942  	0.06337002  
2023-05-29 19:38:07.270: Find a better model.
2023-05-29 19:38:27.390: [iter 20 : loss : 0.9818 = 0.5180 + 0.4630 + 0.0008, time: 20.117118]
2023-05-29 19:38:27.669: epoch 20:	0.02941264  	0.07760617  	0.06473388  
2023-05-29 19:38:27.669: Find a better model.
2023-05-29 19:38:47.592: [iter 21 : loss : 0.9337 = 0.4660 + 0.4666 + 0.0011, time: 19.920360]
2023-05-29 19:38:47.862: epoch 21:	0.02966436  	0.07878850  	0.06515922  
2023-05-29 19:38:47.862: Find a better model.
2023-05-29 19:39:07.933: [iter 22 : loss : 0.8860 = 0.4148 + 0.4698 + 0.0014, time: 20.066738]
2023-05-29 19:39:08.202: epoch 22:	0.02984945  	0.07997119  	0.06524006  
2023-05-29 19:39:08.202: Find a better model.
2023-05-29 19:39:28.558: [iter 23 : loss : 0.8432 = 0.3694 + 0.4722 + 0.0017, time: 20.352090]
2023-05-29 19:39:28.828: epoch 23:	0.02987167  	0.08045392  	0.06528244  
2023-05-29 19:39:28.828: Find a better model.
2023-05-29 19:39:48.946: [iter 24 : loss : 0.8060 = 0.3304 + 0.4736 + 0.0020, time: 20.113020]
2023-05-29 19:39:49.211: epoch 24:	0.02992348  	0.08092499  	0.06556450  
2023-05-29 19:39:49.211: Find a better model.
2023-05-29 19:40:09.323: [iter 25 : loss : 0.7739 = 0.2975 + 0.4741 + 0.0022, time: 20.107324]
2023-05-29 19:40:09.588: epoch 25:	0.03002714  	0.08171427  	0.06586486  
2023-05-29 19:40:09.588: Find a better model.
2023-05-29 19:40:29.726: [iter 26 : loss : 0.7472 = 0.2703 + 0.4744 + 0.0025, time: 20.134005]
2023-05-29 19:40:29.988: epoch 26:	0.03009377  	0.08224244  	0.06621660  
2023-05-29 19:40:29.988: Find a better model.
2023-05-29 19:40:50.138: [iter 27 : loss : 0.7237 = 0.2469 + 0.4740 + 0.0028, time: 20.146040]
2023-05-29 19:40:50.408: epoch 27:	0.03016040  	0.08248997  	0.06635518  
2023-05-29 19:40:50.408: Find a better model.
2023-05-29 19:41:10.514: [iter 28 : loss : 0.7043 = 0.2276 + 0.4737 + 0.0030, time: 20.102304]
2023-05-29 19:41:10.779: epoch 28:	0.03027884  	0.08258584  	0.06646729  
2023-05-29 19:41:10.779: Find a better model.
2023-05-29 19:41:31.092: [iter 29 : loss : 0.6873 = 0.2110 + 0.4730 + 0.0033, time: 20.308033]
2023-05-29 19:41:31.371: epoch 29:	0.03024923  	0.08234838  	0.06635632  
2023-05-29 19:41:51.556: [iter 30 : loss : 0.6719 = 0.1961 + 0.4723 + 0.0035, time: 20.181093]
2023-05-29 19:41:51.819: epoch 30:	0.03022703  	0.08240783  	0.06640757  
2023-05-29 19:42:12.305: [iter 31 : loss : 0.6586 = 0.1833 + 0.4715 + 0.0038, time: 20.483083]
2023-05-29 19:42:12.566: epoch 31:	0.03029365  	0.08275068  	0.06653836  
2023-05-29 19:42:12.566: Find a better model.
2023-05-29 19:42:32.837: [iter 32 : loss : 0.6471 = 0.1723 + 0.4708 + 0.0040, time: 20.267031]
2023-05-29 19:42:33.098: epoch 32:	0.03033067  	0.08269284  	0.06663355  
2023-05-29 19:42:53.460: [iter 33 : loss : 0.6372 = 0.1629 + 0.4701 + 0.0042, time: 20.359049]
2023-05-29 19:42:53.724: epoch 33:	0.03037508  	0.08294506  	0.06672747  
2023-05-29 19:42:53.724: Find a better model.
2023-05-29 19:43:14.094: [iter 34 : loss : 0.6280 = 0.1542 + 0.4694 + 0.0044, time: 20.367068]
2023-05-29 19:43:14.366: epoch 34:	0.03028626  	0.08262977  	0.06688847  
2023-05-29 19:43:34.666: [iter 35 : loss : 0.6192 = 0.1459 + 0.4687 + 0.0046, time: 20.297097]
2023-05-29 19:43:34.930: epoch 35:	0.03030108  	0.08293842  	0.06702489  
2023-05-29 19:43:55.288: [iter 36 : loss : 0.6117 = 0.1389 + 0.4681 + 0.0048, time: 20.353083]
2023-05-29 19:43:55.550: epoch 36:	0.03025667  	0.08281698  	0.06707302  
2023-05-29 19:44:15.805: [iter 37 : loss : 0.6053 = 0.1329 + 0.4675 + 0.0049, time: 20.251002]
2023-05-29 19:44:16.066: epoch 37:	0.03027146  	0.08281198  	0.06705386  
2023-05-29 19:44:36.044: [iter 38 : loss : 0.5987 = 0.1267 + 0.4668 + 0.0051, time: 19.975003]
2023-05-29 19:44:36.305: epoch 38:	0.03016781  	0.08244263  	0.06691924  
2023-05-29 19:44:56.227: [iter 39 : loss : 0.5936 = 0.1220 + 0.4663 + 0.0053, time: 19.915102]
2023-05-29 19:44:56.492: epoch 39:	0.03027886  	0.08282954  	0.06712252  
2023-05-29 19:45:16.628: [iter 40 : loss : 0.5879 = 0.1165 + 0.4659 + 0.0055, time: 20.132005]
2023-05-29 19:45:16.889: epoch 40:	0.03022704  	0.08262865  	0.06707189  
2023-05-29 19:45:36.855: [iter 41 : loss : 0.5828 = 0.1117 + 0.4654 + 0.0056, time: 19.961012]
2023-05-29 19:45:37.116: epoch 41:	0.03023444  	0.08252765  	0.06711899  
2023-05-29 19:45:57.184: [iter 42 : loss : 0.5786 = 0.1079 + 0.4649 + 0.0058, time: 20.065013]
2023-05-29 19:45:57.452: epoch 42:	0.03027886  	0.08247124  	0.06720934  
2023-05-29 19:46:17.774: [iter 43 : loss : 0.5743 = 0.1038 + 0.4645 + 0.0060, time: 20.318513]
2023-05-29 19:46:18.034: epoch 43:	0.03021223  	0.08215335  	0.06709546  
2023-05-29 19:46:38.012: [iter 44 : loss : 0.5714 = 0.1010 + 0.4643 + 0.0061, time: 19.975393]
2023-05-29 19:46:38.277: epoch 44:	0.03010860  	0.08176698  	0.06690919  
2023-05-29 19:46:58.220: [iter 45 : loss : 0.5673 = 0.0972 + 0.4638 + 0.0063, time: 19.939046]
2023-05-29 19:46:58.485: epoch 45:	0.03014561  	0.08200524  	0.06704944  
2023-05-29 19:47:18.631: [iter 46 : loss : 0.5639 = 0.0940 + 0.4634 + 0.0064, time: 20.143108]
2023-05-29 19:47:18.892: epoch 46:	0.03014561  	0.08223550  	0.06711501  
2023-05-29 19:47:38.768: [iter 47 : loss : 0.5607 = 0.0909 + 0.4633 + 0.0066, time: 19.870620]
2023-05-29 19:47:39.029: epoch 47:	0.03013081  	0.08203270  	0.06706212  
2023-05-29 19:47:59.131: [iter 48 : loss : 0.5582 = 0.0886 + 0.4629 + 0.0067, time: 20.099002]
2023-05-29 19:47:59.397: epoch 48:	0.03005677  	0.08159875  	0.06686509  
2023-05-29 19:48:19.551: [iter 49 : loss : 0.5555 = 0.0863 + 0.4624 + 0.0068, time: 20.150001]
2023-05-29 19:48:19.814: epoch 49:	0.03010118  	0.08152556  	0.06686446  
2023-05-29 19:48:39.772: [iter 50 : loss : 0.5529 = 0.0838 + 0.4622 + 0.0070, time: 19.954013]
2023-05-29 19:48:40.034: epoch 50:	0.03011599  	0.08150145  	0.06703007  
2023-05-29 19:49:00.117: [iter 51 : loss : 0.5500 = 0.0811 + 0.4617 + 0.0071, time: 20.078971]
2023-05-29 19:49:00.399: epoch 51:	0.03010118  	0.08155091  	0.06698389  
2023-05-29 19:49:20.521: [iter 52 : loss : 0.5474 = 0.0785 + 0.4616 + 0.0072, time: 20.119108]
2023-05-29 19:49:20.783: epoch 52:	0.03007157  	0.08106910  	0.06685083  
2023-05-29 19:49:40.704: [iter 53 : loss : 0.5465 = 0.0778 + 0.4614 + 0.0074, time: 19.916041]
2023-05-29 19:49:40.965: epoch 53:	0.03001234  	0.08098461  	0.06681590  
2023-05-29 19:50:01.149: [iter 54 : loss : 0.5445 = 0.0758 + 0.4611 + 0.0075, time: 20.179965]
2023-05-29 19:50:01.417: epoch 54:	0.02985689  	0.08049981  	0.06657161  
2023-05-29 19:50:21.738: [iter 55 : loss : 0.5425 = 0.0738 + 0.4610 + 0.0076, time: 20.317119]
2023-05-29 19:50:22.000: epoch 55:	0.02985688  	0.08040725  	0.06650333  
2023-05-29 19:50:41.962: [iter 56 : loss : 0.5401 = 0.0716 + 0.4607 + 0.0077, time: 19.957629]
2023-05-29 19:50:42.222: epoch 56:	0.02983467  	0.08043176  	0.06637095  
2023-05-29 19:51:02.298: [iter 57 : loss : 0.5385 = 0.0702 + 0.4605 + 0.0079, time: 20.072048]
2023-05-29 19:51:02.563: epoch 57:	0.02983467  	0.08032527  	0.06620688  
2023-05-29 19:51:22.511: [iter 58 : loss : 0.5370 = 0.0686 + 0.4604 + 0.0080, time: 19.944553]
2023-05-29 19:51:22.773: epoch 58:	0.02973843  	0.07971195  	0.06600595  
2023-05-29 19:51:22.773: Early stopping is trigger at epoch: 58
2023-05-29 19:51:22.773: best_result@epoch 33:

2023-05-29 19:51:22.773: 		0.0304      	0.0829      	0.0667      
2023-05-29 20:01:34.693: my pid: 14368
2023-05-29 20:01:34.694: model: model.general_recommender.SGL
2023-05-29 20:01:34.694: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 20:01:34.694: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 20:01:38.763: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 20:01:59.420: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.657837]
2023-05-29 20:01:59.693: epoch 1:	0.00136956  	0.00279948  	0.00234637  
2023-05-29 20:01:59.693: Find a better model.
2023-05-29 20:02:20.650: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.952321]
2023-05-29 20:02:20.953: epoch 2:	0.00205064  	0.00342005  	0.00296493  
2023-05-29 20:02:20.953: Find a better model.
2023-05-29 20:02:41.848: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 20.891086]
2023-05-29 20:02:42.141: epoch 3:	0.00248742  	0.00431755  	0.00366544  
2023-05-29 20:02:42.142: Find a better model.
2023-05-29 20:03:02.834: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.689092]
2023-05-29 20:03:03.122: epoch 4:	0.00286497  	0.00544243  	0.00436115  
2023-05-29 20:03:03.123: Find a better model.
2023-05-29 20:03:23.817: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.689947]
2023-05-29 20:03:24.105: epoch 5:	0.00343500  	0.00765780  	0.00567347  
2023-05-29 20:03:24.105: Find a better model.
2023-05-29 20:03:44.825: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.716043]
2023-05-29 20:03:45.111: epoch 6:	0.00409387  	0.01006373  	0.00777498  
2023-05-29 20:03:45.111: Find a better model.
2023-05-29 20:04:05.832: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.718025]
2023-05-29 20:04:06.126: epoch 7:	0.00484157  	0.01193137  	0.00984523  
2023-05-29 20:04:06.126: Find a better model.
2023-05-29 20:04:26.988: [iter 8 : loss : 1.1349 = 0.6918 + 0.4430 + 0.0000, time: 20.859317]
2023-05-29 20:04:27.275: epoch 8:	0.00564849  	0.01441304  	0.01142721  
2023-05-29 20:04:27.275: Find a better model.
2023-05-29 20:04:48.001: [iter 9 : loss : 1.1347 = 0.6912 + 0.4435 + 0.0000, time: 20.723044]
2023-05-29 20:04:48.306: epoch 9:	0.00621852  	0.01618110  	0.01271930  
2023-05-29 20:04:48.306: Find a better model.
2023-05-29 20:05:08.984: [iter 10 : loss : 1.1342 = 0.6901 + 0.4440 + 0.0000, time: 20.674551]
2023-05-29 20:05:09.264: epoch 10:	0.00678854  	0.01731339  	0.01429276  
2023-05-29 20:05:09.264: Find a better model.
2023-05-29 20:05:30.988: [iter 11 : loss : 1.1333 = 0.6887 + 0.4446 + 0.0000, time: 21.720290]
2023-05-29 20:05:31.273: epoch 11:	0.00874292  	0.02294783  	0.01922475  
2023-05-29 20:05:31.273: Find a better model.
2023-05-29 20:05:52.545: [iter 12 : loss : 1.1317 = 0.6865 + 0.4452 + 0.0000, time: 21.268035]
2023-05-29 20:05:52.820: epoch 12:	0.01051966  	0.02778148  	0.02385684  
2023-05-29 20:05:52.820: Find a better model.
2023-05-29 20:06:13.983: [iter 13 : loss : 1.1292 = 0.6832 + 0.4459 + 0.0001, time: 21.158643]
2023-05-29 20:06:14.279: epoch 13:	0.01354753  	0.03542633  	0.03063737  
2023-05-29 20:06:14.279: Find a better model.
2023-05-29 20:06:35.360: [iter 14 : loss : 1.1243 = 0.6776 + 0.4466 + 0.0001, time: 21.078059]
2023-05-29 20:06:35.632: epoch 14:	0.01681234  	0.04353129  	0.03804504  
2023-05-29 20:06:35.632: Find a better model.
2023-05-29 20:06:56.738: [iter 15 : loss : 1.1156 = 0.6679 + 0.4475 + 0.0001, time: 21.103371]
2023-05-29 20:06:57.018: epoch 15:	0.01987724  	0.05154872  	0.04516165  
2023-05-29 20:06:57.018: Find a better model.
2023-05-29 20:07:17.925: [iter 16 : loss : 1.1002 = 0.6513 + 0.4487 + 0.0002, time: 20.903428]
2023-05-29 20:07:18.195: epoch 16:	0.02322353  	0.06099430  	0.05212598  
2023-05-29 20:07:18.195: Find a better model.
2023-05-29 20:07:39.108: [iter 17 : loss : 1.0759 = 0.6254 + 0.4501 + 0.0003, time: 20.909036]
2023-05-29 20:07:39.382: epoch 17:	0.02548894  	0.06747254  	0.05726091  
2023-05-29 20:07:39.382: Find a better model.
2023-05-29 20:08:00.497: [iter 18 : loss : 1.0403 = 0.5876 + 0.4522 + 0.0005, time: 21.111425]
2023-05-29 20:08:00.770: epoch 18:	0.02746560  	0.07327350  	0.06133017  
2023-05-29 20:08:00.770: Find a better model.
2023-05-29 20:08:21.877: [iter 19 : loss : 0.9960 = 0.5403 + 0.4550 + 0.0007, time: 21.102223]
2023-05-29 20:08:22.154: epoch 19:	0.02831697  	0.07598766  	0.06334167  
2023-05-29 20:08:22.154: Find a better model.
2023-05-29 20:08:43.267: [iter 20 : loss : 0.9464 = 0.4872 + 0.4583 + 0.0010, time: 21.109379]
2023-05-29 20:08:43.536: epoch 20:	0.02915354  	0.07800004  	0.06470592  
2023-05-29 20:08:43.536: Find a better model.
2023-05-29 20:09:04.667: [iter 21 : loss : 0.8971 = 0.4342 + 0.4616 + 0.0012, time: 21.126993]
2023-05-29 20:09:04.954: epoch 21:	0.02978280  	0.07948111  	0.06578123  
2023-05-29 20:09:04.954: Find a better model.
2023-05-29 20:09:26.110: [iter 22 : loss : 0.8506 = 0.3848 + 0.4642 + 0.0016, time: 21.152301]
2023-05-29 20:09:26.377: epoch 22:	0.02971620  	0.07944122  	0.06603476  
2023-05-29 20:09:47.453: [iter 23 : loss : 0.8108 = 0.3427 + 0.4662 + 0.0019, time: 21.071505]
2023-05-29 20:09:47.717: epoch 23:	0.02992349  	0.08034475  	0.06623760  
2023-05-29 20:09:47.717: Find a better model.
2023-05-29 20:10:08.711: [iter 24 : loss : 0.7766 = 0.3071 + 0.4673 + 0.0022, time: 20.990333]
2023-05-29 20:10:08.989: epoch 24:	0.03004194  	0.08126947  	0.06661237  
2023-05-29 20:10:08.990: Find a better model.
2023-05-29 20:10:30.030: [iter 25 : loss : 0.7476 = 0.2774 + 0.4678 + 0.0024, time: 21.035721]
2023-05-29 20:10:30.292: epoch 25:	0.03025663  	0.08196534  	0.06713797  
2023-05-29 20:10:30.292: Find a better model.
2023-05-29 20:10:51.618: [iter 26 : loss : 0.7233 = 0.2529 + 0.4677 + 0.0027, time: 21.323240]
2023-05-29 20:10:51.880: epoch 26:	0.03038250  	0.08221652  	0.06739190  
2023-05-29 20:10:51.880: Find a better model.
2023-05-29 20:11:13.039: [iter 27 : loss : 0.7022 = 0.2319 + 0.4674 + 0.0030, time: 21.155179]
2023-05-29 20:11:13.300: epoch 27:	0.03036768  	0.08256984  	0.06755096  
2023-05-29 20:11:13.300: Find a better model.
2023-05-29 20:11:34.398: [iter 28 : loss : 0.6847 = 0.2145 + 0.4670 + 0.0032, time: 21.094075]
2023-05-29 20:11:34.655: epoch 28:	0.03045651  	0.08292735  	0.06756700  
2023-05-29 20:11:34.655: Find a better model.
2023-05-29 20:11:55.799: [iter 29 : loss : 0.6695 = 0.1998 + 0.4663 + 0.0035, time: 21.139029]
2023-05-29 20:11:56.069: epoch 29:	0.03060457  	0.08344127  	0.06784926  
2023-05-29 20:11:56.069: Find a better model.
2023-05-29 20:12:17.027: [iter 30 : loss : 0.6552 = 0.1858 + 0.4657 + 0.0037, time: 20.953012]
2023-05-29 20:12:17.291: epoch 30:	0.03069341  	0.08371217  	0.06795745  
2023-05-29 20:12:17.291: Find a better model.
2023-05-29 20:12:38.583: [iter 31 : loss : 0.6433 = 0.1743 + 0.4651 + 0.0039, time: 21.286375]
2023-05-29 20:12:38.842: epoch 31:	0.03066379  	0.08386821  	0.06805222  
2023-05-29 20:12:38.842: Find a better model.
2023-05-29 20:13:00.026: [iter 32 : loss : 0.6327 = 0.1642 + 0.4644 + 0.0041, time: 21.179062]
2023-05-29 20:13:00.287: epoch 32:	0.03061197  	0.08340771  	0.06788499  
2023-05-29 20:13:21.650: [iter 33 : loss : 0.6238 = 0.1559 + 0.4636 + 0.0043, time: 21.359043]
2023-05-29 20:13:21.943: epoch 33:	0.03066380  	0.08365118  	0.06815556  
2023-05-29 20:13:43.390: [iter 34 : loss : 0.6153 = 0.1477 + 0.4631 + 0.0045, time: 21.443439]
2023-05-29 20:13:43.653: epoch 34:	0.03064900  	0.08360913  	0.06829513  
2023-05-29 20:14:04.999: [iter 35 : loss : 0.6072 = 0.1400 + 0.4624 + 0.0047, time: 21.341065]
2023-05-29 20:14:05.276: epoch 35:	0.03063418  	0.08344086  	0.06835578  
2023-05-29 20:14:26.229: [iter 36 : loss : 0.6005 = 0.1338 + 0.4618 + 0.0049, time: 20.949291]
2023-05-29 20:14:26.507: epoch 36:	0.03055274  	0.08335755  	0.06833819  
2023-05-29 20:14:47.557: [iter 37 : loss : 0.5943 = 0.1279 + 0.4614 + 0.0051, time: 21.045999]
2023-05-29 20:14:47.816: epoch 37:	0.03058235  	0.08350340  	0.06833667  
2023-05-29 20:15:08.755: [iter 38 : loss : 0.5883 = 0.1223 + 0.4607 + 0.0053, time: 20.936269]
2023-05-29 20:15:09.026: epoch 38:	0.03055274  	0.08358633  	0.06834944  
2023-05-29 20:15:29.966: [iter 39 : loss : 0.5834 = 0.1177 + 0.4602 + 0.0054, time: 20.936363]
2023-05-29 20:15:30.237: epoch 39:	0.03045650  	0.08305430  	0.06822702  
2023-05-29 20:15:51.127: [iter 40 : loss : 0.5782 = 0.1128 + 0.4598 + 0.0056, time: 20.887361]
2023-05-29 20:15:51.387: epoch 40:	0.03047872  	0.08357405  	0.06837719  
2023-05-29 20:16:12.505: [iter 41 : loss : 0.5733 = 0.1082 + 0.4594 + 0.0058, time: 21.114086]
2023-05-29 20:16:12.763: epoch 41:	0.03053794  	0.08345215  	0.06851174  
2023-05-29 20:16:33.896: [iter 42 : loss : 0.5693 = 0.1044 + 0.4589 + 0.0059, time: 21.130008]
2023-05-29 20:16:34.158: epoch 42:	0.03050092  	0.08324921  	0.06847495  
2023-05-29 20:16:55.499: [iter 43 : loss : 0.5653 = 0.1005 + 0.4586 + 0.0061, time: 21.338270]
2023-05-29 20:16:55.763: epoch 43:	0.03047131  	0.08302452  	0.06851681  
2023-05-29 20:17:16.908: [iter 44 : loss : 0.5623 = 0.0979 + 0.4582 + 0.0062, time: 21.141111]
2023-05-29 20:17:17.169: epoch 44:	0.03044909  	0.08288225  	0.06850311  
2023-05-29 20:17:38.163: [iter 45 : loss : 0.5588 = 0.0945 + 0.4579 + 0.0064, time: 20.988122]
2023-05-29 20:17:38.440: epoch 45:	0.03036767  	0.08234131  	0.06836573  
2023-05-29 20:17:59.518: [iter 46 : loss : 0.5554 = 0.0914 + 0.4575 + 0.0065, time: 21.075187]
2023-05-29 20:17:59.781: epoch 46:	0.03032325  	0.08228952  	0.06831485  
2023-05-29 20:18:20.718: [iter 47 : loss : 0.5525 = 0.0887 + 0.4572 + 0.0067, time: 20.932787]
2023-05-29 20:18:21.003: epoch 47:	0.03036026  	0.08237332  	0.06831563  
2023-05-29 20:18:42.078: [iter 48 : loss : 0.5499 = 0.0863 + 0.4568 + 0.0068, time: 21.072033]
2023-05-29 20:18:42.337: epoch 48:	0.03029363  	0.08195565  	0.06820961  
2023-05-29 20:19:03.493: [iter 49 : loss : 0.5475 = 0.0839 + 0.4566 + 0.0070, time: 21.152015]
2023-05-29 20:19:03.756: epoch 49:	0.03007154  	0.08149916  	0.06791047  
2023-05-29 20:19:24.677: [iter 50 : loss : 0.5455 = 0.0821 + 0.4563 + 0.0071, time: 20.916017]
2023-05-29 20:19:24.963: epoch 50:	0.02984204  	0.08051424  	0.06751134  
2023-05-29 20:19:45.878: [iter 51 : loss : 0.5426 = 0.0795 + 0.4559 + 0.0072, time: 20.912164]
2023-05-29 20:19:46.142: epoch 51:	0.02976801  	0.08013152  	0.06731907  
2023-05-29 20:20:07.050: [iter 52 : loss : 0.5399 = 0.0768 + 0.4558 + 0.0074, time: 20.904051]
2023-05-29 20:20:07.307: epoch 52:	0.02968657  	0.08004307  	0.06722218  
2023-05-29 20:20:28.237: [iter 53 : loss : 0.5389 = 0.0759 + 0.4555 + 0.0075, time: 20.926065]
2023-05-29 20:20:28.497: epoch 53:	0.02959033  	0.07952758  	0.06709258  
2023-05-29 20:20:49.628: [iter 54 : loss : 0.5367 = 0.0738 + 0.4553 + 0.0076, time: 21.127776]
2023-05-29 20:20:49.905: epoch 54:	0.02959773  	0.07936618  	0.06707726  
2023-05-29 20:21:10.998: [iter 55 : loss : 0.5348 = 0.0719 + 0.4551 + 0.0077, time: 21.082895]
2023-05-29 20:21:11.261: epoch 55:	0.02948668  	0.07896867  	0.06689695  
2023-05-29 20:21:32.218: [iter 56 : loss : 0.5327 = 0.0700 + 0.4549 + 0.0079, time: 20.953034]
2023-05-29 20:21:32.475: epoch 56:	0.02946446  	0.07874826  	0.06689350  
2023-05-29 20:21:32.476: Early stopping is trigger at epoch: 56
2023-05-29 20:21:32.476: best_result@epoch 31:

2023-05-29 20:21:32.476: 		0.0307      	0.0839      	0.0681      
2023-05-29 20:51:36.934: my pid: 2312
2023-05-29 20:51:36.935: model: model.general_recommender.SGL
2023-05-29 20:51:36.935: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 20:51:36.935: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 20:51:41.042: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 20:52:01.667: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.624056]
2023-05-29 20:52:01.941: epoch 1:	0.00142878  	0.00313628  	0.00255931  
2023-05-29 20:52:01.941: Find a better model.
2023-05-29 20:52:22.682: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.737091]
2023-05-29 20:52:22.961: epoch 2:	0.00211727  	0.00401552  	0.00340936  
2023-05-29 20:52:22.961: Find a better model.
2023-05-29 20:52:43.845: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 20.880025]
2023-05-29 20:52:44.128: epoch 3:	0.00255404  	0.00485833  	0.00420555  
2023-05-29 20:52:44.128: Find a better model.
2023-05-29 20:53:04.880: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.749009]
2023-05-29 20:53:05.164: epoch 4:	0.00307225  	0.00639382  	0.00543158  
2023-05-29 20:53:05.164: Find a better model.
2023-05-29 20:53:25.851: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 20.683031]
2023-05-29 20:53:26.137: epoch 5:	0.00341279  	0.00738436  	0.00584296  
2023-05-29 20:53:26.137: Find a better model.
2023-05-29 20:53:46.822: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 20.682224]
2023-05-29 20:53:47.107: epoch 6:	0.00398283  	0.00975071  	0.00760551  
2023-05-29 20:53:47.107: Find a better model.
2023-05-29 20:54:07.871: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 20.760130]
2023-05-29 20:54:08.159: epoch 7:	0.00467130  	0.01121791  	0.00899793  
2023-05-29 20:54:08.159: Find a better model.
2023-05-29 20:54:29.056: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 20.894042]
2023-05-29 20:54:29.336: epoch 8:	0.00550043  	0.01284051  	0.01032016  
2023-05-29 20:54:29.336: Find a better model.
2023-05-29 20:54:50.040: [iter 9 : loss : 1.1347 = 0.6913 + 0.4435 + 0.0000, time: 20.699055]
2023-05-29 20:54:50.320: epoch 9:	0.00588538  	0.01554470  	0.01223183  
2023-05-29 20:54:50.320: Find a better model.
2023-05-29 20:55:10.823: [iter 10 : loss : 1.1342 = 0.6902 + 0.4440 + 0.0000, time: 20.499037]
2023-05-29 20:55:11.108: epoch 10:	0.00652944  	0.01614631  	0.01308230  
2023-05-29 20:55:11.108: Find a better model.
2023-05-29 20:55:32.589: [iter 11 : loss : 1.1334 = 0.6888 + 0.4446 + 0.0000, time: 21.478102]
2023-05-29 20:55:32.872: epoch 11:	0.00819510  	0.02131886  	0.01736750  
2023-05-29 20:55:32.872: Find a better model.
2023-05-29 20:55:54.190: [iter 12 : loss : 1.1319 = 0.6866 + 0.4452 + 0.0000, time: 21.314210]
2023-05-29 20:55:54.487: epoch 12:	0.01053448  	0.02685724  	0.02345542  
2023-05-29 20:55:54.487: Find a better model.
2023-05-29 20:56:15.546: [iter 13 : loss : 1.1294 = 0.6835 + 0.4458 + 0.0001, time: 21.054137]
2023-05-29 20:56:15.835: epoch 13:	0.01363637  	0.03445187  	0.03111444  
2023-05-29 20:56:15.835: Find a better model.
2023-05-29 20:56:36.930: [iter 14 : loss : 1.1245 = 0.6779 + 0.4465 + 0.0001, time: 21.090442]
2023-05-29 20:56:37.201: epoch 14:	0.01664945  	0.04355169  	0.03874265  
2023-05-29 20:56:37.201: Find a better model.
2023-05-29 20:56:58.190: [iter 15 : loss : 1.1159 = 0.6683 + 0.4474 + 0.0001, time: 20.985421]
2023-05-29 20:56:58.466: epoch 15:	0.01994388  	0.05223522  	0.04591706  
2023-05-29 20:56:58.466: Find a better model.
2023-05-29 20:57:19.529: [iter 16 : loss : 1.1008 = 0.6521 + 0.4486 + 0.0002, time: 21.058397]
2023-05-29 20:57:19.812: epoch 16:	0.02277193  	0.06114904  	0.05237814  
2023-05-29 20:57:19.812: Find a better model.
2023-05-29 20:57:40.703: [iter 17 : loss : 1.0767 = 0.6263 + 0.4501 + 0.0003, time: 20.886071]
2023-05-29 20:57:40.973: epoch 17:	0.02533346  	0.06828985  	0.05785664  
2023-05-29 20:57:40.973: Find a better model.
2023-05-29 20:58:01.922: [iter 18 : loss : 1.0415 = 0.5889 + 0.4522 + 0.0005, time: 20.946077]
2023-05-29 20:58:02.193: epoch 18:	0.02711023  	0.07446855  	0.06168895  
2023-05-29 20:58:02.193: Find a better model.
2023-05-29 20:58:23.388: [iter 19 : loss : 0.9974 = 0.5418 + 0.4548 + 0.0007, time: 21.190996]
2023-05-29 20:58:23.682: epoch 19:	0.02815409  	0.07769796  	0.06395942  
2023-05-29 20:58:23.682: Find a better model.
2023-05-29 20:58:44.944: [iter 20 : loss : 0.9481 = 0.4890 + 0.4582 + 0.0010, time: 21.259297]
2023-05-29 20:58:45.216: epoch 20:	0.02916835  	0.07966705  	0.06521653  
2023-05-29 20:58:45.216: Find a better model.
2023-05-29 20:59:06.484: [iter 21 : loss : 0.8988 = 0.4361 + 0.4615 + 0.0012, time: 21.265043]
2023-05-29 20:59:06.776: epoch 21:	0.02944968  	0.08062726  	0.06605899  
2023-05-29 20:59:06.776: Find a better model.
2023-05-29 20:59:27.894: [iter 22 : loss : 0.8520 = 0.3863 + 0.4641 + 0.0015, time: 21.115007]
2023-05-29 20:59:28.162: epoch 22:	0.02964956  	0.08208054  	0.06614710  
2023-05-29 20:59:28.162: Find a better model.
2023-05-29 20:59:49.272: [iter 23 : loss : 0.8121 = 0.3441 + 0.4662 + 0.0018, time: 21.106058]
2023-05-29 20:59:49.540: epoch 23:	0.02986427  	0.08306919  	0.06652988  
2023-05-29 20:59:49.540: Find a better model.
2023-05-29 21:00:10.539: [iter 24 : loss : 0.7776 = 0.3081 + 0.4673 + 0.0021, time: 20.994013]
2023-05-29 21:00:10.814: epoch 24:	0.03001975  	0.08351334  	0.06696943  
2023-05-29 21:00:10.814: Find a better model.
2023-05-29 21:00:31.845: [iter 25 : loss : 0.7481 = 0.2781 + 0.4677 + 0.0024, time: 21.028130]
2023-05-29 21:00:32.114: epoch 25:	0.03013079  	0.08379916  	0.06721834  
2023-05-29 21:00:32.114: Find a better model.
2023-05-29 21:00:53.248: [iter 26 : loss : 0.7237 = 0.2533 + 0.4677 + 0.0027, time: 21.128672]
2023-05-29 21:00:53.512: epoch 26:	0.03020483  	0.08452182  	0.06751037  
2023-05-29 21:00:53.513: Find a better model.
2023-05-29 21:01:14.505: [iter 27 : loss : 0.7028 = 0.2323 + 0.4675 + 0.0030, time: 20.988059]
2023-05-29 21:01:14.780: epoch 27:	0.03020484  	0.08413485  	0.06748643  
2023-05-29 21:01:35.886: [iter 28 : loss : 0.6850 = 0.2147 + 0.4671 + 0.0032, time: 21.102187]
2023-05-29 21:01:36.150: epoch 28:	0.03016041  	0.08417347  	0.06766339  
2023-05-29 21:01:57.238: [iter 29 : loss : 0.6697 = 0.1998 + 0.4664 + 0.0035, time: 21.083229]
2023-05-29 21:01:57.509: epoch 29:	0.03032327  	0.08466050  	0.06801944  
2023-05-29 21:01:57.509: Find a better model.
2023-05-29 21:02:18.643: [iter 30 : loss : 0.6554 = 0.1859 + 0.4658 + 0.0037, time: 21.130235]
2023-05-29 21:02:18.908: epoch 30:	0.03030106  	0.08459625  	0.06813460  
2023-05-29 21:02:40.015: [iter 31 : loss : 0.6434 = 0.1744 + 0.4651 + 0.0039, time: 21.103062]
2023-05-29 21:02:40.277: epoch 31:	0.03030107  	0.08452830  	0.06822462  
2023-05-29 21:03:01.419: [iter 32 : loss : 0.6331 = 0.1645 + 0.4644 + 0.0041, time: 21.139055]
2023-05-29 21:03:01.683: epoch 32:	0.03029365  	0.08414024  	0.06827802  
2023-05-29 21:03:22.789: [iter 33 : loss : 0.6238 = 0.1557 + 0.4637 + 0.0043, time: 21.100055]
2023-05-29 21:03:23.052: epoch 33:	0.03033067  	0.08466166  	0.06835291  
2023-05-29 21:03:23.052: Find a better model.
2023-05-29 21:03:44.187: [iter 34 : loss : 0.6154 = 0.1478 + 0.4631 + 0.0045, time: 21.131133]
2023-05-29 21:03:44.449: epoch 34:	0.03042691  	0.08495411  	0.06856276  
2023-05-29 21:03:44.450: Find a better model.
2023-05-29 21:04:05.432: [iter 35 : loss : 0.6068 = 0.1396 + 0.4625 + 0.0047, time: 20.979165]
2023-05-29 21:04:05.707: epoch 35:	0.03021222  	0.08435971  	0.06849881  
2023-05-29 21:04:26.817: [iter 36 : loss : 0.6002 = 0.1335 + 0.4619 + 0.0049, time: 21.101497]
2023-05-29 21:04:27.080: epoch 36:	0.03016039  	0.08412682  	0.06860726  
2023-05-29 21:04:48.386: [iter 37 : loss : 0.5944 = 0.1280 + 0.4613 + 0.0051, time: 21.303097]
2023-05-29 21:04:48.648: epoch 37:	0.03036027  	0.08467451  	0.06887991  
2023-05-29 21:05:09.958: [iter 38 : loss : 0.5882 = 0.1222 + 0.4607 + 0.0053, time: 21.306056]
2023-05-29 21:05:10.219: epoch 38:	0.03045653  	0.08483492  	0.06896655  
2023-05-29 21:05:31.590: [iter 39 : loss : 0.5830 = 0.1173 + 0.4603 + 0.0054, time: 21.368076]
2023-05-29 21:05:31.862: epoch 39:	0.03039730  	0.08458051  	0.06897271  
2023-05-29 21:05:53.190: [iter 40 : loss : 0.5782 = 0.1126 + 0.4599 + 0.0056, time: 21.324014]
2023-05-29 21:05:53.451: epoch 40:	0.03049354  	0.08478356  	0.06902727  
2023-05-29 21:06:14.754: [iter 41 : loss : 0.5732 = 0.1081 + 0.4593 + 0.0058, time: 21.299387]
2023-05-29 21:06:15.011: epoch 41:	0.03033067  	0.08429478  	0.06895377  
2023-05-29 21:06:36.404: [iter 42 : loss : 0.5692 = 0.1043 + 0.4589 + 0.0059, time: 21.388125]
2023-05-29 21:06:36.680: epoch 42:	0.03026404  	0.08394489  	0.06880801  
2023-05-29 21:06:57.936: [iter 43 : loss : 0.5651 = 0.1004 + 0.4586 + 0.0061, time: 21.252053]
2023-05-29 21:06:58.199: epoch 43:	0.03025663  	0.08344392  	0.06878074  
2023-05-29 21:07:19.552: [iter 44 : loss : 0.5629 = 0.0984 + 0.4582 + 0.0062, time: 21.349108]
2023-05-29 21:07:19.822: epoch 44:	0.03020481  	0.08338510  	0.06853024  
2023-05-29 21:07:41.101: [iter 45 : loss : 0.5587 = 0.0944 + 0.4579 + 0.0064, time: 21.275071]
2023-05-29 21:07:41.361: epoch 45:	0.03016039  	0.08331227  	0.06852273  
2023-05-29 21:08:02.510: [iter 46 : loss : 0.5556 = 0.0916 + 0.4575 + 0.0065, time: 21.144986]
2023-05-29 21:08:02.786: epoch 46:	0.03015298  	0.08352464  	0.06856516  
2023-05-29 21:08:23.916: [iter 47 : loss : 0.5529 = 0.0889 + 0.4573 + 0.0067, time: 21.127497]
2023-05-29 21:08:24.179: epoch 47:	0.03012336  	0.08336917  	0.06844663  
2023-05-29 21:08:45.507: [iter 48 : loss : 0.5498 = 0.0862 + 0.4568 + 0.0068, time: 21.325052]
2023-05-29 21:08:45.812: epoch 48:	0.03001233  	0.08299369  	0.06828232  
2023-05-29 21:09:07.091: [iter 49 : loss : 0.5471 = 0.0837 + 0.4565 + 0.0070, time: 21.273260]
2023-05-29 21:09:07.354: epoch 49:	0.03001972  	0.08282902  	0.06831019  
2023-05-29 21:09:28.689: [iter 50 : loss : 0.5450 = 0.0816 + 0.4563 + 0.0071, time: 21.332639]
2023-05-29 21:09:28.952: epoch 50:	0.02995311  	0.08275048  	0.06815037  
2023-05-29 21:09:50.297: [iter 51 : loss : 0.5423 = 0.0791 + 0.4559 + 0.0072, time: 21.341370]
2023-05-29 21:09:50.555: epoch 51:	0.02976062  	0.08228329  	0.06784294  
2023-05-29 21:10:11.879: [iter 52 : loss : 0.5402 = 0.0770 + 0.4558 + 0.0074, time: 21.320441]
2023-05-29 21:10:12.142: epoch 52:	0.02980504  	0.08226452  	0.06775322  
2023-05-29 21:10:33.494: [iter 53 : loss : 0.5389 = 0.0759 + 0.4555 + 0.0075, time: 21.347524]
2023-05-29 21:10:33.783: epoch 53:	0.02965698  	0.08184253  	0.06755222  
2023-05-29 21:10:55.043: [iter 54 : loss : 0.5368 = 0.0739 + 0.4553 + 0.0076, time: 21.256441]
2023-05-29 21:10:55.304: epoch 54:	0.02953852  	0.08150104  	0.06724253  
2023-05-29 21:11:16.658: [iter 55 : loss : 0.5347 = 0.0719 + 0.4550 + 0.0077, time: 21.351003]
2023-05-29 21:11:16.920: epoch 55:	0.02959035  	0.08137082  	0.06705721  
2023-05-29 21:11:38.251: [iter 56 : loss : 0.5329 = 0.0703 + 0.4548 + 0.0079, time: 21.326032]
2023-05-29 21:11:38.510: epoch 56:	0.02940527  	0.08063296  	0.06693441  
2023-05-29 21:11:59.664: [iter 57 : loss : 0.5310 = 0.0683 + 0.4548 + 0.0080, time: 21.149559]
2023-05-29 21:11:59.932: epoch 57:	0.02933864  	0.08027066  	0.06675630  
2023-05-29 21:12:21.283: [iter 58 : loss : 0.5297 = 0.0670 + 0.4546 + 0.0081, time: 21.347090]
2023-05-29 21:12:21.547: epoch 58:	0.02938307  	0.08020031  	0.06675828  
2023-05-29 21:12:42.823: [iter 59 : loss : 0.5284 = 0.0658 + 0.4544 + 0.0082, time: 21.273108]
2023-05-29 21:12:43.083: epoch 59:	0.02928682  	0.07973857  	0.06650971  
2023-05-29 21:12:43.083: Early stopping is trigger at epoch: 59
2023-05-29 21:12:43.083: best_result@epoch 34:

2023-05-29 21:12:43.083: 		0.0304      	0.0850      	0.0686      
2023-05-29 21:28:29.706: my pid: 13588
2023-05-29 21:28:29.706: model: model.general_recommender.SGL
2023-05-29 21:28:29.706: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 21:28:29.706: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 21:28:33.766: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 21:28:54.687: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.919776]
2023-05-29 21:28:54.958: epoch 1:	0.00122890  	0.00226294  	0.00197152  
2023-05-29 21:28:54.958: Find a better model.
2023-05-29 21:29:16.161: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.198492]
2023-05-29 21:29:16.469: epoch 2:	0.00175452  	0.00322440  	0.00273182  
2023-05-29 21:29:16.469: Find a better model.
2023-05-29 21:29:37.897: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.424941]
2023-05-29 21:29:38.195: epoch 3:	0.00218389  	0.00455071  	0.00341320  
2023-05-29 21:29:38.195: Find a better model.
2023-05-29 21:29:59.304: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.104313]
2023-05-29 21:29:59.598: epoch 4:	0.00265028  	0.00528285  	0.00432979  
2023-05-29 21:29:59.598: Find a better model.
2023-05-29 21:30:20.688: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 21.086644]
2023-05-29 21:30:20.989: epoch 5:	0.00306485  	0.00618714  	0.00509432  
2023-05-29 21:30:20.989: Find a better model.
2023-05-29 21:30:42.288: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.295528]
2023-05-29 21:30:42.584: epoch 6:	0.00401244  	0.00876116  	0.00706466  
2023-05-29 21:30:42.584: Find a better model.
2023-05-29 21:31:03.895: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.308537]
2023-05-29 21:31:04.191: epoch 7:	0.00496742  	0.01205179  	0.00947663  
2023-05-29 21:31:04.191: Find a better model.
2023-05-29 21:31:25.429: [iter 8 : loss : 1.1348 = 0.6919 + 0.4430 + 0.0000, time: 21.234087]
2023-05-29 21:31:25.717: epoch 8:	0.00581135  	0.01444685  	0.01162993  
2023-05-29 21:31:25.717: Find a better model.
2023-05-29 21:31:46.824: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 21.102074]
2023-05-29 21:31:47.113: epoch 9:	0.00647762  	0.01728873  	0.01412234  
2023-05-29 21:31:47.113: Find a better model.
2023-05-29 21:32:08.018: [iter 10 : loss : 1.1344 = 0.6904 + 0.4440 + 0.0000, time: 20.899175]
2023-05-29 21:32:08.324: epoch 10:	0.00704024  	0.01832657  	0.01527409  
2023-05-29 21:32:08.324: Find a better model.
2023-05-29 21:32:29.836: [iter 11 : loss : 1.1335 = 0.6890 + 0.4445 + 0.0000, time: 21.508952]
2023-05-29 21:32:30.122: epoch 11:	0.00913528  	0.02338574  	0.01979628  
2023-05-29 21:32:30.122: Find a better model.
2023-05-29 21:32:51.815: [iter 12 : loss : 1.1320 = 0.6868 + 0.4451 + 0.0000, time: 21.688264]
2023-05-29 21:32:52.099: epoch 12:	0.01124516  	0.02928852  	0.02446174  
2023-05-29 21:32:52.099: Find a better model.
2023-05-29 21:33:13.610: [iter 13 : loss : 1.1295 = 0.6836 + 0.4459 + 0.0001, time: 21.508585]
2023-05-29 21:33:13.898: epoch 13:	0.01388067  	0.03586493  	0.03062748  
2023-05-29 21:33:13.898: Find a better model.
2023-05-29 21:33:35.574: [iter 14 : loss : 1.1246 = 0.6780 + 0.4465 + 0.0001, time: 21.672871]
2023-05-29 21:33:35.855: epoch 14:	0.01720470  	0.04513219  	0.03877018  
2023-05-29 21:33:35.855: Find a better model.
2023-05-29 21:33:57.598: [iter 15 : loss : 1.1160 = 0.6684 + 0.4474 + 0.0001, time: 21.740212]
2023-05-29 21:33:57.899: epoch 15:	0.02041029  	0.05332873  	0.04584238  
2023-05-29 21:33:57.899: Find a better model.
2023-05-29 21:34:19.402: [iter 16 : loss : 1.1008 = 0.6520 + 0.4485 + 0.0002, time: 21.497044]
2023-05-29 21:34:19.682: epoch 16:	0.02335678  	0.06157899  	0.05276772  
2023-05-29 21:34:19.683: Find a better model.
2023-05-29 21:34:41.164: [iter 17 : loss : 1.0766 = 0.6262 + 0.4501 + 0.0003, time: 21.478997]
2023-05-29 21:34:41.452: epoch 17:	0.02550374  	0.06797820  	0.05739845  
2023-05-29 21:34:41.452: Find a better model.
2023-05-29 21:35:03.138: [iter 18 : loss : 1.0412 = 0.5885 + 0.4522 + 0.0005, time: 21.682410]
2023-05-29 21:35:03.429: epoch 18:	0.02699179  	0.07189297  	0.06069726  
2023-05-29 21:35:03.429: Find a better model.
2023-05-29 21:35:24.742: [iter 19 : loss : 0.9967 = 0.5412 + 0.4548 + 0.0007, time: 21.309509]
2023-05-29 21:35:25.022: epoch 19:	0.02820592  	0.07546204  	0.06294865  
2023-05-29 21:35:25.022: Find a better model.
2023-05-29 21:35:46.352: [iter 20 : loss : 0.9472 = 0.4882 + 0.4581 + 0.0010, time: 21.327521]
2023-05-29 21:35:46.651: epoch 20:	0.02892402  	0.07786568  	0.06465118  
2023-05-29 21:35:46.652: Find a better model.
2023-05-29 21:36:07.946: [iter 21 : loss : 0.8979 = 0.4353 + 0.4614 + 0.0012, time: 21.291060]
2023-05-29 21:36:08.223: epoch 21:	0.02953852  	0.07984447  	0.06588358  
2023-05-29 21:36:08.223: Find a better model.
2023-05-29 21:36:29.750: [iter 22 : loss : 0.8512 = 0.3857 + 0.4640 + 0.0015, time: 21.521081]
2023-05-29 21:36:30.028: epoch 22:	0.02959775  	0.08047067  	0.06629837  
2023-05-29 21:36:30.028: Find a better model.
2023-05-29 21:36:51.716: [iter 23 : loss : 0.8113 = 0.3435 + 0.4660 + 0.0018, time: 21.684018]
2023-05-29 21:36:51.996: epoch 23:	0.02992351  	0.08189017  	0.06671085  
2023-05-29 21:36:51.996: Find a better model.
2023-05-29 21:37:13.525: [iter 24 : loss : 0.7771 = 0.3079 + 0.4671 + 0.0021, time: 21.525047]
2023-05-29 21:37:13.801: epoch 24:	0.02980507  	0.08203224  	0.06662888  
2023-05-29 21:37:13.801: Find a better model.
2023-05-29 21:37:35.307: [iter 25 : loss : 0.7476 = 0.2777 + 0.4675 + 0.0024, time: 21.502226]
2023-05-29 21:37:35.579: epoch 25:	0.03001235  	0.08276221  	0.06691626  
2023-05-29 21:37:35.579: Find a better model.
2023-05-29 21:37:56.926: [iter 26 : loss : 0.7232 = 0.2530 + 0.4675 + 0.0027, time: 21.343008]
2023-05-29 21:37:57.194: epoch 26:	0.03004198  	0.08320439  	0.06708025  
2023-05-29 21:37:57.194: Find a better model.
2023-05-29 21:38:18.700: [iter 27 : loss : 0.7022 = 0.2320 + 0.4673 + 0.0030, time: 21.502068]
2023-05-29 21:38:18.993: epoch 27:	0.03005680  	0.08345657  	0.06712938  
2023-05-29 21:38:18.993: Find a better model.
2023-05-29 21:38:40.513: [iter 28 : loss : 0.6843 = 0.2143 + 0.4667 + 0.0032, time: 21.517031]
2023-05-29 21:38:40.786: epoch 28:	0.03028630  	0.08439960  	0.06739070  
2023-05-29 21:38:40.786: Find a better model.
2023-05-29 21:39:02.292: [iter 29 : loss : 0.6692 = 0.1995 + 0.4662 + 0.0035, time: 21.502051]
2023-05-29 21:39:02.563: epoch 29:	0.03024928  	0.08415949  	0.06742034  
2023-05-29 21:39:24.066: [iter 30 : loss : 0.6551 = 0.1859 + 0.4655 + 0.0037, time: 21.500007]
2023-05-29 21:39:24.347: epoch 30:	0.03038254  	0.08473187  	0.06770805  
2023-05-29 21:39:24.348: Find a better model.
2023-05-29 21:39:46.051: [iter 31 : loss : 0.6428 = 0.1740 + 0.4649 + 0.0039, time: 21.700266]
2023-05-29 21:39:46.336: epoch 31:	0.03050099  	0.08510125  	0.06814736  
2023-05-29 21:39:46.336: Find a better model.
2023-05-29 21:40:07.842: [iter 32 : loss : 0.6328 = 0.1644 + 0.4643 + 0.0041, time: 21.501009]
2023-05-29 21:40:08.108: epoch 32:	0.03049359  	0.08502147  	0.06806106  
2023-05-29 21:40:29.481: [iter 33 : loss : 0.6237 = 0.1558 + 0.4636 + 0.0043, time: 21.368682]
2023-05-29 21:40:29.770: epoch 33:	0.03045657  	0.08518786  	0.06806119  
2023-05-29 21:40:29.770: Find a better model.
2023-05-29 21:40:51.262: [iter 34 : loss : 0.6154 = 0.1479 + 0.4630 + 0.0045, time: 21.488179]
2023-05-29 21:40:51.532: epoch 34:	0.03038254  	0.08504999  	0.06786299  
2023-05-29 21:41:13.022: [iter 35 : loss : 0.6066 = 0.1396 + 0.4623 + 0.0047, time: 21.485014]
2023-05-29 21:41:13.311: epoch 35:	0.03041955  	0.08542024  	0.06814382  
2023-05-29 21:41:13.311: Find a better model.
2023-05-29 21:41:35.014: [iter 36 : loss : 0.5998 = 0.1332 + 0.4617 + 0.0049, time: 21.699218]
2023-05-29 21:41:35.299: epoch 36:	0.03047138  	0.08540571  	0.06829688  
2023-05-29 21:41:57.014: [iter 37 : loss : 0.5940 = 0.1277 + 0.4612 + 0.0051, time: 21.710026]
2023-05-29 21:41:57.303: epoch 37:	0.03038994  	0.08513710  	0.06827056  
2023-05-29 21:42:18.858: [iter 38 : loss : 0.5879 = 0.1221 + 0.4606 + 0.0053, time: 21.550916]
2023-05-29 21:42:19.143: epoch 38:	0.03036772  	0.08501027  	0.06833982  
2023-05-29 21:42:40.787: [iter 39 : loss : 0.5832 = 0.1176 + 0.4602 + 0.0054, time: 21.640074]
2023-05-29 21:42:41.054: epoch 39:	0.03052319  	0.08521200  	0.06848532  
2023-05-29 21:43:02.600: [iter 40 : loss : 0.5778 = 0.1126 + 0.4596 + 0.0056, time: 21.543070]
2023-05-29 21:43:02.869: epoch 40:	0.03040474  	0.08499599  	0.06835906  
2023-05-29 21:43:24.566: [iter 41 : loss : 0.5730 = 0.1080 + 0.4593 + 0.0058, time: 21.694101]
2023-05-29 21:43:24.840: epoch 41:	0.03033070  	0.08454181  	0.06834926  
2023-05-29 21:43:46.421: [iter 42 : loss : 0.5690 = 0.1041 + 0.4589 + 0.0059, time: 21.577300]
2023-05-29 21:43:46.705: epoch 42:	0.03027888  	0.08452380  	0.06845471  
2023-05-29 21:44:08.156: [iter 43 : loss : 0.5651 = 0.1005 + 0.4585 + 0.0061, time: 21.447310]
2023-05-29 21:44:08.432: epoch 43:	0.03027148  	0.08437308  	0.06836758  
2023-05-29 21:44:29.969: [iter 44 : loss : 0.5623 = 0.0979 + 0.4581 + 0.0062, time: 21.533068]
2023-05-29 21:44:30.239: epoch 44:	0.03045654  	0.08466638  	0.06858112  
2023-05-29 21:44:51.966: [iter 45 : loss : 0.5587 = 0.0945 + 0.4578 + 0.0064, time: 21.722417]
2023-05-29 21:44:52.232: epoch 45:	0.03030848  	0.08390532  	0.06822130  
2023-05-29 21:45:13.940: [iter 46 : loss : 0.5555 = 0.0915 + 0.4575 + 0.0065, time: 21.703944]
2023-05-29 21:45:14.209: epoch 46:	0.03021964  	0.08357055  	0.06794489  
2023-05-29 21:45:35.535: [iter 47 : loss : 0.5524 = 0.0886 + 0.4571 + 0.0067, time: 21.322213]
2023-05-29 21:45:35.804: epoch 47:	0.03020484  	0.08356310  	0.06778138  
2023-05-29 21:45:57.351: [iter 48 : loss : 0.5496 = 0.0860 + 0.4568 + 0.0068, time: 21.544657]
2023-05-29 21:45:57.618: epoch 48:	0.03003456  	0.08299041  	0.06754318  
2023-05-29 21:46:19.101: [iter 49 : loss : 0.5472 = 0.0838 + 0.4565 + 0.0070, time: 21.478136]
2023-05-29 21:46:19.380: epoch 49:	0.02993832  	0.08251955  	0.06744719  
2023-05-29 21:46:40.902: [iter 50 : loss : 0.5451 = 0.0818 + 0.4563 + 0.0071, time: 21.518021]
2023-05-29 21:46:41.170: epoch 50:	0.02982727  	0.08202127  	0.06724630  
2023-05-29 21:47:02.690: [iter 51 : loss : 0.5425 = 0.0792 + 0.4560 + 0.0072, time: 21.517013]
2023-05-29 21:47:02.958: epoch 51:	0.02982727  	0.08201007  	0.06733855  
2023-05-29 21:47:24.497: [iter 52 : loss : 0.5398 = 0.0768 + 0.4556 + 0.0074, time: 21.535049]
2023-05-29 21:47:24.767: epoch 52:	0.02979766  	0.08194008  	0.06729577  
2023-05-29 21:47:46.293: [iter 53 : loss : 0.5392 = 0.0762 + 0.4555 + 0.0075, time: 21.522792]
2023-05-29 21:47:46.564: epoch 53:	0.02976805  	0.08142278  	0.06714076  
2023-05-29 21:48:08.089: [iter 54 : loss : 0.5366 = 0.0738 + 0.4552 + 0.0076, time: 21.519677]
2023-05-29 21:48:08.372: epoch 54:	0.02971621  	0.08113793  	0.06692290  
2023-05-29 21:48:29.892: [iter 55 : loss : 0.5349 = 0.0721 + 0.4551 + 0.0077, time: 21.514987]
2023-05-29 21:48:30.161: epoch 55:	0.02961257  	0.08094676  	0.06686971  
2023-05-29 21:48:51.673: [iter 56 : loss : 0.5325 = 0.0699 + 0.4548 + 0.0079, time: 21.507994]
2023-05-29 21:48:51.945: epoch 56:	0.02950892  	0.08052436  	0.06664234  
2023-05-29 21:49:13.671: [iter 57 : loss : 0.5309 = 0.0683 + 0.4546 + 0.0080, time: 21.721993]
2023-05-29 21:49:13.939: epoch 57:	0.02944969  	0.08026382  	0.06644963  
2023-05-29 21:49:35.449: [iter 58 : loss : 0.5295 = 0.0669 + 0.4545 + 0.0081, time: 21.505328]
2023-05-29 21:49:35.718: epoch 58:	0.02942008  	0.08003372  	0.06647170  
2023-05-29 21:49:57.098: [iter 59 : loss : 0.5278 = 0.0653 + 0.4543 + 0.0082, time: 21.376465]
2023-05-29 21:49:57.378: epoch 59:	0.02940527  	0.07984038  	0.06641411  
2023-05-29 21:50:18.834: [iter 60 : loss : 0.5269 = 0.0644 + 0.4542 + 0.0083, time: 21.452009]
2023-05-29 21:50:19.118: epoch 60:	0.02922019  	0.07912616  	0.06615149  
2023-05-29 21:50:19.118: Early stopping is trigger at epoch: 60
2023-05-29 21:50:19.118: best_result@epoch 35:

2023-05-29 21:50:19.118: 		0.0304      	0.0854      	0.0681      
2023-05-29 22:12:53.466: my pid: 4132
2023-05-29 22:12:53.466: model: model.general_recommender.SGL
2023-05-29 22:12:53.466: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-29 22:12:53.466: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-29 22:12:57.548: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-29 22:13:18.646: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.097460]
2023-05-29 22:13:18.910: epoch 1:	0.00117708  	0.00238847  	0.00195853  
2023-05-29 22:13:18.910: Find a better model.
2023-05-29 22:13:40.026: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.111099]
2023-05-29 22:13:40.311: epoch 2:	0.00173231  	0.00331134  	0.00281544  
2023-05-29 22:13:40.311: Find a better model.
2023-05-29 22:14:01.429: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.114168]
2023-05-29 22:14:01.728: epoch 3:	0.00208025  	0.00380434  	0.00316227  
2023-05-29 22:14:01.728: Find a better model.
2023-05-29 22:14:22.832: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 21.101362]
2023-05-29 22:14:23.121: epoch 4:	0.00262067  	0.00586857  	0.00470534  
2023-05-29 22:14:23.121: Find a better model.
2023-05-29 22:14:44.375: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.250052]
2023-05-29 22:14:44.675: epoch 5:	0.00330175  	0.00698588  	0.00595037  
2023-05-29 22:14:44.675: Find a better model.
2023-05-29 22:15:05.847: [iter 6 : loss : 1.1346 = 0.6925 + 0.4422 + 0.0000, time: 21.168038]
2023-05-29 22:15:06.132: epoch 6:	0.00409387  	0.00935021  	0.00745241  
2023-05-29 22:15:06.132: Find a better model.
2023-05-29 22:15:27.201: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 21.066141]
2023-05-29 22:15:27.486: epoch 7:	0.00459727  	0.01091589  	0.00877803  
2023-05-29 22:15:27.487: Find a better model.
2023-05-29 22:15:48.583: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 21.092974]
2023-05-29 22:15:48.870: epoch 8:	0.00533756  	0.01303778  	0.01070128  
2023-05-29 22:15:48.870: Find a better model.
2023-05-29 22:16:10.000: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 21.127062]
2023-05-29 22:16:10.282: epoch 9:	0.00662568  	0.01718501  	0.01418958  
2023-05-29 22:16:10.282: Find a better model.
2023-05-29 22:16:31.030: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 20.744488]
2023-05-29 22:16:31.310: epoch 10:	0.00767690  	0.02035583  	0.01690010  
2023-05-29 22:16:31.310: Find a better model.
2023-05-29 22:16:52.833: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 21.518116]
2023-05-29 22:16:53.139: epoch 11:	0.00929814  	0.02413489  	0.02076268  
2023-05-29 22:16:53.139: Find a better model.
2023-05-29 22:17:14.739: [iter 12 : loss : 1.1322 = 0.6872 + 0.4449 + 0.0000, time: 21.594301]
2023-05-29 22:17:15.015: epoch 12:	0.01095645  	0.02973337  	0.02501794  
2023-05-29 22:17:15.015: Find a better model.
2023-05-29 22:17:36.374: [iter 13 : loss : 1.1297 = 0.6840 + 0.4456 + 0.0001, time: 21.353989]
2023-05-29 22:17:36.663: epoch 13:	0.01356235  	0.03632323  	0.03112083  
2023-05-29 22:17:36.663: Find a better model.
2023-05-29 22:17:58.137: [iter 14 : loss : 1.1252 = 0.6787 + 0.4464 + 0.0001, time: 21.470537]
2023-05-29 22:17:58.409: epoch 14:	0.01659025  	0.04321189  	0.03849950  
2023-05-29 22:17:58.409: Find a better model.
2023-05-29 22:18:19.744: [iter 15 : loss : 1.1170 = 0.6697 + 0.4473 + 0.0001, time: 21.331637]
2023-05-29 22:18:20.015: epoch 15:	0.01953671  	0.05143584  	0.04570454  
2023-05-29 22:18:20.015: Find a better model.
2023-05-29 22:18:41.312: [iter 16 : loss : 1.1027 = 0.6541 + 0.4484 + 0.0002, time: 21.292082]
2023-05-29 22:18:41.591: epoch 16:	0.02243140  	0.05851616  	0.05177237  
2023-05-29 22:18:41.592: Find a better model.
2023-05-29 22:19:02.704: [iter 17 : loss : 1.0794 = 0.6293 + 0.4498 + 0.0003, time: 21.104336]
2023-05-29 22:19:02.978: epoch 17:	0.02511877  	0.06621171  	0.05721997  
2023-05-29 22:19:02.978: Find a better model.
2023-05-29 22:19:24.323: [iter 18 : loss : 1.0448 = 0.5925 + 0.4519 + 0.0005, time: 21.340658]
2023-05-29 22:19:24.614: epoch 18:	0.02687335  	0.07136759  	0.06064217  
2023-05-29 22:19:24.614: Find a better model.
2023-05-29 22:19:45.714: [iter 19 : loss : 1.0008 = 0.5455 + 0.4546 + 0.0007, time: 21.095500]
2023-05-29 22:19:45.985: epoch 19:	0.02831697  	0.07573871  	0.06368828  
2023-05-29 22:19:45.986: Find a better model.
2023-05-29 22:20:07.481: [iter 20 : loss : 0.9513 = 0.4924 + 0.4580 + 0.0009, time: 21.491036]
2023-05-29 22:20:07.758: epoch 20:	0.02899066  	0.07743463  	0.06509680  
2023-05-29 22:20:07.758: Find a better model.
2023-05-29 22:20:29.070: [iter 21 : loss : 0.9016 = 0.4390 + 0.4613 + 0.0012, time: 21.307940]
2023-05-29 22:20:29.340: epoch 21:	0.02936083  	0.07923056  	0.06587304  
2023-05-29 22:20:29.340: Find a better model.
2023-05-29 22:20:50.673: [iter 22 : loss : 0.8543 = 0.3888 + 0.4640 + 0.0015, time: 21.329992]
2023-05-29 22:20:50.942: epoch 22:	0.02953113  	0.08018424  	0.06599887  
2023-05-29 22:20:50.942: Find a better model.
2023-05-29 22:21:12.460: [iter 23 : loss : 0.8137 = 0.3458 + 0.4660 + 0.0018, time: 21.515402]
2023-05-29 22:21:12.736: epoch 23:	0.02967918  	0.08115955  	0.06634459  
2023-05-29 22:21:12.736: Find a better model.
2023-05-29 22:21:34.083: [iter 24 : loss : 0.7790 = 0.3097 + 0.4672 + 0.0021, time: 21.341308]
2023-05-29 22:21:34.354: epoch 24:	0.02952371  	0.08072570  	0.06602786  
2023-05-29 22:21:55.684: [iter 25 : loss : 0.7494 = 0.2794 + 0.4676 + 0.0024, time: 21.327564]
2023-05-29 22:21:55.949: epoch 25:	0.02969398  	0.08125395  	0.06652848  
2023-05-29 22:21:55.950: Find a better model.
2023-05-29 22:22:17.258: [iter 26 : loss : 0.7247 = 0.2544 + 0.4675 + 0.0027, time: 21.302554]
2023-05-29 22:22:17.526: epoch 26:	0.02987166  	0.08219053  	0.06690960  
2023-05-29 22:22:17.526: Find a better model.
2023-05-29 22:22:38.634: [iter 27 : loss : 0.7036 = 0.2334 + 0.4673 + 0.0029, time: 21.104182]
2023-05-29 22:22:38.902: epoch 27:	0.02993829  	0.08249117  	0.06711684  
2023-05-29 22:22:38.902: Find a better model.
2023-05-29 22:23:00.199: [iter 28 : loss : 0.6853 = 0.2152 + 0.4669 + 0.0032, time: 21.293077]
2023-05-29 22:23:00.465: epoch 28:	0.03003454  	0.08284874  	0.06725258  
2023-05-29 22:23:00.466: Find a better model.
2023-05-29 22:23:21.610: [iter 29 : loss : 0.6701 = 0.2004 + 0.4662 + 0.0034, time: 21.141331]
2023-05-29 22:23:21.878: epoch 29:	0.03009377  	0.08296613  	0.06724824  
2023-05-29 22:23:21.878: Find a better model.
2023-05-29 22:23:43.224: [iter 30 : loss : 0.6557 = 0.1865 + 0.4656 + 0.0037, time: 21.341507]
2023-05-29 22:23:43.490: epoch 30:	0.03010857  	0.08297113  	0.06741296  
2023-05-29 22:23:43.490: Find a better model.
2023-05-29 22:24:05.007: [iter 31 : loss : 0.6437 = 0.1748 + 0.4650 + 0.0039, time: 21.514138]
2023-05-29 22:24:05.277: epoch 31:	0.03034547  	0.08328481  	0.06767945  
2023-05-29 22:24:05.277: Find a better model.
2023-05-29 22:24:26.987: [iter 32 : loss : 0.6334 = 0.1650 + 0.4642 + 0.0041, time: 21.705043]
2023-05-29 22:24:27.253: epoch 32:	0.03030845  	0.08308510  	0.06759270  
2023-05-29 22:24:48.784: [iter 33 : loss : 0.6241 = 0.1562 + 0.4636 + 0.0043, time: 21.527338]
2023-05-29 22:24:49.049: epoch 33:	0.03027143  	0.08281142  	0.06756657  
2023-05-29 22:25:10.606: [iter 34 : loss : 0.6154 = 0.1480 + 0.4629 + 0.0045, time: 21.552189]
2023-05-29 22:25:10.870: epoch 34:	0.03042689  	0.08303608  	0.06763756  
2023-05-29 22:25:32.573: [iter 35 : loss : 0.6070 = 0.1400 + 0.4623 + 0.0047, time: 21.697994]
2023-05-29 22:25:32.839: epoch 35:	0.03033065  	0.08279600  	0.06764857  
2023-05-29 22:25:54.542: [iter 36 : loss : 0.6008 = 0.1343 + 0.4616 + 0.0049, time: 21.698984]
2023-05-29 22:25:54.814: epoch 36:	0.03032326  	0.08303824  	0.06775040  
2023-05-29 22:26:16.366: [iter 37 : loss : 0.5945 = 0.1283 + 0.4611 + 0.0051, time: 21.548023]
2023-05-29 22:26:16.649: epoch 37:	0.03026403  	0.08253923  	0.06755294  
2023-05-29 22:26:38.360: [iter 38 : loss : 0.5886 = 0.1228 + 0.4605 + 0.0053, time: 21.707916]
2023-05-29 22:26:38.640: epoch 38:	0.03036027  	0.08288837  	0.06769051  
2023-05-29 22:27:00.086: [iter 39 : loss : 0.5831 = 0.1177 + 0.4600 + 0.0054, time: 21.441384]
2023-05-29 22:27:00.353: epoch 39:	0.03049353  	0.08328409  	0.06798475  
2023-05-29 22:27:21.745: [iter 40 : loss : 0.5782 = 0.1129 + 0.4597 + 0.0056, time: 21.388250]
2023-05-29 22:27:22.008: epoch 40:	0.03044911  	0.08316316  	0.06794527  
2023-05-29 22:27:43.520: [iter 41 : loss : 0.5734 = 0.1085 + 0.4592 + 0.0058, time: 21.508263]
2023-05-29 22:27:43.789: epoch 41:	0.03047132  	0.08283770  	0.06799116  
2023-05-29 22:28:05.131: [iter 42 : loss : 0.5692 = 0.1045 + 0.4588 + 0.0059, time: 21.337990]
2023-05-29 22:28:05.396: epoch 42:	0.03047872  	0.08283310  	0.06790768  
2023-05-29 22:28:26.703: [iter 43 : loss : 0.5652 = 0.1008 + 0.4583 + 0.0061, time: 21.302520]
2023-05-29 22:28:26.968: epoch 43:	0.03034547  	0.08272782  	0.06778798  
2023-05-29 22:28:48.499: [iter 44 : loss : 0.5626 = 0.0984 + 0.4580 + 0.0062, time: 21.527231]
2023-05-29 22:28:48.770: epoch 44:	0.03027144  	0.08252951  	0.06760699  
2023-05-29 22:29:10.062: [iter 45 : loss : 0.5588 = 0.0947 + 0.4576 + 0.0064, time: 21.287559]
2023-05-29 22:29:10.324: epoch 45:	0.03036027  	0.08308306  	0.06792735  
2023-05-29 22:29:31.686: [iter 46 : loss : 0.5555 = 0.0917 + 0.4573 + 0.0065, time: 21.358069]
2023-05-29 22:29:31.949: epoch 46:	0.03021962  	0.08258638  	0.06758370  
2023-05-29 22:29:53.311: [iter 47 : loss : 0.5525 = 0.0888 + 0.4570 + 0.0067, time: 21.359004]
2023-05-29 22:29:53.574: epoch 47:	0.03031586  	0.08259830  	0.06768923  
2023-05-29 22:30:14.858: [iter 48 : loss : 0.5498 = 0.0863 + 0.4567 + 0.0068, time: 21.278163]
2023-05-29 22:30:15.123: epoch 48:	0.03018999  	0.08251445  	0.06764515  
2023-05-29 22:30:36.459: [iter 49 : loss : 0.5475 = 0.0842 + 0.4563 + 0.0070, time: 21.331583]
2023-05-29 22:30:36.728: epoch 49:	0.03005674  	0.08226742  	0.06752726  
2023-05-29 22:30:58.247: [iter 50 : loss : 0.5450 = 0.0819 + 0.4561 + 0.0071, time: 21.514047]
2023-05-29 22:30:58.510: epoch 50:	0.02996790  	0.08178540  	0.06727679  
2023-05-29 22:31:19.639: [iter 51 : loss : 0.5425 = 0.0795 + 0.4558 + 0.0072, time: 21.125112]
2023-05-29 22:31:19.921: epoch 51:	0.02989387  	0.08125932  	0.06714483  
2023-05-29 22:31:41.256: [iter 52 : loss : 0.5399 = 0.0770 + 0.4556 + 0.0073, time: 21.328553]
2023-05-29 22:31:41.520: epoch 52:	0.02988647  	0.08148816  	0.06713062  
2023-05-29 22:32:02.803: [iter 53 : loss : 0.5389 = 0.0761 + 0.4553 + 0.0075, time: 21.278654]
2023-05-29 22:32:03.066: epoch 53:	0.02979763  	0.08131259  	0.06692781  
2023-05-29 22:32:24.205: [iter 54 : loss : 0.5368 = 0.0741 + 0.4551 + 0.0076, time: 21.136053]
2023-05-29 22:32:24.470: epoch 54:	0.02981243  	0.08119213  	0.06685507  
2023-05-29 22:32:45.618: [iter 55 : loss : 0.5345 = 0.0719 + 0.4549 + 0.0077, time: 21.144084]
2023-05-29 22:32:45.881: epoch 55:	0.02980502  	0.08114198  	0.06677867  
2023-05-29 22:33:07.220: [iter 56 : loss : 0.5327 = 0.0701 + 0.4547 + 0.0078, time: 21.335039]
2023-05-29 22:33:07.483: epoch 56:	0.02970138  	0.08108134  	0.06668236  
2023-05-29 22:33:07.483: Early stopping is trigger at epoch: 56
2023-05-29 22:33:07.483: best_result@epoch 31:

2023-05-29 22:33:07.483: 		0.0303      	0.0833      	0.0677      
2023-05-30 09:13:24.241: my pid: 14980
2023-05-30 09:13:24.241: model: model.general_recommender.SGL
2023-05-30 09:13:24.241: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 09:13:24.241: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 09:13:28.322: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 09:13:49.022: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.698245]
2023-05-30 09:13:49.288: epoch 1:	0.00147320  	0.00311625  	0.00246160  
2023-05-30 09:13:49.288: Find a better model.
2023-05-30 09:14:10.288: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.996053]
2023-05-30 09:14:10.575: epoch 2:	0.00186556  	0.00367127  	0.00304520  
2023-05-30 09:14:10.575: Find a better model.
2023-05-30 09:14:31.460: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 20.882866]
2023-05-30 09:14:31.751: epoch 3:	0.00208025  	0.00367878  	0.00312150  
2023-05-30 09:14:31.751: Find a better model.
2023-05-30 09:14:52.457: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.702040]
2023-05-30 09:14:52.745: epoch 4:	0.00270951  	0.00538028  	0.00431047  
2023-05-30 09:14:52.745: Find a better model.
2023-05-30 09:15:13.638: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 20.888020]
2023-05-30 09:15:13.938: epoch 5:	0.00339059  	0.00700443  	0.00559875  
2023-05-30 09:15:13.938: Find a better model.
2023-05-30 09:15:34.737: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 20.795528]
2023-05-30 09:15:35.036: epoch 6:	0.00381996  	0.00861051  	0.00682860  
2023-05-30 09:15:35.036: Find a better model.
2023-05-30 09:15:56.037: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.997394]
2023-05-30 09:15:56.323: epoch 7:	0.00472312  	0.01130587  	0.00911435  
2023-05-30 09:15:56.323: Find a better model.
2023-05-30 09:16:17.407: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 21.081024]
2023-05-30 09:16:17.692: epoch 8:	0.00569291  	0.01371649  	0.01111854  
2023-05-30 09:16:17.692: Find a better model.
2023-05-30 09:16:38.593: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 20.896023]
2023-05-30 09:16:38.893: epoch 9:	0.00649983  	0.01589953  	0.01310365  
2023-05-30 09:16:38.893: Find a better model.
2023-05-30 09:16:59.985: [iter 10 : loss : 1.1343 = 0.6904 + 0.4439 + 0.0000, time: 21.087058]
2023-05-30 09:17:00.272: epoch 10:	0.00720311  	0.01824263  	0.01473298  
2023-05-30 09:17:00.272: Find a better model.
2023-05-30 09:17:21.583: [iter 11 : loss : 1.1335 = 0.6889 + 0.4445 + 0.0000, time: 21.308026]
2023-05-30 09:17:21.879: epoch 11:	0.00846901  	0.02231394  	0.01851178  
2023-05-30 09:17:21.879: Find a better model.
2023-05-30 09:17:43.008: [iter 12 : loss : 1.1319 = 0.6867 + 0.4451 + 0.0000, time: 21.125247]
2023-05-30 09:17:43.287: epoch 12:	0.01058630  	0.02856528  	0.02372318  
2023-05-30 09:17:43.287: Find a better model.
2023-05-30 09:18:04.588: [iter 13 : loss : 1.1292 = 0.6834 + 0.4458 + 0.0001, time: 21.296685]
2023-05-30 09:18:04.879: epoch 13:	0.01297010  	0.03461844  	0.02947227  
2023-05-30 09:18:04.879: Find a better model.
2023-05-30 09:18:25.976: [iter 14 : loss : 1.1244 = 0.6777 + 0.4466 + 0.0001, time: 21.091154]
2023-05-30 09:18:26.249: epoch 14:	0.01617569  	0.04309722  	0.03684816  
2023-05-30 09:18:26.249: Find a better model.
2023-05-30 09:18:47.510: [iter 15 : loss : 1.1156 = 0.6680 + 0.4475 + 0.0001, time: 21.257215]
2023-05-30 09:18:47.781: epoch 15:	0.01954411  	0.05185821  	0.04457111  
2023-05-30 09:18:47.781: Find a better model.
2023-05-30 09:19:08.954: [iter 16 : loss : 1.1002 = 0.6513 + 0.4487 + 0.0002, time: 21.168440]
2023-05-30 09:19:09.228: epoch 16:	0.02261647  	0.06047874  	0.05179189  
2023-05-30 09:19:09.228: Find a better model.
2023-05-30 09:19:30.352: [iter 17 : loss : 1.0756 = 0.6252 + 0.4501 + 0.0003, time: 21.117106]
2023-05-30 09:19:30.625: epoch 17:	0.02489668  	0.06578263  	0.05666751  
2023-05-30 09:19:30.625: Find a better model.
2023-05-30 09:19:51.749: [iter 18 : loss : 1.0398 = 0.5871 + 0.4522 + 0.0005, time: 21.118025]
2023-05-30 09:19:52.029: epoch 18:	0.02688814  	0.07093685  	0.06086945  
2023-05-30 09:19:52.029: Find a better model.
2023-05-30 09:20:13.135: [iter 19 : loss : 0.9952 = 0.5395 + 0.4550 + 0.0007, time: 21.102065]
2023-05-30 09:20:13.405: epoch 19:	0.02812449  	0.07498893  	0.06379868  
2023-05-30 09:20:13.405: Find a better model.
2023-05-30 09:20:34.695: [iter 20 : loss : 0.9456 = 0.4863 + 0.4583 + 0.0010, time: 21.285297]
2023-05-30 09:20:34.970: epoch 20:	0.02900546  	0.07766180  	0.06544329  
2023-05-30 09:20:34.971: Find a better model.
2023-05-30 09:20:56.122: [iter 21 : loss : 0.8962 = 0.4334 + 0.4616 + 0.0013, time: 21.147150]
2023-05-30 09:20:56.404: epoch 21:	0.02944227  	0.07901374  	0.06616320  
2023-05-30 09:20:56.405: Find a better model.
2023-05-30 09:21:17.498: [iter 22 : loss : 0.8496 = 0.3837 + 0.4643 + 0.0016, time: 21.088708]
2023-05-30 09:21:17.762: epoch 22:	0.02976801  	0.07990331  	0.06660653  
2023-05-30 09:21:17.762: Find a better model.
2023-05-30 09:21:39.104: [iter 23 : loss : 0.8100 = 0.3418 + 0.4663 + 0.0019, time: 21.338099]
2023-05-30 09:21:39.368: epoch 23:	0.02984945  	0.08037931  	0.06651245  
2023-05-30 09:21:39.368: Find a better model.
2023-05-30 09:22:00.655: [iter 24 : loss : 0.7758 = 0.3064 + 0.4673 + 0.0022, time: 21.282940]
2023-05-30 09:22:00.935: epoch 24:	0.03000493  	0.08134454  	0.06667059  
2023-05-30 09:22:00.935: Find a better model.
2023-05-30 09:22:22.423: [iter 25 : loss : 0.7467 = 0.2766 + 0.4677 + 0.0024, time: 21.484615]
2023-05-30 09:22:22.708: epoch 25:	0.03016039  	0.08209261  	0.06695230  
2023-05-30 09:22:22.708: Find a better model.
2023-05-30 09:22:44.049: [iter 26 : loss : 0.7224 = 0.2519 + 0.4678 + 0.0027, time: 21.337891]
2023-05-30 09:22:44.316: epoch 26:	0.03008636  	0.08186457  	0.06686167  
2023-05-30 09:23:05.865: [iter 27 : loss : 0.7015 = 0.2312 + 0.4674 + 0.0030, time: 21.543312]
2023-05-30 09:23:06.130: epoch 27:	0.03027143  	0.08234195  	0.06706014  
2023-05-30 09:23:06.130: Find a better model.
2023-05-30 09:23:27.657: [iter 28 : loss : 0.6837 = 0.2135 + 0.4670 + 0.0032, time: 21.522319]
2023-05-30 09:23:27.934: epoch 28:	0.03016780  	0.08201972  	0.06713319  
2023-05-30 09:23:49.039: [iter 29 : loss : 0.6685 = 0.1988 + 0.4662 + 0.0035, time: 21.101606]
2023-05-30 09:23:49.300: epoch 29:	0.03028625  	0.08222726  	0.06726661  
2023-05-30 09:24:10.574: [iter 30 : loss : 0.6544 = 0.1850 + 0.4657 + 0.0037, time: 21.270060]
2023-05-30 09:24:10.837: epoch 30:	0.03014558  	0.08200259  	0.06728805  
2023-05-30 09:24:32.035: [iter 31 : loss : 0.6424 = 0.1734 + 0.4650 + 0.0039, time: 21.193020]
2023-05-30 09:24:32.301: epoch 31:	0.03025663  	0.08232427  	0.06748854  
2023-05-30 09:24:53.419: [iter 32 : loss : 0.6323 = 0.1638 + 0.4644 + 0.0041, time: 21.112206]
2023-05-30 09:24:53.679: epoch 32:	0.03024181  	0.08190129  	0.06740524  
2023-05-30 09:25:15.048: [iter 33 : loss : 0.6233 = 0.1554 + 0.4636 + 0.0043, time: 21.364858]
2023-05-30 09:25:15.328: epoch 33:	0.03038987  	0.08225208  	0.06752604  
2023-05-30 09:25:36.801: [iter 34 : loss : 0.6148 = 0.1471 + 0.4632 + 0.0045, time: 21.466569]
2023-05-30 09:25:37.069: epoch 34:	0.03043429  	0.08264211  	0.06787194  
2023-05-30 09:25:37.069: Find a better model.
2023-05-30 09:25:58.405: [iter 35 : loss : 0.6066 = 0.1395 + 0.4624 + 0.0047, time: 21.332096]
2023-05-30 09:25:58.684: epoch 35:	0.03044170  	0.08279520  	0.06784667  
2023-05-30 09:25:58.684: Find a better model.
2023-05-30 09:26:19.996: [iter 36 : loss : 0.5998 = 0.1330 + 0.4619 + 0.0049, time: 21.308060]
2023-05-30 09:26:20.276: epoch 36:	0.03047131  	0.08281334  	0.06791890  
2023-05-30 09:26:20.276: Find a better model.
2023-05-30 09:26:41.778: [iter 37 : loss : 0.5938 = 0.1273 + 0.4614 + 0.0051, time: 21.497143]
2023-05-30 09:26:42.045: epoch 37:	0.03045650  	0.08269417  	0.06793500  
2023-05-30 09:27:03.862: [iter 38 : loss : 0.5879 = 0.1218 + 0.4608 + 0.0053, time: 21.812891]
2023-05-30 09:27:04.130: epoch 38:	0.03057495  	0.08287087  	0.06805944  
2023-05-30 09:27:04.130: Find a better model.
2023-05-30 09:27:25.313: [iter 39 : loss : 0.5830 = 0.1172 + 0.4604 + 0.0055, time: 21.177993]
2023-05-30 09:27:25.577: epoch 39:	0.03054533  	0.08298840  	0.06807926  
2023-05-30 09:27:25.577: Find a better model.
2023-05-30 09:27:46.963: [iter 40 : loss : 0.5779 = 0.1125 + 0.4598 + 0.0056, time: 21.382203]
2023-05-30 09:27:47.225: epoch 40:	0.03055274  	0.08259941  	0.06795365  
2023-05-30 09:28:08.186: [iter 41 : loss : 0.5729 = 0.1079 + 0.4593 + 0.0058, time: 20.956323]
2023-05-30 09:28:08.452: epoch 41:	0.03045650  	0.08220492  	0.06778265  
2023-05-30 09:28:29.553: [iter 42 : loss : 0.5687 = 0.1038 + 0.4590 + 0.0059, time: 21.097656]
2023-05-30 09:28:29.834: epoch 42:	0.03034545  	0.08154184  	0.06760263  
2023-05-30 09:28:50.921: [iter 43 : loss : 0.5648 = 0.1002 + 0.4585 + 0.0061, time: 21.083061]
2023-05-30 09:28:51.183: epoch 43:	0.03036026  	0.08173927  	0.06771395  
2023-05-30 09:29:12.113: [iter 44 : loss : 0.5621 = 0.0977 + 0.4581 + 0.0063, time: 20.927317]
2023-05-30 09:29:12.379: epoch 44:	0.03026403  	0.08141962  	0.06768635  
2023-05-30 09:29:33.180: [iter 45 : loss : 0.5583 = 0.0941 + 0.4578 + 0.0064, time: 20.797387]
2023-05-30 09:29:33.442: epoch 45:	0.03027883  	0.08159705  	0.06764006  
2023-05-30 09:29:54.552: [iter 46 : loss : 0.5554 = 0.0914 + 0.4575 + 0.0065, time: 21.106194]
2023-05-30 09:29:54.815: epoch 46:	0.03018258  	0.08128436  	0.06744972  
2023-05-30 09:30:15.726: [iter 47 : loss : 0.5522 = 0.0884 + 0.4571 + 0.0067, time: 20.907508]
2023-05-30 09:30:16.003: epoch 47:	0.03021959  	0.08146529  	0.06736501  
2023-05-30 09:30:36.924: [iter 48 : loss : 0.5496 = 0.0859 + 0.4569 + 0.0068, time: 20.916943]
2023-05-30 09:30:37.189: epoch 48:	0.03009375  	0.08142675  	0.06725356  
2023-05-30 09:30:58.321: [iter 49 : loss : 0.5471 = 0.0836 + 0.4565 + 0.0070, time: 21.127041]
2023-05-30 09:30:58.583: epoch 49:	0.03007894  	0.08100604  	0.06715915  
2023-05-30 09:31:19.517: [iter 50 : loss : 0.5449 = 0.0816 + 0.4563 + 0.0071, time: 20.931407]
2023-05-30 09:31:19.780: epoch 50:	0.02996790  	0.08058503  	0.06682819  
2023-05-30 09:31:40.690: [iter 51 : loss : 0.5420 = 0.0788 + 0.4560 + 0.0072, time: 20.905408]
2023-05-30 09:31:40.963: epoch 51:	0.02980503  	0.08017537  	0.06663949  
2023-05-30 09:32:02.064: [iter 52 : loss : 0.5397 = 0.0765 + 0.4558 + 0.0074, time: 21.096466]
2023-05-30 09:32:02.328: epoch 52:	0.02972359  	0.07995428  	0.06658687  
2023-05-30 09:32:23.256: [iter 53 : loss : 0.5387 = 0.0757 + 0.4555 + 0.0075, time: 20.925002]
2023-05-30 09:32:23.518: epoch 53:	0.02964955  	0.07984371  	0.06641087  
2023-05-30 09:32:44.648: [iter 54 : loss : 0.5367 = 0.0737 + 0.4553 + 0.0076, time: 21.125374]
2023-05-30 09:32:44.940: epoch 54:	0.02964215  	0.07981543  	0.06620803  
2023-05-30 09:33:06.026: [iter 55 : loss : 0.5349 = 0.0720 + 0.4551 + 0.0077, time: 21.082360]
2023-05-30 09:33:06.289: epoch 55:	0.02968657  	0.07960211  	0.06623063  
2023-05-30 09:33:27.239: [iter 56 : loss : 0.5328 = 0.0701 + 0.4549 + 0.0079, time: 20.947176]
2023-05-30 09:33:27.501: epoch 56:	0.02950149  	0.07940616  	0.06608487  
2023-05-30 09:33:48.433: [iter 57 : loss : 0.5309 = 0.0683 + 0.4547 + 0.0080, time: 20.928109]
2023-05-30 09:33:48.695: epoch 57:	0.02949409  	0.07945395  	0.06587925  
2023-05-30 09:34:09.812: [iter 58 : loss : 0.5296 = 0.0669 + 0.4546 + 0.0081, time: 21.112171]
2023-05-30 09:34:10.082: epoch 58:	0.02947928  	0.07945687  	0.06583461  
2023-05-30 09:34:31.019: [iter 59 : loss : 0.5283 = 0.0658 + 0.4544 + 0.0082, time: 20.933583]
2023-05-30 09:34:31.280: epoch 59:	0.02947188  	0.07929619  	0.06588542  
2023-05-30 09:34:52.418: [iter 60 : loss : 0.5267 = 0.0641 + 0.4543 + 0.0083, time: 21.134050]
2023-05-30 09:34:52.680: epoch 60:	0.02933122  	0.07891502  	0.06570277  
2023-05-30 09:35:13.821: [iter 61 : loss : 0.5255 = 0.0630 + 0.4540 + 0.0084, time: 21.136385]
2023-05-30 09:35:14.103: epoch 61:	0.02935342  	0.07883693  	0.06554963  
2023-05-30 09:35:35.024: [iter 62 : loss : 0.5246 = 0.0621 + 0.4539 + 0.0085, time: 20.918021]
2023-05-30 09:35:35.306: epoch 62:	0.02920536  	0.07829788  	0.06533652  
2023-05-30 09:35:56.382: [iter 63 : loss : 0.5232 = 0.0608 + 0.4538 + 0.0087, time: 21.072107]
2023-05-30 09:35:56.645: epoch 63:	0.02917574  	0.07788441  	0.06509475  
2023-05-30 09:36:17.790: [iter 64 : loss : 0.5221 = 0.0597 + 0.4536 + 0.0088, time: 21.142079]
2023-05-30 09:36:18.058: epoch 64:	0.02910911  	0.07761452  	0.06501514  
2023-05-30 09:36:18.058: Early stopping is trigger at epoch: 64
2023-05-30 09:36:18.058: best_result@epoch 39:

2023-05-30 09:36:18.058: 		0.0305      	0.0830      	0.0681      
2023-05-30 09:37:44.704: my pid: 5576
2023-05-30 09:37:44.704: model: model.general_recommender.SGL
2023-05-30 09:37:44.704: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 09:37:44.704: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 09:37:48.785: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 09:38:09.710: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.923798]
2023-05-30 09:38:09.972: epoch 1:	0.00143619  	0.00279789  	0.00238986  
2023-05-30 09:38:09.972: Find a better model.
2023-05-30 09:38:30.735: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.759860]
2023-05-30 09:38:31.019: epoch 2:	0.00163607  	0.00299639  	0.00261196  
2023-05-30 09:38:31.019: Find a better model.
2023-05-30 09:38:51.743: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 20.719910]
2023-05-30 09:38:52.021: epoch 3:	0.00233195  	0.00473967  	0.00384713  
2023-05-30 09:38:52.021: Find a better model.
2023-05-30 09:39:12.556: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 20.531346]
2023-05-30 09:39:12.838: epoch 4:	0.00266509  	0.00578939  	0.00482899  
2023-05-30 09:39:12.838: Find a better model.
2023-05-30 09:39:33.344: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.502392]
2023-05-30 09:39:33.624: epoch 5:	0.00312408  	0.00735851  	0.00571394  
2023-05-30 09:39:33.624: Find a better model.
2023-05-30 09:39:54.129: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.500990]
2023-05-30 09:39:54.408: epoch 6:	0.00362748  	0.00846281  	0.00667769  
2023-05-30 09:39:54.408: Find a better model.
2023-05-30 09:40:15.117: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 20.706104]
2023-05-30 09:40:15.398: epoch 7:	0.00430856  	0.01082550  	0.00835015  
2023-05-30 09:40:15.398: Find a better model.
2023-05-30 09:40:35.942: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 20.538744]
2023-05-30 09:40:36.228: epoch 8:	0.00510067  	0.01332103  	0.01061002  
2023-05-30 09:40:36.228: Find a better model.
2023-05-30 09:40:56.852: [iter 9 : loss : 1.1348 = 0.6914 + 0.4433 + 0.0000, time: 20.620986]
2023-05-30 09:40:57.146: epoch 9:	0.00615929  	0.01527517  	0.01266936  
2023-05-30 09:40:57.146: Find a better model.
2023-05-30 09:41:17.739: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 20.589751]
2023-05-30 09:41:18.022: epoch 10:	0.00656645  	0.01766069  	0.01459780  
2023-05-30 09:41:18.022: Find a better model.
2023-05-30 09:41:39.046: [iter 11 : loss : 1.1337 = 0.6893 + 0.4444 + 0.0000, time: 21.021055]
2023-05-30 09:41:39.321: epoch 11:	0.00794340  	0.02143277  	0.01756434  
2023-05-30 09:41:39.321: Find a better model.
2023-05-30 09:42:00.271: [iter 12 : loss : 1.1322 = 0.6872 + 0.4450 + 0.0000, time: 20.946038]
2023-05-30 09:42:00.542: epoch 12:	0.00979415  	0.02589114  	0.02187687  
2023-05-30 09:42:00.542: Find a better model.
2023-05-30 09:42:21.659: [iter 13 : loss : 1.1299 = 0.6842 + 0.4457 + 0.0001, time: 21.113030]
2023-05-30 09:42:21.929: epoch 13:	0.01248888  	0.03301231  	0.02827781  
2023-05-30 09:42:21.930: Find a better model.
2023-05-30 09:42:42.846: [iter 14 : loss : 1.1256 = 0.6791 + 0.4464 + 0.0001, time: 20.913070]
2023-05-30 09:42:43.143: epoch 14:	0.01571669  	0.04222341  	0.03563575  
2023-05-30 09:42:43.143: Find a better model.
2023-05-30 09:43:04.021: [iter 15 : loss : 1.1178 = 0.6704 + 0.4473 + 0.0001, time: 20.874399]
2023-05-30 09:43:04.292: epoch 15:	0.01923319  	0.05058340  	0.04353353  
2023-05-30 09:43:04.293: Find a better model.
2023-05-30 09:43:25.248: [iter 16 : loss : 1.1041 = 0.6555 + 0.4484 + 0.0002, time: 20.951049]
2023-05-30 09:43:25.516: epoch 16:	0.02226112  	0.05868068  	0.05065655  
2023-05-30 09:43:25.516: Find a better model.
2023-05-30 09:43:46.420: [iter 17 : loss : 1.0816 = 0.6316 + 0.4497 + 0.0003, time: 20.900021]
2023-05-30 09:43:46.687: epoch 17:	0.02500771  	0.06594792  	0.05625511  
2023-05-30 09:43:46.687: Find a better model.
2023-05-30 09:44:07.795: [iter 18 : loss : 1.0481 = 0.5960 + 0.4517 + 0.0004, time: 21.104733]
2023-05-30 09:44:08.086: epoch 18:	0.02701401  	0.07090528  	0.06026289  
2023-05-30 09:44:08.086: Find a better model.
2023-05-30 09:44:28.996: [iter 19 : loss : 1.0055 = 0.5505 + 0.4543 + 0.0007, time: 20.907032]
2023-05-30 09:44:29.270: epoch 19:	0.02816892  	0.07468475  	0.06292476  
2023-05-30 09:44:29.270: Find a better model.
2023-05-30 09:44:50.216: [iter 20 : loss : 0.9565 = 0.4983 + 0.4574 + 0.0009, time: 20.942786]
2023-05-30 09:44:50.486: epoch 20:	0.02904250  	0.07737146  	0.06440412  
2023-05-30 09:44:50.486: Find a better model.
2023-05-30 09:45:11.346: [iter 21 : loss : 0.9070 = 0.4451 + 0.4607 + 0.0012, time: 20.855094]
2023-05-30 09:45:11.612: epoch 21:	0.02969400  	0.07965931  	0.06554002  
2023-05-30 09:45:11.612: Find a better model.
2023-05-30 09:45:32.551: [iter 22 : loss : 0.8597 = 0.3947 + 0.4635 + 0.0015, time: 20.935027]
2023-05-30 09:45:32.818: epoch 22:	0.02971620  	0.08025368  	0.06540408  
2023-05-30 09:45:32.818: Find a better model.
2023-05-30 09:45:53.745: [iter 23 : loss : 0.8185 = 0.3512 + 0.4655 + 0.0018, time: 20.923045]
2023-05-30 09:45:54.009: epoch 23:	0.02967918  	0.08093945  	0.06557886  
2023-05-30 09:45:54.009: Find a better model.
2023-05-30 09:46:14.933: [iter 24 : loss : 0.7832 = 0.3143 + 0.4668 + 0.0021, time: 20.920606]
2023-05-30 09:46:15.205: epoch 24:	0.02970880  	0.08071246  	0.06580397  
2023-05-30 09:46:36.101: [iter 25 : loss : 0.7530 = 0.2832 + 0.4674 + 0.0024, time: 20.892527]
2023-05-30 09:46:36.364: epoch 25:	0.02987167  	0.08137021  	0.06613321  
2023-05-30 09:46:36.364: Find a better model.
2023-05-30 09:46:57.312: [iter 26 : loss : 0.7280 = 0.2577 + 0.4676 + 0.0027, time: 20.944067]
2023-05-30 09:46:57.575: epoch 26:	0.03011598  	0.08198804  	0.06664753  
2023-05-30 09:46:57.575: Find a better model.
2023-05-30 09:47:18.714: [iter 27 : loss : 0.7061 = 0.2358 + 0.4673 + 0.0029, time: 21.134262]
2023-05-30 09:47:18.974: epoch 27:	0.03004194  	0.08208177  	0.06681848  
2023-05-30 09:47:18.975: Find a better model.
2023-05-30 09:47:40.008: [iter 28 : loss : 0.6877 = 0.2175 + 0.4670 + 0.0032, time: 21.030067]
2023-05-30 09:47:40.291: epoch 28:	0.03019001  	0.08236125  	0.06700685  
2023-05-30 09:47:40.291: Find a better model.
2023-05-30 09:48:01.327: [iter 29 : loss : 0.6723 = 0.2026 + 0.4664 + 0.0034, time: 21.033091]
2023-05-30 09:48:01.590: epoch 29:	0.03019001  	0.08211362  	0.06703723  
2023-05-30 09:48:22.534: [iter 30 : loss : 0.6579 = 0.1884 + 0.4658 + 0.0036, time: 20.940099]
2023-05-30 09:48:22.813: epoch 30:	0.03029365  	0.08204505  	0.06725628  
2023-05-30 09:48:43.703: [iter 31 : loss : 0.6454 = 0.1764 + 0.4652 + 0.0039, time: 20.885818]
2023-05-30 09:48:43.964: epoch 31:	0.03013079  	0.08128243  	0.06711975  
2023-05-30 09:49:04.913: [iter 32 : loss : 0.6346 = 0.1661 + 0.4644 + 0.0041, time: 20.945017]
2023-05-30 09:49:05.183: epoch 32:	0.03021961  	0.08131531  	0.06701668  
2023-05-30 09:49:26.256: [iter 33 : loss : 0.6256 = 0.1575 + 0.4639 + 0.0043, time: 21.068366]
2023-05-30 09:49:26.517: epoch 33:	0.03027143  	0.08169530  	0.06716684  
2023-05-30 09:49:47.681: [iter 34 : loss : 0.6167 = 0.1489 + 0.4632 + 0.0045, time: 21.161781]
2023-05-30 09:49:47.945: epoch 34:	0.03039727  	0.08221702  	0.06742050  
2023-05-30 09:50:08.886: [iter 35 : loss : 0.6083 = 0.1411 + 0.4625 + 0.0047, time: 20.938046]
2023-05-30 09:50:09.160: epoch 35:	0.03033805  	0.08161132  	0.06727410  
2023-05-30 09:50:30.048: [iter 36 : loss : 0.6016 = 0.1347 + 0.4620 + 0.0049, time: 20.883518]
2023-05-30 09:50:30.327: epoch 36:	0.03048612  	0.08213746  	0.06754611  
2023-05-30 09:50:51.432: [iter 37 : loss : 0.5953 = 0.1288 + 0.4614 + 0.0051, time: 21.101047]
2023-05-30 09:50:51.692: epoch 37:	0.03047132  	0.08211746  	0.06753267  
2023-05-30 09:51:12.624: [iter 38 : loss : 0.5893 = 0.1231 + 0.4610 + 0.0052, time: 20.928055]
2023-05-30 09:51:12.885: epoch 38:	0.03047872  	0.08217905  	0.06743170  
2023-05-30 09:51:33.834: [iter 39 : loss : 0.5842 = 0.1184 + 0.4604 + 0.0054, time: 20.945068]
2023-05-30 09:51:34.109: epoch 39:	0.03056756  	0.08207993  	0.06753327  
2023-05-30 09:51:55.091: [iter 40 : loss : 0.5789 = 0.1134 + 0.4599 + 0.0056, time: 20.978101]
2023-05-30 09:51:55.351: epoch 40:	0.03051573  	0.08163163  	0.06737655  
2023-05-30 09:52:16.095: [iter 41 : loss : 0.5740 = 0.1087 + 0.4595 + 0.0057, time: 20.739604]
2023-05-30 09:52:16.355: epoch 41:	0.03050833  	0.08135756  	0.06749806  
2023-05-30 09:52:37.411: [iter 42 : loss : 0.5698 = 0.1048 + 0.4591 + 0.0059, time: 21.052039]
2023-05-30 09:52:37.671: epoch 42:	0.03051574  	0.08156749  	0.06755637  
2023-05-30 09:52:58.831: [iter 43 : loss : 0.5660 = 0.1012 + 0.4587 + 0.0061, time: 21.157481]
2023-05-30 09:52:59.106: epoch 43:	0.03053055  	0.08181962  	0.06761358  
2023-05-30 09:53:19.853: [iter 44 : loss : 0.5631 = 0.0985 + 0.4583 + 0.0062, time: 20.743308]
2023-05-30 09:53:20.126: epoch 44:	0.03059718  	0.08198888  	0.06777374  
2023-05-30 09:53:41.176: [iter 45 : loss : 0.5592 = 0.0948 + 0.4580 + 0.0064, time: 21.045032]
2023-05-30 09:53:41.437: epoch 45:	0.03047132  	0.08163038  	0.06745321  
2023-05-30 09:54:02.582: [iter 46 : loss : 0.5564 = 0.0923 + 0.4576 + 0.0065, time: 21.142475]
2023-05-30 09:54:02.864: epoch 46:	0.03044911  	0.08107305  	0.06724082  
2023-05-30 09:54:23.750: [iter 47 : loss : 0.5526 = 0.0887 + 0.4573 + 0.0067, time: 20.882089]
2023-05-30 09:54:24.010: epoch 47:	0.03047132  	0.08124132  	0.06719187  
2023-05-30 09:54:44.960: [iter 48 : loss : 0.5503 = 0.0866 + 0.4569 + 0.0068, time: 20.945258]
2023-05-30 09:54:45.225: epoch 48:	0.03030104  	0.08080685  	0.06689484  
2023-05-30 09:55:06.162: [iter 49 : loss : 0.5478 = 0.0841 + 0.4567 + 0.0070, time: 20.933499]
2023-05-30 09:55:06.423: epoch 49:	0.03036027  	0.08069289  	0.06688614  
2023-05-30 09:55:27.147: [iter 50 : loss : 0.5457 = 0.0823 + 0.4564 + 0.0071, time: 20.719031]
2023-05-30 09:55:27.423: epoch 50:	0.03019740  	0.08015290  	0.06664813  
2023-05-30 09:55:48.121: [iter 51 : loss : 0.5429 = 0.0796 + 0.4561 + 0.0072, time: 20.693393]
2023-05-30 09:55:48.382: epoch 51:	0.03024923  	0.08023343  	0.06661434  
2023-05-30 09:56:09.177: [iter 52 : loss : 0.5402 = 0.0770 + 0.4559 + 0.0073, time: 20.790246]
2023-05-30 09:56:09.454: epoch 52:	0.03021962  	0.08003160  	0.06661715  
2023-05-30 09:56:30.161: [iter 53 : loss : 0.5391 = 0.0760 + 0.4557 + 0.0075, time: 20.702066]
2023-05-30 09:56:30.423: epoch 53:	0.03021961  	0.08030614  	0.06670306  
2023-05-30 09:56:30.423: Early stopping is trigger at epoch: 53
2023-05-30 09:56:30.423: best_result@epoch 28:

2023-05-30 09:56:30.423: 		0.0302      	0.0824      	0.0670      
2023-05-30 09:57:42.043: my pid: 11804
2023-05-30 09:57:42.043: model: model.general_recommender.SGL
2023-05-30 09:57:42.043: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 09:57:42.043: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 09:57:46.125: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 09:58:06.949: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.824329]
2023-05-30 09:58:07.220: epoch 1:	0.00152502  	0.00288674  	0.00249656  
2023-05-30 09:58:07.221: Find a better model.
2023-05-30 09:58:27.965: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.739528]
2023-05-30 09:58:28.254: epoch 2:	0.00173971  	0.00366615  	0.00283590  
2023-05-30 09:58:28.255: Find a better model.
2023-05-30 09:58:49.090: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 20.832051]
2023-05-30 09:58:49.376: epoch 3:	0.00206544  	0.00432867  	0.00328964  
2023-05-30 09:58:49.376: Find a better model.
2023-05-30 09:59:10.151: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.771038]
2023-05-30 09:59:10.439: epoch 4:	0.00250962  	0.00550617  	0.00454090  
2023-05-30 09:59:10.439: Find a better model.
2023-05-30 09:59:31.111: [iter 5 : loss : 1.1346 = 0.6927 + 0.4419 + 0.0000, time: 20.667638]
2023-05-30 09:59:31.398: epoch 5:	0.00295380  	0.00700134  	0.00564569  
2023-05-30 09:59:31.398: Find a better model.
2023-05-30 09:59:52.081: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.680070]
2023-05-30 09:59:52.367: epoch 6:	0.00356085  	0.00800331  	0.00659939  
2023-05-30 09:59:52.367: Find a better model.
2023-05-30 10:00:13.085: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 20.714015]
2023-05-30 10:00:13.364: epoch 7:	0.00433076  	0.01014128  	0.00838940  
2023-05-30 10:00:13.365: Find a better model.
2023-05-30 10:00:34.067: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 20.699104]
2023-05-30 10:00:34.348: epoch 8:	0.00517470  	0.01309659  	0.01049579  
2023-05-30 10:00:34.349: Find a better model.
2023-05-30 10:00:55.047: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 20.695364]
2023-05-30 10:00:55.328: epoch 9:	0.00575953  	0.01504536  	0.01227965  
2023-05-30 10:00:55.329: Find a better model.
2023-05-30 10:01:16.060: [iter 10 : loss : 1.1344 = 0.6905 + 0.4438 + 0.0000, time: 20.727772]
2023-05-30 10:01:16.346: epoch 10:	0.00660347  	0.01779297  	0.01441823  
2023-05-30 10:01:16.347: Find a better model.
2023-05-30 10:01:37.651: [iter 11 : loss : 1.1336 = 0.6892 + 0.4444 + 0.0000, time: 21.299739]
2023-05-30 10:01:37.928: epoch 11:	0.00809146  	0.02141933  	0.01797302  
2023-05-30 10:01:37.929: Find a better model.
2023-05-30 10:01:59.226: [iter 12 : loss : 1.1322 = 0.6871 + 0.4450 + 0.0000, time: 21.292206]
2023-05-30 10:01:59.501: epoch 12:	0.01004586  	0.02644558  	0.02275927  
2023-05-30 10:01:59.501: Find a better model.
2023-05-30 10:02:20.824: [iter 13 : loss : 1.1298 = 0.6841 + 0.4457 + 0.0001, time: 21.319039]
2023-05-30 10:02:21.114: epoch 13:	0.01265175  	0.03466173  	0.02943822  
2023-05-30 10:02:21.114: Find a better model.
2023-05-30 10:02:42.279: [iter 14 : loss : 1.1255 = 0.6790 + 0.4464 + 0.0001, time: 21.161461]
2023-05-30 10:02:42.567: epoch 14:	0.01566485  	0.04250325  	0.03687845  
2023-05-30 10:02:42.567: Find a better model.
2023-05-30 10:03:03.806: [iter 15 : loss : 1.1177 = 0.6702 + 0.4474 + 0.0001, time: 21.235377]
2023-05-30 10:03:04.079: epoch 15:	0.01900368  	0.05052520  	0.04404338  
2023-05-30 10:03:04.079: Find a better model.
2023-05-30 10:03:25.187: [iter 16 : loss : 1.1037 = 0.6551 + 0.4484 + 0.0002, time: 21.099057]
2023-05-30 10:03:25.459: epoch 16:	0.02203160  	0.05928954  	0.05088942  
2023-05-30 10:03:25.460: Find a better model.
2023-05-30 10:03:46.378: [iter 17 : loss : 1.0812 = 0.6310 + 0.4498 + 0.0003, time: 20.915405]
2023-05-30 10:03:46.648: epoch 17:	0.02445249  	0.06569379  	0.05588813  
2023-05-30 10:03:46.648: Find a better model.
2023-05-30 10:04:07.790: [iter 18 : loss : 1.0473 = 0.5951 + 0.4518 + 0.0004, time: 21.137113]
2023-05-30 10:04:08.059: epoch 18:	0.02676970  	0.07201917  	0.06025305  
2023-05-30 10:04:08.059: Find a better model.
2023-05-30 10:04:29.001: [iter 19 : loss : 1.0044 = 0.5492 + 0.4546 + 0.0007, time: 20.939005]
2023-05-30 10:04:29.282: epoch 19:	0.02805048  	0.07590090  	0.06351955  
2023-05-30 10:04:29.282: Find a better model.
2023-05-30 10:04:50.415: [iter 20 : loss : 0.9555 = 0.4967 + 0.4578 + 0.0009, time: 21.128351]
2023-05-30 10:04:50.706: epoch 20:	0.02879822  	0.07850733  	0.06519356  
2023-05-30 10:04:50.706: Find a better model.
2023-05-30 10:05:11.562: [iter 21 : loss : 0.9056 = 0.4432 + 0.4612 + 0.0012, time: 20.852290]
2023-05-30 10:05:11.835: epoch 21:	0.02933126  	0.08007914  	0.06597082  
2023-05-30 10:05:11.835: Find a better model.
2023-05-30 10:05:32.758: [iter 22 : loss : 0.8585 = 0.3931 + 0.4640 + 0.0015, time: 20.919057]
2023-05-30 10:05:33.024: epoch 22:	0.02952374  	0.08100585  	0.06632937  
2023-05-30 10:05:33.024: Find a better model.
2023-05-30 10:05:53.957: [iter 23 : loss : 0.8174 = 0.3497 + 0.4659 + 0.0018, time: 20.929351]
2023-05-30 10:05:54.232: epoch 23:	0.02973103  	0.08142219  	0.06658577  
2023-05-30 10:05:54.232: Find a better model.
2023-05-30 10:06:15.339: [iter 24 : loss : 0.7826 = 0.3132 + 0.4673 + 0.0021, time: 21.102989]
2023-05-30 10:06:15.603: epoch 24:	0.02981987  	0.08252467  	0.06688479  
2023-05-30 10:06:15.603: Find a better model.
2023-05-30 10:06:36.724: [iter 25 : loss : 0.7525 = 0.2823 + 0.4678 + 0.0024, time: 21.117064]
2023-05-30 10:06:36.986: epoch 25:	0.02984948  	0.08288917  	0.06712542  
2023-05-30 10:06:36.986: Find a better model.
2023-05-30 10:06:57.951: [iter 26 : loss : 0.7276 = 0.2570 + 0.4679 + 0.0027, time: 20.961010]
2023-05-30 10:06:58.223: epoch 26:	0.03007156  	0.08359074  	0.06732026  
2023-05-30 10:06:58.223: Find a better model.
2023-05-30 10:07:19.294: [iter 27 : loss : 0.7060 = 0.2355 + 0.4676 + 0.0029, time: 21.067643]
2023-05-30 10:07:19.555: epoch 27:	0.03015300  	0.08411120  	0.06755522  
2023-05-30 10:07:19.556: Find a better model.
2023-05-30 10:07:40.701: [iter 28 : loss : 0.6881 = 0.2177 + 0.4672 + 0.0032, time: 21.142207]
2023-05-30 10:07:40.962: epoch 28:	0.03028627  	0.08444532  	0.06778744  
2023-05-30 10:07:40.962: Find a better model.
2023-05-30 10:08:02.124: [iter 29 : loss : 0.6719 = 0.2020 + 0.4665 + 0.0034, time: 21.157220]
2023-05-30 10:08:02.388: epoch 29:	0.03021225  	0.08395956  	0.06783397  
2023-05-30 10:08:23.493: [iter 30 : loss : 0.6578 = 0.1883 + 0.4659 + 0.0036, time: 21.102044]
2023-05-30 10:08:23.756: epoch 30:	0.03031589  	0.08437490  	0.06811873  
2023-05-30 10:08:44.914: [iter 31 : loss : 0.6454 = 0.1763 + 0.4653 + 0.0039, time: 21.154045]
2023-05-30 10:08:45.185: epoch 31:	0.03029368  	0.08419721  	0.06829993  
2023-05-30 10:09:06.278: [iter 32 : loss : 0.6347 = 0.1661 + 0.4645 + 0.0041, time: 21.089066]
2023-05-30 10:09:06.542: epoch 32:	0.03034550  	0.08440252  	0.06833761  
2023-05-30 10:09:27.473: [iter 33 : loss : 0.6256 = 0.1575 + 0.4639 + 0.0043, time: 20.926991]
2023-05-30 10:09:27.737: epoch 33:	0.03041213  	0.08433522  	0.06834368  
2023-05-30 10:09:49.061: [iter 34 : loss : 0.6169 = 0.1492 + 0.4632 + 0.0045, time: 21.319346]
2023-05-30 10:09:49.323: epoch 34:	0.03048616  	0.08428254  	0.06831005  
2023-05-30 10:10:10.645: [iter 35 : loss : 0.6083 = 0.1411 + 0.4625 + 0.0047, time: 21.316472]
2023-05-30 10:10:10.908: epoch 35:	0.03050097  	0.08421726  	0.06839032  
2023-05-30 10:10:32.037: [iter 36 : loss : 0.6015 = 0.1347 + 0.4620 + 0.0049, time: 21.126009]
2023-05-30 10:10:32.302: epoch 36:	0.03061942  	0.08471069  	0.06879371  
2023-05-30 10:10:32.303: Find a better model.
2023-05-30 10:10:53.618: [iter 37 : loss : 0.5956 = 0.1291 + 0.4614 + 0.0051, time: 21.311066]
2023-05-30 10:10:53.878: epoch 37:	0.03059720  	0.08447859  	0.06864913  
2023-05-30 10:11:15.106: [iter 38 : loss : 0.5895 = 0.1233 + 0.4609 + 0.0052, time: 21.224033]
2023-05-30 10:11:15.384: epoch 38:	0.03063422  	0.08458614  	0.06861342  
2023-05-30 10:11:36.610: [iter 39 : loss : 0.5845 = 0.1187 + 0.4604 + 0.0054, time: 21.223013]
2023-05-30 10:11:36.869: epoch 39:	0.03061941  	0.08439743  	0.06855823  
2023-05-30 10:11:58.219: [iter 40 : loss : 0.5792 = 0.1137 + 0.4599 + 0.0056, time: 21.346059]
2023-05-30 10:11:58.476: epoch 40:	0.03047876  	0.08395130  	0.06848544  
2023-05-30 10:12:19.622: [iter 41 : loss : 0.5740 = 0.1088 + 0.4594 + 0.0057, time: 21.140999]
2023-05-30 10:12:19.881: epoch 41:	0.03057500  	0.08425933  	0.06863886  
2023-05-30 10:12:41.014: [iter 42 : loss : 0.5699 = 0.1050 + 0.4590 + 0.0059, time: 21.128039]
2023-05-30 10:12:41.285: epoch 42:	0.03057500  	0.08400731  	0.06858317  
2023-05-30 10:13:02.739: [iter 43 : loss : 0.5659 = 0.1012 + 0.4587 + 0.0061, time: 21.450009]
2023-05-30 10:13:03.004: epoch 43:	0.03048616  	0.08371537  	0.06857716  
2023-05-30 10:13:24.219: [iter 44 : loss : 0.5632 = 0.0988 + 0.4581 + 0.0062, time: 21.211144]
2023-05-30 10:13:24.489: epoch 44:	0.03052317  	0.08374678  	0.06862794  
2023-05-30 10:13:45.594: [iter 45 : loss : 0.5593 = 0.0951 + 0.4579 + 0.0064, time: 21.101051]
2023-05-30 10:13:45.855: epoch 45:	0.03041213  	0.08318365  	0.06849930  
2023-05-30 10:14:07.888: [iter 46 : loss : 0.5562 = 0.0921 + 0.4576 + 0.0065, time: 22.028218]
2023-05-30 10:14:08.180: epoch 46:	0.03044914  	0.08322016  	0.06836146  
2023-05-30 10:14:17.274: my pid: 10904
2023-05-30 10:14:17.275: model: model.general_recommender.SGL
2023-05-30 10:14:17.275: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 10:14:17.275: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 10:14:21.418: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 10:14:42.702: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.284099]
2023-05-30 10:14:42.999: epoch 1:	0.00133254  	0.00242429  	0.00229936  
2023-05-30 10:14:42.999: Find a better model.
2023-05-30 10:15:04.504: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.501641]
2023-05-30 10:15:04.803: epoch 2:	0.00159165  	0.00301562  	0.00245519  
2023-05-30 10:15:04.804: Find a better model.
2023-05-30 10:15:26.031: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 21.223104]
2023-05-30 10:15:26.318: epoch 3:	0.00212467  	0.00433169  	0.00345513  
2023-05-30 10:15:26.318: Find a better model.
2023-05-30 10:15:47.373: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.052149]
2023-05-30 10:15:47.661: epoch 4:	0.00226533  	0.00527602  	0.00419133  
2023-05-30 10:15:47.661: Find a better model.
2023-05-30 10:16:08.795: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.131071]
2023-05-30 10:16:09.089: epoch 5:	0.00313888  	0.00706341  	0.00561087  
2023-05-30 10:16:09.090: Find a better model.
2023-05-30 10:16:30.173: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.075594]
2023-05-30 10:16:30.461: epoch 6:	0.00373853  	0.00858318  	0.00683319  
2023-05-30 10:16:30.462: Find a better model.
2023-05-30 10:16:51.554: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 21.088106]
2023-05-30 10:16:51.857: epoch 7:	0.00430115  	0.00994332  	0.00789069  
2023-05-30 10:16:51.858: Find a better model.
2023-05-30 10:17:12.947: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 21.084584]
2023-05-30 10:17:13.239: epoch 8:	0.00493781  	0.01233227  	0.00956739  
2023-05-30 10:17:13.239: Find a better model.
2023-05-30 10:17:34.135: [iter 9 : loss : 1.1347 = 0.6915 + 0.4433 + 0.0000, time: 20.893011]
2023-05-30 10:17:34.414: epoch 9:	0.00572992  	0.01495090  	0.01179039  
2023-05-30 10:17:34.414: Find a better model.
2023-05-30 10:17:55.495: [iter 10 : loss : 1.1345 = 0.6907 + 0.4438 + 0.0000, time: 21.077231]
2023-05-30 10:17:55.774: epoch 10:	0.00665529  	0.01726708  	0.01411907  
2023-05-30 10:17:55.774: Find a better model.
2023-05-30 10:18:17.504: [iter 11 : loss : 1.1338 = 0.6894 + 0.4444 + 0.0000, time: 21.726336]
2023-05-30 10:18:17.777: epoch 11:	0.00783236  	0.02001826  	0.01752204  
2023-05-30 10:18:17.777: Find a better model.
2023-05-30 10:18:39.304: [iter 12 : loss : 1.1323 = 0.6873 + 0.4449 + 0.0000, time: 21.521045]
2023-05-30 10:18:39.603: epoch 12:	0.00966830  	0.02510972  	0.02154080  
2023-05-30 10:18:39.603: Find a better model.
2023-05-30 10:19:01.127: [iter 13 : loss : 1.1301 = 0.6844 + 0.4456 + 0.0001, time: 21.520069]
2023-05-30 10:19:01.400: epoch 13:	0.01218535  	0.03184890  	0.02740814  
2023-05-30 10:19:01.400: Find a better model.
2023-05-30 10:19:22.883: [iter 14 : loss : 1.1259 = 0.6795 + 0.4463 + 0.0001, time: 21.480229]
2023-05-30 10:19:23.166: epoch 14:	0.01514663  	0.03991314  	0.03430554  
2023-05-30 10:19:23.166: Find a better model.
2023-05-30 10:19:44.672: [iter 15 : loss : 1.1184 = 0.6711 + 0.4472 + 0.0001, time: 21.501052]
2023-05-30 10:19:44.942: epoch 15:	0.01858169  	0.04872575  	0.04184959  
2023-05-30 10:19:44.942: Find a better model.
2023-05-30 10:20:06.452: [iter 16 : loss : 1.1050 = 0.6565 + 0.4482 + 0.0002, time: 21.506067]
2023-05-30 10:20:06.724: epoch 16:	0.02157261  	0.05562798  	0.04863644  
2023-05-30 10:20:06.724: Find a better model.
2023-05-30 10:20:28.069: [iter 17 : loss : 1.0832 = 0.6333 + 0.4497 + 0.0003, time: 21.342031]
2023-05-30 10:20:28.342: epoch 17:	0.02428960  	0.06345171  	0.05526109  
2023-05-30 10:20:28.342: Find a better model.
2023-05-30 10:20:49.837: [iter 18 : loss : 1.0504 = 0.5984 + 0.4516 + 0.0004, time: 21.489557]
2023-05-30 10:20:50.128: epoch 18:	0.02617746  	0.06869370  	0.05953713  
2023-05-30 10:20:50.128: Find a better model.
2023-05-30 10:21:11.450: [iter 19 : loss : 1.0081 = 0.5534 + 0.4541 + 0.0006, time: 21.319025]
2023-05-30 10:21:11.720: epoch 19:	0.02756925  	0.07300542  	0.06245149  
2023-05-30 10:21:11.720: Find a better model.
2023-05-30 10:21:33.046: [iter 20 : loss : 0.9595 = 0.5014 + 0.4573 + 0.0009, time: 21.322120]
2023-05-30 10:21:33.318: epoch 20:	0.02835400  	0.07522873  	0.06430458  
2023-05-30 10:21:33.319: Find a better model.
2023-05-30 10:21:54.644: [iter 21 : loss : 0.9098 = 0.4482 + 0.4605 + 0.0012, time: 21.321391]
2023-05-30 10:21:54.913: epoch 21:	0.02896105  	0.07697280  	0.06537870  
2023-05-30 10:21:54.913: Find a better model.
2023-05-30 10:22:16.200: [iter 22 : loss : 0.8621 = 0.3973 + 0.4633 + 0.0015, time: 21.282117]
2023-05-30 10:22:16.467: epoch 22:	0.02919796  	0.07766502  	0.06538142  
2023-05-30 10:22:16.468: Find a better model.
2023-05-30 10:22:37.813: [iter 23 : loss : 0.8206 = 0.3533 + 0.4655 + 0.0018, time: 21.341522]
2023-05-30 10:22:38.087: epoch 23:	0.02930162  	0.07782688  	0.06541558  
2023-05-30 10:22:38.087: Find a better model.
2023-05-30 10:22:59.401: [iter 24 : loss : 0.7852 = 0.3163 + 0.4668 + 0.0021, time: 21.305068]
2023-05-30 10:22:59.669: epoch 24:	0.02935344  	0.07860504  	0.06551865  
2023-05-30 10:22:59.669: Find a better model.
2023-05-30 10:23:21.157: [iter 25 : loss : 0.7545 = 0.2846 + 0.4675 + 0.0024, time: 21.484703]
2023-05-30 10:23:21.423: epoch 25:	0.02949411  	0.07928423  	0.06602482  
2023-05-30 10:23:21.423: Find a better model.
2023-05-30 10:23:42.987: [iter 26 : loss : 0.7295 = 0.2592 + 0.4677 + 0.0026, time: 21.560734]
2023-05-30 10:23:43.259: epoch 26:	0.02959776  	0.07962874  	0.06622851  
2023-05-30 10:23:43.259: Find a better model.
2023-05-30 10:24:04.761: [iter 27 : loss : 0.7075 = 0.2372 + 0.4674 + 0.0029, time: 21.498082]
2023-05-30 10:24:05.025: epoch 27:	0.02971622  	0.08029776  	0.06660553  
2023-05-30 10:24:05.025: Find a better model.
2023-05-30 10:24:26.571: [iter 28 : loss : 0.6890 = 0.2188 + 0.4670 + 0.0032, time: 21.540420]
2023-05-30 10:24:26.850: epoch 28:	0.02982726  	0.08040804  	0.06680238  
2023-05-30 10:24:26.851: Find a better model.
2023-05-30 10:24:48.357: [iter 29 : loss : 0.6734 = 0.2036 + 0.4664 + 0.0034, time: 21.503144]
2023-05-30 10:24:48.626: epoch 29:	0.02990870  	0.08104666  	0.06712354  
2023-05-30 10:24:48.626: Find a better model.
2023-05-30 10:25:10.170: [iter 30 : loss : 0.6585 = 0.1892 + 0.4657 + 0.0036, time: 21.540041]
2023-05-30 10:25:10.434: epoch 30:	0.02987167  	0.08100766  	0.06716669  
2023-05-30 10:25:32.130: [iter 31 : loss : 0.6463 = 0.1772 + 0.4653 + 0.0039, time: 21.692088]
2023-05-30 10:25:32.393: epoch 31:	0.02999751  	0.08132748  	0.06740244  
2023-05-30 10:25:32.393: Find a better model.
2023-05-30 10:25:53.997: [iter 32 : loss : 0.6353 = 0.1667 + 0.4645 + 0.0041, time: 21.600153]
2023-05-30 10:25:54.268: epoch 32:	0.02999752  	0.08131266  	0.06737320  
2023-05-30 10:26:15.747: [iter 33 : loss : 0.6266 = 0.1585 + 0.4638 + 0.0043, time: 21.475341]
2023-05-30 10:26:16.013: epoch 33:	0.02993830  	0.08115869  	0.06730454  
2023-05-30 10:26:37.948: [iter 34 : loss : 0.6174 = 0.1496 + 0.4633 + 0.0045, time: 21.932043]
2023-05-30 10:26:38.219: epoch 34:	0.03000493  	0.08109504  	0.06734483  
2023-05-30 10:26:59.786: [iter 35 : loss : 0.6085 = 0.1412 + 0.4626 + 0.0047, time: 21.562457]
2023-05-30 10:27:00.074: epoch 35:	0.03003453  	0.08127522  	0.06740560  
2023-05-30 10:27:21.888: [iter 36 : loss : 0.6021 = 0.1353 + 0.4620 + 0.0049, time: 21.806227]
2023-05-30 10:27:22.167: epoch 36:	0.03004934  	0.08132728  	0.06735712  
2023-05-30 10:27:43.877: [iter 37 : loss : 0.5957 = 0.1292 + 0.4615 + 0.0051, time: 21.705078]
2023-05-30 10:27:44.153: epoch 37:	0.03014557  	0.08120096  	0.06746902  
2023-05-30 10:28:06.105: [iter 38 : loss : 0.5896 = 0.1235 + 0.4608 + 0.0052, time: 21.948026]
2023-05-30 10:28:06.369: epoch 38:	0.02998270  	0.08092346  	0.06739764  
2023-05-30 10:28:28.140: [iter 39 : loss : 0.5846 = 0.1187 + 0.4604 + 0.0054, time: 21.767246]
2023-05-30 10:28:28.401: epoch 39:	0.03001230  	0.08043649  	0.06714727  
2023-05-30 10:28:50.482: [iter 40 : loss : 0.5791 = 0.1136 + 0.4599 + 0.0056, time: 22.076019]
2023-05-30 10:28:50.746: epoch 40:	0.02996787  	0.08048632  	0.06716547  
2023-05-30 10:29:12.686: [iter 41 : loss : 0.5742 = 0.1090 + 0.4594 + 0.0057, time: 21.937395]
2023-05-30 10:29:12.949: epoch 41:	0.02990866  	0.08024074  	0.06706897  
2023-05-30 10:29:34.691: [iter 42 : loss : 0.5702 = 0.1053 + 0.4591 + 0.0059, time: 21.737305]
2023-05-30 10:29:34.955: epoch 42:	0.02986423  	0.08031641  	0.06717027  
2023-05-30 10:29:57.060: [iter 43 : loss : 0.5663 = 0.1015 + 0.4587 + 0.0061, time: 22.101850]
2023-05-30 10:29:57.324: epoch 43:	0.02993087  	0.08035017  	0.06726988  
2023-05-30 10:30:19.264: [iter 44 : loss : 0.5636 = 0.0991 + 0.4583 + 0.0062, time: 21.935978]
2023-05-30 10:30:19.530: epoch 44:	0.02976059  	0.07964319  	0.06693938  
2023-05-30 10:30:41.262: [iter 45 : loss : 0.5595 = 0.0953 + 0.4579 + 0.0064, time: 21.729002]
2023-05-30 10:30:41.524: epoch 45:	0.02976800  	0.07968154  	0.06706143  
2023-05-30 10:31:03.460: [iter 46 : loss : 0.5565 = 0.0924 + 0.4576 + 0.0065, time: 21.932069]
2023-05-30 10:31:03.720: epoch 46:	0.02967916  	0.07898278  	0.06690038  
2023-05-30 10:31:25.659: [iter 47 : loss : 0.5533 = 0.0895 + 0.4571 + 0.0067, time: 21.935102]
2023-05-30 10:31:25.919: epoch 47:	0.02967916  	0.07895491  	0.06685279  
2023-05-30 10:31:47.807: [iter 48 : loss : 0.5507 = 0.0869 + 0.4570 + 0.0068, time: 21.883990]
2023-05-30 10:31:48.071: epoch 48:	0.02970878  	0.07921103  	0.06688909  
2023-05-30 10:32:10.012: [iter 49 : loss : 0.5479 = 0.0843 + 0.4566 + 0.0069, time: 21.935285]
2023-05-30 10:32:10.283: epoch 49:	0.02975320  	0.07907173  	0.06693264  
2023-05-30 10:32:32.001: [iter 50 : loss : 0.5458 = 0.0824 + 0.4563 + 0.0071, time: 21.714245]
2023-05-30 10:32:32.268: epoch 50:	0.02962734  	0.07857658  	0.06666876  
2023-05-30 10:32:53.807: [iter 51 : loss : 0.5431 = 0.0799 + 0.4561 + 0.0072, time: 21.534068]
2023-05-30 10:32:54.071: epoch 51:	0.02963474  	0.07886043  	0.06657343  
2023-05-30 10:33:16.008: [iter 52 : loss : 0.5404 = 0.0773 + 0.4558 + 0.0073, time: 21.933639]
2023-05-30 10:33:16.277: epoch 52:	0.02961253  	0.07852316  	0.06655675  
2023-05-30 10:33:38.054: [iter 53 : loss : 0.5392 = 0.0761 + 0.4556 + 0.0075, time: 21.772234]
2023-05-30 10:33:38.316: epoch 53:	0.02959772  	0.07841082  	0.06639012  
2023-05-30 10:33:59.951: [iter 54 : loss : 0.5370 = 0.0741 + 0.4553 + 0.0076, time: 21.632113]
2023-05-30 10:34:00.225: epoch 54:	0.02944225  	0.07830658  	0.06624591  
2023-05-30 10:34:21.989: [iter 55 : loss : 0.5350 = 0.0721 + 0.4552 + 0.0077, time: 21.761004]
2023-05-30 10:34:22.257: epoch 55:	0.02939042  	0.07803030  	0.06615697  
2023-05-30 10:34:44.002: [iter 56 : loss : 0.5330 = 0.0702 + 0.4550 + 0.0078, time: 21.742415]
2023-05-30 10:34:44.277: epoch 56:	0.02922015  	0.07779566  	0.06582581  
2023-05-30 10:34:44.278: Early stopping is trigger at epoch: 56
2023-05-30 10:34:44.278: best_result@epoch 31:

2023-05-30 10:34:44.278: 		0.0300      	0.0813      	0.0674      
2023-05-30 11:00:08.369: my pid: 11388
2023-05-30 11:00:08.369: model: model.general_recommender.SGL
2023-05-30 11:00:08.369: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 11:00:08.369: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 11:00:12.543: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 11:00:33.678: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.135029]
2023-05-30 11:00:33.948: epoch 1:	0.00145099  	0.00305217  	0.00248638  
2023-05-30 11:00:33.948: Find a better model.
2023-05-30 11:00:55.053: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.101519]
2023-05-30 11:00:55.350: epoch 2:	0.00154723  	0.00343966  	0.00259538  
2023-05-30 11:00:55.350: Find a better model.
2023-05-30 11:01:16.687: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 21.332520]
2023-05-30 11:01:16.972: epoch 3:	0.00211727  	0.00448373  	0.00361433  
2023-05-30 11:01:16.972: Find a better model.
2023-05-30 11:01:38.033: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.057029]
2023-05-30 11:01:38.328: epoch 4:	0.00253183  	0.00514022  	0.00415657  
2023-05-30 11:01:38.329: Find a better model.
2023-05-30 11:01:59.615: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.282146]
2023-05-30 11:01:59.911: epoch 5:	0.00280575  	0.00613118  	0.00493031  
2023-05-30 11:01:59.912: Find a better model.
2023-05-30 11:02:21.066: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 21.150076]
2023-05-30 11:02:21.357: epoch 6:	0.00354605  	0.00821576  	0.00671523  
2023-05-30 11:02:21.358: Find a better model.
2023-05-30 11:02:42.420: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 21.059253]
2023-05-30 11:02:42.705: epoch 7:	0.00422712  	0.00953222  	0.00811874  
2023-05-30 11:02:42.705: Find a better model.
2023-05-30 11:03:03.602: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 20.894104]
2023-05-30 11:03:03.884: epoch 8:	0.00473793  	0.01126515  	0.00990216  
2023-05-30 11:03:03.884: Find a better model.
2023-05-30 11:03:24.839: [iter 9 : loss : 1.1346 = 0.6914 + 0.4432 + 0.0000, time: 20.951580]
2023-05-30 11:03:25.121: epoch 9:	0.00593721  	0.01560480  	0.01263500  
2023-05-30 11:03:25.121: Find a better model.
2023-05-30 11:03:45.950: [iter 10 : loss : 1.1344 = 0.6906 + 0.4437 + 0.0000, time: 20.825108]
2023-05-30 11:03:46.243: epoch 10:	0.00660347  	0.01653010  	0.01381708  
2023-05-30 11:03:46.243: Find a better model.
2023-05-30 11:04:07.752: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 21.505279]
2023-05-30 11:04:08.028: epoch 11:	0.00769170  	0.01973671  	0.01684522  
2023-05-30 11:04:08.028: Find a better model.
2023-05-30 11:04:29.620: [iter 12 : loss : 1.1322 = 0.6873 + 0.4449 + 0.0000, time: 21.587980]
2023-05-30 11:04:29.899: epoch 12:	0.00942399  	0.02539745  	0.02157186  
2023-05-30 11:04:29.899: Find a better model.
2023-05-30 11:04:51.390: [iter 13 : loss : 1.1299 = 0.6843 + 0.4455 + 0.0001, time: 21.486660]
2023-05-30 11:04:51.667: epoch 13:	0.01216314  	0.03279828  	0.02857495  
2023-05-30 11:04:51.667: Find a better model.
2023-05-30 11:05:13.113: [iter 14 : loss : 1.1257 = 0.6793 + 0.4463 + 0.0001, time: 21.440172]
2023-05-30 11:05:13.390: epoch 14:	0.01525766  	0.04191081  	0.03588969  
2023-05-30 11:05:13.390: Find a better model.
2023-05-30 11:05:34.957: [iter 15 : loss : 1.1179 = 0.6707 + 0.4472 + 0.0001, time: 21.562882]
2023-05-30 11:05:35.244: epoch 15:	0.01845583  	0.05020019  	0.04305759  
2023-05-30 11:05:35.244: Find a better model.
2023-05-30 11:05:56.696: [iter 16 : loss : 1.1042 = 0.6558 + 0.4483 + 0.0002, time: 21.448279]
2023-05-30 11:05:56.973: epoch 16:	0.02174288  	0.05805091  	0.04932577  
2023-05-30 11:05:56.973: Find a better model.
2023-05-30 11:06:18.334: [iter 17 : loss : 1.0820 = 0.6320 + 0.4497 + 0.0003, time: 21.356014]
2023-05-30 11:06:18.608: epoch 17:	0.02416374  	0.06432983  	0.05461850  
2023-05-30 11:06:18.608: Find a better model.
2023-05-30 11:06:39.948: [iter 18 : loss : 1.0486 = 0.5966 + 0.4516 + 0.0004, time: 21.337232]
2023-05-30 11:06:40.231: epoch 18:	0.02636251  	0.07037887  	0.05899161  
2023-05-30 11:06:40.231: Find a better model.
2023-05-30 11:07:01.311: [iter 19 : loss : 1.0059 = 0.5511 + 0.4541 + 0.0007, time: 21.076310]
2023-05-30 11:07:01.578: epoch 19:	0.02786538  	0.07477433  	0.06241347  
2023-05-30 11:07:01.578: Find a better model.
2023-05-30 11:07:22.876: [iter 20 : loss : 0.9573 = 0.4992 + 0.4573 + 0.0009, time: 21.292667]
2023-05-30 11:07:23.182: epoch 20:	0.02853907  	0.07737816  	0.06390592  
2023-05-30 11:07:23.182: Find a better model.
2023-05-30 11:07:44.281: [iter 21 : loss : 0.9075 = 0.4458 + 0.4605 + 0.0012, time: 21.092981]
2023-05-30 11:07:44.548: epoch 21:	0.02926458  	0.07952537  	0.06497233  
2023-05-30 11:07:44.548: Find a better model.
2023-05-30 11:08:05.869: [iter 22 : loss : 0.8600 = 0.3953 + 0.4632 + 0.0015, time: 21.316327]
2023-05-30 11:08:06.136: epoch 22:	0.02964217  	0.08094296  	0.06585860  
2023-05-30 11:08:06.136: Find a better model.
2023-05-30 11:08:27.629: [iter 23 : loss : 0.8188 = 0.3517 + 0.4654 + 0.0018, time: 21.489087]
2023-05-30 11:08:27.900: epoch 23:	0.02973101  	0.08145039  	0.06607974  
2023-05-30 11:08:27.900: Find a better model.
2023-05-30 11:08:49.124: [iter 24 : loss : 0.7834 = 0.3146 + 0.4667 + 0.0021, time: 21.219582]
2023-05-30 11:08:49.413: epoch 24:	0.02975322  	0.08158741  	0.06647728  
2023-05-30 11:08:49.413: Find a better model.
2023-05-30 11:09:10.835: [iter 25 : loss : 0.7530 = 0.2834 + 0.4673 + 0.0024, time: 21.417419]
2023-05-30 11:09:11.100: epoch 25:	0.02991610  	0.08233562  	0.06691892  
2023-05-30 11:09:11.100: Find a better model.
2023-05-30 11:09:32.622: [iter 26 : loss : 0.7280 = 0.2578 + 0.4676 + 0.0027, time: 21.518067]
2023-05-30 11:09:32.885: epoch 26:	0.03005677  	0.08264446  	0.06733292  
2023-05-30 11:09:32.885: Find a better model.
2023-05-30 11:09:54.260: [iter 27 : loss : 0.7064 = 0.2360 + 0.4675 + 0.0029, time: 21.371061]
2023-05-30 11:09:54.523: epoch 27:	0.03005677  	0.08300320  	0.06731350  
2023-05-30 11:09:54.523: Find a better model.
2023-05-30 11:10:16.007: [iter 28 : loss : 0.6881 = 0.2181 + 0.4668 + 0.0032, time: 21.479262]
2023-05-30 11:10:16.280: epoch 28:	0.03020483  	0.08380026  	0.06779974  
2023-05-30 11:10:16.280: Find a better model.
2023-05-30 11:10:37.782: [iter 29 : loss : 0.6724 = 0.2026 + 0.4664 + 0.0034, time: 21.499181]
2023-05-30 11:10:38.044: epoch 29:	0.03019742  	0.08317019  	0.06769685  
2023-05-30 11:10:59.417: [iter 30 : loss : 0.6578 = 0.1884 + 0.4658 + 0.0036, time: 21.370285]
2023-05-30 11:10:59.679: epoch 30:	0.03006415  	0.08300213  	0.06744291  
2023-05-30 11:11:21.007: [iter 31 : loss : 0.6457 = 0.1766 + 0.4652 + 0.0039, time: 21.323152]
2023-05-30 11:11:21.281: epoch 31:	0.03009376  	0.08273704  	0.06751623  
2023-05-30 11:11:42.763: [iter 32 : loss : 0.6347 = 0.1662 + 0.4644 + 0.0041, time: 21.479568]
2023-05-30 11:11:43.027: epoch 32:	0.03024183  	0.08353291  	0.06777635  
2023-05-30 11:12:04.249: [iter 33 : loss : 0.6254 = 0.1573 + 0.4639 + 0.0043, time: 21.217676]
2023-05-30 11:12:04.510: epoch 33:	0.03020482  	0.08322026  	0.06785636  
2023-05-30 11:12:25.830: [iter 34 : loss : 0.6169 = 0.1493 + 0.4632 + 0.0045, time: 21.315821]
2023-05-30 11:12:26.114: epoch 34:	0.03016779  	0.08350596  	0.06802962  
2023-05-30 11:12:47.540: [iter 35 : loss : 0.6082 = 0.1410 + 0.4625 + 0.0047, time: 21.422486]
2023-05-30 11:12:47.805: epoch 35:	0.03026405  	0.08384357  	0.06840298  
2023-05-30 11:12:47.805: Find a better model.
2023-05-30 11:13:09.198: [iter 36 : loss : 0.6017 = 0.1349 + 0.4619 + 0.0049, time: 21.389552]
2023-05-30 11:13:09.465: epoch 36:	0.03019742  	0.08345933  	0.06846202  
2023-05-30 11:13:30.974: [iter 37 : loss : 0.5954 = 0.1289 + 0.4614 + 0.0051, time: 21.505127]
2023-05-30 11:13:31.249: epoch 37:	0.03019742  	0.08314931  	0.06849029  
2023-05-30 11:13:52.580: [iter 38 : loss : 0.5892 = 0.1231 + 0.4608 + 0.0052, time: 21.327335]
2023-05-30 11:13:52.843: epoch 38:	0.03029366  	0.08325373  	0.06868935  
2023-05-30 11:14:14.203: [iter 39 : loss : 0.5842 = 0.1184 + 0.4603 + 0.0054, time: 21.355552]
2023-05-30 11:14:14.483: epoch 39:	0.03031587  	0.08310802  	0.06870017  
2023-05-30 11:14:35.789: [iter 40 : loss : 0.5792 = 0.1137 + 0.4599 + 0.0056, time: 21.301044]
2023-05-30 11:14:36.052: epoch 40:	0.03036029  	0.08312338  	0.06886128  
2023-05-30 11:14:57.333: [iter 41 : loss : 0.5743 = 0.1091 + 0.4594 + 0.0057, time: 21.278648]
2023-05-30 11:14:57.594: epoch 41:	0.03036028  	0.08326909  	0.06896398  
2023-05-30 11:15:18.935: [iter 42 : loss : 0.5700 = 0.1051 + 0.4590 + 0.0059, time: 21.337114]
2023-05-30 11:15:19.213: epoch 42:	0.03038989  	0.08326782  	0.06903204  
2023-05-30 11:15:40.705: [iter 43 : loss : 0.5657 = 0.1009 + 0.4587 + 0.0061, time: 21.487121]
2023-05-30 11:15:40.972: epoch 43:	0.03052314  	0.08354489  	0.06925052  
2023-05-30 11:16:02.319: [iter 44 : loss : 0.5631 = 0.0986 + 0.4583 + 0.0062, time: 21.343981]
2023-05-30 11:16:02.582: epoch 44:	0.03045651  	0.08328369  	0.06910504  
2023-05-30 11:16:23.731: [iter 45 : loss : 0.5592 = 0.0949 + 0.4579 + 0.0064, time: 21.145045]
2023-05-30 11:16:23.996: epoch 45:	0.03034546  	0.08263011  	0.06879758  
2023-05-30 11:16:45.107: [iter 46 : loss : 0.5566 = 0.0925 + 0.4576 + 0.0065, time: 21.106839]
2023-05-30 11:16:45.372: epoch 46:	0.03033065  	0.08247580  	0.06862523  
2023-05-30 11:17:06.503: [iter 47 : loss : 0.5529 = 0.0890 + 0.4572 + 0.0067, time: 21.127311]
2023-05-30 11:17:06.769: epoch 47:	0.03010117  	0.08201304  	0.06834839  
2023-05-30 11:17:28.093: [iter 48 : loss : 0.5504 = 0.0867 + 0.4569 + 0.0068, time: 21.320126]
2023-05-30 11:17:28.361: epoch 48:	0.03014558  	0.08186129  	0.06834386  
2023-05-30 11:17:49.653: [iter 49 : loss : 0.5479 = 0.0844 + 0.4566 + 0.0069, time: 21.286532]
2023-05-30 11:17:49.915: epoch 49:	0.03002713  	0.08149251  	0.06805625  
2023-05-30 11:18:11.072: [iter 50 : loss : 0.5456 = 0.0822 + 0.4563 + 0.0071, time: 21.153021]
2023-05-30 11:18:11.346: epoch 50:	0.03001232  	0.08122265  	0.06800180  
2023-05-30 11:18:32.487: [iter 51 : loss : 0.5428 = 0.0795 + 0.4561 + 0.0072, time: 21.136672]
2023-05-30 11:18:32.774: epoch 51:	0.02999011  	0.08103859  	0.06807116  
2023-05-30 11:18:54.066: [iter 52 : loss : 0.5405 = 0.0773 + 0.4558 + 0.0073, time: 21.288071]
2023-05-30 11:18:54.342: epoch 52:	0.03007155  	0.08135024  	0.06814467  
2023-05-30 11:19:15.628: [iter 53 : loss : 0.5398 = 0.0768 + 0.4556 + 0.0075, time: 21.283002]
2023-05-30 11:19:15.892: epoch 53:	0.03012337  	0.08097336  	0.06806095  
2023-05-30 11:19:37.097: [iter 54 : loss : 0.5370 = 0.0742 + 0.4553 + 0.0076, time: 21.201480]
2023-05-30 11:19:37.361: epoch 54:	0.03002712  	0.08050512  	0.06764292  
2023-05-30 11:19:58.615: [iter 55 : loss : 0.5352 = 0.0724 + 0.4551 + 0.0077, time: 21.248688]
2023-05-30 11:19:58.879: epoch 55:	0.02996049  	0.08042532  	0.06756150  
2023-05-30 11:20:20.308: [iter 56 : loss : 0.5331 = 0.0704 + 0.4549 + 0.0078, time: 21.424750]
2023-05-30 11:20:20.582: epoch 56:	0.02984203  	0.08000040  	0.06735555  
2023-05-30 11:20:41.851: [iter 57 : loss : 0.5314 = 0.0687 + 0.4547 + 0.0080, time: 21.264074]
2023-05-30 11:20:42.111: epoch 57:	0.02967916  	0.07955848  	0.06709383  
2023-05-30 11:21:03.218: [iter 58 : loss : 0.5299 = 0.0672 + 0.4546 + 0.0081, time: 21.102176]
2023-05-30 11:21:03.482: epoch 58:	0.02963476  	0.07948808  	0.06706079  
2023-05-30 11:21:24.578: [iter 59 : loss : 0.5286 = 0.0660 + 0.4544 + 0.0082, time: 21.092517]
2023-05-30 11:21:24.844: epoch 59:	0.02956072  	0.07914282  	0.06698418  
2023-05-30 11:21:46.198: [iter 60 : loss : 0.5273 = 0.0648 + 0.4541 + 0.0083, time: 21.350067]
2023-05-30 11:21:46.463: epoch 60:	0.02948670  	0.07880368  	0.06670593  
2023-05-30 11:21:46.464: Early stopping is trigger at epoch: 60
2023-05-30 11:21:46.464: best_result@epoch 35:

2023-05-30 11:21:46.464: 		0.0303      	0.0838      	0.0684      
2023-05-30 11:24:40.831: my pid: 13844
2023-05-30 11:24:40.831: model: model.general_recommender.SGL
2023-05-30 11:24:40.831: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 11:24:40.831: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 11:24:44.868: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 11:25:05.477: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.607417]
2023-05-30 11:25:05.739: epoch 1:	0.00118448  	0.00217694  	0.00181670  
2023-05-30 11:25:05.740: Find a better model.
2023-05-30 11:25:26.475: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.731020]
2023-05-30 11:25:26.745: epoch 2:	0.00186556  	0.00329996  	0.00273242  
2023-05-30 11:25:26.745: Find a better model.
2023-05-30 11:25:47.475: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.726072]
2023-05-30 11:25:47.761: epoch 3:	0.00208025  	0.00438991  	0.00338974  
2023-05-30 11:25:47.761: Find a better model.
2023-05-30 11:26:08.445: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.681102]
2023-05-30 11:26:08.733: epoch 4:	0.00233936  	0.00486474  	0.00395372  
2023-05-30 11:26:08.733: Find a better model.
2023-05-30 11:26:29.214: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.478381]
2023-05-30 11:26:29.497: epoch 5:	0.00284276  	0.00635210  	0.00501918  
2023-05-30 11:26:29.497: Find a better model.
2023-05-30 11:26:49.983: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.483047]
2023-05-30 11:26:50.264: epoch 6:	0.00338318  	0.00739066  	0.00625299  
2023-05-30 11:26:50.264: Find a better model.
2023-05-30 11:27:10.837: [iter 7 : loss : 1.1346 = 0.6923 + 0.4424 + 0.0000, time: 20.569114]
2023-05-30 11:27:11.116: epoch 7:	0.00397542  	0.00937399  	0.00799503  
2023-05-30 11:27:11.116: Find a better model.
2023-05-30 11:27:31.632: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.513086]
2023-05-30 11:27:31.903: epoch 8:	0.00498963  	0.01217237  	0.01022033  
2023-05-30 11:27:31.903: Find a better model.
2023-05-30 11:27:52.408: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.500733]
2023-05-30 11:27:52.682: epoch 9:	0.00593720  	0.01496360  	0.01228638  
2023-05-30 11:27:52.682: Find a better model.
2023-05-30 11:28:13.228: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.542251]
2023-05-30 11:28:13.501: epoch 10:	0.00755104  	0.01971685  	0.01591508  
2023-05-30 11:28:13.501: Find a better model.
2023-05-30 11:28:34.408: [iter 11 : loss : 1.1338 = 0.6897 + 0.4441 + 0.0000, time: 20.902315]
2023-05-30 11:28:34.681: epoch 11:	0.00891319  	0.02249682  	0.01892234  
2023-05-30 11:28:34.681: Find a better model.
2023-05-30 11:28:55.371: [iter 12 : loss : 1.1325 = 0.6878 + 0.4447 + 0.0000, time: 20.686473]
2023-05-30 11:28:55.646: epoch 12:	0.01032718  	0.02757012  	0.02324647  
2023-05-30 11:28:55.646: Find a better model.
2023-05-30 11:29:16.375: [iter 13 : loss : 1.1303 = 0.6848 + 0.4455 + 0.0001, time: 20.725076]
2023-05-30 11:29:16.648: epoch 13:	0.01257032  	0.03370536  	0.02893015  
2023-05-30 11:29:16.648: Find a better model.
2023-05-30 11:29:37.395: [iter 14 : loss : 1.1262 = 0.6799 + 0.4462 + 0.0001, time: 20.741570]
2023-05-30 11:29:37.666: epoch 14:	0.01559822  	0.04163234  	0.03626463  
2023-05-30 11:29:37.666: Find a better model.
2023-05-30 11:29:58.539: [iter 15 : loss : 1.1188 = 0.6716 + 0.4470 + 0.0001, time: 20.869177]
2023-05-30 11:29:58.810: epoch 15:	0.01895186  	0.05030667  	0.04339201  
2023-05-30 11:29:58.810: Find a better model.
2023-05-30 11:30:19.567: [iter 16 : loss : 1.1057 = 0.6573 + 0.4482 + 0.0002, time: 20.753471]
2023-05-30 11:30:19.833: epoch 16:	0.02182431  	0.05701141  	0.04976201  
2023-05-30 11:30:19.833: Find a better model.
2023-05-30 11:30:40.502: [iter 17 : loss : 1.0841 = 0.6343 + 0.4495 + 0.0003, time: 20.664033]
2023-05-30 11:30:40.767: epoch 17:	0.02440804  	0.06365603  	0.05518655  
2023-05-30 11:30:40.767: Find a better model.
2023-05-30 11:31:01.380: [iter 18 : loss : 1.0515 = 0.5996 + 0.4515 + 0.0004, time: 20.608242]
2023-05-30 11:31:01.649: epoch 18:	0.02634769  	0.06886414  	0.05910381  
2023-05-30 11:31:01.650: Find a better model.
2023-05-30 11:31:22.299: [iter 19 : loss : 1.0091 = 0.5546 + 0.4539 + 0.0006, time: 20.645077]
2023-05-30 11:31:22.572: epoch 19:	0.02799123  	0.07322901  	0.06222122  
2023-05-30 11:31:22.572: Find a better model.
2023-05-30 11:31:43.163: [iter 20 : loss : 0.9605 = 0.5025 + 0.4571 + 0.0009, time: 20.586072]
2023-05-30 11:31:43.424: epoch 20:	0.02875376  	0.07621743  	0.06384427  
2023-05-30 11:31:43.424: Find a better model.
2023-05-30 11:32:04.285: [iter 21 : loss : 0.9107 = 0.4490 + 0.4605 + 0.0012, time: 20.858096]
2023-05-30 11:32:04.559: epoch 21:	0.02925719  	0.07753700  	0.06460705  
2023-05-30 11:32:04.559: Find a better model.
2023-05-30 11:32:25.502: [iter 22 : loss : 0.8628 = 0.3981 + 0.4633 + 0.0015, time: 20.937687]
2023-05-30 11:32:25.763: epoch 22:	0.02941266  	0.07867808  	0.06504201  
2023-05-30 11:32:25.763: Find a better model.
2023-05-30 11:32:46.490: [iter 23 : loss : 0.8210 = 0.3538 + 0.4654 + 0.0018, time: 20.721999]
2023-05-30 11:32:46.751: epoch 23:	0.02964216  	0.08020777  	0.06581181  
2023-05-30 11:32:46.751: Find a better model.
2023-05-30 11:33:07.539: [iter 24 : loss : 0.7854 = 0.3165 + 0.4668 + 0.0021, time: 20.783695]
2023-05-30 11:33:07.800: epoch 24:	0.02950152  	0.08009923  	0.06560209  
2023-05-30 11:33:28.842: [iter 25 : loss : 0.7547 = 0.2849 + 0.4674 + 0.0024, time: 21.037638]
2023-05-30 11:33:29.101: epoch 25:	0.02956075  	0.08040723  	0.06582958  
2023-05-30 11:33:29.101: Find a better model.
2023-05-30 11:33:50.074: [iter 26 : loss : 0.7297 = 0.2594 + 0.4676 + 0.0026, time: 20.969359]
2023-05-30 11:33:50.332: epoch 26:	0.02953854  	0.08070347  	0.06612528  
2023-05-30 11:33:50.332: Find a better model.
2023-05-30 11:34:11.236: [iter 27 : loss : 0.7076 = 0.2373 + 0.4673 + 0.0029, time: 20.901128]
2023-05-30 11:34:11.494: epoch 27:	0.02976064  	0.08122475  	0.06656695  
2023-05-30 11:34:11.494: Find a better model.
2023-05-30 11:34:32.468: [iter 28 : loss : 0.6891 = 0.2190 + 0.4669 + 0.0032, time: 20.971041]
2023-05-30 11:34:32.728: epoch 28:	0.02965699  	0.08079128  	0.06631326  
2023-05-30 11:34:53.643: [iter 29 : loss : 0.6732 = 0.2035 + 0.4664 + 0.0034, time: 20.911033]
2023-05-30 11:34:53.900: epoch 29:	0.02950893  	0.08079501  	0.06645337  
2023-05-30 11:35:14.843: [iter 30 : loss : 0.6588 = 0.1894 + 0.4657 + 0.0036, time: 20.939017]
2023-05-30 11:35:15.102: epoch 30:	0.02970882  	0.08122042  	0.06666131  
2023-05-30 11:35:36.012: [iter 31 : loss : 0.6461 = 0.1772 + 0.4650 + 0.0039, time: 20.907089]
2023-05-30 11:35:36.270: epoch 31:	0.02973843  	0.08157641  	0.06673700  
2023-05-30 11:35:36.270: Find a better model.
2023-05-30 11:35:57.234: [iter 32 : loss : 0.6352 = 0.1667 + 0.4644 + 0.0041, time: 20.961037]
2023-05-30 11:35:57.491: epoch 32:	0.02976064  	0.08168489  	0.06687000  
2023-05-30 11:35:57.491: Find a better model.
2023-05-30 11:36:18.384: [iter 33 : loss : 0.6261 = 0.1582 + 0.4636 + 0.0043, time: 20.888617]
2023-05-30 11:36:18.645: epoch 33:	0.02986427  	0.08165145  	0.06699960  
2023-05-30 11:36:39.584: [iter 34 : loss : 0.6175 = 0.1499 + 0.4631 + 0.0045, time: 20.936133]
2023-05-30 11:36:39.842: epoch 34:	0.02987909  	0.08173111  	0.06703789  
2023-05-30 11:36:39.842: Find a better model.
2023-05-30 11:37:00.777: [iter 35 : loss : 0.6088 = 0.1417 + 0.4625 + 0.0047, time: 20.930069]
2023-05-30 11:37:01.037: epoch 35:	0.02991609  	0.08179538  	0.06698304  
2023-05-30 11:37:01.037: Find a better model.
2023-05-30 11:37:22.020: [iter 36 : loss : 0.6022 = 0.1355 + 0.4618 + 0.0049, time: 20.980058]
2023-05-30 11:37:22.276: epoch 36:	0.02993830  	0.08187557  	0.06714766  
2023-05-30 11:37:22.276: Find a better model.
2023-05-30 11:37:43.184: [iter 37 : loss : 0.5958 = 0.1295 + 0.4613 + 0.0050, time: 20.903908]
2023-05-30 11:37:43.442: epoch 37:	0.02999013  	0.08175421  	0.06719133  
2023-05-30 11:38:04.196: [iter 38 : loss : 0.5896 = 0.1236 + 0.4608 + 0.0052, time: 20.751032]
2023-05-30 11:38:04.455: epoch 38:	0.03004936  	0.08155663  	0.06721083  
2023-05-30 11:38:25.339: [iter 39 : loss : 0.5844 = 0.1187 + 0.4602 + 0.0054, time: 20.880113]
2023-05-30 11:38:25.604: epoch 39:	0.03011598  	0.08193470  	0.06732904  
2023-05-30 11:38:25.604: Find a better model.
2023-05-30 11:38:46.379: [iter 40 : loss : 0.5791 = 0.1137 + 0.4598 + 0.0056, time: 20.772162]
2023-05-30 11:38:46.640: epoch 40:	0.03001974  	0.08138176  	0.06723520  
2023-05-30 11:39:07.324: [iter 41 : loss : 0.5741 = 0.1091 + 0.4593 + 0.0057, time: 20.680046]
2023-05-30 11:39:07.590: epoch 41:	0.03006416  	0.08164207  	0.06725804  
2023-05-30 11:39:28.334: [iter 42 : loss : 0.5701 = 0.1054 + 0.4588 + 0.0059, time: 20.741148]
2023-05-30 11:39:28.599: epoch 42:	0.03013820  	0.08169616  	0.06736601  
2023-05-30 11:39:49.323: [iter 43 : loss : 0.5660 = 0.1015 + 0.4585 + 0.0061, time: 20.720495]
2023-05-30 11:39:49.591: epoch 43:	0.03010860  	0.08174746  	0.06731448  
2023-05-30 11:40:10.359: [iter 44 : loss : 0.5633 = 0.0990 + 0.4581 + 0.0062, time: 20.763072]
2023-05-30 11:40:10.621: epoch 44:	0.03004936  	0.08161980  	0.06725100  
2023-05-30 11:40:31.300: [iter 45 : loss : 0.5593 = 0.0953 + 0.4577 + 0.0064, time: 20.675165]
2023-05-30 11:40:31.568: epoch 45:	0.03007898  	0.08161542  	0.06724010  
2023-05-30 11:40:52.527: [iter 46 : loss : 0.5563 = 0.0924 + 0.4574 + 0.0065, time: 20.954276]
2023-05-30 11:40:52.784: epoch 46:	0.02997532  	0.08146870  	0.06717484  
2023-05-30 11:41:13.295: [iter 47 : loss : 0.5529 = 0.0892 + 0.4570 + 0.0067, time: 20.508009]
2023-05-30 11:41:13.564: epoch 47:	0.02987909  	0.08126207  	0.06715359  
2023-05-30 11:41:34.153: [iter 48 : loss : 0.5506 = 0.0871 + 0.4568 + 0.0068, time: 20.585113]
2023-05-30 11:41:34.410: epoch 48:	0.02992350  	0.08114046  	0.06720947  
2023-05-30 11:41:55.264: [iter 49 : loss : 0.5479 = 0.0845 + 0.4564 + 0.0069, time: 20.850028]
2023-05-30 11:41:55.529: epoch 49:	0.02998273  	0.08100877  	0.06726878  
2023-05-30 11:42:16.281: [iter 50 : loss : 0.5456 = 0.0824 + 0.4562 + 0.0071, time: 20.745091]
2023-05-30 11:42:16.551: epoch 50:	0.02985688  	0.08071131  	0.06701021  
2023-05-30 11:42:37.253: [iter 51 : loss : 0.5427 = 0.0796 + 0.4559 + 0.0072, time: 20.698019]
2023-05-30 11:42:37.510: epoch 51:	0.02984948  	0.08078422  	0.06703553  
2023-05-30 11:42:58.287: [iter 52 : loss : 0.5407 = 0.0776 + 0.4557 + 0.0073, time: 20.774198]
2023-05-30 11:42:58.558: epoch 52:	0.02988649  	0.08078218  	0.06702212  
2023-05-30 11:43:19.235: [iter 53 : loss : 0.5395 = 0.0766 + 0.4555 + 0.0075, time: 20.672033]
2023-05-30 11:43:19.491: epoch 53:	0.02984948  	0.08033694  	0.06693440  
2023-05-30 11:43:40.292: [iter 54 : loss : 0.5372 = 0.0744 + 0.4552 + 0.0076, time: 20.797031]
2023-05-30 11:43:40.561: epoch 54:	0.02976804  	0.08006343  	0.06661697  
2023-05-30 11:44:01.242: [iter 55 : loss : 0.5355 = 0.0728 + 0.4550 + 0.0077, time: 20.678158]
2023-05-30 11:44:01.498: epoch 55:	0.02969400  	0.07980733  	0.06657100  
2023-05-30 11:44:22.253: [iter 56 : loss : 0.5331 = 0.0706 + 0.4547 + 0.0078, time: 20.751990]
2023-05-30 11:44:22.509: epoch 56:	0.02956815  	0.07942489  	0.06633637  
2023-05-30 11:44:43.221: [iter 57 : loss : 0.5315 = 0.0689 + 0.4546 + 0.0079, time: 20.707569]
2023-05-30 11:44:43.479: epoch 57:	0.02961996  	0.07946445  	0.06627326  
2023-05-30 11:45:04.433: [iter 58 : loss : 0.5298 = 0.0673 + 0.4544 + 0.0081, time: 20.951044]
2023-05-30 11:45:04.697: epoch 58:	0.02953112  	0.07913611  	0.06610488  
2023-05-30 11:45:25.184: [iter 59 : loss : 0.5283 = 0.0658 + 0.4543 + 0.0082, time: 20.484201]
2023-05-30 11:45:25.442: epoch 59:	0.02941268  	0.07886057  	0.06595191  
2023-05-30 11:45:46.200: [iter 60 : loss : 0.5274 = 0.0650 + 0.4541 + 0.0083, time: 20.755169]
2023-05-30 11:45:46.458: epoch 60:	0.02935345  	0.07839665  	0.06580840  
2023-05-30 11:46:07.231: [iter 61 : loss : 0.5260 = 0.0636 + 0.4539 + 0.0084, time: 20.768589]
2023-05-30 11:46:07.490: epoch 61:	0.02932384  	0.07823322  	0.06566240  
2023-05-30 11:46:28.220: [iter 62 : loss : 0.5248 = 0.0625 + 0.4538 + 0.0085, time: 20.726137]
2023-05-30 11:46:28.479: epoch 62:	0.02924982  	0.07805127  	0.06557451  
2023-05-30 11:46:49.189: [iter 63 : loss : 0.5237 = 0.0614 + 0.4536 + 0.0086, time: 20.707010]
2023-05-30 11:46:49.449: epoch 63:	0.02917579  	0.07773060  	0.06554095  
2023-05-30 11:47:10.208: [iter 64 : loss : 0.5223 = 0.0600 + 0.4535 + 0.0087, time: 20.756530]
2023-05-30 11:47:10.465: epoch 64:	0.02904994  	0.07713327  	0.06525870  
2023-05-30 11:47:10.466: Early stopping is trigger at epoch: 64
2023-05-30 11:47:10.466: best_result@epoch 39:

2023-05-30 11:47:10.466: 		0.0301      	0.0819      	0.0673      
2023-05-30 14:35:16.752: my pid: 736
2023-05-30 14:35:16.752: model: model.general_recommender.SGL
2023-05-30 14:35:16.752: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 14:35:16.752: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 14:35:20.755: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 14:35:41.363: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.607078]
2023-05-30 14:35:41.631: epoch 1:	0.00118448  	0.00235553  	0.00202550  
2023-05-30 14:35:41.631: Find a better model.
2023-05-30 14:36:02.349: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.715091]
2023-05-30 14:36:02.630: epoch 2:	0.00175452  	0.00295176  	0.00257103  
2023-05-30 14:36:02.630: Find a better model.
2023-05-30 14:36:23.373: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.739045]
2023-05-30 14:36:23.665: epoch 3:	0.00190258  	0.00367540  	0.00296255  
2023-05-30 14:36:23.665: Find a better model.
2023-05-30 14:36:44.227: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.559177]
2023-05-30 14:36:44.525: epoch 4:	0.00230234  	0.00403839  	0.00364342  
2023-05-30 14:36:44.525: Find a better model.
2023-05-30 14:37:05.327: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.798078]
2023-05-30 14:37:05.616: epoch 5:	0.00267989  	0.00633098  	0.00468608  
2023-05-30 14:37:05.616: Find a better model.
2023-05-30 14:37:26.156: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.537297]
2023-05-30 14:37:26.456: epoch 6:	0.00348682  	0.00871261  	0.00649859  
2023-05-30 14:37:26.456: Find a better model.
2023-05-30 14:37:46.904: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 20.443316]
2023-05-30 14:37:47.191: epoch 7:	0.00369411  	0.00897768  	0.00703101  
2023-05-30 14:37:47.191: Find a better model.
2023-05-30 14:38:07.712: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.517436]
2023-05-30 14:38:07.995: epoch 8:	0.00444181  	0.01066499  	0.00830655  
2023-05-30 14:38:07.995: Find a better model.
2023-05-30 14:38:28.345: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.346031]
2023-05-30 14:38:28.628: epoch 9:	0.00570031  	0.01516679  	0.01134446  
2023-05-30 14:38:28.629: Find a better model.
2023-05-30 14:38:49.134: [iter 10 : loss : 1.1345 = 0.6908 + 0.4437 + 0.0000, time: 20.501717]
2023-05-30 14:38:49.430: epoch 10:	0.00611487  	0.01646584  	0.01252020  
2023-05-30 14:38:49.430: Find a better model.
2023-05-30 14:39:10.485: [iter 11 : loss : 1.1338 = 0.6895 + 0.4442 + 0.0000, time: 21.050522]
2023-05-30 14:39:10.762: epoch 11:	0.00717350  	0.01896319  	0.01520445  
2023-05-30 14:39:10.762: Find a better model.
2023-05-30 14:39:31.503: [iter 12 : loss : 1.1324 = 0.6875 + 0.4448 + 0.0000, time: 20.736519]
2023-05-30 14:39:31.784: epoch 12:	0.00869850  	0.02353199  	0.01921308  
2023-05-30 14:39:31.784: Find a better model.
2023-05-30 14:39:52.461: [iter 13 : loss : 1.1302 = 0.6846 + 0.4455 + 0.0001, time: 20.672795]
2023-05-30 14:39:52.734: epoch 13:	0.01096386  	0.02917426  	0.02471424  
2023-05-30 14:39:52.734: Find a better model.
2023-05-30 14:40:13.454: [iter 14 : loss : 1.1262 = 0.6798 + 0.4463 + 0.0001, time: 20.715070]
2023-05-30 14:40:13.728: epoch 14:	0.01450995  	0.03915835  	0.03252212  
2023-05-30 14:40:13.728: Find a better model.
2023-05-30 14:40:34.492: [iter 15 : loss : 1.1190 = 0.6717 + 0.4472 + 0.0001, time: 20.760045]
2023-05-30 14:40:34.764: epoch 15:	0.01777474  	0.04687107  	0.04025767  
2023-05-30 14:40:34.764: Find a better model.
2023-05-30 14:40:55.461: [iter 16 : loss : 1.1059 = 0.6576 + 0.4482 + 0.0002, time: 20.693428]
2023-05-30 14:40:55.737: epoch 16:	0.02089151  	0.05531746  	0.04731930  
2023-05-30 14:40:55.737: Find a better model.
2023-05-30 14:41:16.448: [iter 17 : loss : 1.0848 = 0.6349 + 0.4496 + 0.0003, time: 20.707210]
2023-05-30 14:41:16.721: epoch 17:	0.02376396  	0.06286122  	0.05284021  
2023-05-30 14:41:16.721: Find a better model.
2023-05-30 14:41:37.473: [iter 18 : loss : 1.0528 = 0.6009 + 0.4514 + 0.0004, time: 20.747446]
2023-05-30 14:41:37.738: epoch 18:	0.02611821  	0.06942166  	0.05734945  
2023-05-30 14:41:37.738: Find a better model.
2023-05-30 14:41:58.402: [iter 19 : loss : 1.0111 = 0.5566 + 0.4539 + 0.0006, time: 20.659178]
2023-05-30 14:41:58.675: epoch 19:	0.02748783  	0.07368214  	0.06047270  
2023-05-30 14:41:58.675: Find a better model.
2023-05-30 14:42:19.184: [iter 20 : loss : 0.9629 = 0.5051 + 0.4569 + 0.0009, time: 20.504008]
2023-05-30 14:42:19.462: epoch 20:	0.02850947  	0.07701169  	0.06254889  
2023-05-30 14:42:19.462: Find a better model.
2023-05-30 14:42:40.387: [iter 21 : loss : 0.9135 = 0.4523 + 0.4601 + 0.0012, time: 20.921026]
2023-05-30 14:42:40.658: epoch 21:	0.02910912  	0.07928073  	0.06395615  
2023-05-30 14:42:40.658: Find a better model.
2023-05-30 14:43:01.219: [iter 22 : loss : 0.8658 = 0.4014 + 0.4630 + 0.0015, time: 20.557109]
2023-05-30 14:43:01.487: epoch 22:	0.02942746  	0.08012225  	0.06440645  
2023-05-30 14:43:01.487: Find a better model.
2023-05-30 14:43:22.380: [iter 23 : loss : 0.8240 = 0.3571 + 0.4651 + 0.0018, time: 20.889135]
2023-05-30 14:43:22.646: epoch 23:	0.02949410  	0.08063522  	0.06461886  
2023-05-30 14:43:22.646: Find a better model.
2023-05-30 14:43:43.407: [iter 24 : loss : 0.7880 = 0.3194 + 0.4666 + 0.0021, time: 20.757496]
2023-05-30 14:43:43.675: epoch 24:	0.02951632  	0.08112472  	0.06502760  
2023-05-30 14:43:43.675: Find a better model.
2023-05-30 14:44:04.356: [iter 25 : loss : 0.7573 = 0.2877 + 0.4672 + 0.0023, time: 20.678195]
2023-05-30 14:44:04.625: epoch 25:	0.02982725  	0.08166830  	0.06541141  
2023-05-30 14:44:04.625: Find a better model.
2023-05-30 14:44:25.346: [iter 26 : loss : 0.7319 = 0.2618 + 0.4675 + 0.0026, time: 20.717147]
2023-05-30 14:44:25.622: epoch 26:	0.02979023  	0.08187310  	0.06554944  
2023-05-30 14:44:25.622: Find a better model.
2023-05-30 14:44:46.384: [iter 27 : loss : 0.7094 = 0.2394 + 0.4671 + 0.0029, time: 20.757754]
2023-05-30 14:44:46.651: epoch 27:	0.02982725  	0.08197903  	0.06565619  
2023-05-30 14:44:46.651: Find a better model.
2023-05-30 14:45:07.400: [iter 28 : loss : 0.6908 = 0.2207 + 0.4669 + 0.0031, time: 20.746477]
2023-05-30 14:45:07.665: epoch 28:	0.02976803  	0.08188455  	0.06586368  
2023-05-30 14:45:28.375: [iter 29 : loss : 0.6746 = 0.2049 + 0.4663 + 0.0034, time: 20.706573]
2023-05-30 14:45:28.639: epoch 29:	0.02969400  	0.08166551  	0.06575090  
2023-05-30 14:45:49.388: [iter 30 : loss : 0.6597 = 0.1904 + 0.4658 + 0.0036, time: 20.744060]
2023-05-30 14:45:49.650: epoch 30:	0.02965698  	0.08140353  	0.06575436  
2023-05-30 14:46:10.510: [iter 31 : loss : 0.6473 = 0.1783 + 0.4651 + 0.0038, time: 20.857414]
2023-05-30 14:46:10.773: epoch 31:	0.02975322  	0.08185739  	0.06614368  
2023-05-30 14:46:31.347: [iter 32 : loss : 0.6363 = 0.1677 + 0.4645 + 0.0041, time: 20.569303]
2023-05-30 14:46:31.611: epoch 32:	0.02976802  	0.08204365  	0.06619075  
2023-05-30 14:46:31.611: Find a better model.
2023-05-30 14:46:52.383: [iter 33 : loss : 0.6272 = 0.1591 + 0.4638 + 0.0043, time: 20.768180]
2023-05-30 14:46:52.647: epoch 33:	0.02976802  	0.08193111  	0.06631786  
2023-05-30 14:47:13.339: [iter 34 : loss : 0.6180 = 0.1504 + 0.4632 + 0.0045, time: 20.687849]
2023-05-30 14:47:13.602: epoch 34:	0.02978282  	0.08199639  	0.06628556  
2023-05-30 14:47:34.295: [iter 35 : loss : 0.6095 = 0.1424 + 0.4625 + 0.0047, time: 20.689033]
2023-05-30 14:47:34.564: epoch 35:	0.02976062  	0.08224373  	0.06648934  
2023-05-30 14:47:34.564: Find a better model.
2023-05-30 14:47:55.299: [iter 36 : loss : 0.6029 = 0.1360 + 0.4620 + 0.0049, time: 20.731576]
2023-05-30 14:47:55.559: epoch 36:	0.02982724  	0.08254615  	0.06661976  
2023-05-30 14:47:55.560: Find a better model.
2023-05-30 14:48:16.257: [iter 37 : loss : 0.5966 = 0.1300 + 0.4615 + 0.0050, time: 20.693695]
2023-05-30 14:48:16.522: epoch 37:	0.02979762  	0.08224007  	0.06654164  
2023-05-30 14:48:37.285: [iter 38 : loss : 0.5901 = 0.1239 + 0.4609 + 0.0052, time: 20.758687]
2023-05-30 14:48:37.548: epoch 38:	0.02973100  	0.08217515  	0.06645609  
2023-05-30 14:48:58.303: [iter 39 : loss : 0.5850 = 0.1192 + 0.4603 + 0.0054, time: 20.751127]
2023-05-30 14:48:58.570: epoch 39:	0.02979763  	0.08258635  	0.06648557  
2023-05-30 14:48:58.570: Find a better model.
2023-05-30 14:49:19.498: [iter 40 : loss : 0.5797 = 0.1142 + 0.4599 + 0.0056, time: 20.923952]
2023-05-30 14:49:19.761: epoch 40:	0.02979023  	0.08243385  	0.06654557  
2023-05-30 14:49:40.423: [iter 41 : loss : 0.5747 = 0.1096 + 0.4594 + 0.0057, time: 20.657575]
2023-05-30 14:49:40.687: epoch 41:	0.02981243  	0.08233472  	0.06651079  
2023-05-30 14:50:01.487: [iter 42 : loss : 0.5705 = 0.1055 + 0.4591 + 0.0059, time: 20.796338]
2023-05-30 14:50:01.750: epoch 42:	0.02977541  	0.08197550  	0.06642506  
2023-05-30 14:50:22.420: [iter 43 : loss : 0.5666 = 0.1019 + 0.4587 + 0.0061, time: 20.667367]
2023-05-30 14:50:22.686: epoch 43:	0.02973839  	0.08188937  	0.06622775  
2023-05-30 14:50:43.431: [iter 44 : loss : 0.5636 = 0.0991 + 0.4583 + 0.0062, time: 20.741647]
2023-05-30 14:50:43.694: epoch 44:	0.02968657  	0.08129624  	0.06605448  
2023-05-30 14:51:04.486: [iter 45 : loss : 0.5598 = 0.0954 + 0.4580 + 0.0064, time: 20.788027]
2023-05-30 14:51:04.751: epoch 45:	0.02970138  	0.08139928  	0.06617776  
2023-05-30 14:51:25.479: [iter 46 : loss : 0.5565 = 0.0924 + 0.4576 + 0.0065, time: 20.724532]
2023-05-30 14:51:25.746: epoch 46:	0.02959773  	0.08088197  	0.06590965  
2023-05-30 14:51:46.406: [iter 47 : loss : 0.5534 = 0.0894 + 0.4573 + 0.0067, time: 20.657053]
2023-05-30 14:51:46.670: epoch 47:	0.02964216  	0.08057121  	0.06593905  
2023-05-30 14:52:07.465: [iter 48 : loss : 0.5508 = 0.0870 + 0.4570 + 0.0068, time: 20.792045]
2023-05-30 14:52:07.730: epoch 48:	0.02955331  	0.07991995  	0.06565697  
2023-05-30 14:52:28.404: [iter 49 : loss : 0.5480 = 0.0845 + 0.4566 + 0.0069, time: 20.670043]
2023-05-30 14:52:28.667: epoch 49:	0.02965696  	0.08017085  	0.06579532  
2023-05-30 14:52:49.207: [iter 50 : loss : 0.5459 = 0.0824 + 0.4565 + 0.0071, time: 20.535290]
2023-05-30 14:52:49.475: epoch 50:	0.02948669  	0.07962557  	0.06563585  
2023-05-30 14:53:10.220: [iter 51 : loss : 0.5434 = 0.0800 + 0.4561 + 0.0072, time: 20.741350]
2023-05-30 14:53:10.489: epoch 51:	0.02958293  	0.07983561  	0.06562274  
2023-05-30 14:53:31.233: [iter 52 : loss : 0.5406 = 0.0774 + 0.4559 + 0.0073, time: 20.741238]
2023-05-30 14:53:31.503: epoch 52:	0.02950148  	0.07938991  	0.06535544  
2023-05-30 14:53:52.159: [iter 53 : loss : 0.5398 = 0.0767 + 0.4557 + 0.0075, time: 20.653339]
2023-05-30 14:53:52.438: epoch 53:	0.02944967  	0.07926674  	0.06533948  
2023-05-30 14:54:13.185: [iter 54 : loss : 0.5375 = 0.0744 + 0.4555 + 0.0076, time: 20.742635]
2023-05-30 14:54:13.462: epoch 54:	0.02936083  	0.07929013  	0.06523259  
2023-05-30 14:54:34.201: [iter 55 : loss : 0.5355 = 0.0726 + 0.4552 + 0.0077, time: 20.735179]
2023-05-30 14:54:34.472: epoch 55:	0.02919056  	0.07898048  	0.06507774  
2023-05-30 14:54:55.151: [iter 56 : loss : 0.5333 = 0.0705 + 0.4549 + 0.0078, time: 20.675428]
2023-05-30 14:54:55.430: epoch 56:	0.02922017  	0.07883427  	0.06499647  
2023-05-30 14:55:16.188: [iter 57 : loss : 0.5312 = 0.0684 + 0.4549 + 0.0080, time: 20.752613]
2023-05-30 14:55:16.464: epoch 57:	0.02905731  	0.07829185  	0.06469234  
2023-05-30 14:55:37.180: [iter 58 : loss : 0.5302 = 0.0675 + 0.4546 + 0.0081, time: 20.713253]
2023-05-30 14:55:37.456: epoch 58:	0.02903510  	0.07799857  	0.06441642  
2023-05-30 14:55:58.111: [iter 59 : loss : 0.5287 = 0.0661 + 0.4544 + 0.0082, time: 20.651770]
2023-05-30 14:55:58.377: epoch 59:	0.02896846  	0.07755924  	0.06439038  
2023-05-30 14:56:19.151: [iter 60 : loss : 0.5274 = 0.0648 + 0.4543 + 0.0083, time: 20.770552]
2023-05-30 14:56:19.428: epoch 60:	0.02892405  	0.07727589  	0.06426182  
2023-05-30 14:56:40.122: [iter 61 : loss : 0.5258 = 0.0632 + 0.4541 + 0.0084, time: 20.680014]
2023-05-30 14:56:40.386: epoch 61:	0.02885742  	0.07715952  	0.06411611  
2023-05-30 14:57:01.138: [iter 62 : loss : 0.5249 = 0.0624 + 0.4540 + 0.0085, time: 20.748050]
2023-05-30 14:57:01.418: epoch 62:	0.02888702  	0.07712736  	0.06402708  
2023-05-30 14:57:22.171: [iter 63 : loss : 0.5234 = 0.0610 + 0.4538 + 0.0086, time: 20.749010]
2023-05-30 14:57:22.445: epoch 63:	0.02885000  	0.07688340  	0.06376272  
2023-05-30 14:57:43.141: [iter 64 : loss : 0.5225 = 0.0601 + 0.4537 + 0.0087, time: 20.692986]
2023-05-30 14:57:43.423: epoch 64:	0.02877598  	0.07659722  	0.06361388  
2023-05-30 14:57:43.423: Early stopping is trigger at epoch: 64
2023-05-30 14:57:43.423: best_result@epoch 39:

2023-05-30 14:57:43.423: 		0.0298      	0.0826      	0.0665      
2023-05-30 15:02:01.673: my pid: 13716
2023-05-30 15:02:01.673: model: model.general_recommender.SGL
2023-05-30 15:02:01.673: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 15:02:01.673: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 15:02:05.664: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 15:02:26.011: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.346365]
2023-05-30 15:02:26.278: epoch 1:	0.00115487  	0.00228493  	0.00190254  
2023-05-30 15:02:26.278: Find a better model.
2023-05-30 15:02:46.776: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.494746]
2023-05-30 15:02:47.077: epoch 2:	0.00166568  	0.00371545  	0.00289654  
2023-05-30 15:02:47.078: Find a better model.
2023-05-30 15:03:07.436: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.353444]
2023-05-30 15:03:07.729: epoch 3:	0.00216168  	0.00515186  	0.00374820  
2023-05-30 15:03:07.729: Find a better model.
2023-05-30 15:03:28.177: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.444181]
2023-05-30 15:03:28.469: epoch 4:	0.00239858  	0.00474883  	0.00393039  
2023-05-30 15:03:48.977: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.505077]
2023-05-30 15:03:49.268: epoch 5:	0.00287978  	0.00620720  	0.00517905  
2023-05-30 15:03:49.268: Find a better model.
2023-05-30 15:04:09.770: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.497483]
2023-05-30 15:04:10.073: epoch 6:	0.00331655  	0.00705786  	0.00590326  
2023-05-30 15:04:10.073: Find a better model.
2023-05-30 15:04:30.381: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.303152]
2023-05-30 15:04:30.668: epoch 7:	0.00387178  	0.00933125  	0.00724821  
2023-05-30 15:04:30.668: Find a better model.
2023-05-30 15:04:50.980: [iter 8 : loss : 1.1347 = 0.6920 + 0.4428 + 0.0000, time: 20.308892]
2023-05-30 15:04:51.269: epoch 8:	0.00461948  	0.01189526  	0.00976882  
2023-05-30 15:04:51.269: Find a better model.
2023-05-30 15:05:11.712: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.440028]
2023-05-30 15:05:11.996: epoch 9:	0.00548562  	0.01473232  	0.01190058  
2023-05-30 15:05:11.996: Find a better model.
2023-05-30 15:05:32.158: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.158057]
2023-05-30 15:05:32.443: epoch 10:	0.00594461  	0.01562324  	0.01311378  
2023-05-30 15:05:32.444: Find a better model.
2023-05-30 15:05:53.372: [iter 11 : loss : 1.1337 = 0.6895 + 0.4443 + 0.0000, time: 20.925169]
2023-05-30 15:05:53.656: epoch 11:	0.00722532  	0.01915654  	0.01592501  
2023-05-30 15:05:53.656: Find a better model.
2023-05-30 15:06:14.689: [iter 12 : loss : 1.1323 = 0.6875 + 0.4448 + 0.0000, time: 21.029367]
2023-05-30 15:06:14.967: epoch 12:	0.00879474  	0.02382101  	0.01975275  
2023-05-30 15:06:14.967: Find a better model.
2023-05-30 15:06:35.697: [iter 13 : loss : 1.1303 = 0.6847 + 0.4455 + 0.0001, time: 20.727300]
2023-05-30 15:06:35.974: epoch 13:	0.01143024  	0.03022899  	0.02628684  
2023-05-30 15:06:35.974: Find a better model.
2023-05-30 15:06:56.693: [iter 14 : loss : 1.1264 = 0.6800 + 0.4463 + 0.0001, time: 20.715107]
2023-05-30 15:06:56.965: epoch 14:	0.01430266  	0.03759331  	0.03285302  
2023-05-30 15:06:56.965: Find a better model.
2023-05-30 15:07:17.695: [iter 15 : loss : 1.1193 = 0.6721 + 0.4471 + 0.0001, time: 20.726914]
2023-05-30 15:07:17.970: epoch 15:	0.01774513  	0.04613693  	0.04019225  
2023-05-30 15:07:17.970: Find a better model.
2023-05-30 15:07:38.725: [iter 16 : loss : 1.1066 = 0.6583 + 0.4482 + 0.0002, time: 20.750340]
2023-05-30 15:07:38.998: epoch 16:	0.02107658  	0.05520765  	0.04796500  
2023-05-30 15:07:38.998: Find a better model.
2023-05-30 15:07:59.698: [iter 17 : loss : 1.0858 = 0.6359 + 0.4496 + 0.0003, time: 20.697031]
2023-05-30 15:07:59.972: epoch 17:	0.02378617  	0.06274851  	0.05416557  
2023-05-30 15:07:59.972: Find a better model.
2023-05-30 15:08:20.667: [iter 18 : loss : 1.0539 = 0.6021 + 0.4514 + 0.0004, time: 20.692132]
2023-05-30 15:08:20.942: epoch 18:	0.02569623  	0.06823061  	0.05815412  
2023-05-30 15:08:20.942: Find a better model.
2023-05-30 15:08:41.489: [iter 19 : loss : 1.0125 = 0.5579 + 0.4539 + 0.0006, time: 20.543036]
2023-05-30 15:08:41.766: epoch 19:	0.02765067  	0.07327034  	0.06157713  
2023-05-30 15:08:41.766: Find a better model.
2023-05-30 15:09:02.630: [iter 20 : loss : 0.9642 = 0.5063 + 0.4571 + 0.0009, time: 20.860134]
2023-05-30 15:09:02.899: epoch 20:	0.02850947  	0.07592601  	0.06317300  
2023-05-30 15:09:02.899: Find a better model.
2023-05-30 15:09:23.634: [iter 21 : loss : 0.9145 = 0.4529 + 0.4605 + 0.0011, time: 20.730039]
2023-05-30 15:09:23.902: epoch 21:	0.02892407  	0.07762614  	0.06400023  
2023-05-30 15:09:23.902: Find a better model.
2023-05-30 15:09:44.624: [iter 22 : loss : 0.8666 = 0.4017 + 0.4634 + 0.0014, time: 20.719089]
2023-05-30 15:09:44.894: epoch 22:	0.02932383  	0.07887889  	0.06441283  
2023-05-30 15:09:44.894: Find a better model.
2023-05-30 15:10:05.438: [iter 23 : loss : 0.8245 = 0.3572 + 0.4655 + 0.0018, time: 20.539971]
2023-05-30 15:10:05.706: epoch 23:	0.02949413  	0.07948158  	0.06460994  
2023-05-30 15:10:05.706: Find a better model.
2023-05-30 15:10:26.642: [iter 24 : loss : 0.7883 = 0.3192 + 0.4670 + 0.0020, time: 20.933019]
2023-05-30 15:10:26.908: epoch 24:	0.02940529  	0.07989885  	0.06473147  
2023-05-30 15:10:26.908: Find a better model.
2023-05-30 15:10:47.625: [iter 25 : loss : 0.7574 = 0.2875 + 0.4676 + 0.0023, time: 20.714451]
2023-05-30 15:10:47.889: epoch 25:	0.02947192  	0.08024773  	0.06462567  
2023-05-30 15:10:47.889: Find a better model.
2023-05-30 15:11:08.603: [iter 26 : loss : 0.7318 = 0.2613 + 0.4679 + 0.0026, time: 20.710945]
2023-05-30 15:11:08.869: epoch 26:	0.02958296  	0.08051294  	0.06504255  
2023-05-30 15:11:08.869: Find a better model.
2023-05-30 15:11:29.650: [iter 27 : loss : 0.7096 = 0.2391 + 0.4676 + 0.0029, time: 20.777092]
2023-05-30 15:11:29.922: epoch 27:	0.02967922  	0.08101958  	0.06534716  
2023-05-30 15:11:29.922: Find a better model.
2023-05-30 15:11:50.586: [iter 28 : loss : 0.6910 = 0.2207 + 0.4672 + 0.0031, time: 20.661154]
2023-05-30 15:11:50.850: epoch 28:	0.02970882  	0.08055973  	0.06527855  
2023-05-30 15:12:11.594: [iter 29 : loss : 0.6747 = 0.2046 + 0.4667 + 0.0034, time: 20.740894]
2023-05-30 15:12:11.859: epoch 29:	0.02979767  	0.08099356  	0.06567917  
2023-05-30 15:12:32.579: [iter 30 : loss : 0.6601 = 0.1904 + 0.4661 + 0.0036, time: 20.717064]
2023-05-30 15:12:32.842: epoch 30:	0.02988650  	0.08123291  	0.06594662  
2023-05-30 15:12:32.842: Find a better model.
2023-05-30 15:12:53.736: [iter 31 : loss : 0.6475 = 0.1783 + 0.4654 + 0.0038, time: 20.891419]
2023-05-30 15:12:54.002: epoch 31:	0.02998273  	0.08172950  	0.06611375  
2023-05-30 15:12:54.002: Find a better model.
2023-05-30 15:13:14.783: [iter 32 : loss : 0.6366 = 0.1679 + 0.4647 + 0.0041, time: 20.777075]
2023-05-30 15:13:15.060: epoch 32:	0.03007898  	0.08224618  	0.06645329  
2023-05-30 15:13:15.060: Find a better model.
2023-05-30 15:13:35.935: [iter 33 : loss : 0.6277 = 0.1593 + 0.4641 + 0.0043, time: 20.872088]
2023-05-30 15:13:36.204: epoch 33:	0.03008637  	0.08175632  	0.06629318  
2023-05-30 15:13:57.140: [iter 34 : loss : 0.6185 = 0.1506 + 0.4635 + 0.0045, time: 20.932174]
2023-05-30 15:13:57.402: epoch 34:	0.03017521  	0.08225592  	0.06649518  
2023-05-30 15:13:57.402: Find a better model.
2023-05-30 15:14:18.016: [iter 35 : loss : 0.6097 = 0.1423 + 0.4627 + 0.0047, time: 20.610223]
2023-05-30 15:14:18.277: epoch 35:	0.03025664  	0.08252110  	0.06647963  
2023-05-30 15:14:18.277: Find a better model.
2023-05-30 15:14:39.118: [iter 36 : loss : 0.6031 = 0.1362 + 0.4621 + 0.0048, time: 20.837471]
2023-05-30 15:14:39.383: epoch 36:	0.03018261  	0.08236197  	0.06629713  
2023-05-30 15:15:00.331: [iter 37 : loss : 0.5968 = 0.1302 + 0.4616 + 0.0050, time: 20.944656]
2023-05-30 15:15:00.599: epoch 37:	0.03018260  	0.08200022  	0.06614324  
2023-05-30 15:15:21.514: [iter 38 : loss : 0.5906 = 0.1243 + 0.4610 + 0.0052, time: 20.909515]
2023-05-30 15:15:21.777: epoch 38:	0.03024923  	0.08244565  	0.06639805  
2023-05-30 15:15:42.514: [iter 39 : loss : 0.5855 = 0.1196 + 0.4606 + 0.0054, time: 20.732086]
2023-05-30 15:15:42.776: epoch 39:	0.03016779  	0.08237681  	0.06630418  
2023-05-30 15:16:03.709: [iter 40 : loss : 0.5799 = 0.1143 + 0.4600 + 0.0056, time: 20.929226]
2023-05-30 15:16:03.975: epoch 40:	0.03015299  	0.08196068  	0.06621641  
2023-05-30 15:16:24.672: [iter 41 : loss : 0.5747 = 0.1095 + 0.4595 + 0.0057, time: 20.692027]
2023-05-30 15:16:24.935: epoch 41:	0.03000492  	0.08159408  	0.06604555  
2023-05-30 15:16:45.682: [iter 42 : loss : 0.5707 = 0.1057 + 0.4592 + 0.0059, time: 20.742179]
2023-05-30 15:16:45.943: epoch 42:	0.03004934  	0.08164024  	0.06601893  
2023-05-30 15:17:07.226: [iter 43 : loss : 0.5667 = 0.1019 + 0.4588 + 0.0060, time: 21.278518]
2023-05-30 15:17:07.499: epoch 43:	0.03014559  	0.08199221  	0.06607418  
2023-05-30 15:17:35.558: my pid: 3972
2023-05-30 15:17:35.558: model: model.general_recommender.SGL
2023-05-30 15:17:35.558: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 15:17:35.558: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 15:17:39.959: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 15:18:01.761: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.801529]
2023-05-30 15:18:02.020: epoch 1:	0.00125111  	0.00247722  	0.00205225  
2023-05-30 15:18:02.020: Find a better model.
2023-05-30 15:18:22.720: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.696386]
2023-05-30 15:18:23.022: epoch 2:	0.00168789  	0.00378389  	0.00315659  
2023-05-30 15:18:23.022: Find a better model.
2023-05-30 15:18:43.680: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.653739]
2023-05-30 15:18:43.973: epoch 3:	0.00186556  	0.00432073  	0.00327114  
2023-05-30 15:18:43.973: Find a better model.
2023-05-30 15:19:04.540: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.563913]
2023-05-30 15:19:04.839: epoch 4:	0.00234676  	0.00479000  	0.00384169  
2023-05-30 15:19:04.839: Find a better model.
2023-05-30 15:19:25.463: [iter 5 : loss : 1.1344 = 0.6927 + 0.4418 + 0.0000, time: 20.621261]
2023-05-30 15:19:25.757: epoch 5:	0.00284276  	0.00605042  	0.00474088  
2023-05-30 15:19:25.757: Find a better model.
2023-05-30 15:19:46.284: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.523441]
2023-05-30 15:19:46.578: epoch 6:	0.00338318  	0.00700927  	0.00595354  
2023-05-30 15:19:46.578: Find a better model.
2023-05-30 15:20:07.252: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.670595]
2023-05-30 15:20:07.543: epoch 7:	0.00407166  	0.00968357  	0.00757574  
2023-05-30 15:20:07.543: Find a better model.
2023-05-30 15:20:27.912: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.364415]
2023-05-30 15:20:28.201: epoch 8:	0.00466390  	0.01199285  	0.00957031  
2023-05-30 15:20:28.201: Find a better model.
2023-05-30 15:20:48.836: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.631051]
2023-05-30 15:20:49.131: epoch 9:	0.00541159  	0.01427385  	0.01157928  
2023-05-30 15:20:49.131: Find a better model.
2023-05-30 15:21:09.655: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.521172]
2023-05-30 15:21:09.941: epoch 10:	0.00589278  	0.01632822  	0.01247539  
2023-05-30 15:21:09.941: Find a better model.
2023-05-30 15:21:30.797: [iter 11 : loss : 1.1337 = 0.6894 + 0.4443 + 0.0000, time: 20.851298]
2023-05-30 15:21:31.089: epoch 11:	0.00677373  	0.01902436  	0.01520705  
2023-05-30 15:21:31.089: Find a better model.
2023-05-30 15:21:52.004: [iter 12 : loss : 1.1323 = 0.6874 + 0.4449 + 0.0000, time: 20.910937]
2023-05-30 15:21:52.288: epoch 12:	0.00852083  	0.02406400  	0.01923458  
2023-05-30 15:21:52.288: Find a better model.
2023-05-30 15:22:13.186: [iter 13 : loss : 1.1302 = 0.6846 + 0.4456 + 0.0001, time: 20.894949]
2023-05-30 15:22:13.466: epoch 13:	0.01122296  	0.03245069  	0.02584941  
2023-05-30 15:22:13.466: Find a better model.
2023-05-30 15:22:34.426: [iter 14 : loss : 1.1262 = 0.6800 + 0.4462 + 0.0001, time: 20.957095]
2023-05-30 15:22:34.702: epoch 14:	0.01431746  	0.04040460  	0.03313872  
2023-05-30 15:22:34.702: Find a better model.
2023-05-30 15:22:55.609: [iter 15 : loss : 1.1193 = 0.6720 + 0.4472 + 0.0001, time: 20.903724]
2023-05-30 15:22:55.886: epoch 15:	0.01771553  	0.04951868  	0.04118072  
2023-05-30 15:22:55.886: Find a better model.
2023-05-30 15:23:16.831: [iter 16 : loss : 1.1064 = 0.6581 + 0.4482 + 0.0002, time: 20.942065]
2023-05-30 15:23:17.122: epoch 16:	0.02100258  	0.05714227  	0.04827401  
2023-05-30 15:23:17.123: Find a better model.
2023-05-30 15:23:37.942: [iter 17 : loss : 1.0856 = 0.6357 + 0.4496 + 0.0003, time: 20.815331]
2023-05-30 15:23:38.226: epoch 17:	0.02405272  	0.06501446  	0.05451954  
2023-05-30 15:23:38.226: Find a better model.
2023-05-30 15:23:59.175: [iter 18 : loss : 1.0538 = 0.6019 + 0.4515 + 0.0004, time: 20.944272]
2023-05-30 15:23:59.453: epoch 18:	0.02618487  	0.07115767  	0.05905434  
2023-05-30 15:23:59.453: Find a better model.
2023-05-30 15:24:20.144: [iter 19 : loss : 1.0124 = 0.5579 + 0.4539 + 0.0006, time: 20.688015]
2023-05-30 15:24:20.421: epoch 19:	0.02756187  	0.07524527  	0.06217200  
2023-05-30 15:24:20.422: Find a better model.
2023-05-30 15:24:41.161: [iter 20 : loss : 0.9643 = 0.5065 + 0.4570 + 0.0009, time: 20.735400]
2023-05-30 15:24:41.435: epoch 20:	0.02851689  	0.07830256  	0.06433767  
2023-05-30 15:24:41.435: Find a better model.
2023-05-30 15:25:02.144: [iter 21 : loss : 0.9147 = 0.4533 + 0.4603 + 0.0011, time: 20.703741]
2023-05-30 15:25:02.420: epoch 21:	0.02893146  	0.07953784  	0.06470004  
2023-05-30 15:25:02.420: Find a better model.
2023-05-30 15:25:23.157: [iter 22 : loss : 0.8671 = 0.4024 + 0.4632 + 0.0014, time: 20.733530]
2023-05-30 15:25:23.430: epoch 22:	0.02945710  	0.08103196  	0.06576282  
2023-05-30 15:25:23.431: Find a better model.
2023-05-30 15:25:44.120: [iter 23 : loss : 0.8250 = 0.3580 + 0.4652 + 0.0017, time: 20.686846]
2023-05-30 15:25:44.392: epoch 23:	0.02962736  	0.08196144  	0.06616237  
2023-05-30 15:25:44.392: Find a better model.
2023-05-30 15:26:05.128: [iter 24 : loss : 0.7890 = 0.3202 + 0.4668 + 0.0020, time: 20.730269]
2023-05-30 15:26:05.401: epoch 24:	0.02993830  	0.08320016  	0.06669634  
2023-05-30 15:26:05.401: Find a better model.
2023-05-30 15:26:26.103: [iter 25 : loss : 0.7580 = 0.2882 + 0.4675 + 0.0023, time: 20.698189]
2023-05-30 15:26:26.371: epoch 25:	0.02991610  	0.08304257  	0.06671335  
2023-05-30 15:26:47.108: [iter 26 : loss : 0.7323 = 0.2621 + 0.4677 + 0.0026, time: 20.733496]
2023-05-30 15:26:47.382: epoch 26:	0.02999013  	0.08343855  	0.06691445  
2023-05-30 15:26:47.382: Find a better model.
2023-05-30 15:27:08.302: [iter 27 : loss : 0.7101 = 0.2397 + 0.4675 + 0.0029, time: 20.917084]
2023-05-30 15:27:08.574: epoch 27:	0.03008637  	0.08380883  	0.06703568  
2023-05-30 15:27:08.574: Find a better model.
2023-05-30 15:27:29.322: [iter 28 : loss : 0.6913 = 0.2211 + 0.4671 + 0.0031, time: 20.744984]
2023-05-30 15:27:29.591: epoch 28:	0.03021963  	0.08413381  	0.06735230  
2023-05-30 15:27:29.591: Find a better model.
2023-05-30 15:27:50.482: [iter 29 : loss : 0.6751 = 0.2052 + 0.4665 + 0.0034, time: 20.888036]
2023-05-30 15:27:50.752: epoch 29:	0.03037509  	0.08420275  	0.06730930  
2023-05-30 15:27:50.752: Find a better model.
2023-05-30 15:28:11.671: [iter 30 : loss : 0.6603 = 0.1907 + 0.4660 + 0.0036, time: 20.915731]
2023-05-30 15:28:11.937: epoch 30:	0.03047873  	0.08451252  	0.06746436  
2023-05-30 15:28:11.937: Find a better model.
2023-05-30 15:28:32.872: [iter 31 : loss : 0.6476 = 0.1785 + 0.4653 + 0.0038, time: 20.931348]
2023-05-30 15:28:33.145: epoch 31:	0.03033067  	0.08427270  	0.06753269  
2023-05-30 15:28:54.072: [iter 32 : loss : 0.6369 = 0.1683 + 0.4646 + 0.0041, time: 20.923586]
2023-05-30 15:28:54.338: epoch 32:	0.03028626  	0.08436571  	0.06751527  
2023-05-30 15:29:15.211: [iter 33 : loss : 0.6274 = 0.1592 + 0.4640 + 0.0043, time: 20.869411]
2023-05-30 15:29:15.478: epoch 33:	0.03031587  	0.08441965  	0.06760930  
2023-05-30 15:29:36.423: [iter 34 : loss : 0.6187 = 0.1509 + 0.4633 + 0.0045, time: 20.941114]
2023-05-30 15:29:36.687: epoch 34:	0.03035289  	0.08460181  	0.06755958  
2023-05-30 15:29:36.687: Find a better model.
2023-05-30 15:29:57.600: [iter 35 : loss : 0.6098 = 0.1426 + 0.4626 + 0.0047, time: 20.908380]
2023-05-30 15:29:57.870: epoch 35:	0.03020482  	0.08390861  	0.06748971  
2023-05-30 15:30:18.866: [iter 36 : loss : 0.6030 = 0.1360 + 0.4621 + 0.0048, time: 20.991604]
2023-05-30 15:30:19.138: epoch 36:	0.03030105  	0.08380348  	0.06742253  
2023-05-30 15:30:40.002: [iter 37 : loss : 0.5971 = 0.1304 + 0.4617 + 0.0050, time: 20.860448]
2023-05-30 15:30:40.270: epoch 37:	0.03022702  	0.08364520  	0.06718619  
2023-05-30 15:31:01.021: [iter 38 : loss : 0.5903 = 0.1242 + 0.4609 + 0.0052, time: 20.747512]
2023-05-30 15:31:01.290: epoch 38:	0.03019741  	0.08346917  	0.06700403  
2023-05-30 15:31:22.007: [iter 39 : loss : 0.5853 = 0.1195 + 0.4604 + 0.0054, time: 20.714059]
2023-05-30 15:31:22.274: epoch 39:	0.03013818  	0.08331908  	0.06703484  
2023-05-30 15:31:43.204: [iter 40 : loss : 0.5797 = 0.1143 + 0.4599 + 0.0056, time: 20.927487]
2023-05-30 15:31:43.469: epoch 40:	0.03024183  	0.08332666  	0.06712638  
2023-05-30 15:32:04.208: [iter 41 : loss : 0.5753 = 0.1100 + 0.4595 + 0.0057, time: 20.735029]
2023-05-30 15:32:04.482: epoch 41:	0.03010117  	0.08281059  	0.06692696  
2023-05-30 15:32:25.820: [iter 42 : loss : 0.5710 = 0.1060 + 0.4592 + 0.0059, time: 21.334139]
2023-05-30 15:32:26.100: epoch 42:	0.03010117  	0.08280939  	0.06698111  
2023-05-30 15:32:49.939: my pid: 10208
2023-05-30 15:32:49.939: model: model.general_recommender.SGL
2023-05-30 15:32:49.939: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 15:32:49.939: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 15:32:54.362: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 15:33:16.370: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 22.006276]
2023-05-30 15:33:16.643: epoch 1:	0.00146580  	0.00305013  	0.00238767  
2023-05-30 15:33:16.643: Find a better model.
2023-05-30 15:33:37.588: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.941814]
2023-05-30 15:33:37.878: epoch 2:	0.00165087  	0.00300768  	0.00260583  
2023-05-30 15:33:58.563: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.680112]
2023-05-30 15:33:58.856: epoch 3:	0.00204324  	0.00359939  	0.00307318  
2023-05-30 15:33:58.856: Find a better model.
2023-05-30 15:34:19.566: [iter 4 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 20.707157]
2023-05-30 15:34:19.862: epoch 4:	0.00232455  	0.00508259  	0.00407583  
2023-05-30 15:34:19.862: Find a better model.
2023-05-30 15:34:40.758: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.892277]
2023-05-30 15:34:41.066: epoch 5:	0.00288718  	0.00652283  	0.00503758  
2023-05-30 15:34:41.066: Find a better model.
2023-05-30 15:35:01.603: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.532432]
2023-05-30 15:35:01.900: epoch 6:	0.00360527  	0.00839662  	0.00653703  
2023-05-30 15:35:01.900: Find a better model.
2023-05-30 15:35:22.600: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.695964]
2023-05-30 15:35:22.894: epoch 7:	0.00425674  	0.01087602  	0.00874199  
2023-05-30 15:35:22.894: Find a better model.
2023-05-30 15:35:43.549: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.651789]
2023-05-30 15:35:43.837: epoch 8:	0.00514509  	0.01359256  	0.01089599  
2023-05-30 15:35:43.837: Find a better model.
2023-05-30 15:36:04.513: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.672079]
2023-05-30 15:36:04.802: epoch 9:	0.00587058  	0.01670792  	0.01328709  
2023-05-30 15:36:04.802: Find a better model.
2023-05-30 15:36:25.188: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.382043]
2023-05-30 15:36:25.476: epoch 10:	0.00609267  	0.01687275  	0.01390026  
2023-05-30 15:36:25.476: Find a better model.
2023-05-30 15:36:46.727: [iter 11 : loss : 1.1338 = 0.6895 + 0.4442 + 0.0000, time: 21.246065]
2023-05-30 15:36:47.013: epoch 11:	0.00715129  	0.01932851  	0.01609239  
2023-05-30 15:36:47.013: Find a better model.
2023-05-30 15:37:08.255: [iter 12 : loss : 1.1325 = 0.6876 + 0.4449 + 0.0000, time: 21.238416]
2023-05-30 15:37:08.541: epoch 12:	0.00905385  	0.02530864  	0.02043262  
2023-05-30 15:37:08.542: Find a better model.
2023-05-30 15:37:29.665: [iter 13 : loss : 1.1303 = 0.6848 + 0.4455 + 0.0001, time: 21.120147]
2023-05-30 15:37:29.948: epoch 13:	0.01148206  	0.03181471  	0.02613983  
2023-05-30 15:37:29.948: Find a better model.
2023-05-30 15:37:50.975: [iter 14 : loss : 1.1265 = 0.6802 + 0.4462 + 0.0001, time: 21.023306]
2023-05-30 15:37:51.257: epoch 14:	0.01473204  	0.04041758  	0.03398840  
2023-05-30 15:37:51.257: Find a better model.
2023-05-30 15:38:12.109: [iter 15 : loss : 1.1195 = 0.6724 + 0.4471 + 0.0001, time: 20.849104]
2023-05-30 15:38:12.388: epoch 15:	0.01810790  	0.04901825  	0.04164684  
2023-05-30 15:38:12.389: Find a better model.
2023-05-30 15:38:33.259: [iter 16 : loss : 1.1070 = 0.6587 + 0.4482 + 0.0002, time: 20.867523]
2023-05-30 15:38:33.538: epoch 16:	0.02126168  	0.05714571  	0.04887157  
2023-05-30 15:38:33.538: Find a better model.
2023-05-30 15:38:54.412: [iter 17 : loss : 1.0863 = 0.6366 + 0.4494 + 0.0003, time: 20.869441]
2023-05-30 15:38:54.689: epoch 17:	0.02397866  	0.06404207  	0.05507441  
2023-05-30 15:38:54.689: Find a better model.
2023-05-30 15:39:15.694: [iter 18 : loss : 1.0548 = 0.6031 + 0.4514 + 0.0004, time: 21.001556]
2023-05-30 15:39:15.968: epoch 18:	0.02621447  	0.06990678  	0.05956331  
2023-05-30 15:39:15.968: Find a better model.
2023-05-30 15:39:36.836: [iter 19 : loss : 1.0136 = 0.5591 + 0.4539 + 0.0006, time: 20.863402]
2023-05-30 15:39:37.123: epoch 19:	0.02770251  	0.07361108  	0.06289643  
2023-05-30 15:39:37.123: Find a better model.
2023-05-30 15:39:58.023: [iter 20 : loss : 0.9655 = 0.5076 + 0.4570 + 0.0009, time: 20.897049]
2023-05-30 15:39:58.300: epoch 20:	0.02859830  	0.07631100  	0.06467495  
2023-05-30 15:39:58.300: Find a better model.
2023-05-30 15:40:19.206: [iter 21 : loss : 0.9157 = 0.4542 + 0.4604 + 0.0011, time: 20.902396]
2023-05-30 15:40:19.481: epoch 21:	0.02890184  	0.07747532  	0.06553588  
2023-05-30 15:40:19.481: Find a better model.
2023-05-30 15:40:40.452: [iter 22 : loss : 0.8678 = 0.4030 + 0.4634 + 0.0014, time: 20.966470]
2023-05-30 15:40:40.726: epoch 22:	0.02913876  	0.07845467  	0.06585978  
2023-05-30 15:40:40.726: Find a better model.
2023-05-30 15:41:01.818: [iter 23 : loss : 0.8258 = 0.3585 + 0.4655 + 0.0017, time: 21.087041]
2023-05-30 15:41:02.106: epoch 23:	0.02923499  	0.07904553  	0.06619187  
2023-05-30 15:41:02.106: Find a better model.
2023-05-30 15:41:23.009: [iter 24 : loss : 0.7898 = 0.3208 + 0.4670 + 0.0020, time: 20.900535]
2023-05-30 15:41:23.287: epoch 24:	0.02937566  	0.08013514  	0.06664193  
2023-05-30 15:41:23.287: Find a better model.
2023-05-30 15:41:44.150: [iter 25 : loss : 0.7587 = 0.2887 + 0.4676 + 0.0023, time: 20.858519]
2023-05-30 15:41:44.417: epoch 25:	0.02950892  	0.08032459  	0.06666259  
2023-05-30 15:41:44.417: Find a better model.
2023-05-30 15:42:05.373: [iter 26 : loss : 0.7329 = 0.2625 + 0.4678 + 0.0026, time: 20.950539]
2023-05-30 15:42:05.645: epoch 26:	0.02969401  	0.08088550  	0.06698873  
2023-05-30 15:42:05.645: Find a better model.
2023-05-30 15:42:26.611: [iter 27 : loss : 0.7106 = 0.2400 + 0.4677 + 0.0029, time: 20.963384]
2023-05-30 15:42:26.878: epoch 27:	0.02973843  	0.08118657  	0.06699167  
2023-05-30 15:42:26.878: Find a better model.
2023-05-30 15:42:47.940: [iter 28 : loss : 0.6919 = 0.2216 + 0.4672 + 0.0031, time: 21.057034]
2023-05-30 15:42:48.211: epoch 28:	0.02984207  	0.08142342  	0.06732415  
2023-05-30 15:42:48.211: Find a better model.
2023-05-30 15:43:09.314: [iter 29 : loss : 0.6757 = 0.2056 + 0.4667 + 0.0034, time: 21.098691]
2023-05-30 15:43:09.579: epoch 29:	0.02994571  	0.08184891  	0.06755163  
2023-05-30 15:43:09.579: Find a better model.
2023-05-30 15:43:30.771: [iter 30 : loss : 0.6610 = 0.1914 + 0.4660 + 0.0036, time: 21.188997]
2023-05-30 15:43:31.045: epoch 30:	0.03002715  	0.08252111  	0.06797120  
2023-05-30 15:43:31.045: Find a better model.
2023-05-30 15:43:52.142: [iter 31 : loss : 0.6484 = 0.1792 + 0.4654 + 0.0038, time: 21.088060]
2023-05-30 15:43:52.409: epoch 31:	0.02999013  	0.08244334  	0.06795349  
2023-05-30 15:44:13.507: [iter 32 : loss : 0.6375 = 0.1687 + 0.4647 + 0.0040, time: 21.094158]
2023-05-30 15:44:13.776: epoch 32:	0.02977543  	0.08186679  	0.06772517  
2023-05-30 15:44:34.936: [iter 33 : loss : 0.6281 = 0.1598 + 0.4641 + 0.0042, time: 21.157150]
2023-05-30 15:44:35.209: epoch 33:	0.02997532  	0.08254898  	0.06805568  
2023-05-30 15:44:35.209: Find a better model.
2023-05-30 15:44:56.516: [iter 34 : loss : 0.6194 = 0.1515 + 0.4634 + 0.0045, time: 21.302142]
2023-05-30 15:44:56.782: epoch 34:	0.02997532  	0.08239353  	0.06797118  
2023-05-30 15:45:18.304: [iter 35 : loss : 0.6104 = 0.1431 + 0.4627 + 0.0047, time: 21.518200]
2023-05-30 15:45:18.572: epoch 35:	0.02996051  	0.08263211  	0.06810267  
2023-05-30 15:45:18.572: Find a better model.
2023-05-30 15:45:39.856: [iter 36 : loss : 0.6033 = 0.1364 + 0.4621 + 0.0048, time: 21.281112]
2023-05-30 15:45:40.125: epoch 36:	0.03015300  	0.08263049  	0.06820589  
2023-05-30 15:46:01.311: [iter 37 : loss : 0.5975 = 0.1308 + 0.4616 + 0.0050, time: 21.181035]
2023-05-30 15:46:01.575: epoch 37:	0.03019002  	0.08251359  	0.06810792  
2023-05-30 15:46:23.102: [iter 38 : loss : 0.5911 = 0.1249 + 0.4610 + 0.0052, time: 21.522070]
2023-05-30 15:46:23.370: epoch 38:	0.03019002  	0.08298878  	0.06822776  
2023-05-30 15:46:23.370: Find a better model.
2023-05-30 15:46:44.508: [iter 39 : loss : 0.5859 = 0.1201 + 0.4604 + 0.0054, time: 21.133553]
2023-05-30 15:46:44.775: epoch 39:	0.03006416  	0.08255875  	0.06813539  
2023-05-30 15:47:06.041: [iter 40 : loss : 0.5804 = 0.1149 + 0.4600 + 0.0055, time: 21.262281]
2023-05-30 15:47:06.312: epoch 40:	0.03017521  	0.08248813  	0.06809052  
2023-05-30 15:47:27.644: [iter 41 : loss : 0.5756 = 0.1103 + 0.4596 + 0.0057, time: 21.328039]
2023-05-30 15:47:27.912: epoch 41:	0.03012339  	0.08226649  	0.06810191  
2023-05-30 15:47:49.108: [iter 42 : loss : 0.5711 = 0.1061 + 0.4591 + 0.0059, time: 21.192079]
2023-05-30 15:47:49.372: epoch 42:	0.03015300  	0.08234756  	0.06813805  
2023-05-30 15:48:10.650: [iter 43 : loss : 0.5672 = 0.1024 + 0.4587 + 0.0060, time: 21.273381]
2023-05-30 15:48:10.913: epoch 43:	0.03013079  	0.08243873  	0.06798096  
2023-05-30 15:48:32.205: [iter 44 : loss : 0.5643 = 0.0998 + 0.4583 + 0.0062, time: 21.288281]
2023-05-30 15:48:32.473: epoch 44:	0.03008636  	0.08214296  	0.06775603  
2023-05-30 15:48:53.428: [iter 45 : loss : 0.5604 = 0.0960 + 0.4580 + 0.0063, time: 20.952027]
2023-05-30 15:48:53.692: epoch 45:	0.03001234  	0.08169764  	0.06761853  
2023-05-30 15:49:14.843: [iter 46 : loss : 0.5573 = 0.0931 + 0.4577 + 0.0065, time: 21.148101]
2023-05-30 15:49:15.117: epoch 46:	0.02996051  	0.08139954  	0.06754877  
2023-05-30 15:49:36.426: [iter 47 : loss : 0.5539 = 0.0900 + 0.4573 + 0.0066, time: 21.304372]
2023-05-30 15:49:36.695: epoch 47:	0.02997531  	0.08133209  	0.06744356  
2023-05-30 15:49:57.985: [iter 48 : loss : 0.5514 = 0.0876 + 0.4570 + 0.0068, time: 21.286056]
2023-05-30 15:49:58.256: epoch 48:	0.02996051  	0.08137188  	0.06745837  
2023-05-30 15:50:19.395: [iter 49 : loss : 0.5485 = 0.0849 + 0.4567 + 0.0069, time: 21.136012]
2023-05-30 15:50:19.659: epoch 49:	0.02984206  	0.08106731  	0.06722194  
2023-05-30 15:50:40.821: [iter 50 : loss : 0.5463 = 0.0829 + 0.4564 + 0.0070, time: 21.157990]
2023-05-30 15:50:41.098: epoch 50:	0.02986428  	0.08087652  	0.06717934  
2023-05-30 15:51:02.053: [iter 51 : loss : 0.5436 = 0.0803 + 0.4561 + 0.0072, time: 20.951602]
2023-05-30 15:51:02.318: epoch 51:	0.02981246  	0.08081856  	0.06710510  
2023-05-30 15:51:23.950: [iter 52 : loss : 0.5412 = 0.0780 + 0.4558 + 0.0073, time: 21.627229]
2023-05-30 15:51:24.240: epoch 52:	0.02972361  	0.08024856  	0.06689941  
2023-05-30 15:51:45.498: my pid: 11176
2023-05-30 15:51:45.498: model: model.general_recommender.SGL
2023-05-30 15:51:45.498: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 15:51:45.498: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 15:51:49.922: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 15:52:11.835: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.913592]
2023-05-30 15:52:12.148: epoch 1:	0.00133254  	0.00246236  	0.00203011  
2023-05-30 15:52:12.148: Find a better model.
2023-05-30 15:52:33.021: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.869453]
2023-05-30 15:52:33.312: epoch 2:	0.00149541  	0.00344904  	0.00255212  
2023-05-30 15:52:33.312: Find a better model.
2023-05-30 15:52:54.406: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.091222]
2023-05-30 15:52:54.696: epoch 3:	0.00207285  	0.00460897  	0.00341093  
2023-05-30 15:52:54.696: Find a better model.
2023-05-30 15:53:15.624: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.924980]
2023-05-30 15:53:15.914: epoch 4:	0.00243559  	0.00550925  	0.00407362  
2023-05-30 15:53:15.914: Find a better model.
2023-05-30 15:53:36.985: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.067518]
2023-05-30 15:53:37.276: epoch 5:	0.00286497  	0.00627007  	0.00480228  
2023-05-30 15:53:37.276: Find a better model.
2023-05-30 15:53:58.175: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.896665]
2023-05-30 15:53:58.461: epoch 6:	0.00336838  	0.00795605  	0.00598359  
2023-05-30 15:53:58.461: Find a better model.
2023-05-30 15:54:19.010: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.546075]
2023-05-30 15:54:19.298: epoch 7:	0.00432336  	0.01079184  	0.00843039  
2023-05-30 15:54:19.298: Find a better model.
2023-05-30 15:54:40.171: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.868620]
2023-05-30 15:54:40.458: epoch 8:	0.00509327  	0.01276874  	0.01023484  
2023-05-30 15:54:40.459: Find a better model.
2023-05-30 15:55:01.168: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.704676]
2023-05-30 15:55:01.455: epoch 9:	0.00585577  	0.01458428  	0.01233231  
2023-05-30 15:55:01.455: Find a better model.
2023-05-30 15:55:22.148: [iter 10 : loss : 1.1345 = 0.6909 + 0.4436 + 0.0000, time: 20.689202]
2023-05-30 15:55:22.430: epoch 10:	0.00688478  	0.01861026  	0.01524156  
2023-05-30 15:55:22.430: Find a better model.
2023-05-30 15:55:43.746: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 21.312068]
2023-05-30 15:55:44.030: epoch 11:	0.00831355  	0.02307408  	0.01890976  
2023-05-30 15:55:44.030: Find a better model.
2023-05-30 15:56:05.371: [iter 12 : loss : 1.1327 = 0.6880 + 0.4447 + 0.0000, time: 21.331102]
2023-05-30 15:56:05.652: epoch 12:	0.00952763  	0.02663949  	0.02214441  
2023-05-30 15:56:05.652: Find a better model.
2023-05-30 15:56:27.117: [iter 13 : loss : 1.1306 = 0.6852 + 0.4453 + 0.0001, time: 21.462259]
2023-05-30 15:56:27.393: epoch 13:	0.01140803  	0.03254397  	0.02768199  
2023-05-30 15:56:27.393: Find a better model.
2023-05-30 15:56:48.707: [iter 14 : loss : 1.1268 = 0.6807 + 0.4460 + 0.0001, time: 21.310093]
2023-05-30 15:56:48.986: epoch 14:	0.01469504  	0.04026849  	0.03495651  
2023-05-30 15:56:48.986: Find a better model.
2023-05-30 15:57:10.106: [iter 15 : loss : 1.1201 = 0.6732 + 0.4468 + 0.0001, time: 21.117042]
2023-05-30 15:57:10.380: epoch 15:	0.01779696  	0.04862374  	0.04204214  
2023-05-30 15:57:10.380: Find a better model.
2023-05-30 15:57:31.496: [iter 16 : loss : 1.1082 = 0.6601 + 0.4479 + 0.0002, time: 21.112430]
2023-05-30 15:57:31.772: epoch 16:	0.02126908  	0.05759775  	0.04965958  
2023-05-30 15:57:31.772: Find a better model.
2023-05-30 15:57:52.882: [iter 17 : loss : 1.0884 = 0.6389 + 0.4493 + 0.0003, time: 21.107376]
2023-05-30 15:57:53.168: epoch 17:	0.02421559  	0.06524631  	0.05599463  
2023-05-30 15:57:53.168: Find a better model.
2023-05-30 15:58:14.473: [iter 18 : loss : 1.0577 = 0.6063 + 0.4510 + 0.0004, time: 21.300051]
2023-05-30 15:58:14.747: epoch 18:	0.02645877  	0.07125945  	0.06031471  
2023-05-30 15:58:14.747: Find a better model.
2023-05-30 15:58:35.872: [iter 19 : loss : 1.0170 = 0.5631 + 0.4534 + 0.0006, time: 21.122030]
2023-05-30 15:58:36.153: epoch 19:	0.02812449  	0.07526372  	0.06350026  
2023-05-30 15:58:36.153: Find a better model.
2023-05-30 15:58:57.056: [iter 20 : loss : 0.9694 = 0.5122 + 0.4564 + 0.0008, time: 20.899002]
2023-05-30 15:58:57.331: epoch 20:	0.02924238  	0.07901306  	0.06572361  
2023-05-30 15:58:57.331: Find a better model.
2023-05-30 15:59:18.277: [iter 21 : loss : 0.9197 = 0.4589 + 0.4597 + 0.0011, time: 20.941754]
2023-05-30 15:59:18.550: epoch 21:	0.02962735  	0.07981199  	0.06601127  
2023-05-30 15:59:18.550: Find a better model.
2023-05-30 15:59:39.640: [iter 22 : loss : 0.8711 = 0.4070 + 0.4627 + 0.0014, time: 21.085695]
2023-05-30 15:59:39.911: epoch 22:	0.02994570  	0.08107476  	0.06664015  
2023-05-30 15:59:39.911: Find a better model.
2023-05-30 16:00:00.834: [iter 23 : loss : 0.8285 = 0.3618 + 0.4650 + 0.0017, time: 20.920341]
2023-05-30 16:00:01.111: epoch 23:	0.03001973  	0.08195775  	0.06677327  
2023-05-30 16:00:01.111: Find a better model.
2023-05-30 16:00:22.026: [iter 24 : loss : 0.7917 = 0.3232 + 0.4664 + 0.0020, time: 20.911005]
2023-05-30 16:00:22.294: epoch 24:	0.03023442  	0.08257192  	0.06726234  
2023-05-30 16:00:22.294: Find a better model.
2023-05-30 16:00:43.238: [iter 25 : loss : 0.7602 = 0.2907 + 0.4672 + 0.0023, time: 20.938770]
2023-05-30 16:00:43.506: epoch 25:	0.03036768  	0.08308926  	0.06768861  
2023-05-30 16:00:43.506: Find a better model.
2023-05-30 16:01:04.603: [iter 26 : loss : 0.7340 = 0.2640 + 0.4675 + 0.0026, time: 21.094168]
2023-05-30 16:01:04.868: epoch 26:	0.03036768  	0.08340530  	0.06770677  
2023-05-30 16:01:04.868: Find a better model.
2023-05-30 16:01:25.791: [iter 27 : loss : 0.7114 = 0.2413 + 0.4673 + 0.0029, time: 20.920016]
2023-05-30 16:01:26.072: epoch 27:	0.03044911  	0.08395997  	0.06798638  
2023-05-30 16:01:26.072: Find a better model.
2023-05-30 16:01:47.006: [iter 28 : loss : 0.6924 = 0.2224 + 0.4669 + 0.0031, time: 20.929509]
2023-05-30 16:01:47.278: epoch 28:	0.03050833  	0.08443453  	0.06822518  
2023-05-30 16:01:47.278: Find a better model.
2023-05-30 16:02:08.223: [iter 29 : loss : 0.6760 = 0.2063 + 0.4664 + 0.0034, time: 20.941051]
2023-05-30 16:02:08.489: epoch 29:	0.03075264  	0.08460002  	0.06849788  
2023-05-30 16:02:08.489: Find a better model.
2023-05-30 16:02:29.432: [iter 30 : loss : 0.6610 = 0.1917 + 0.4657 + 0.0036, time: 20.940047]
2023-05-30 16:02:29.698: epoch 30:	0.03070822  	0.08430080  	0.06840493  
2023-05-30 16:02:50.999: [iter 31 : loss : 0.6482 = 0.1793 + 0.4651 + 0.0038, time: 21.297164]
2023-05-30 16:02:51.266: epoch 31:	0.03059716  	0.08443527  	0.06848937  
2023-05-30 16:03:12.558: [iter 32 : loss : 0.6372 = 0.1688 + 0.4644 + 0.0040, time: 21.289146]
2023-05-30 16:03:12.826: epoch 32:	0.03055274  	0.08414956  	0.06869075  
2023-05-30 16:03:33.967: [iter 33 : loss : 0.6278 = 0.1599 + 0.4637 + 0.0042, time: 21.138545]
2023-05-30 16:03:34.234: epoch 33:	0.03067119  	0.08438443  	0.06886004  
2023-05-30 16:03:55.399: [iter 34 : loss : 0.6188 = 0.1512 + 0.4632 + 0.0044, time: 21.162034]
2023-05-30 16:03:55.668: epoch 34:	0.03079704  	0.08464871  	0.06904534  
2023-05-30 16:03:55.668: Find a better model.
2023-05-30 16:04:16.974: [iter 35 : loss : 0.6102 = 0.1430 + 0.4625 + 0.0046, time: 21.302102]
2023-05-30 16:04:17.240: epoch 35:	0.03089330  	0.08503873  	0.06914957  
2023-05-30 16:04:17.240: Find a better model.
2023-05-30 16:04:38.388: [iter 36 : loss : 0.6030 = 0.1365 + 0.4618 + 0.0048, time: 21.144315]
2023-05-30 16:04:38.652: epoch 36:	0.03097473  	0.08521109  	0.06937592  
2023-05-30 16:04:38.652: Find a better model.
2023-05-30 16:04:59.950: [iter 37 : loss : 0.5968 = 0.1305 + 0.4613 + 0.0050, time: 21.294045]
2023-05-30 16:05:00.220: epoch 37:	0.03085628  	0.08492313  	0.06916193  
2023-05-30 16:05:21.543: [iter 38 : loss : 0.5905 = 0.1245 + 0.4608 + 0.0052, time: 21.319308]
2023-05-30 16:05:21.807: epoch 38:	0.03080445  	0.08485669  	0.06907971  
2023-05-30 16:05:42.924: [iter 39 : loss : 0.5853 = 0.1197 + 0.4602 + 0.0054, time: 21.113105]
2023-05-30 16:05:43.186: epoch 39:	0.03078224  	0.08462219  	0.06916284  
2023-05-30 16:06:04.171: [iter 40 : loss : 0.5801 = 0.1148 + 0.4597 + 0.0055, time: 20.981265]
2023-05-30 16:06:04.435: epoch 40:	0.03067860  	0.08446869  	0.06909434  
2023-05-30 16:06:25.547: [iter 41 : loss : 0.5750 = 0.1100 + 0.4593 + 0.0057, time: 21.108435]
2023-05-30 16:06:25.812: epoch 41:	0.03062678  	0.08431382  	0.06913123  
2023-05-30 16:06:46.932: [iter 42 : loss : 0.5706 = 0.1060 + 0.4588 + 0.0059, time: 21.117102]
2023-05-30 16:06:47.204: epoch 42:	0.03062678  	0.08439143  	0.06914342  
2023-05-30 16:07:08.548: [iter 43 : loss : 0.5666 = 0.1021 + 0.4585 + 0.0060, time: 21.341453]
2023-05-30 16:07:08.815: epoch 43:	0.03047871  	0.08380531  	0.06882507  
2023-05-30 16:07:30.328: [iter 44 : loss : 0.5637 = 0.0994 + 0.4581 + 0.0062, time: 21.509996]
2023-05-30 16:07:30.591: epoch 44:	0.03041949  	0.08333680  	0.06877020  
2023-05-30 16:07:51.735: [iter 45 : loss : 0.5600 = 0.0959 + 0.4577 + 0.0063, time: 21.141253]
2023-05-30 16:07:51.998: epoch 45:	0.03035286  	0.08314038  	0.06871482  
2023-05-30 16:08:12.898: [iter 46 : loss : 0.5571 = 0.0931 + 0.4575 + 0.0065, time: 20.897070]
2023-05-30 16:08:13.164: epoch 46:	0.03016779  	0.08239531  	0.06845723  
2023-05-30 16:08:34.301: [iter 47 : loss : 0.5534 = 0.0898 + 0.4570 + 0.0066, time: 21.132010]
2023-05-30 16:08:34.565: epoch 47:	0.03028623  	0.08220961  	0.06838158  
2023-05-30 16:08:55.642: [iter 48 : loss : 0.5507 = 0.0872 + 0.4567 + 0.0068, time: 21.073251]
2023-05-30 16:08:55.907: epoch 48:	0.03026402  	0.08192624  	0.06828203  
2023-05-30 16:09:16.853: [iter 49 : loss : 0.5483 = 0.0850 + 0.4563 + 0.0069, time: 20.941801]
2023-05-30 16:09:17.128: epoch 49:	0.03012336  	0.08184120  	0.06825091  
2023-05-30 16:09:38.036: [iter 50 : loss : 0.5462 = 0.0829 + 0.4562 + 0.0070, time: 20.905160]
2023-05-30 16:09:38.301: epoch 50:	0.03009375  	0.08186369  	0.06829128  
2023-05-30 16:09:59.221: [iter 51 : loss : 0.5433 = 0.0802 + 0.4560 + 0.0072, time: 20.917049]
2023-05-30 16:09:59.487: epoch 51:	0.03009374  	0.08152324  	0.06833672  
2023-05-30 16:10:20.647: [iter 52 : loss : 0.5405 = 0.0775 + 0.4557 + 0.0073, time: 21.157089]
2023-05-30 16:10:20.911: epoch 52:	0.02993087  	0.08065287  	0.06804390  
2023-05-30 16:10:42.047: [iter 53 : loss : 0.5397 = 0.0769 + 0.4554 + 0.0074, time: 21.133311]
2023-05-30 16:10:42.314: epoch 53:	0.02979021  	0.08051387  	0.06792397  
2023-05-30 16:11:03.419: [iter 54 : loss : 0.5376 = 0.0748 + 0.4552 + 0.0076, time: 21.102113]
2023-05-30 16:11:03.684: epoch 54:	0.02981242  	0.08074039  	0.06789593  
2023-05-30 16:11:25.002: [iter 55 : loss : 0.5356 = 0.0729 + 0.4551 + 0.0077, time: 21.314023]
2023-05-30 16:11:25.268: epoch 55:	0.02967174  	0.08018268  	0.06762917  
2023-05-30 16:11:46.390: [iter 56 : loss : 0.5335 = 0.0709 + 0.4548 + 0.0078, time: 21.118678]
2023-05-30 16:11:46.657: epoch 56:	0.02961252  	0.08012200  	0.06743786  
2023-05-30 16:12:07.581: [iter 57 : loss : 0.5316 = 0.0691 + 0.4546 + 0.0079, time: 20.920310]
2023-05-30 16:12:07.845: epoch 57:	0.02949406  	0.07970144  	0.06724986  
2023-05-30 16:12:28.784: [iter 58 : loss : 0.5302 = 0.0678 + 0.4544 + 0.0080, time: 20.935431]
2023-05-30 16:12:29.063: epoch 58:	0.02946445  	0.07948985  	0.06722693  
2023-05-30 16:12:50.376: [iter 59 : loss : 0.5284 = 0.0660 + 0.4542 + 0.0082, time: 21.309097]
2023-05-30 16:12:50.642: epoch 59:	0.02936081  	0.07936998  	0.06701729  
2023-05-30 16:13:11.600: [iter 60 : loss : 0.5274 = 0.0651 + 0.4541 + 0.0083, time: 20.954505]
2023-05-30 16:13:11.865: epoch 60:	0.02948666  	0.07923218  	0.06705327  
2023-05-30 16:13:32.823: [iter 61 : loss : 0.5260 = 0.0637 + 0.4539 + 0.0084, time: 20.953485]
2023-05-30 16:13:33.099: epoch 61:	0.02947926  	0.07894051  	0.06678183  
2023-05-30 16:13:33.099: Early stopping is trigger at epoch: 61
2023-05-30 16:13:33.099: best_result@epoch 36:

2023-05-30 16:13:33.099: 		0.0310      	0.0852      	0.0694      
2023-05-30 16:19:03.903: my pid: 11680
2023-05-30 16:19:03.903: model: model.general_recommender.SGL
2023-05-30 16:19:03.903: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 16:19:03.903: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 16:19:07.918: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 16:19:29.111: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.192590]
2023-05-30 16:19:29.388: epoch 1:	0.00131774  	0.00271001  	0.00221953  
2023-05-30 16:19:29.388: Find a better model.
2023-05-30 16:19:50.706: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.314583]
2023-05-30 16:19:50.995: epoch 2:	0.00142138  	0.00310625  	0.00259956  
2023-05-30 16:19:50.995: Find a better model.
2023-05-30 16:20:12.493: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.493049]
2023-05-30 16:20:12.793: epoch 3:	0.00203583  	0.00375826  	0.00331528  
2023-05-30 16:20:12.793: Find a better model.
2023-05-30 16:20:34.053: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.257501]
2023-05-30 16:20:34.348: epoch 4:	0.00208025  	0.00460798  	0.00345129  
2023-05-30 16:20:34.348: Find a better model.
2023-05-30 16:20:55.694: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.343120]
2023-05-30 16:20:55.988: epoch 5:	0.00282795  	0.00685647  	0.00512996  
2023-05-30 16:20:55.988: Find a better model.
2023-05-30 16:21:17.503: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.510108]
2023-05-30 16:21:17.804: epoch 6:	0.00331655  	0.00722213  	0.00584395  
2023-05-30 16:21:17.804: Find a better model.
2023-05-30 16:21:39.253: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.445063]
2023-05-30 16:21:39.549: epoch 7:	0.00396062  	0.00925946  	0.00724987  
2023-05-30 16:21:39.549: Find a better model.
2023-05-30 16:22:00.836: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.283186]
2023-05-30 16:22:01.129: epoch 8:	0.00473793  	0.01148590  	0.00928387  
2023-05-30 16:22:01.129: Find a better model.
2023-05-30 16:22:22.437: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.302887]
2023-05-30 16:22:22.742: epoch 9:	0.00569291  	0.01434855  	0.01180723  
2023-05-30 16:22:22.742: Find a better model.
2023-05-30 16:22:43.863: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.117002]
2023-05-30 16:22:44.141: epoch 10:	0.00641839  	0.01677264  	0.01389488  
2023-05-30 16:22:44.141: Find a better model.
2023-05-30 16:23:05.863: [iter 11 : loss : 1.1339 = 0.6901 + 0.4438 + 0.0000, time: 21.719040]
2023-05-30 16:23:06.152: epoch 11:	0.00781014  	0.02143826  	0.01718048  
2023-05-30 16:23:06.152: Find a better model.
2023-05-30 16:23:27.803: [iter 12 : loss : 1.1328 = 0.6884 + 0.4444 + 0.0000, time: 21.648158]
2023-05-30 16:23:28.093: epoch 12:	0.00917969  	0.02478529  	0.02039620  
2023-05-30 16:23:28.093: Find a better model.
2023-05-30 16:23:49.989: [iter 13 : loss : 1.1308 = 0.6857 + 0.4451 + 0.0000, time: 21.891842]
2023-05-30 16:23:50.277: epoch 13:	0.01144504  	0.03108247  	0.02600728  
2023-05-30 16:23:50.277: Find a better model.
2023-05-30 16:24:12.202: [iter 14 : loss : 1.1274 = 0.6815 + 0.4459 + 0.0001, time: 21.920622]
2023-05-30 16:24:12.484: epoch 14:	0.01473205  	0.04021747  	0.03388812  
2023-05-30 16:24:12.484: Find a better model.
2023-05-30 16:24:34.192: [iter 15 : loss : 1.1212 = 0.6745 + 0.4467 + 0.0001, time: 21.704624]
2023-05-30 16:24:34.453: epoch 15:	0.01801165  	0.04850266  	0.04139964  
2023-05-30 16:24:34.453: Find a better model.
2023-05-30 16:24:56.195: [iter 16 : loss : 1.1099 = 0.6622 + 0.4476 + 0.0002, time: 21.738754]
2023-05-30 16:24:56.477: epoch 16:	0.02118023  	0.05688530  	0.04825964  
2023-05-30 16:24:56.477: Find a better model.
2023-05-30 16:25:18.171: [iter 17 : loss : 1.0913 = 0.6421 + 0.4489 + 0.0002, time: 21.690394]
2023-05-30 16:25:18.447: epoch 17:	0.02387501  	0.06305332  	0.05423879  
2023-05-30 16:25:18.447: Find a better model.
2023-05-30 16:25:40.165: [iter 18 : loss : 1.0620 = 0.6109 + 0.4506 + 0.0004, time: 21.714052]
2023-05-30 16:25:40.447: epoch 18:	0.02561478  	0.06839725  	0.05816112  
2023-05-30 16:25:40.447: Find a better model.
2023-05-30 16:26:02.158: [iter 19 : loss : 1.0224 = 0.5689 + 0.4530 + 0.0006, time: 21.705602]
2023-05-30 16:26:02.438: epoch 19:	0.02696217  	0.07252885  	0.06149409  
2023-05-30 16:26:02.438: Find a better model.
2023-05-30 16:26:24.333: [iter 20 : loss : 0.9752 = 0.5185 + 0.4559 + 0.0008, time: 21.890084]
2023-05-30 16:26:24.608: epoch 20:	0.02810968  	0.07537892  	0.06392586  
2023-05-30 16:26:24.608: Find a better model.
2023-05-30 16:26:46.175: [iter 21 : loss : 0.9255 = 0.4652 + 0.4592 + 0.0011, time: 21.562550]
2023-05-30 16:26:46.456: epoch 21:	0.02846505  	0.07681388  	0.06480287  
2023-05-30 16:26:46.456: Find a better model.
2023-05-30 16:27:08.155: [iter 22 : loss : 0.8766 = 0.4131 + 0.4621 + 0.0014, time: 21.696034]
2023-05-30 16:27:08.434: epoch 22:	0.02896847  	0.07825187  	0.06548374  
2023-05-30 16:27:08.434: Find a better model.
2023-05-30 16:27:30.104: [iter 23 : loss : 0.8336 = 0.3672 + 0.4647 + 0.0017, time: 21.666917]
2023-05-30 16:27:30.382: epoch 23:	0.02909433  	0.07901412  	0.06572407  
2023-05-30 16:27:30.382: Find a better model.
2023-05-30 16:27:52.104: [iter 24 : loss : 0.7957 = 0.3277 + 0.4660 + 0.0020, time: 21.718822]
2023-05-30 16:27:52.375: epoch 24:	0.02923499  	0.07972630  	0.06603374  
2023-05-30 16:27:52.375: Find a better model.
2023-05-30 16:28:14.075: [iter 25 : loss : 0.7635 = 0.2944 + 0.4669 + 0.0023, time: 21.696630]
2023-05-30 16:28:14.343: epoch 25:	0.02944969  	0.08026857  	0.06632394  
2023-05-30 16:28:14.343: Find a better model.
2023-05-30 16:28:36.061: [iter 26 : loss : 0.7370 = 0.2671 + 0.4673 + 0.0026, time: 21.714071]
2023-05-30 16:28:36.330: epoch 26:	0.02958295  	0.08094128  	0.06670110  
2023-05-30 16:28:36.330: Find a better model.
2023-05-30 16:28:58.077: [iter 27 : loss : 0.7142 = 0.2442 + 0.4671 + 0.0028, time: 21.743103]
2023-05-30 16:28:58.347: epoch 27:	0.02950151  	0.08093877  	0.06661324  
2023-05-30 16:29:20.080: [iter 28 : loss : 0.6947 = 0.2248 + 0.4667 + 0.0031, time: 21.729025]
2023-05-30 16:29:20.350: epoch 28:	0.02967178  	0.08157295  	0.06702184  
2023-05-30 16:29:20.350: Find a better model.
2023-05-30 16:29:41.859: [iter 29 : loss : 0.6780 = 0.2085 + 0.4662 + 0.0033, time: 21.505354]
2023-05-30 16:29:42.127: epoch 29:	0.02960516  	0.08159290  	0.06718831  
2023-05-30 16:29:42.127: Find a better model.
2023-05-30 16:30:04.055: [iter 30 : loss : 0.6628 = 0.1936 + 0.4656 + 0.0036, time: 21.923594]
2023-05-30 16:30:04.323: epoch 30:	0.02971621  	0.08202451  	0.06740097  
2023-05-30 16:30:04.323: Find a better model.
2023-05-30 16:30:26.074: [iter 31 : loss : 0.6499 = 0.1811 + 0.4649 + 0.0038, time: 21.746087]
2023-05-30 16:30:26.344: epoch 31:	0.02976063  	0.08193122  	0.06745957  
2023-05-30 16:30:48.233: [iter 32 : loss : 0.6387 = 0.1704 + 0.4643 + 0.0040, time: 21.886602]
2023-05-30 16:30:48.501: epoch 32:	0.02976803  	0.08212139  	0.06740293  
2023-05-30 16:30:48.501: Find a better model.
2023-05-30 16:31:10.457: [iter 33 : loss : 0.6292 = 0.1614 + 0.4636 + 0.0042, time: 21.950137]
2023-05-30 16:31:10.743: epoch 33:	0.02992350  	0.08217335  	0.06766658  
2023-05-30 16:31:10.743: Find a better model.
2023-05-30 16:31:32.636: [iter 34 : loss : 0.6198 = 0.1525 + 0.4629 + 0.0044, time: 21.888173]
2023-05-30 16:31:32.909: epoch 34:	0.02979764  	0.08181605  	0.06750522  
2023-05-30 16:31:54.603: [iter 35 : loss : 0.6112 = 0.1443 + 0.4623 + 0.0046, time: 21.690163]
2023-05-30 16:31:54.876: epoch 35:	0.02979764  	0.08155647  	0.06745174  
2023-05-30 16:32:16.809: [iter 36 : loss : 0.6041 = 0.1376 + 0.4617 + 0.0048, time: 21.928310]
2023-05-30 16:32:17.081: epoch 36:	0.02973101  	0.08097813  	0.06720302  
2023-05-30 16:32:38.825: [iter 37 : loss : 0.5977 = 0.1315 + 0.4612 + 0.0050, time: 21.741160]
2023-05-30 16:32:39.096: epoch 37:	0.02981985  	0.08102752  	0.06744119  
2023-05-30 16:33:01.024: [iter 38 : loss : 0.5912 = 0.1254 + 0.4606 + 0.0052, time: 21.924969]
2023-05-30 16:33:01.283: epoch 38:	0.02981984  	0.08039292  	0.06721137  
2023-05-30 16:33:23.382: [iter 39 : loss : 0.5859 = 0.1205 + 0.4600 + 0.0054, time: 22.095149]
2023-05-30 16:33:23.649: epoch 39:	0.02983465  	0.08073729  	0.06726686  
2023-05-30 16:33:45.402: [iter 40 : loss : 0.5805 = 0.1155 + 0.4595 + 0.0055, time: 21.750296]
2023-05-30 16:33:45.674: epoch 40:	0.02973101  	0.08033016  	0.06707763  
2023-05-30 16:34:07.404: [iter 41 : loss : 0.5757 = 0.1109 + 0.4591 + 0.0057, time: 21.726819]
2023-05-30 16:34:07.673: epoch 41:	0.02980504  	0.08074251  	0.06720257  
2023-05-30 16:34:29.563: [iter 42 : loss : 0.5711 = 0.1066 + 0.4586 + 0.0059, time: 21.886302]
2023-05-30 16:34:29.834: epoch 42:	0.02977543  	0.08059106  	0.06729985  
2023-05-30 16:34:51.746: [iter 43 : loss : 0.5672 = 0.1029 + 0.4583 + 0.0060, time: 21.907107]
2023-05-30 16:34:52.012: epoch 43:	0.02976802  	0.08049185  	0.06719434  
2023-05-30 16:35:13.715: [iter 44 : loss : 0.5644 = 0.1003 + 0.4579 + 0.0062, time: 21.698545]
2023-05-30 16:35:13.989: epoch 44:	0.02962736  	0.08013203  	0.06717338  
2023-05-30 16:35:35.715: [iter 45 : loss : 0.5605 = 0.0967 + 0.4575 + 0.0063, time: 21.723199]
2023-05-30 16:35:35.983: epoch 45:	0.02972360  	0.08018166  	0.06710853  
2023-05-30 16:35:57.562: [iter 46 : loss : 0.5571 = 0.0935 + 0.4572 + 0.0065, time: 21.576181]
2023-05-30 16:35:57.836: epoch 46:	0.02968658  	0.08020082  	0.06694768  
2023-05-30 16:36:19.328: [iter 47 : loss : 0.5536 = 0.0902 + 0.4568 + 0.0066, time: 21.489031]
2023-05-30 16:36:19.599: epoch 47:	0.02957552  	0.07943971  	0.06652884  
2023-05-30 16:36:41.313: [iter 48 : loss : 0.5513 = 0.0880 + 0.4565 + 0.0068, time: 21.710790]
2023-05-30 16:36:41.582: epoch 48:	0.02967917  	0.07981067  	0.06666233  
2023-05-30 16:37:03.346: [iter 49 : loss : 0.5486 = 0.0855 + 0.4562 + 0.0069, time: 21.760027]
2023-05-30 16:37:03.622: epoch 49:	0.02947929  	0.07938207  	0.06626103  
2023-05-30 16:37:25.493: [iter 50 : loss : 0.5465 = 0.0835 + 0.4560 + 0.0070, time: 21.867474]
2023-05-30 16:37:25.769: epoch 50:	0.02947188  	0.07907709  	0.06612037  
2023-05-30 16:37:47.530: [iter 51 : loss : 0.5434 = 0.0805 + 0.4557 + 0.0072, time: 21.755607]
2023-05-30 16:37:47.800: epoch 51:	0.02942746  	0.07876528  	0.06596206  
2023-05-30 16:38:09.499: [iter 52 : loss : 0.5410 = 0.0782 + 0.4555 + 0.0073, time: 21.696141]
2023-05-30 16:38:09.779: epoch 52:	0.02931641  	0.07868541  	0.06582293  
2023-05-30 16:38:31.312: [iter 53 : loss : 0.5400 = 0.0774 + 0.4552 + 0.0074, time: 21.528588]
2023-05-30 16:38:31.580: epoch 53:	0.02916094  	0.07803435  	0.06543252  
2023-05-30 16:38:53.300: [iter 54 : loss : 0.5379 = 0.0754 + 0.4549 + 0.0076, time: 21.716490]
2023-05-30 16:38:53.566: epoch 54:	0.02915354  	0.07792085  	0.06530198  
2023-05-30 16:39:15.114: [iter 55 : loss : 0.5358 = 0.0733 + 0.4548 + 0.0077, time: 21.544045]
2023-05-30 16:39:15.381: epoch 55:	0.02904990  	0.07782105  	0.06506331  
2023-05-30 16:39:37.065: [iter 56 : loss : 0.5334 = 0.0710 + 0.4546 + 0.0078, time: 21.679286]
2023-05-30 16:39:37.331: epoch 56:	0.02899067  	0.07710550  	0.06483610  
2023-05-30 16:39:59.078: [iter 57 : loss : 0.5317 = 0.0694 + 0.4543 + 0.0079, time: 21.744027]
2023-05-30 16:39:59.344: epoch 57:	0.02884260  	0.07692195  	0.06461910  
2023-05-30 16:40:21.029: [iter 58 : loss : 0.5305 = 0.0682 + 0.4542 + 0.0080, time: 21.681015]
2023-05-30 16:40:21.294: epoch 58:	0.02879078  	0.07661971  	0.06451403  
2023-05-30 16:40:21.294: Early stopping is trigger at epoch: 58
2023-05-30 16:40:21.294: best_result@epoch 33:

2023-05-30 16:40:21.294: 		0.0299      	0.0822      	0.0677      
2023-05-30 16:50:37.487: my pid: 9472
2023-05-30 16:50:37.487: model: model.general_recommender.SGL
2023-05-30 16:50:37.487: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 16:50:37.487: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 16:50:41.660: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 16:51:02.898: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.237923]
2023-05-30 16:51:03.204: epoch 1:	0.00134735  	0.00257531  	0.00218738  
2023-05-30 16:51:03.204: Find a better model.
2023-05-30 16:51:23.582: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.374012]
2023-05-30 16:51:23.873: epoch 2:	0.00165087  	0.00333653  	0.00281177  
2023-05-30 16:51:23.873: Find a better model.
2023-05-30 16:51:44.215: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.339275]
2023-05-30 16:51:44.510: epoch 3:	0.00208025  	0.00488818  	0.00377989  
2023-05-30 16:51:44.510: Find a better model.
2023-05-30 16:52:04.957: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.444453]
2023-05-30 16:52:05.250: epoch 4:	0.00228754  	0.00482927  	0.00426536  
2023-05-30 16:52:25.774: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.520450]
2023-05-30 16:52:26.054: epoch 5:	0.00313148  	0.00693002  	0.00532840  
2023-05-30 16:52:26.055: Find a better model.
2023-05-30 16:52:46.553: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.495038]
2023-05-30 16:52:46.847: epoch 6:	0.00345721  	0.00852081  	0.00692000  
2023-05-30 16:52:46.847: Find a better model.
2023-05-30 16:53:07.365: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.515248]
2023-05-30 16:53:07.656: epoch 7:	0.00393841  	0.00974718  	0.00777298  
2023-05-30 16:53:07.656: Find a better model.
2023-05-30 16:53:27.895: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.235946]
2023-05-30 16:53:28.190: epoch 8:	0.00464909  	0.01282058  	0.00977537  
2023-05-30 16:53:28.190: Find a better model.
2023-05-30 16:53:48.522: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.327013]
2023-05-30 16:53:48.807: epoch 9:	0.00537458  	0.01509071  	0.01143250  
2023-05-30 16:53:48.807: Find a better model.
2023-05-30 16:54:09.152: [iter 10 : loss : 1.1344 = 0.6908 + 0.4435 + 0.0000, time: 20.340170]
2023-05-30 16:54:09.433: epoch 10:	0.00586318  	0.01641391  	0.01295427  
2023-05-30 16:54:09.433: Find a better model.
2023-05-30 16:54:30.512: [iter 11 : loss : 1.1338 = 0.6896 + 0.4442 + 0.0000, time: 21.074791]
2023-05-30 16:54:30.796: epoch 11:	0.00688478  	0.02017885  	0.01609604  
2023-05-30 16:54:30.796: Find a better model.
2023-05-30 16:54:51.904: [iter 12 : loss : 1.1324 = 0.6877 + 0.4447 + 0.0000, time: 21.104641]
2023-05-30 16:54:52.190: epoch 12:	0.00872071  	0.02553624  	0.02075295  
2023-05-30 16:54:52.190: Find a better model.
2023-05-30 16:55:13.117: [iter 13 : loss : 1.1304 = 0.6850 + 0.4454 + 0.0001, time: 20.922086]
2023-05-30 16:55:13.398: epoch 13:	0.01077878  	0.03126983  	0.02593843  
2023-05-30 16:55:13.398: Find a better model.
2023-05-30 16:55:34.135: [iter 14 : loss : 1.1266 = 0.6805 + 0.4461 + 0.0001, time: 20.731591]
2023-05-30 16:55:34.409: epoch 14:	0.01385847  	0.03963701  	0.03274507  
2023-05-30 16:55:34.409: Find a better model.
2023-05-30 16:55:55.142: [iter 15 : loss : 1.1198 = 0.6727 + 0.4470 + 0.0001, time: 20.728620]
2023-05-30 16:55:55.416: epoch 15:	0.01727134  	0.04891167  	0.04115500  
2023-05-30 16:55:55.416: Find a better model.
2023-05-30 16:56:16.108: [iter 16 : loss : 1.1075 = 0.6593 + 0.4481 + 0.0002, time: 20.689173]
2023-05-30 16:56:16.381: epoch 16:	0.02086190  	0.05817496  	0.04912736  
2023-05-30 16:56:16.381: Find a better model.
2023-05-30 16:56:36.854: [iter 17 : loss : 1.0872 = 0.6375 + 0.4494 + 0.0003, time: 20.469412]
2023-05-30 16:56:37.147: epoch 17:	0.02357890  	0.06495874  	0.05519296  
2023-05-30 16:56:37.148: Find a better model.
2023-05-30 16:56:57.859: [iter 18 : loss : 1.0560 = 0.6043 + 0.4513 + 0.0004, time: 20.707139]
2023-05-30 16:56:58.148: epoch 18:	0.02556298  	0.06919236  	0.05912412  
2023-05-30 16:56:58.148: Find a better model.
2023-05-30 16:57:19.034: [iter 19 : loss : 1.0151 = 0.5606 + 0.4539 + 0.0006, time: 20.882361]
2023-05-30 16:57:19.312: epoch 19:	0.02733976  	0.07360283  	0.06305857  
2023-05-30 16:57:19.312: Find a better model.
2023-05-30 16:57:40.059: [iter 20 : loss : 0.9673 = 0.5095 + 0.4569 + 0.0009, time: 20.743031]
2023-05-30 16:57:40.336: epoch 20:	0.02830217  	0.07645787  	0.06512719  
2023-05-30 16:57:40.336: Find a better model.
2023-05-30 16:58:01.044: [iter 21 : loss : 0.9176 = 0.4563 + 0.4602 + 0.0011, time: 20.704031]
2023-05-30 16:58:01.319: epoch 21:	0.02895367  	0.07799258  	0.06580301  
2023-05-30 16:58:01.319: Find a better model.
2023-05-30 16:58:21.998: [iter 22 : loss : 0.8694 = 0.4050 + 0.4630 + 0.0014, time: 20.675065]
2023-05-30 16:58:22.271: epoch 22:	0.02917578  	0.07888988  	0.06618117  
2023-05-30 16:58:22.271: Find a better model.
2023-05-30 16:58:42.863: [iter 23 : loss : 0.8273 = 0.3603 + 0.4653 + 0.0017, time: 20.587577]
2023-05-30 16:58:43.147: epoch 23:	0.02943490  	0.08025572  	0.06671831  
2023-05-30 16:58:43.147: Find a better model.
2023-05-30 16:59:03.794: [iter 24 : loss : 0.7912 = 0.3222 + 0.4669 + 0.0020, time: 20.641331]
2023-05-30 16:59:04.064: epoch 24:	0.02944971  	0.08122410  	0.06708942  
2023-05-30 16:59:04.064: Find a better model.
2023-05-30 16:59:24.773: [iter 25 : loss : 0.7599 = 0.2900 + 0.4675 + 0.0023, time: 20.705339]
2023-05-30 16:59:25.037: epoch 25:	0.02960518  	0.08179011  	0.06733576  
2023-05-30 16:59:25.037: Find a better model.
2023-05-30 16:59:45.775: [iter 26 : loss : 0.7339 = 0.2635 + 0.4678 + 0.0026, time: 20.735085]
2023-05-30 16:59:46.042: epoch 26:	0.02970882  	0.08210824  	0.06765924  
2023-05-30 16:59:46.042: Find a better model.
2023-05-30 17:00:06.974: [iter 27 : loss : 0.7117 = 0.2411 + 0.4678 + 0.0029, time: 20.928165]
2023-05-30 17:00:07.244: epoch 27:	0.02976805  	0.08275904  	0.06782660  
2023-05-30 17:00:07.244: Find a better model.
2023-05-30 17:00:27.968: [iter 28 : loss : 0.6928 = 0.2224 + 0.4673 + 0.0031, time: 20.720084]
2023-05-30 17:00:28.236: epoch 28:	0.02963480  	0.08207218  	0.06774846  
2023-05-30 17:00:49.146: [iter 29 : loss : 0.6766 = 0.2064 + 0.4668 + 0.0034, time: 20.906070]
2023-05-30 17:00:49.412: epoch 29:	0.02972364  	0.08268357  	0.06794600  
2023-05-30 17:01:10.150: [iter 30 : loss : 0.6615 = 0.1920 + 0.4660 + 0.0036, time: 20.734822]
2023-05-30 17:01:10.416: epoch 30:	0.02973104  	0.08304126  	0.06824820  
2023-05-30 17:01:10.416: Find a better model.
2023-05-30 17:01:30.952: [iter 31 : loss : 0.6489 = 0.1796 + 0.4655 + 0.0038, time: 20.532689]
2023-05-30 17:01:31.218: epoch 31:	0.02973104  	0.08343651  	0.06827351  
2023-05-30 17:01:31.218: Find a better model.
2023-05-30 17:01:51.925: [iter 32 : loss : 0.6379 = 0.1691 + 0.4647 + 0.0040, time: 20.703168]
2023-05-30 17:01:52.197: epoch 32:	0.02983468  	0.08330963  	0.06857936  
2023-05-30 17:02:13.146: [iter 33 : loss : 0.6285 = 0.1602 + 0.4640 + 0.0042, time: 20.946114]
2023-05-30 17:02:13.414: epoch 33:	0.02987169  	0.08342780  	0.06886267  
2023-05-30 17:02:34.330: [iter 34 : loss : 0.6196 = 0.1517 + 0.4635 + 0.0044, time: 20.912080]
2023-05-30 17:02:34.597: epoch 34:	0.03004937  	0.08382567  	0.06897399  
2023-05-30 17:02:34.597: Find a better model.
2023-05-30 17:02:55.165: [iter 35 : loss : 0.6107 = 0.1433 + 0.4627 + 0.0046, time: 20.564143]
2023-05-30 17:02:55.429: epoch 35:	0.03008639  	0.08429243  	0.06905802  
2023-05-30 17:02:55.429: Find a better model.
2023-05-30 17:03:16.143: [iter 36 : loss : 0.6040 = 0.1370 + 0.4622 + 0.0048, time: 20.708398]
2023-05-30 17:03:16.410: epoch 36:	0.03004938  	0.08376329  	0.06900581  
2023-05-30 17:03:37.144: [iter 37 : loss : 0.5977 = 0.1310 + 0.4617 + 0.0050, time: 20.730284]
2023-05-30 17:03:37.409: epoch 37:	0.03020484  	0.08405643  	0.06926133  
2023-05-30 17:03:58.099: [iter 38 : loss : 0.5911 = 0.1249 + 0.4610 + 0.0052, time: 20.686023]
2023-05-30 17:03:58.364: epoch 38:	0.03015302  	0.08357077  	0.06911898  
2023-05-30 17:04:19.118: [iter 39 : loss : 0.5861 = 0.1202 + 0.4605 + 0.0054, time: 20.750154]
2023-05-30 17:04:19.383: epoch 39:	0.03004198  	0.08310880  	0.06888695  
2023-05-30 17:04:40.127: [iter 40 : loss : 0.5806 = 0.1150 + 0.4601 + 0.0055, time: 20.739981]
2023-05-30 17:04:40.392: epoch 40:	0.03000496  	0.08324781  	0.06881861  
2023-05-30 17:05:01.105: [iter 41 : loss : 0.5756 = 0.1103 + 0.4596 + 0.0057, time: 20.709106]
2023-05-30 17:05:01.371: epoch 41:	0.03000495  	0.08341842  	0.06878032  
2023-05-30 17:05:22.120: [iter 42 : loss : 0.5713 = 0.1062 + 0.4592 + 0.0059, time: 20.745669]
2023-05-30 17:05:22.386: epoch 42:	0.02993832  	0.08328219  	0.06881210  
2023-05-30 17:05:43.044: [iter 43 : loss : 0.5673 = 0.1025 + 0.4588 + 0.0060, time: 20.655119]
2023-05-30 17:05:43.313: epoch 43:	0.02993092  	0.08290029  	0.06863827  
2023-05-30 17:06:04.029: [iter 44 : loss : 0.5649 = 0.1003 + 0.4585 + 0.0062, time: 20.712044]
2023-05-30 17:06:04.296: epoch 44:	0.02990871  	0.08285283  	0.06867231  
2023-05-30 17:06:25.040: [iter 45 : loss : 0.5605 = 0.0960 + 0.4581 + 0.0063, time: 20.739921]
2023-05-30 17:06:25.307: epoch 45:	0.02984948  	0.08293457  	0.06871702  
2023-05-30 17:06:45.903: [iter 46 : loss : 0.5572 = 0.0931 + 0.4577 + 0.0065, time: 20.593167]
2023-05-30 17:06:46.177: epoch 46:	0.02984948  	0.08319467  	0.06872769  
2023-05-30 17:07:06.885: [iter 47 : loss : 0.5542 = 0.0901 + 0.4575 + 0.0066, time: 20.703253]
2023-05-30 17:07:07.160: epoch 47:	0.02984207  	0.08286561  	0.06853265  
2023-05-30 17:07:28.060: [iter 48 : loss : 0.5516 = 0.0878 + 0.4571 + 0.0068, time: 20.897039]
2023-05-30 17:07:28.328: epoch 48:	0.02976064  	0.08246265  	0.06837760  
2023-05-30 17:07:49.024: [iter 49 : loss : 0.5487 = 0.0851 + 0.4566 + 0.0069, time: 20.692469]
2023-05-30 17:07:49.293: epoch 49:	0.02972361  	0.08212584  	0.06834603  
2023-05-30 17:08:09.831: [iter 50 : loss : 0.5466 = 0.0832 + 0.4564 + 0.0070, time: 20.532072]
2023-05-30 17:08:10.100: epoch 50:	0.02964217  	0.08159222  	0.06807693  
2023-05-30 17:08:30.851: [iter 51 : loss : 0.5439 = 0.0806 + 0.4562 + 0.0072, time: 20.747134]
2023-05-30 17:08:31.120: epoch 51:	0.02961255  	0.08143733  	0.06806774  
2023-05-30 17:08:52.000: [iter 52 : loss : 0.5411 = 0.0779 + 0.4559 + 0.0073, time: 20.872063]
2023-05-30 17:08:52.270: epoch 52:	0.02951631  	0.08143672  	0.06788751  
2023-05-30 17:09:12.987: [iter 53 : loss : 0.5402 = 0.0770 + 0.4557 + 0.0074, time: 20.714161]
2023-05-30 17:09:13.259: epoch 53:	0.02951631  	0.08142706  	0.06781162  
2023-05-30 17:09:33.975: [iter 54 : loss : 0.5381 = 0.0750 + 0.4555 + 0.0076, time: 20.711623]
2023-05-30 17:09:34.248: epoch 54:	0.02939785  	0.08087140  	0.06751873  
2023-05-30 17:09:54.998: [iter 55 : loss : 0.5359 = 0.0730 + 0.4552 + 0.0077, time: 20.747289]
2023-05-30 17:09:55.269: epoch 55:	0.02931642  	0.08043649  	0.06725337  
2023-05-30 17:10:16.169: [iter 56 : loss : 0.5340 = 0.0712 + 0.4550 + 0.0078, time: 20.897586]
2023-05-30 17:10:16.436: epoch 56:	0.02933863  	0.08059175  	0.06718469  
2023-05-30 17:10:37.145: [iter 57 : loss : 0.5321 = 0.0694 + 0.4549 + 0.0079, time: 20.705215]
2023-05-30 17:10:37.409: epoch 57:	0.02929421  	0.08024787  	0.06701501  
2023-05-30 17:10:58.143: [iter 58 : loss : 0.5308 = 0.0680 + 0.4548 + 0.0080, time: 20.731573]
2023-05-30 17:10:58.407: epoch 58:	0.02924979  	0.08013984  	0.06693261  
2023-05-30 17:11:18.927: [iter 59 : loss : 0.5292 = 0.0665 + 0.4545 + 0.0082, time: 20.516228]
2023-05-30 17:11:19.198: epoch 59:	0.02913134  	0.07999334  	0.06684176  
2023-05-30 17:11:39.928: [iter 60 : loss : 0.5282 = 0.0656 + 0.4544 + 0.0083, time: 20.727027]
2023-05-30 17:11:40.200: epoch 60:	0.02905730  	0.07939188  	0.06651347  
2023-05-30 17:11:40.201: Early stopping is trigger at epoch: 60
2023-05-30 17:11:40.201: best_result@epoch 35:

2023-05-30 17:11:40.201: 		0.0301      	0.0843      	0.0691      
2023-05-30 17:18:16.778: my pid: 15368
2023-05-30 17:18:16.778: model: model.general_recommender.SGL
2023-05-30 17:18:16.778: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 17:18:16.778: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 17:18:20.730: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 17:18:41.410: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.679161]
2023-05-30 17:18:41.675: epoch 1:	0.00116968  	0.00239628  	0.00200123  
2023-05-30 17:18:41.675: Find a better model.
2023-05-30 17:19:02.249: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.570710]
2023-05-30 17:19:02.531: epoch 2:	0.00203583  	0.00378073  	0.00307750  
2023-05-30 17:19:02.531: Find a better model.
2023-05-30 17:19:23.209: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.675209]
2023-05-30 17:19:23.501: epoch 3:	0.00197661  	0.00415934  	0.00384676  
2023-05-30 17:19:23.501: Find a better model.
2023-05-30 17:19:43.997: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.492498]
2023-05-30 17:19:44.291: epoch 4:	0.00228754  	0.00432986  	0.00357868  
2023-05-30 17:19:44.291: Find a better model.
2023-05-30 17:20:04.740: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.440384]
2023-05-30 17:20:05.027: epoch 5:	0.00282795  	0.00562049  	0.00482064  
2023-05-30 17:20:05.027: Find a better model.
2023-05-30 17:20:25.375: [iter 6 : loss : 1.1346 = 0.6925 + 0.4420 + 0.0000, time: 20.345011]
2023-05-30 17:20:25.667: epoch 6:	0.00316849  	0.00747804  	0.00596360  
2023-05-30 17:20:25.667: Find a better model.
2023-05-30 17:20:46.127: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.457200]
2023-05-30 17:20:46.424: epoch 7:	0.00359787  	0.00831012  	0.00693833  
2023-05-30 17:20:46.424: Find a better model.
2023-05-30 17:21:06.938: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.511284]
2023-05-30 17:21:07.222: epoch 8:	0.00433817  	0.01074012  	0.00873781  
2023-05-30 17:21:07.222: Find a better model.
2023-05-30 17:21:27.553: [iter 9 : loss : 1.1346 = 0.6915 + 0.4430 + 0.0000, time: 20.327734]
2023-05-30 17:21:27.840: epoch 9:	0.00490820  	0.01295017  	0.01028293  
2023-05-30 17:21:27.840: Find a better model.
2023-05-30 17:21:48.131: [iter 10 : loss : 1.1345 = 0.6908 + 0.4436 + 0.0000, time: 20.287029]
2023-05-30 17:21:48.415: epoch 10:	0.00527094  	0.01445080  	0.01142647  
2023-05-30 17:21:48.415: Find a better model.
2023-05-30 17:22:09.352: [iter 11 : loss : 1.1338 = 0.6896 + 0.4441 + 0.0000, time: 20.932719]
2023-05-30 17:22:09.635: epoch 11:	0.00646281  	0.01860618  	0.01440425  
2023-05-30 17:22:09.635: Find a better model.
2023-05-30 17:22:30.509: [iter 12 : loss : 1.1325 = 0.6877 + 0.4447 + 0.0000, time: 20.870818]
2023-05-30 17:22:30.790: epoch 12:	0.00821731  	0.02256173  	0.01876342  
2023-05-30 17:22:30.790: Find a better model.
2023-05-30 17:22:51.685: [iter 13 : loss : 1.1304 = 0.6850 + 0.4453 + 0.0001, time: 20.891155]
2023-05-30 17:22:51.968: epoch 13:	0.01050486  	0.02976885  	0.02477117  
2023-05-30 17:22:51.968: Find a better model.
2023-05-30 17:23:12.889: [iter 14 : loss : 1.1268 = 0.6805 + 0.4461 + 0.0001, time: 20.916621]
2023-05-30 17:23:13.165: epoch 14:	0.01336245  	0.03652759  	0.03165090  
2023-05-30 17:23:13.165: Find a better model.
2023-05-30 17:23:34.076: [iter 15 : loss : 1.1200 = 0.6729 + 0.4470 + 0.0001, time: 20.905693]
2023-05-30 17:23:34.363: epoch 15:	0.01699002  	0.04516105  	0.03903812  
2023-05-30 17:23:34.363: Find a better model.
2023-05-30 17:23:55.256: [iter 16 : loss : 1.1078 = 0.6597 + 0.4480 + 0.0002, time: 20.889369]
2023-05-30 17:23:55.531: epoch 16:	0.02043252  	0.05430775  	0.04654848  
2023-05-30 17:23:55.531: Find a better model.
2023-05-30 17:24:16.650: [iter 17 : loss : 1.0878 = 0.6381 + 0.4494 + 0.0003, time: 21.114053]
2023-05-30 17:24:16.927: epoch 17:	0.02326797  	0.06191768  	0.05299786  
2023-05-30 17:24:16.927: Find a better model.
2023-05-30 17:24:37.845: [iter 18 : loss : 1.0567 = 0.6050 + 0.4513 + 0.0004, time: 20.915392]
2023-05-30 17:24:38.122: epoch 18:	0.02543712  	0.06790315  	0.05743519  
2023-05-30 17:24:38.122: Find a better model.
2023-05-30 17:24:58.885: [iter 19 : loss : 1.0158 = 0.5614 + 0.4538 + 0.0006, time: 20.759140]
2023-05-30 17:24:59.161: epoch 19:	0.02722130  	0.07147637  	0.06059688  
2023-05-30 17:24:59.161: Find a better model.
2023-05-30 17:25:19.845: [iter 20 : loss : 0.9678 = 0.5099 + 0.4570 + 0.0008, time: 20.679183]
2023-05-30 17:25:20.119: epoch 20:	0.02821333  	0.07468385  	0.06240429  
2023-05-30 17:25:20.119: Find a better model.
2023-05-30 17:25:40.824: [iter 21 : loss : 0.9180 = 0.4565 + 0.4604 + 0.0011, time: 20.701126]
2023-05-30 17:25:41.096: epoch 21:	0.02891664  	0.07659083  	0.06359816  
2023-05-30 17:25:41.096: Find a better model.
2023-05-30 17:26:01.854: [iter 22 : loss : 0.8699 = 0.4051 + 0.4633 + 0.0014, time: 20.754130]
2023-05-30 17:26:02.126: epoch 22:	0.02902030  	0.07761189  	0.06424951  
2023-05-30 17:26:02.126: Find a better model.
2023-05-30 17:26:23.050: [iter 23 : loss : 0.8274 = 0.3602 + 0.4655 + 0.0017, time: 20.920063]
2023-05-30 17:26:23.336: epoch 23:	0.02909433  	0.07841853  	0.06440839  
2023-05-30 17:26:23.336: Find a better model.
2023-05-30 17:26:44.238: [iter 24 : loss : 0.7912 = 0.3220 + 0.4672 + 0.0020, time: 20.898446]
2023-05-30 17:26:44.510: epoch 24:	0.02929422  	0.07914007  	0.06463509  
2023-05-30 17:26:44.510: Find a better model.
2023-05-30 17:27:05.417: [iter 25 : loss : 0.7600 = 0.2899 + 0.4678 + 0.0023, time: 20.901762]
2023-05-30 17:27:05.687: epoch 25:	0.02939787  	0.07939089  	0.06486852  
2023-05-30 17:27:05.687: Find a better model.
2023-05-30 17:27:26.419: [iter 26 : loss : 0.7343 = 0.2638 + 0.4679 + 0.0026, time: 20.728156]
2023-05-30 17:27:26.686: epoch 26:	0.02940528  	0.07990987  	0.06519275  
2023-05-30 17:27:26.686: Find a better model.
2023-05-30 17:27:47.436: [iter 27 : loss : 0.7116 = 0.2410 + 0.4677 + 0.0029, time: 20.746591]
2023-05-30 17:27:47.703: epoch 27:	0.02960516  	0.08013047  	0.06534029  
2023-05-30 17:27:47.703: Find a better model.
2023-05-30 17:28:08.407: [iter 28 : loss : 0.6927 = 0.2223 + 0.4673 + 0.0031, time: 20.701361]
2023-05-30 17:28:08.674: epoch 28:	0.02970139  	0.08027100  	0.06538998  
2023-05-30 17:28:08.674: Find a better model.
2023-05-30 17:28:29.389: [iter 29 : loss : 0.6766 = 0.2064 + 0.4668 + 0.0034, time: 20.711280]
2023-05-30 17:28:29.656: epoch 29:	0.02982725  	0.08060168  	0.06552501  
2023-05-30 17:28:29.656: Find a better model.
2023-05-30 17:28:50.571: [iter 30 : loss : 0.6618 = 0.1922 + 0.4661 + 0.0036, time: 20.911990]
2023-05-30 17:28:50.839: epoch 30:	0.02993088  	0.08078300  	0.06572260  
2023-05-30 17:28:50.839: Find a better model.
2023-05-30 17:29:11.984: [iter 31 : loss : 0.6489 = 0.1797 + 0.4654 + 0.0038, time: 21.141091]
2023-05-30 17:29:12.249: epoch 31:	0.02979763  	0.08069624  	0.06577850  
2023-05-30 17:29:33.160: [iter 32 : loss : 0.6379 = 0.1691 + 0.4647 + 0.0040, time: 20.907064]
2023-05-30 17:29:33.425: epoch 32:	0.02979763  	0.08067472  	0.06580909  
2023-05-30 17:29:54.355: [iter 33 : loss : 0.6285 = 0.1602 + 0.4641 + 0.0042, time: 20.925208]
2023-05-30 17:29:54.623: epoch 33:	0.02984205  	0.08059730  	0.06589722  
2023-05-30 17:30:15.560: [iter 34 : loss : 0.6194 = 0.1516 + 0.4633 + 0.0044, time: 20.934073]
2023-05-30 17:30:15.826: epoch 34:	0.02990868  	0.08055978  	0.06603364  
2023-05-30 17:30:36.812: [iter 35 : loss : 0.6108 = 0.1435 + 0.4627 + 0.0046, time: 20.983119]
2023-05-30 17:30:37.076: epoch 35:	0.02992349  	0.08071715  	0.06611607  
2023-05-30 17:30:58.132: [iter 36 : loss : 0.6043 = 0.1373 + 0.4622 + 0.0048, time: 21.052121]
2023-05-30 17:30:58.405: epoch 36:	0.03004194  	0.08101908  	0.06634019  
2023-05-30 17:30:58.405: Find a better model.
2023-05-30 17:31:19.180: [iter 37 : loss : 0.5978 = 0.1311 + 0.4617 + 0.0050, time: 20.770463]
2023-05-30 17:31:19.445: epoch 37:	0.02994570  	0.08118783  	0.06648795  
2023-05-30 17:31:19.445: Find a better model.
2023-05-30 17:31:40.324: [iter 38 : loss : 0.5914 = 0.1251 + 0.4611 + 0.0052, time: 20.875024]
2023-05-30 17:31:40.591: epoch 38:	0.02990867  	0.08104969  	0.06644012  
2023-05-30 17:32:01.312: [iter 39 : loss : 0.5860 = 0.1202 + 0.4605 + 0.0054, time: 20.717011]
2023-05-30 17:32:01.575: epoch 39:	0.02990127  	0.08139790  	0.06660804  
2023-05-30 17:32:01.575: Find a better model.
2023-05-30 17:32:22.333: [iter 40 : loss : 0.5805 = 0.1150 + 0.4600 + 0.0055, time: 20.755069]
2023-05-30 17:32:22.595: epoch 40:	0.03001231  	0.08130868  	0.06657887  
2023-05-30 17:32:43.331: [iter 41 : loss : 0.5758 = 0.1106 + 0.4596 + 0.0057, time: 20.731059]
2023-05-30 17:32:43.596: epoch 41:	0.02995310  	0.08110223  	0.06641950  
2023-05-30 17:33:04.485: [iter 42 : loss : 0.5712 = 0.1061 + 0.4592 + 0.0059, time: 20.885170]
2023-05-30 17:33:04.751: epoch 42:	0.02994569  	0.08097392  	0.06647845  
2023-05-30 17:33:25.535: [iter 43 : loss : 0.5674 = 0.1026 + 0.4588 + 0.0060, time: 20.780158]
2023-05-30 17:33:25.803: epoch 43:	0.02981244  	0.08001186  	0.06616555  
2023-05-30 17:33:46.689: [iter 44 : loss : 0.5643 = 0.0997 + 0.4584 + 0.0062, time: 20.883105]
2023-05-30 17:33:46.955: epoch 44:	0.02981984  	0.07964617  	0.06601526  
2023-05-30 17:34:08.063: [iter 45 : loss : 0.5605 = 0.0962 + 0.4579 + 0.0063, time: 21.103577]
2023-05-30 17:34:08.338: epoch 45:	0.02976060  	0.07957854  	0.06596501  
2023-05-30 17:34:29.065: [iter 46 : loss : 0.5576 = 0.0935 + 0.4577 + 0.0065, time: 20.723028]
2023-05-30 17:34:29.344: epoch 46:	0.02954592  	0.07876295  	0.06563471  
2023-05-30 17:34:50.080: [iter 47 : loss : 0.5540 = 0.0900 + 0.4574 + 0.0066, time: 20.732085]
2023-05-30 17:34:50.355: epoch 47:	0.02953112  	0.07839426  	0.06562047  
2023-05-30 17:35:11.261: [iter 48 : loss : 0.5515 = 0.0878 + 0.4570 + 0.0068, time: 20.901227]
2023-05-30 17:35:11.530: epoch 48:	0.02937565  	0.07793052  	0.06528121  
2023-05-30 17:35:32.310: [iter 49 : loss : 0.5489 = 0.0853 + 0.4568 + 0.0069, time: 20.777055]
2023-05-30 17:35:32.564: epoch 49:	0.02944228  	0.07787731  	0.06520562  
2023-05-30 17:35:53.440: [iter 50 : loss : 0.5465 = 0.0831 + 0.4564 + 0.0070, time: 20.871709]
2023-05-30 17:35:53.705: epoch 50:	0.02928680  	0.07751542  	0.06504409  
2023-05-30 17:36:14.675: [iter 51 : loss : 0.5436 = 0.0803 + 0.4561 + 0.0072, time: 20.966003]
2023-05-30 17:36:14.941: epoch 51:	0.02923499  	0.07752249  	0.06501538  
2023-05-30 17:36:35.832: [iter 52 : loss : 0.5412 = 0.0780 + 0.4559 + 0.0073, time: 20.888007]
2023-05-30 17:36:36.097: epoch 52:	0.02914615  	0.07713573  	0.06489878  
2023-05-30 17:36:56.821: [iter 53 : loss : 0.5402 = 0.0771 + 0.4556 + 0.0074, time: 20.720039]
2023-05-30 17:36:57.085: epoch 53:	0.02911654  	0.07693443  	0.06477915  
2023-05-30 17:37:18.011: [iter 54 : loss : 0.5378 = 0.0748 + 0.4555 + 0.0076, time: 20.921467]
2023-05-30 17:37:18.275: epoch 54:	0.02907953  	0.07690312  	0.06474974  
2023-05-30 17:37:38.985: [iter 55 : loss : 0.5358 = 0.0730 + 0.4551 + 0.0077, time: 20.707118]
2023-05-30 17:37:39.251: epoch 55:	0.02896847  	0.07674877  	0.06450095  
2023-05-30 17:37:59.994: [iter 56 : loss : 0.5336 = 0.0708 + 0.4549 + 0.0078, time: 20.740110]
2023-05-30 17:38:00.255: epoch 56:	0.02894626  	0.07689633  	0.06456403  
2023-05-30 17:38:21.029: [iter 57 : loss : 0.5321 = 0.0694 + 0.4548 + 0.0079, time: 20.771083]
2023-05-30 17:38:21.310: epoch 57:	0.02876118  	0.07604459  	0.06422122  
2023-05-30 17:38:42.161: [iter 58 : loss : 0.5306 = 0.0679 + 0.4546 + 0.0080, time: 20.847054]
2023-05-30 17:38:42.430: epoch 58:	0.02875377  	0.07590854  	0.06412890  
2023-05-30 17:39:03.220: [iter 59 : loss : 0.5291 = 0.0664 + 0.4545 + 0.0082, time: 20.786353]
2023-05-30 17:39:03.490: epoch 59:	0.02879819  	0.07611512  	0.06424665  
2023-05-30 17:39:24.161: [iter 60 : loss : 0.5281 = 0.0655 + 0.4543 + 0.0083, time: 20.668335]
2023-05-30 17:39:24.427: epoch 60:	0.02862050  	0.07545617  	0.06393287  
2023-05-30 17:39:45.134: [iter 61 : loss : 0.5262 = 0.0636 + 0.4541 + 0.0084, time: 20.702187]
2023-05-30 17:39:45.402: epoch 61:	0.02852427  	0.07507102  	0.06368849  
2023-05-30 17:40:06.130: [iter 62 : loss : 0.5252 = 0.0626 + 0.4541 + 0.0085, time: 20.723146]
2023-05-30 17:40:06.397: epoch 62:	0.02848725  	0.07493404  	0.06358401  
2023-05-30 17:40:27.114: [iter 63 : loss : 0.5240 = 0.0615 + 0.4539 + 0.0086, time: 20.714008]
2023-05-30 17:40:27.386: epoch 63:	0.02842803  	0.07461565  	0.06337588  
2023-05-30 17:40:48.111: [iter 64 : loss : 0.5228 = 0.0603 + 0.4537 + 0.0087, time: 20.722040]
2023-05-30 17:40:48.380: epoch 64:	0.02836880  	0.07470453  	0.06325317  
2023-05-30 17:40:48.380: Early stopping is trigger at epoch: 64
2023-05-30 17:40:48.380: best_result@epoch 39:

2023-05-30 17:40:48.380: 		0.0299      	0.0814      	0.0666      
2023-05-30 18:20:02.682: my pid: 12956
2023-05-30 18:20:02.682: model: model.general_recommender.SGL
2023-05-30 18:20:02.682: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 18:20:02.683: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 18:20:07.005: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 18:20:28.144: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.138811]
2023-05-30 18:20:28.430: epoch 1:	0.00148060  	0.00328196  	0.00256696  
2023-05-30 18:20:28.431: Find a better model.
2023-05-30 18:20:48.970: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.534629]
2023-05-30 18:20:49.262: epoch 2:	0.00157684  	0.00337158  	0.00271587  
2023-05-30 18:20:49.262: Find a better model.
2023-05-30 18:21:09.969: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.702092]
2023-05-30 18:21:10.261: epoch 3:	0.00220610  	0.00506938  	0.00370112  
2023-05-30 18:21:10.261: Find a better model.
2023-05-30 18:21:30.905: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.639503]
2023-05-30 18:21:31.196: epoch 4:	0.00247261  	0.00541289  	0.00402794  
2023-05-30 18:21:31.196: Find a better model.
2023-05-30 18:21:51.917: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.717570]
2023-05-30 18:21:52.209: epoch 5:	0.00267989  	0.00612855  	0.00466047  
2023-05-30 18:21:52.209: Find a better model.
2023-05-30 18:22:12.912: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.698591]
2023-05-30 18:22:13.205: epoch 6:	0.00310927  	0.00760012  	0.00584339  
2023-05-30 18:22:13.205: Find a better model.
2023-05-30 18:22:33.728: [iter 7 : loss : 1.1347 = 0.6923 + 0.4423 + 0.0000, time: 20.518507]
2023-05-30 18:22:34.022: epoch 7:	0.00393101  	0.00991382  	0.00839197  
2023-05-30 18:22:34.022: Find a better model.
2023-05-30 18:22:54.705: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.678591]
2023-05-30 18:22:54.992: epoch 8:	0.00449363  	0.01201707  	0.00928528  
2023-05-30 18:22:54.992: Find a better model.
2023-05-30 18:23:15.553: [iter 9 : loss : 1.1346 = 0.6915 + 0.4431 + 0.0000, time: 20.557183]
2023-05-30 18:23:15.842: epoch 9:	0.00492300  	0.01320091  	0.01045080  
2023-05-30 18:23:15.842: Find a better model.
2023-05-30 18:23:36.499: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.652307]
2023-05-30 18:23:36.785: epoch 10:	0.00572992  	0.01514250  	0.01219411  
2023-05-30 18:23:36.785: Find a better model.
2023-05-30 18:23:58.051: [iter 11 : loss : 1.1337 = 0.6896 + 0.4441 + 0.0000, time: 21.261352]
2023-05-30 18:23:58.338: epoch 11:	0.00669971  	0.01822843  	0.01469681  
2023-05-30 18:23:58.338: Find a better model.
2023-05-30 18:24:19.454: [iter 12 : loss : 1.1325 = 0.6877 + 0.4447 + 0.0000, time: 21.113501]
2023-05-30 18:24:19.739: epoch 12:	0.00815069  	0.02269813  	0.01843647  
2023-05-30 18:24:19.739: Find a better model.
2023-05-30 18:24:40.873: [iter 13 : loss : 1.1304 = 0.6850 + 0.4454 + 0.0001, time: 21.130972]
2023-05-30 18:24:41.155: epoch 13:	0.01028277  	0.02882969  	0.02392549  
2023-05-30 18:24:41.155: Find a better model.
2023-05-30 18:25:02.259: [iter 14 : loss : 1.1267 = 0.6805 + 0.4461 + 0.0001, time: 21.101178]
2023-05-30 18:25:02.575: epoch 14:	0.01333285  	0.03718260  	0.03120223  
2023-05-30 18:25:02.575: Find a better model.
2023-05-30 18:25:23.818: [iter 15 : loss : 1.1199 = 0.6729 + 0.4469 + 0.0001, time: 21.238044]
2023-05-30 18:25:24.098: epoch 15:	0.01681974  	0.04620444  	0.03914149  
2023-05-30 18:25:24.098: Find a better model.
2023-05-30 18:25:45.198: [iter 16 : loss : 1.1077 = 0.6595 + 0.4480 + 0.0002, time: 21.096317]
2023-05-30 18:25:45.493: epoch 16:	0.02010676  	0.05511802  	0.04682399  
2023-05-30 18:25:45.493: Find a better model.
2023-05-30 18:26:06.627: [iter 17 : loss : 1.0875 = 0.6379 + 0.4494 + 0.0003, time: 21.131335]
2023-05-30 18:26:06.906: epoch 17:	0.02327536  	0.06257610  	0.05277204  
2023-05-30 18:26:06.906: Find a better model.
2023-05-30 18:26:28.020: [iter 18 : loss : 1.0565 = 0.6048 + 0.4512 + 0.0004, time: 21.109440]
2023-05-30 18:26:28.297: epoch 18:	0.02553336  	0.06791298  	0.05732450  
2023-05-30 18:26:28.297: Find a better model.
2023-05-30 18:26:49.248: [iter 19 : loss : 1.0156 = 0.5613 + 0.4537 + 0.0006, time: 20.947453]
2023-05-30 18:26:49.535: epoch 19:	0.02736936  	0.07284774  	0.06030323  
2023-05-30 18:26:49.535: Find a better model.
2023-05-30 18:27:10.565: [iter 20 : loss : 0.9678 = 0.5099 + 0.4570 + 0.0008, time: 21.024686]
2023-05-30 18:27:10.842: epoch 20:	0.02848727  	0.07665239  	0.06266208  
2023-05-30 18:27:10.842: Find a better model.
2023-05-30 18:27:31.769: [iter 21 : loss : 0.9180 = 0.4565 + 0.4604 + 0.0011, time: 20.923114]
2023-05-30 18:27:32.048: epoch 21:	0.02893146  	0.07885800  	0.06395460  
2023-05-30 18:27:32.048: Find a better model.
2023-05-30 18:27:53.197: [iter 22 : loss : 0.8699 = 0.4051 + 0.4633 + 0.0014, time: 21.146325]
2023-05-30 18:27:53.482: epoch 22:	0.02921278  	0.07986794  	0.06474759  
2023-05-30 18:27:53.482: Find a better model.
2023-05-30 18:28:14.399: [iter 23 : loss : 0.8276 = 0.3602 + 0.4656 + 0.0017, time: 20.911394]
2023-05-30 18:28:14.675: epoch 23:	0.02942009  	0.08047202  	0.06488933  
2023-05-30 18:28:14.675: Find a better model.
2023-05-30 18:28:35.751: [iter 24 : loss : 0.7915 = 0.3223 + 0.4672 + 0.0020, time: 21.072300]
2023-05-30 18:28:36.029: epoch 24:	0.02960516  	0.08143564  	0.06546023  
2023-05-30 18:28:36.029: Find a better model.
2023-05-30 18:28:56.949: [iter 25 : loss : 0.7601 = 0.2900 + 0.4678 + 0.0023, time: 20.915799]
2023-05-30 18:28:57.226: epoch 25:	0.02986427  	0.08184158  	0.06590580  
2023-05-30 18:28:57.226: Find a better model.
2023-05-30 18:29:18.179: [iter 26 : loss : 0.7341 = 0.2636 + 0.4679 + 0.0026, time: 20.947840]
2023-05-30 18:29:18.464: epoch 26:	0.03001233  	0.08253340  	0.06616378  
2023-05-30 18:29:18.464: Find a better model.
2023-05-30 18:29:39.567: [iter 27 : loss : 0.7118 = 0.2411 + 0.4679 + 0.0029, time: 21.099826]
2023-05-30 18:29:39.845: epoch 27:	0.03015300  	0.08370128  	0.06646238  
2023-05-30 18:29:39.845: Find a better model.
2023-05-30 18:30:00.765: [iter 28 : loss : 0.6929 = 0.2223 + 0.4674 + 0.0031, time: 20.916682]
2023-05-30 18:30:01.043: epoch 28:	0.03010118  	0.08348858  	0.06648264  
2023-05-30 18:30:22.125: [iter 29 : loss : 0.6768 = 0.2066 + 0.4669 + 0.0034, time: 21.078536]
2023-05-30 18:30:22.397: epoch 29:	0.03016040  	0.08337406  	0.06670003  
2023-05-30 18:30:43.505: [iter 30 : loss : 0.6619 = 0.1920 + 0.4663 + 0.0036, time: 21.105418]
2023-05-30 18:30:43.777: epoch 30:	0.03017521  	0.08323757  	0.06678375  
2023-05-30 18:31:04.766: [iter 31 : loss : 0.6491 = 0.1797 + 0.4656 + 0.0038, time: 20.985101]
2023-05-30 18:31:05.035: epoch 31:	0.03027886  	0.08387628  	0.06684628  
2023-05-30 18:31:05.035: Find a better model.
2023-05-30 18:31:26.134: [iter 32 : loss : 0.6383 = 0.1694 + 0.4648 + 0.0040, time: 21.095246]
2023-05-30 18:31:26.402: epoch 32:	0.03045654  	0.08432314  	0.06718357  
2023-05-30 18:31:26.402: Find a better model.
2023-05-30 18:31:47.492: [iter 33 : loss : 0.6284 = 0.1601 + 0.4641 + 0.0042, time: 21.083265]
2023-05-30 18:31:47.763: epoch 33:	0.03044172  	0.08430733  	0.06738791  
2023-05-30 18:32:08.723: [iter 34 : loss : 0.6197 = 0.1517 + 0.4636 + 0.0044, time: 20.956119]
2023-05-30 18:32:08.994: epoch 34:	0.03048614  	0.08424965  	0.06763241  
2023-05-30 18:32:29.915: [iter 35 : loss : 0.6110 = 0.1436 + 0.4628 + 0.0046, time: 20.918021]
2023-05-30 18:32:30.183: epoch 35:	0.03052315  	0.08426800  	0.06786890  
2023-05-30 18:32:51.113: [iter 36 : loss : 0.6041 = 0.1370 + 0.4623 + 0.0048, time: 20.927130]
2023-05-30 18:32:51.385: epoch 36:	0.03047873  	0.08372188  	0.06761079  
2023-05-30 18:33:12.477: [iter 37 : loss : 0.5975 = 0.1309 + 0.4616 + 0.0050, time: 21.089086]
2023-05-30 18:33:12.745: epoch 37:	0.03047873  	0.08382244  	0.06758579  
2023-05-30 18:33:33.424: [iter 38 : loss : 0.5915 = 0.1252 + 0.4611 + 0.0052, time: 20.675310]
2023-05-30 18:33:33.692: epoch 38:	0.03047133  	0.08378040  	0.06754199  
2023-05-30 18:33:54.415: [iter 39 : loss : 0.5861 = 0.1203 + 0.4605 + 0.0054, time: 20.720519]
2023-05-30 18:33:54.684: epoch 39:	0.03050834  	0.08429921  	0.06764799  
2023-05-30 18:34:15.695: [iter 40 : loss : 0.5806 = 0.1151 + 0.4600 + 0.0055, time: 21.008154]
2023-05-30 18:34:15.962: epoch 40:	0.03050094  	0.08415575  	0.06770850  
2023-05-30 18:34:37.061: [iter 41 : loss : 0.5757 = 0.1105 + 0.4595 + 0.0057, time: 21.094328]
2023-05-30 18:34:37.330: epoch 41:	0.03057498  	0.08418904  	0.06797396  
2023-05-30 18:34:58.416: [iter 42 : loss : 0.5717 = 0.1066 + 0.4593 + 0.0059, time: 21.082395]
2023-05-30 18:34:58.686: epoch 42:	0.03059719  	0.08364474  	0.06784581  
2023-05-30 18:35:19.623: [iter 43 : loss : 0.5672 = 0.1023 + 0.4589 + 0.0060, time: 20.932386]
2023-05-30 18:35:19.890: epoch 43:	0.03041210  	0.08327686  	0.06763579  
2023-05-30 18:35:41.029: [iter 44 : loss : 0.5646 = 0.1000 + 0.4584 + 0.0062, time: 21.136097]
2023-05-30 18:35:41.292: epoch 44:	0.03036768  	0.08315569  	0.06746861  
2023-05-30 18:36:02.213: [iter 45 : loss : 0.5603 = 0.0959 + 0.4580 + 0.0063, time: 20.916117]
2023-05-30 18:36:02.485: epoch 45:	0.03055276  	0.08374880  	0.06774099  
2023-05-30 18:36:23.606: [iter 46 : loss : 0.5572 = 0.0931 + 0.4577 + 0.0065, time: 21.116063]
2023-05-30 18:36:23.871: epoch 46:	0.03056758  	0.08384519  	0.06766748  
2023-05-30 18:36:44.968: [iter 47 : loss : 0.5542 = 0.0903 + 0.4574 + 0.0066, time: 21.094028]
2023-05-30 18:36:45.234: epoch 47:	0.03054537  	0.08385925  	0.06761277  
2023-05-30 18:37:06.392: [iter 48 : loss : 0.5513 = 0.0877 + 0.4569 + 0.0068, time: 21.154580]
2023-05-30 18:37:06.659: epoch 48:	0.03051575  	0.08336576  	0.06745485  
2023-05-30 18:37:27.821: [iter 49 : loss : 0.5487 = 0.0851 + 0.4567 + 0.0069, time: 21.158606]
2023-05-30 18:37:28.089: epoch 49:	0.03048613  	0.08320250  	0.06729922  
2023-05-30 18:37:49.195: [iter 50 : loss : 0.5466 = 0.0832 + 0.4564 + 0.0070, time: 21.101074]
2023-05-30 18:37:49.475: epoch 50:	0.03047133  	0.08286723  	0.06735391  
2023-05-30 18:38:10.573: [iter 51 : loss : 0.5437 = 0.0804 + 0.4561 + 0.0072, time: 21.094330]
2023-05-30 18:38:10.837: epoch 51:	0.03033067  	0.08249544  	0.06712387  
2023-05-30 18:38:32.137: [iter 52 : loss : 0.5413 = 0.0781 + 0.4559 + 0.0073, time: 21.296050]
2023-05-30 18:38:32.403: epoch 52:	0.03021962  	0.08212208  	0.06702200  
2023-05-30 18:38:53.766: [iter 53 : loss : 0.5403 = 0.0771 + 0.4558 + 0.0074, time: 21.356054]
2023-05-30 18:38:54.037: epoch 53:	0.03010117  	0.08174908  	0.06686836  
2023-05-30 18:39:15.348: [iter 54 : loss : 0.5379 = 0.0749 + 0.4554 + 0.0076, time: 21.307053]
2023-05-30 18:39:15.619: epoch 54:	0.03014558  	0.08145987  	0.06680766  
2023-05-30 18:39:36.961: [iter 55 : loss : 0.5358 = 0.0729 + 0.4552 + 0.0077, time: 21.339004]
2023-05-30 18:39:37.225: epoch 55:	0.02996790  	0.08085532  	0.06643508  
2023-05-30 18:39:58.497: [iter 56 : loss : 0.5337 = 0.0710 + 0.4549 + 0.0078, time: 21.268030]
2023-05-30 18:39:58.762: epoch 56:	0.02999752  	0.08093586  	0.06645121  
2023-05-30 18:40:19.974: [iter 57 : loss : 0.5320 = 0.0692 + 0.4548 + 0.0079, time: 21.208070]
2023-05-30 18:40:20.243: epoch 57:	0.02998271  	0.08051434  	0.06619769  
2023-05-30 18:40:20.243: Early stopping is trigger at epoch: 57
2023-05-30 18:40:20.243: best_result@epoch 32:

2023-05-30 18:40:20.243: 		0.0305      	0.0843      	0.0672      
2023-05-30 19:50:40.409: my pid: 5160
2023-05-30 19:50:40.409: model: model.general_recommender.SGL
2023-05-30 19:50:40.409: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 19:50:40.409: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 19:50:44.371: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 19:51:05.156: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.783718]
2023-05-30 19:51:05.426: epoch 1:	0.00123630  	0.00223099  	0.00183067  
2023-05-30 19:51:05.426: Find a better model.
2023-05-30 19:51:26.095: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.666502]
2023-05-30 19:51:26.382: epoch 2:	0.00176192  	0.00308313  	0.00265128  
2023-05-30 19:51:26.382: Find a better model.
2023-05-30 19:51:47.292: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.906892]
2023-05-30 19:51:47.586: epoch 3:	0.00208765  	0.00401357  	0.00308850  
2023-05-30 19:51:47.586: Find a better model.
2023-05-30 19:52:08.330: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.740891]
2023-05-30 19:52:08.622: epoch 4:	0.00216909  	0.00490258  	0.00379989  
2023-05-30 19:52:08.622: Find a better model.
2023-05-30 19:52:29.273: [iter 5 : loss : 1.1344 = 0.6927 + 0.4416 + 0.0000, time: 20.648051]
2023-05-30 19:52:29.562: epoch 5:	0.00256885  	0.00558320  	0.00451249  
2023-05-30 19:52:29.562: Find a better model.
2023-05-30 19:52:50.260: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.694242]
2023-05-30 19:52:50.550: epoch 6:	0.00309446  	0.00711338  	0.00547966  
2023-05-30 19:52:50.550: Find a better model.
2023-05-30 19:53:11.247: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.693086]
2023-05-30 19:53:11.531: epoch 7:	0.00362748  	0.00833921  	0.00699639  
2023-05-30 19:53:11.531: Find a better model.
2023-05-30 19:53:32.281: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 20.747389]
2023-05-30 19:53:32.567: epoch 8:	0.00439739  	0.01126788  	0.00874842  
2023-05-30 19:53:32.567: Find a better model.
2023-05-30 19:53:53.268: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 20.697539]
2023-05-30 19:53:53.547: epoch 9:	0.00518210  	0.01356109  	0.01072247  
2023-05-30 19:53:53.547: Find a better model.
2023-05-30 19:54:14.055: [iter 10 : loss : 1.1345 = 0.6910 + 0.4435 + 0.0000, time: 20.505249]
2023-05-30 19:54:14.333: epoch 10:	0.00633696  	0.01757422  	0.01383409  
2023-05-30 19:54:14.333: Find a better model.
2023-05-30 19:54:35.427: [iter 11 : loss : 1.1340 = 0.6899 + 0.4440 + 0.0000, time: 21.090373]
2023-05-30 19:54:35.705: epoch 11:	0.00739559  	0.02083148  	0.01721038  
2023-05-30 19:54:35.705: Find a better model.
2023-05-30 19:54:56.786: [iter 12 : loss : 1.1327 = 0.6881 + 0.4446 + 0.0000, time: 21.078200]
2023-05-30 19:54:57.063: epoch 12:	0.00865409  	0.02348229  	0.01940284  
2023-05-30 19:54:57.063: Find a better model.
2023-05-30 19:55:17.984: [iter 13 : loss : 1.1308 = 0.6854 + 0.4453 + 0.0000, time: 20.916777]
2023-05-30 19:55:18.259: epoch 13:	0.01059369  	0.02865230  	0.02457189  
2023-05-30 19:55:18.259: Find a better model.
2023-05-30 19:55:39.197: [iter 14 : loss : 1.1271 = 0.6811 + 0.4459 + 0.0001, time: 20.934204]
2023-05-30 19:55:39.473: epoch 14:	0.01399172  	0.03839997  	0.03258275  
2023-05-30 19:55:39.473: Find a better model.
2023-05-30 19:56:00.602: [iter 15 : loss : 1.1208 = 0.6738 + 0.4469 + 0.0001, time: 21.125044]
2023-05-30 19:56:00.890: epoch 15:	0.01686415  	0.04628135  	0.03996161  
2023-05-30 19:56:00.890: Find a better model.
2023-05-30 19:56:21.944: [iter 16 : loss : 1.1092 = 0.6611 + 0.4479 + 0.0002, time: 21.048485]
2023-05-30 19:56:22.220: epoch 16:	0.02038069  	0.05580198  	0.04753730  
2023-05-30 19:56:22.220: Find a better model.
2023-05-30 19:56:43.139: [iter 17 : loss : 1.0898 = 0.6404 + 0.4492 + 0.0003, time: 20.913217]
2023-05-30 19:56:43.416: epoch 17:	0.02339380  	0.06348112  	0.05408895  
2023-05-30 19:56:43.416: Find a better model.
2023-05-30 19:57:04.380: [iter 18 : loss : 1.0597 = 0.6083 + 0.4510 + 0.0004, time: 20.959696]
2023-05-30 19:57:04.658: epoch 18:	0.02546669  	0.06864620  	0.05835868  
2023-05-30 19:57:04.658: Find a better model.
2023-05-30 19:57:25.570: [iter 19 : loss : 1.0195 = 0.5654 + 0.4536 + 0.0006, time: 20.908154]
2023-05-30 19:57:25.856: epoch 19:	0.02694736  	0.07280439  	0.06125782  
2023-05-30 19:57:25.856: Find a better model.
2023-05-30 19:57:46.738: [iter 20 : loss : 0.9722 = 0.5147 + 0.4566 + 0.0008, time: 20.878155]
2023-05-30 19:57:47.013: epoch 20:	0.02803564  	0.07643045  	0.06358014  
2023-05-30 19:57:47.013: Find a better model.
2023-05-30 19:58:07.725: [iter 21 : loss : 0.9224 = 0.4612 + 0.4601 + 0.0011, time: 20.707381]
2023-05-30 19:58:08.004: epoch 21:	0.02858349  	0.07797232  	0.06478202  
2023-05-30 19:58:08.004: Find a better model.
2023-05-30 19:58:28.726: [iter 22 : loss : 0.8738 = 0.4093 + 0.4630 + 0.0014, time: 20.718073]
2023-05-30 19:58:28.998: epoch 22:	0.02890184  	0.07900351  	0.06544107  
2023-05-30 19:58:28.998: Find a better model.
2023-05-30 19:58:49.765: [iter 23 : loss : 0.8310 = 0.3640 + 0.4653 + 0.0017, time: 20.763489]
2023-05-30 19:58:50.034: epoch 23:	0.02896846  	0.07949241  	0.06566970  
2023-05-30 19:58:50.034: Find a better model.
2023-05-30 19:59:10.957: [iter 24 : loss : 0.7942 = 0.3253 + 0.4669 + 0.0020, time: 20.919331]
2023-05-30 19:59:11.225: epoch 24:	0.02914614  	0.08017595  	0.06601056  
2023-05-30 19:59:11.225: Find a better model.
2023-05-30 19:59:32.304: [iter 25 : loss : 0.7626 = 0.2928 + 0.4675 + 0.0023, time: 21.074048]
2023-05-30 19:59:32.571: epoch 25:	0.02936824  	0.08060358  	0.06636755  
2023-05-30 19:59:32.571: Find a better model.
2023-05-30 19:59:53.723: [iter 26 : loss : 0.7363 = 0.2660 + 0.4678 + 0.0026, time: 21.149037]
2023-05-30 19:59:53.996: epoch 26:	0.02944229  	0.08108979  	0.06661129  
2023-05-30 19:59:53.996: Find a better model.
2023-05-30 20:00:15.276: [iter 27 : loss : 0.7137 = 0.2432 + 0.4677 + 0.0028, time: 21.276330]
2023-05-30 20:00:15.546: epoch 27:	0.02951631  	0.08108842  	0.06688539  
2023-05-30 20:00:36.865: [iter 28 : loss : 0.6945 = 0.2240 + 0.4673 + 0.0031, time: 21.314970]
2023-05-30 20:00:37.131: epoch 28:	0.02960516  	0.08152012  	0.06707491  
2023-05-30 20:00:37.131: Find a better model.
2023-05-30 20:00:58.467: [iter 29 : loss : 0.6779 = 0.2080 + 0.4666 + 0.0033, time: 21.331402]
2023-05-30 20:00:58.736: epoch 29:	0.02962736  	0.08149043  	0.06710692  
2023-05-30 20:01:20.052: [iter 30 : loss : 0.6631 = 0.1934 + 0.4661 + 0.0036, time: 21.313292]
2023-05-30 20:01:20.317: epoch 30:	0.02972360  	0.08189978  	0.06721249  
2023-05-30 20:01:20.317: Find a better model.
2023-05-30 20:01:41.646: [iter 31 : loss : 0.6503 = 0.1811 + 0.4655 + 0.0038, time: 21.326026]
2023-05-30 20:01:41.910: epoch 31:	0.02967178  	0.08212283  	0.06734517  
2023-05-30 20:01:41.910: Find a better model.
2023-05-30 20:02:03.256: [iter 32 : loss : 0.6390 = 0.1704 + 0.4646 + 0.0040, time: 21.343431]
2023-05-30 20:02:03.521: epoch 32:	0.02966438  	0.08191121  	0.06720029  
2023-05-30 20:02:24.671: [iter 33 : loss : 0.6296 = 0.1614 + 0.4639 + 0.0042, time: 21.146016]
2023-05-30 20:02:24.939: epoch 33:	0.02979023  	0.08242243  	0.06742638  
2023-05-30 20:02:24.939: Find a better model.
2023-05-30 20:02:46.234: [iter 34 : loss : 0.6202 = 0.1524 + 0.4634 + 0.0044, time: 21.291414]
2023-05-30 20:02:46.501: epoch 34:	0.03000492  	0.08267370  	0.06758982  
2023-05-30 20:02:46.501: Find a better model.
2023-05-30 20:03:07.821: [iter 35 : loss : 0.6117 = 0.1444 + 0.4627 + 0.0046, time: 21.317251]
2023-05-30 20:03:08.086: epoch 35:	0.03004194  	0.08273649  	0.06757821  
2023-05-30 20:03:08.086: Find a better model.
2023-05-30 20:03:29.181: [iter 36 : loss : 0.6043 = 0.1374 + 0.4621 + 0.0048, time: 21.091079]
2023-05-30 20:03:29.447: epoch 36:	0.03005675  	0.08281125  	0.06762705  
2023-05-30 20:03:29.447: Find a better model.
2023-05-30 20:03:50.823: [iter 37 : loss : 0.5984 = 0.1318 + 0.4616 + 0.0050, time: 21.373051]
2023-05-30 20:03:51.089: epoch 37:	0.03001975  	0.08315705  	0.06773078  
2023-05-30 20:03:51.089: Find a better model.
2023-05-30 20:04:12.424: [iter 38 : loss : 0.5918 = 0.1257 + 0.4609 + 0.0052, time: 21.330542]
2023-05-30 20:04:12.686: epoch 38:	0.02997534  	0.08292315  	0.06757703  
2023-05-30 20:04:33.826: [iter 39 : loss : 0.5865 = 0.1207 + 0.4604 + 0.0054, time: 21.137140]
2023-05-30 20:04:34.092: epoch 39:	0.02991611  	0.08248252  	0.06753436  
2023-05-30 20:04:55.420: [iter 40 : loss : 0.5811 = 0.1156 + 0.4600 + 0.0055, time: 21.324284]
2023-05-30 20:04:55.686: epoch 40:	0.02993091  	0.08265290  	0.06760361  
2023-05-30 20:05:16.811: [iter 41 : loss : 0.5763 = 0.1111 + 0.4595 + 0.0057, time: 21.121127]
2023-05-30 20:05:17.076: epoch 41:	0.02989389  	0.08263334  	0.06755944  
2023-05-30 20:05:38.004: [iter 42 : loss : 0.5719 = 0.1069 + 0.4591 + 0.0059, time: 20.924093]
2023-05-30 20:05:38.270: epoch 42:	0.02992350  	0.08227249  	0.06749345  
2023-05-30 20:05:59.595: [iter 43 : loss : 0.5675 = 0.1027 + 0.4588 + 0.0060, time: 21.321824]
2023-05-30 20:05:59.871: epoch 43:	0.02996791  	0.08194047  	0.06746699  
2023-05-30 20:06:20.792: [iter 44 : loss : 0.5647 = 0.1002 + 0.4583 + 0.0062, time: 20.916503]
2023-05-30 20:06:21.059: epoch 44:	0.02984207  	0.08169141  	0.06734210  
2023-05-30 20:06:41.967: [iter 45 : loss : 0.5607 = 0.0963 + 0.4581 + 0.0063, time: 20.904434]
2023-05-30 20:06:42.233: epoch 45:	0.02983466  	0.08151884  	0.06724019  
2023-05-30 20:07:03.332: [iter 46 : loss : 0.5575 = 0.0934 + 0.4576 + 0.0065, time: 21.094600]
2023-05-30 20:07:03.597: epoch 46:	0.02977543  	0.08131092  	0.06702255  
2023-05-30 20:07:24.738: [iter 47 : loss : 0.5543 = 0.0904 + 0.4573 + 0.0066, time: 21.136167]
2023-05-30 20:07:25.006: epoch 47:	0.02973101  	0.08120436  	0.06691869  
2023-05-30 20:07:45.932: [iter 48 : loss : 0.5517 = 0.0880 + 0.4570 + 0.0068, time: 20.920487]
2023-05-30 20:07:46.196: epoch 48:	0.02957554  	0.08073597  	0.06658909  
2023-05-30 20:08:07.115: [iter 49 : loss : 0.5491 = 0.0856 + 0.4566 + 0.0069, time: 20.915129]
2023-05-30 20:08:07.380: epoch 49:	0.02944227  	0.08061597  	0.06647560  
2023-05-30 20:08:28.493: [iter 50 : loss : 0.5464 = 0.0830 + 0.4564 + 0.0070, time: 21.110237]
2023-05-30 20:08:28.759: epoch 50:	0.02939046  	0.08041457  	0.06634576  
2023-05-30 20:08:49.676: [iter 51 : loss : 0.5439 = 0.0806 + 0.4561 + 0.0072, time: 20.914200]
2023-05-30 20:08:49.941: epoch 51:	0.02930162  	0.08029976  	0.06634907  
2023-05-30 20:09:10.903: [iter 52 : loss : 0.5415 = 0.0784 + 0.4558 + 0.0073, time: 20.957085]
2023-05-30 20:09:11.168: epoch 52:	0.02935343  	0.07978822  	0.06622870  
2023-05-30 20:09:32.117: [iter 53 : loss : 0.5406 = 0.0776 + 0.4556 + 0.0074, time: 20.946597]
2023-05-30 20:09:32.382: epoch 53:	0.02924979  	0.07942656  	0.06597195  
2023-05-30 20:09:54.076: [iter 54 : loss : 0.5376 = 0.0746 + 0.4554 + 0.0076, time: 21.689693]
2023-05-30 20:09:54.333: epoch 54:	0.02914615  	0.07893831  	0.06570352  
2023-05-30 20:10:15.552: [iter 55 : loss : 0.5360 = 0.0731 + 0.4552 + 0.0077, time: 21.215126]
2023-05-30 20:10:15.827: epoch 55:	0.02902030  	0.07868638  	0.06558418  
2023-05-30 20:10:36.865: [iter 56 : loss : 0.5341 = 0.0714 + 0.4549 + 0.0078, time: 21.029263]
2023-05-30 20:10:37.130: epoch 56:	0.02905731  	0.07876803  	0.06564373  
2023-05-30 20:10:57.864: [iter 57 : loss : 0.5321 = 0.0694 + 0.4548 + 0.0079, time: 20.730098]
2023-05-30 20:10:58.128: epoch 57:	0.02888704  	0.07855178  	0.06525405  
2023-05-30 20:11:19.048: [iter 58 : loss : 0.5307 = 0.0680 + 0.4546 + 0.0080, time: 20.915251]
2023-05-30 20:11:19.313: epoch 58:	0.02891666  	0.07850249  	0.06502701  
2023-05-30 20:11:40.235: [iter 59 : loss : 0.5292 = 0.0667 + 0.4543 + 0.0082, time: 20.918106]
2023-05-30 20:11:40.498: epoch 59:	0.02896847  	0.07846998  	0.06508923  
2023-05-30 20:12:01.449: [iter 60 : loss : 0.5279 = 0.0653 + 0.4543 + 0.0083, time: 20.946501]
2023-05-30 20:12:01.714: epoch 60:	0.02890924  	0.07868663  	0.06508385  
2023-05-30 20:12:22.624: [iter 61 : loss : 0.5267 = 0.0642 + 0.4541 + 0.0084, time: 20.907063]
2023-05-30 20:12:22.898: epoch 61:	0.02885742  	0.07786664  	0.06472519  
2023-05-30 20:12:43.842: [iter 62 : loss : 0.5253 = 0.0628 + 0.4539 + 0.0085, time: 20.940482]
2023-05-30 20:12:44.107: epoch 62:	0.02876857  	0.07736397  	0.06447744  
2023-05-30 20:12:44.107: Early stopping is trigger at epoch: 62
2023-05-30 20:12:44.107: best_result@epoch 37:

2023-05-30 20:12:44.107: 		0.0300      	0.0832      	0.0677      
2023-05-30 20:19:11.631: my pid: 5624
2023-05-30 20:19:11.631: model: model.general_recommender.SGL
2023-05-30 20:19:11.631: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 20:19:11.632: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 20:19:15.638: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 20:19:36.548: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.908595]
2023-05-30 20:19:36.815: epoch 1:	0.00149541  	0.00331865  	0.00249180  
2023-05-30 20:19:36.815: Find a better model.
2023-05-30 20:19:57.737: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.919476]
2023-05-30 20:19:58.023: epoch 2:	0.00184335  	0.00355939  	0.00286668  
2023-05-30 20:19:58.023: Find a better model.
2023-05-30 20:20:18.957: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 20.930750]
2023-05-30 20:20:19.256: epoch 3:	0.00181374  	0.00381438  	0.00313240  
2023-05-30 20:20:19.256: Find a better model.
2023-05-30 20:20:40.145: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.886021]
2023-05-30 20:20:40.425: epoch 4:	0.00225792  	0.00498765  	0.00388928  
2023-05-30 20:20:40.425: Find a better model.
2023-05-30 20:21:01.337: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.908566]
2023-05-30 20:21:01.624: epoch 5:	0.00248001  	0.00510824  	0.00423793  
2023-05-30 20:21:01.624: Find a better model.
2023-05-30 20:21:22.317: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 20.690275]
2023-05-30 20:21:22.603: epoch 6:	0.00310927  	0.00688189  	0.00551158  
2023-05-30 20:21:22.603: Find a better model.
2023-05-30 20:21:43.498: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 20.890329]
2023-05-30 20:21:43.782: epoch 7:	0.00374593  	0.00913534  	0.00725597  
2023-05-30 20:21:43.782: Find a better model.
2023-05-30 20:22:04.483: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 20.698027]
2023-05-30 20:22:04.767: epoch 8:	0.00465649  	0.01205722  	0.00915721  
2023-05-30 20:22:04.767: Find a better model.
2023-05-30 20:22:25.340: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 20.570077]
2023-05-30 20:22:25.620: epoch 9:	0.00545601  	0.01437823  	0.01117027  
2023-05-30 20:22:25.620: Find a better model.
2023-05-30 20:22:46.104: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.481194]
2023-05-30 20:22:46.389: epoch 10:	0.00647022  	0.01813536  	0.01450604  
2023-05-30 20:22:46.389: Find a better model.
2023-05-30 20:23:07.658: [iter 11 : loss : 1.1340 = 0.6901 + 0.4438 + 0.0000, time: 21.266093]
2023-05-30 20:23:07.935: epoch 11:	0.00798782  	0.02215927  	0.01784789  
2023-05-30 20:23:07.935: Find a better model.
2023-05-30 20:23:29.261: [iter 12 : loss : 1.1329 = 0.6885 + 0.4444 + 0.0000, time: 21.322626]
2023-05-30 20:23:29.543: epoch 12:	0.00909826  	0.02588002  	0.02073781  
2023-05-30 20:23:29.543: Find a better model.
2023-05-30 20:23:50.846: [iter 13 : loss : 1.1310 = 0.6858 + 0.4451 + 0.0000, time: 21.299059]
2023-05-30 20:23:51.120: epoch 13:	0.01124516  	0.03096368  	0.02613633  
2023-05-30 20:23:51.120: Find a better model.
2023-05-30 20:24:12.245: [iter 14 : loss : 1.1274 = 0.6815 + 0.4459 + 0.0001, time: 21.122501]
2023-05-30 20:24:12.521: epoch 14:	0.01458398  	0.03873464  	0.03350208  
2023-05-30 20:24:12.521: Find a better model.
2023-05-30 20:24:33.655: [iter 15 : loss : 1.1212 = 0.6743 + 0.4467 + 0.0001, time: 21.129327]
2023-05-30 20:24:33.925: epoch 15:	0.01773772  	0.04688229  	0.04086318  
2023-05-30 20:24:33.925: Find a better model.
2023-05-30 20:24:55.006: [iter 16 : loss : 1.1097 = 0.6618 + 0.4478 + 0.0002, time: 21.077898]
2023-05-30 20:24:55.286: epoch 16:	0.02092852  	0.05495931  	0.04798819  
2023-05-30 20:24:55.286: Find a better model.
2023-05-30 20:25:16.242: [iter 17 : loss : 1.0907 = 0.6413 + 0.4491 + 0.0002, time: 20.950645]
2023-05-30 20:25:16.525: epoch 17:	0.02354187  	0.06255062  	0.05370350  
2023-05-30 20:25:16.525: Find a better model.
2023-05-30 20:25:37.593: [iter 18 : loss : 1.0607 = 0.6094 + 0.4509 + 0.0004, time: 21.063668]
2023-05-30 20:25:37.867: epoch 18:	0.02582948  	0.06806915  	0.05805444  
2023-05-30 20:25:37.867: Find a better model.
2023-05-30 20:25:59.004: [iter 19 : loss : 1.0206 = 0.5668 + 0.4533 + 0.0006, time: 21.133037]
2023-05-30 20:25:59.286: epoch 19:	0.02716947  	0.07154021  	0.06100050  
2023-05-30 20:25:59.286: Find a better model.
2023-05-30 20:26:19.996: [iter 20 : loss : 0.9733 = 0.5161 + 0.4564 + 0.0008, time: 20.706556]
2023-05-30 20:26:20.276: epoch 20:	0.02829474  	0.07432245  	0.06294513  
2023-05-30 20:26:20.276: Find a better model.
2023-05-30 20:26:41.394: [iter 21 : loss : 0.9234 = 0.4627 + 0.4597 + 0.0011, time: 21.114126]
2023-05-30 20:26:41.665: epoch 21:	0.02873154  	0.07591984  	0.06367243  
2023-05-30 20:26:41.665: Find a better model.
2023-05-30 20:27:02.774: [iter 22 : loss : 0.8749 = 0.4109 + 0.4626 + 0.0014, time: 21.105128]
2023-05-30 20:27:03.052: epoch 22:	0.02902768  	0.07671810  	0.06394732  
2023-05-30 20:27:03.052: Find a better model.
2023-05-30 20:27:24.004: [iter 23 : loss : 0.8321 = 0.3654 + 0.4650 + 0.0017, time: 20.948874]
2023-05-30 20:27:24.279: epoch 23:	0.02903508  	0.07718265  	0.06421417  
2023-05-30 20:27:24.279: Find a better model.
2023-05-30 20:27:45.172: [iter 24 : loss : 0.7950 = 0.3265 + 0.4665 + 0.0020, time: 20.889081]
2023-05-30 20:27:45.437: epoch 24:	0.02930162  	0.07824106  	0.06486986  
2023-05-30 20:27:45.437: Find a better model.
2023-05-30 20:28:06.344: [iter 25 : loss : 0.7632 = 0.2938 + 0.4672 + 0.0023, time: 20.904627]
2023-05-30 20:28:06.610: epoch 25:	0.02945708  	0.07846043  	0.06502701  
2023-05-30 20:28:06.610: Find a better model.
2023-05-30 20:28:27.811: [iter 26 : loss : 0.7367 = 0.2666 + 0.4675 + 0.0026, time: 21.197321]
2023-05-30 20:28:28.094: epoch 26:	0.02943487  	0.07901824  	0.06504386  
2023-05-30 20:28:28.095: Find a better model.
2023-05-30 20:28:49.969: [iter 27 : loss : 0.7137 = 0.2436 + 0.4673 + 0.0028, time: 21.870944]
2023-05-30 20:28:50.267: epoch 27:	0.02954592  	0.07987181  	0.06541216  
2023-05-30 20:28:50.268: Find a better model.
2023-05-30 20:29:11.951: [iter 28 : loss : 0.6944 = 0.2245 + 0.4669 + 0.0031, time: 21.680853]
2023-05-30 20:29:12.235: epoch 28:	0.02959774  	0.08005548  	0.06564511  
2023-05-30 20:29:12.235: Find a better model.
2023-05-30 20:29:33.869: [iter 29 : loss : 0.6779 = 0.2084 + 0.4662 + 0.0033, time: 21.629754]
2023-05-30 20:29:34.122: epoch 29:	0.02967177  	0.08024393  	0.06582160  
2023-05-30 20:29:34.122: Find a better model.
2023-05-30 20:29:55.301: [iter 30 : loss : 0.6629 = 0.1937 + 0.4656 + 0.0036, time: 21.175327]
2023-05-30 20:29:55.570: epoch 30:	0.02973841  	0.08062494  	0.06590591  
2023-05-30 20:29:55.570: Find a better model.
2023-05-30 20:30:16.698: [iter 31 : loss : 0.6500 = 0.1812 + 0.4650 + 0.0038, time: 21.123541]
2023-05-30 20:30:16.962: epoch 31:	0.02975321  	0.08084814  	0.06612339  
2023-05-30 20:30:16.962: Find a better model.
2023-05-30 20:30:38.064: [iter 32 : loss : 0.6390 = 0.1705 + 0.4645 + 0.0040, time: 21.098025]
2023-05-30 20:30:38.333: epoch 32:	0.02973840  	0.08043607  	0.06592266  
2023-05-30 20:30:59.276: [iter 33 : loss : 0.6291 = 0.1612 + 0.4637 + 0.0042, time: 20.939687]
2023-05-30 20:30:59.541: epoch 33:	0.02974580  	0.08018763  	0.06587558  
2023-05-30 20:31:20.656: [iter 34 : loss : 0.6202 = 0.1527 + 0.4631 + 0.0044, time: 21.111408]
2023-05-30 20:31:20.920: epoch 34:	0.02971618  	0.08003382  	0.06587388  
2023-05-30 20:31:42.052: [iter 35 : loss : 0.6114 = 0.1444 + 0.4623 + 0.0046, time: 21.129331]
2023-05-30 20:31:42.318: epoch 35:	0.02989386  	0.08038380  	0.06619029  
2023-05-30 20:32:03.326: [iter 36 : loss : 0.6042 = 0.1376 + 0.4618 + 0.0048, time: 21.004071]
2023-05-30 20:32:03.590: epoch 36:	0.02991607  	0.08004361  	0.06616576  
2023-05-30 20:32:24.855: [iter 37 : loss : 0.5981 = 0.1318 + 0.4613 + 0.0050, time: 21.261253]
2023-05-30 20:32:25.120: epoch 37:	0.02996050  	0.07995903  	0.06622984  
2023-05-30 20:32:46.038: [iter 38 : loss : 0.5914 = 0.1255 + 0.4608 + 0.0052, time: 20.914530]
2023-05-30 20:32:46.305: epoch 38:	0.02991608  	0.07983109  	0.06626946  
2023-05-30 20:33:07.253: [iter 39 : loss : 0.5864 = 0.1209 + 0.4602 + 0.0054, time: 20.943807]
2023-05-30 20:33:07.517: epoch 39:	0.02980503  	0.07950933  	0.06622840  
2023-05-30 20:33:28.426: [iter 40 : loss : 0.5810 = 0.1157 + 0.4597 + 0.0055, time: 20.904024]
2023-05-30 20:33:28.689: epoch 40:	0.02999751  	0.08013568  	0.06643137  
2023-05-30 20:33:49.625: [iter 41 : loss : 0.5757 = 0.1109 + 0.4591 + 0.0057, time: 20.933507]
2023-05-30 20:33:49.893: epoch 41:	0.02984204  	0.07974677  	0.06622812  
2023-05-30 20:34:11.005: [iter 42 : loss : 0.5716 = 0.1069 + 0.4588 + 0.0059, time: 21.107200]
2023-05-30 20:34:11.277: epoch 42:	0.02990867  	0.07988843  	0.06635422  
2023-05-30 20:34:32.240: [iter 43 : loss : 0.5675 = 0.1030 + 0.4584 + 0.0060, time: 20.959096]
2023-05-30 20:34:32.503: epoch 43:	0.02999750  	0.07975262  	0.06637607  
2023-05-30 20:34:53.623: [iter 44 : loss : 0.5646 = 0.1004 + 0.4581 + 0.0062, time: 21.115084]
2023-05-30 20:34:53.884: epoch 44:	0.02993087  	0.07992252  	0.06636751  
2023-05-30 20:35:14.984: [iter 45 : loss : 0.5604 = 0.0964 + 0.4577 + 0.0063, time: 21.096443]
2023-05-30 20:35:15.257: epoch 45:	0.02976060  	0.07909603  	0.06611672  
2023-05-30 20:35:36.194: [iter 46 : loss : 0.5572 = 0.0934 + 0.4574 + 0.0065, time: 20.933067]
2023-05-30 20:35:36.461: epoch 46:	0.02980502  	0.07895561  	0.06601571  
2023-05-30 20:35:57.585: [iter 47 : loss : 0.5538 = 0.0902 + 0.4570 + 0.0066, time: 21.119948]
2023-05-30 20:35:57.846: epoch 47:	0.02964215  	0.07859945  	0.06588083  
2023-05-30 20:36:18.761: [iter 48 : loss : 0.5514 = 0.0880 + 0.4566 + 0.0068, time: 20.911168]
2023-05-30 20:36:19.026: epoch 48:	0.02964215  	0.07833958  	0.06589134  
2023-05-30 20:36:40.159: [iter 49 : loss : 0.5485 = 0.0853 + 0.4564 + 0.0069, time: 21.128154]
2023-05-30 20:36:40.425: epoch 49:	0.02971619  	0.07846801  	0.06587546  
2023-05-30 20:37:01.531: [iter 50 : loss : 0.5464 = 0.0833 + 0.4561 + 0.0070, time: 21.103051]
2023-05-30 20:37:01.796: epoch 50:	0.02960514  	0.07866614  	0.06595580  
2023-05-30 20:37:22.765: [iter 51 : loss : 0.5437 = 0.0806 + 0.4558 + 0.0072, time: 20.966018]
2023-05-30 20:37:23.029: epoch 51:	0.02941265  	0.07794102  	0.06569646  
2023-05-30 20:37:43.902: [iter 52 : loss : 0.5412 = 0.0783 + 0.4556 + 0.0073, time: 20.870253]
2023-05-30 20:37:44.166: epoch 52:	0.02939784  	0.07761592  	0.06543268  
2023-05-30 20:38:05.331: [iter 53 : loss : 0.5402 = 0.0772 + 0.4555 + 0.0074, time: 21.162365]
2023-05-30 20:38:05.593: epoch 53:	0.02926459  	0.07705110  	0.06504734  
2023-05-30 20:38:26.533: [iter 54 : loss : 0.5378 = 0.0750 + 0.4552 + 0.0076, time: 20.937059]
2023-05-30 20:38:26.797: epoch 54:	0.02915354  	0.07688170  	0.06489999  
2023-05-30 20:38:47.708: [iter 55 : loss : 0.5355 = 0.0729 + 0.4549 + 0.0077, time: 20.907193]
2023-05-30 20:38:47.972: epoch 55:	0.02905731  	0.07653498  	0.06464191  
2023-05-30 20:39:08.945: [iter 56 : loss : 0.5338 = 0.0713 + 0.4547 + 0.0078, time: 20.969281]
2023-05-30 20:39:09.222: epoch 56:	0.02904249  	0.07627556  	0.06452122  
2023-05-30 20:39:09.223: Early stopping is trigger at epoch: 56
2023-05-30 20:39:09.223: best_result@epoch 31:

2023-05-30 20:39:09.223: 		0.0298      	0.0808      	0.0661      
2023-05-30 20:45:14.675: my pid: 7584
2023-05-30 20:45:14.675: model: model.general_recommender.SGL
2023-05-30 20:45:14.675: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 20:45:14.676: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 20:45:18.725: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 20:45:39.343: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.617111]
2023-05-30 20:45:39.614: epoch 1:	0.00142138  	0.00269782  	0.00237468  
2023-05-30 20:45:39.614: Find a better model.
2023-05-30 20:45:59.941: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.324129]
2023-05-30 20:46:00.228: epoch 2:	0.00170270  	0.00375430  	0.00287233  
2023-05-30 20:46:00.228: Find a better model.
2023-05-30 20:46:20.587: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.354052]
2023-05-30 20:46:20.876: epoch 3:	0.00216909  	0.00455388  	0.00361294  
2023-05-30 20:46:20.876: Find a better model.
2023-05-30 20:46:41.115: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.234152]
2023-05-30 20:46:41.416: epoch 4:	0.00225052  	0.00480796  	0.00368408  
2023-05-30 20:46:41.416: Find a better model.
2023-05-30 20:47:01.752: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.333115]
2023-05-30 20:47:02.047: epoch 5:	0.00264288  	0.00624910  	0.00459153  
2023-05-30 20:47:02.047: Find a better model.
2023-05-30 20:47:22.708: [iter 6 : loss : 1.1346 = 0.6925 + 0.4420 + 0.0000, time: 20.656536]
2023-05-30 20:47:22.995: epoch 6:	0.00316849  	0.00777314  	0.00560608  
2023-05-30 20:47:22.995: Find a better model.
2023-05-30 20:47:43.304: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.306602]
2023-05-30 20:47:43.595: epoch 7:	0.00381996  	0.00962996  	0.00736241  
2023-05-30 20:47:43.595: Find a better model.
2023-05-30 20:48:04.098: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.500000]
2023-05-30 20:48:04.395: epoch 8:	0.00437518  	0.01165567  	0.00878493  
2023-05-30 20:48:04.396: Find a better model.
2023-05-30 20:48:24.705: [iter 9 : loss : 1.1346 = 0.6915 + 0.4431 + 0.0000, time: 20.301982]
2023-05-30 20:48:24.984: epoch 9:	0.00526354  	0.01445002  	0.01141427  
2023-05-30 20:48:24.984: Find a better model.
2023-05-30 20:48:45.260: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.271707]
2023-05-30 20:48:45.540: epoch 10:	0.00567070  	0.01558789  	0.01314498  
2023-05-30 20:48:45.540: Find a better model.
2023-05-30 20:49:06.264: [iter 11 : loss : 1.1337 = 0.6895 + 0.4441 + 0.0000, time: 20.720264]
2023-05-30 20:49:06.544: epoch 11:	0.00635177  	0.01777657  	0.01453771  
2023-05-30 20:49:06.544: Find a better model.
2023-05-30 20:49:27.286: [iter 12 : loss : 1.1324 = 0.6876 + 0.4447 + 0.0000, time: 20.737114]
2023-05-30 20:49:27.568: epoch 12:	0.00812847  	0.02254211  	0.01885452  
2023-05-30 20:49:27.568: Find a better model.
2023-05-30 20:49:48.440: [iter 13 : loss : 1.1304 = 0.6850 + 0.4454 + 0.0001, time: 20.867302]
2023-05-30 20:49:48.716: epoch 13:	0.01043823  	0.02912708  	0.02458614  
2023-05-30 20:49:48.716: Find a better model.
2023-05-30 20:50:09.299: [iter 14 : loss : 1.1266 = 0.6804 + 0.4461 + 0.0001, time: 20.579073]
2023-05-30 20:50:09.580: epoch 14:	0.01388068  	0.03847559  	0.03205103  
2023-05-30 20:50:09.580: Find a better model.
2023-05-30 20:50:30.573: [iter 15 : loss : 1.1199 = 0.6727 + 0.4470 + 0.0001, time: 20.989377]
2023-05-30 20:50:30.845: epoch 15:	0.01733055  	0.04760133  	0.04043208  
2023-05-30 20:50:30.845: Find a better model.
2023-05-30 20:50:51.456: [iter 16 : loss : 1.1076 = 0.6593 + 0.4481 + 0.0002, time: 20.608019]
2023-05-30 20:50:51.731: epoch 16:	0.02063980  	0.05512820  	0.04748008  
2023-05-30 20:50:51.731: Find a better model.
2023-05-30 20:51:12.407: [iter 17 : loss : 1.0873 = 0.6376 + 0.4495 + 0.0003, time: 20.670612]
2023-05-30 20:51:12.684: epoch 17:	0.02358628  	0.06168555  	0.05372640  
2023-05-30 20:51:12.684: Find a better model.
2023-05-30 20:51:33.219: [iter 18 : loss : 1.0561 = 0.6043 + 0.4513 + 0.0004, time: 20.531070]
2023-05-30 20:51:33.495: epoch 18:	0.02591091  	0.06833293  	0.05845450  
2023-05-30 20:51:33.496: Find a better model.
2023-05-30 20:51:54.199: [iter 19 : loss : 1.0151 = 0.5605 + 0.4540 + 0.0006, time: 20.700062]
2023-05-30 20:51:54.480: epoch 19:	0.02745078  	0.07310096  	0.06194368  
2023-05-30 20:51:54.480: Find a better model.
2023-05-30 20:52:15.206: [iter 20 : loss : 0.9670 = 0.5090 + 0.4571 + 0.0009, time: 20.721514]
2023-05-30 20:52:15.484: epoch 20:	0.02813188  	0.07512262  	0.06354118  
2023-05-30 20:52:15.484: Find a better model.
2023-05-30 20:52:36.039: [iter 21 : loss : 0.9173 = 0.4557 + 0.4605 + 0.0011, time: 20.550997]
2023-05-30 20:52:36.308: epoch 21:	0.02868712  	0.07711518  	0.06419464  
2023-05-30 20:52:36.308: Find a better model.
2023-05-30 20:52:57.152: [iter 22 : loss : 0.8691 = 0.4040 + 0.4636 + 0.0014, time: 20.840033]
2023-05-30 20:52:57.435: epoch 22:	0.02896106  	0.07824250  	0.06462226  
2023-05-30 20:52:57.435: Find a better model.
2023-05-30 20:53:18.250: [iter 23 : loss : 0.8269 = 0.3594 + 0.4658 + 0.0017, time: 20.811397]
2023-05-30 20:53:18.517: epoch 23:	0.02917576  	0.07924923  	0.06475788  
2023-05-30 20:53:18.517: Find a better model.
2023-05-30 20:53:39.372: [iter 24 : loss : 0.7908 = 0.3215 + 0.4673 + 0.0020, time: 20.851425]
2023-05-30 20:53:39.663: epoch 24:	0.02952370  	0.08009284  	0.06537768  
2023-05-30 20:53:39.663: Find a better model.
2023-05-30 20:54:00.375: [iter 25 : loss : 0.7596 = 0.2893 + 0.4680 + 0.0023, time: 20.707492]
2023-05-30 20:54:00.628: epoch 25:	0.02954591  	0.08038359  	0.06542935  
2023-05-30 20:54:00.628: Find a better model.
2023-05-30 20:54:21.346: [iter 26 : loss : 0.7335 = 0.2629 + 0.4680 + 0.0026, time: 20.713025]
2023-05-30 20:54:21.611: epoch 26:	0.02980503  	0.08108398  	0.06587389  
2023-05-30 20:54:21.611: Find a better model.
2023-05-30 20:54:42.346: [iter 27 : loss : 0.7114 = 0.2407 + 0.4679 + 0.0029, time: 20.730397]
2023-05-30 20:54:42.614: epoch 27:	0.02983464  	0.08112137  	0.06592206  
2023-05-30 20:54:42.614: Find a better model.
2023-05-30 20:55:03.425: [iter 28 : loss : 0.6925 = 0.2218 + 0.4675 + 0.0031, time: 20.806196]
2023-05-30 20:55:03.688: epoch 28:	0.02991608  	0.08132830  	0.06619354  
2023-05-30 20:55:03.688: Find a better model.
2023-05-30 20:55:24.350: [iter 29 : loss : 0.6763 = 0.2061 + 0.4669 + 0.0034, time: 20.658027]
2023-05-30 20:55:24.616: epoch 29:	0.02999011  	0.08138189  	0.06619905  
2023-05-30 20:55:24.616: Find a better model.
2023-05-30 20:55:45.338: [iter 30 : loss : 0.6617 = 0.1919 + 0.4663 + 0.0036, time: 20.717103]
2023-05-30 20:55:45.601: epoch 30:	0.02997531  	0.08103607  	0.06612320  
2023-05-30 20:56:06.351: [iter 31 : loss : 0.6488 = 0.1794 + 0.4656 + 0.0038, time: 20.747258]
2023-05-30 20:56:06.615: epoch 31:	0.03004193  	0.08108579  	0.06634777  
2023-05-30 20:56:27.372: [iter 32 : loss : 0.6380 = 0.1690 + 0.4650 + 0.0040, time: 20.752102]
2023-05-30 20:56:27.639: epoch 32:	0.03004194  	0.08091380  	0.06621337  
2023-05-30 20:56:48.559: [iter 33 : loss : 0.6286 = 0.1602 + 0.4642 + 0.0042, time: 20.916105]
2023-05-30 20:56:48.823: epoch 33:	0.03007895  	0.08074674  	0.06628810  
2023-05-30 20:57:09.710: [iter 34 : loss : 0.6194 = 0.1514 + 0.4635 + 0.0044, time: 20.884380]
2023-05-30 20:57:09.971: epoch 34:	0.03010116  	0.08090138  	0.06644979  
2023-05-30 20:57:30.904: [iter 35 : loss : 0.6108 = 0.1433 + 0.4628 + 0.0046, time: 20.929930]
2023-05-30 20:57:31.171: epoch 35:	0.03014558  	0.08106139  	0.06658781  
2023-05-30 20:57:51.878: [iter 36 : loss : 0.6036 = 0.1366 + 0.4622 + 0.0048, time: 20.703800]
2023-05-30 20:57:52.142: epoch 36:	0.03013077  	0.08081486  	0.06656577  
2023-05-30 20:58:13.062: [iter 37 : loss : 0.5975 = 0.1308 + 0.4617 + 0.0050, time: 20.915490]
2023-05-30 20:58:13.329: epoch 37:	0.03012338  	0.08097503  	0.06670129  
2023-05-30 20:58:34.289: [iter 38 : loss : 0.5911 = 0.1247 + 0.4612 + 0.0052, time: 20.956510]
2023-05-30 20:58:34.557: epoch 38:	0.03020480  	0.08101495  	0.06666487  
2023-05-30 20:58:55.335: [iter 39 : loss : 0.5860 = 0.1202 + 0.4604 + 0.0054, time: 20.774242]
2023-05-30 20:58:55.602: epoch 39:	0.03015297  	0.08115063  	0.06663352  
2023-05-30 20:59:16.490: [iter 40 : loss : 0.5803 = 0.1147 + 0.4601 + 0.0055, time: 20.884414]
2023-05-30 20:59:16.752: epoch 40:	0.03019740  	0.08115232  	0.06667980  
2023-05-30 20:59:37.887: [iter 41 : loss : 0.5754 = 0.1100 + 0.4597 + 0.0057, time: 21.131463]
2023-05-30 20:59:38.150: epoch 41:	0.03023440  	0.08163941  	0.06684626  
2023-05-30 20:59:38.150: Find a better model.
2023-05-30 20:59:59.263: [iter 42 : loss : 0.5713 = 0.1062 + 0.4592 + 0.0059, time: 21.108702]
2023-05-30 20:59:59.537: epoch 42:	0.03032324  	0.08208378  	0.06691049  
2023-05-30 20:59:59.537: Find a better model.
2023-05-30 21:00:20.680: [iter 43 : loss : 0.5670 = 0.1021 + 0.4589 + 0.0060, time: 21.140276]
2023-05-30 21:00:20.947: epoch 43:	0.03016037  	0.08125022  	0.06654168  
2023-05-30 21:00:42.065: [iter 44 : loss : 0.5641 = 0.0995 + 0.4584 + 0.0062, time: 21.113339]
2023-05-30 21:00:42.331: epoch 44:	0.03014557  	0.08088276  	0.06650414  
2023-05-30 21:01:03.439: [iter 45 : loss : 0.5606 = 0.0962 + 0.4581 + 0.0063, time: 21.103030]
2023-05-30 21:01:03.705: epoch 45:	0.03021222  	0.08086780  	0.06645117  
2023-05-30 21:01:25.053: [iter 46 : loss : 0.5573 = 0.0930 + 0.4578 + 0.0065, time: 21.345054]
2023-05-30 21:01:25.320: epoch 46:	0.03020481  	0.08073564  	0.06645935  
2023-05-30 21:01:47.442: [iter 47 : loss : 0.5544 = 0.0904 + 0.4574 + 0.0066, time: 22.119537]
2023-05-30 21:01:47.720: epoch 47:	0.03016039  	0.08056356  	0.06636257  
2023-05-30 21:02:06.414: my pid: 4796
2023-05-30 21:02:06.414: model: model.general_recommender.SGL
2023-05-30 21:02:06.414: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 21:02:06.414: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 21:02:10.813: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 21:02:32.980: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.165815]
2023-05-30 21:02:33.273: epoch 1:	0.00120669  	0.00241532  	0.00189338  
2023-05-30 21:02:33.273: Find a better model.
2023-05-30 21:02:54.460: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.183440]
2023-05-30 21:02:54.752: epoch 2:	0.00179153  	0.00332253  	0.00294850  
2023-05-30 21:02:54.752: Find a better model.
2023-05-30 21:03:15.637: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.882332]
2023-05-30 21:03:15.933: epoch 3:	0.00205804  	0.00447957  	0.00345028  
2023-05-30 21:03:15.933: Find a better model.
2023-05-30 21:03:36.839: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.903455]
2023-05-30 21:03:37.137: epoch 4:	0.00199141  	0.00433370  	0.00366472  
2023-05-30 21:03:58.009: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.869020]
2023-05-30 21:03:58.307: epoch 5:	0.00280574  	0.00650740  	0.00489819  
2023-05-30 21:03:58.307: Find a better model.
2023-05-30 21:04:19.000: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.688545]
2023-05-30 21:04:19.297: epoch 6:	0.00294640  	0.00738273  	0.00549240  
2023-05-30 21:04:19.297: Find a better model.
2023-05-30 21:04:40.031: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.730113]
2023-05-30 21:04:40.327: epoch 7:	0.00357566  	0.00907046  	0.00679777  
2023-05-30 21:04:40.327: Find a better model.
2023-05-30 21:05:01.178: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.847705]
2023-05-30 21:05:01.474: epoch 8:	0.00437518  	0.01090658  	0.00868914  
2023-05-30 21:05:01.474: Find a better model.
2023-05-30 21:05:22.173: [iter 9 : loss : 1.1347 = 0.6916 + 0.4431 + 0.0000, time: 20.695433]
2023-05-30 21:05:22.469: epoch 9:	0.00508586  	0.01361679  	0.01032700  
2023-05-30 21:05:22.469: Find a better model.
2023-05-30 21:05:43.178: [iter 10 : loss : 1.1344 = 0.6909 + 0.4435 + 0.0000, time: 20.706506]
2023-05-30 21:05:43.474: epoch 10:	0.00578174  	0.01630088  	0.01221930  
2023-05-30 21:05:43.475: Find a better model.
2023-05-30 21:06:04.774: [iter 11 : loss : 1.1338 = 0.6898 + 0.4440 + 0.0000, time: 21.296007]
2023-05-30 21:06:05.057: epoch 11:	0.00655905  	0.01994221  	0.01491930  
2023-05-30 21:06:05.057: Find a better model.
2023-05-30 21:06:26.337: [iter 12 : loss : 1.1326 = 0.6879 + 0.4446 + 0.0000, time: 21.276005]
2023-05-30 21:06:26.624: epoch 12:	0.00802483  	0.02398554  	0.01873304  
2023-05-30 21:06:26.624: Find a better model.
2023-05-30 21:06:47.971: [iter 13 : loss : 1.1307 = 0.6853 + 0.4453 + 0.0000, time: 21.342581]
2023-05-30 21:06:48.258: epoch 13:	0.01017172  	0.03024547  	0.02437784  
2023-05-30 21:06:48.258: Find a better model.
2023-05-30 21:07:09.545: [iter 14 : loss : 1.1272 = 0.6810 + 0.4461 + 0.0001, time: 21.283363]
2023-05-30 21:07:09.823: epoch 14:	0.01325141  	0.03840404  	0.03142869  
2023-05-30 21:07:09.823: Find a better model.
2023-05-30 21:07:30.905: [iter 15 : loss : 1.1208 = 0.6738 + 0.4469 + 0.0001, time: 21.078306]
2023-05-30 21:07:31.186: epoch 15:	0.01675312  	0.04672579  	0.03892367  
2023-05-30 21:07:31.186: Find a better model.
2023-05-30 21:07:52.144: [iter 16 : loss : 1.1092 = 0.6611 + 0.4479 + 0.0002, time: 20.954502]
2023-05-30 21:07:52.439: epoch 16:	0.02032147  	0.05596767  	0.04649884  
2023-05-30 21:07:52.439: Find a better model.
2023-05-30 21:08:13.331: [iter 17 : loss : 1.0900 = 0.6404 + 0.4493 + 0.0003, time: 20.889107]
2023-05-30 21:08:13.615: epoch 17:	0.02328276  	0.06334151  	0.05294894  
2023-05-30 21:08:13.615: Find a better model.
2023-05-30 21:08:34.712: [iter 18 : loss : 1.0598 = 0.6084 + 0.4510 + 0.0004, time: 21.093158]
2023-05-30 21:08:34.988: epoch 18:	0.02569622  	0.06911952  	0.05739954  
2023-05-30 21:08:34.988: Find a better model.
2023-05-30 21:08:56.050: [iter 19 : loss : 1.0199 = 0.5658 + 0.4535 + 0.0006, time: 21.058117]
2023-05-30 21:08:56.330: epoch 19:	0.02723611  	0.07352336  	0.06072082  
2023-05-30 21:08:56.330: Find a better model.
2023-05-30 21:09:17.489: [iter 20 : loss : 0.9728 = 0.5153 + 0.4568 + 0.0008, time: 21.156072]
2023-05-30 21:09:17.764: epoch 20:	0.02813191  	0.07616562  	0.06260053  
2023-05-30 21:09:17.764: Find a better model.
2023-05-30 21:09:38.845: [iter 21 : loss : 0.9231 = 0.4620 + 0.4600 + 0.0011, time: 21.077590]
2023-05-30 21:09:39.126: epoch 21:	0.02848726  	0.07724686  	0.06328377  
2023-05-30 21:09:39.126: Find a better model.
2023-05-30 21:10:00.091: [iter 22 : loss : 0.8746 = 0.4102 + 0.4630 + 0.0014, time: 20.961041]
2023-05-30 21:10:00.352: epoch 22:	0.02892405  	0.07881445  	0.06407680  
2023-05-30 21:10:00.352: Find a better model.
2023-05-30 21:10:21.432: [iter 23 : loss : 0.8321 = 0.3650 + 0.4654 + 0.0017, time: 21.076447]
2023-05-30 21:10:21.702: epoch 23:	0.02910913  	0.07956208  	0.06450129  
2023-05-30 21:10:21.702: Find a better model.
2023-05-30 21:10:42.655: [iter 24 : loss : 0.7950 = 0.3260 + 0.4669 + 0.0020, time: 20.948162]
2023-05-30 21:10:42.925: epoch 24:	0.02929422  	0.08051959  	0.06482742  
2023-05-30 21:10:42.925: Find a better model.
2023-05-30 21:11:04.046: [iter 25 : loss : 0.7633 = 0.2935 + 0.4675 + 0.0023, time: 21.116340]
2023-05-30 21:11:04.313: epoch 25:	0.02941267  	0.08062044  	0.06505026  
2023-05-30 21:11:04.313: Find a better model.
2023-05-30 21:11:25.430: [iter 26 : loss : 0.7370 = 0.2666 + 0.4679 + 0.0026, time: 21.113840]
2023-05-30 21:11:25.698: epoch 26:	0.02952373  	0.08094672  	0.06525305  
2023-05-30 21:11:25.698: Find a better model.
2023-05-30 21:11:46.693: [iter 27 : loss : 0.7144 = 0.2438 + 0.4677 + 0.0028, time: 20.990539]
2023-05-30 21:11:46.964: epoch 27:	0.02949412  	0.08082008  	0.06534993  
2023-05-30 21:12:08.213: [iter 28 : loss : 0.6953 = 0.2248 + 0.4674 + 0.0031, time: 21.244759]
2023-05-30 21:12:08.485: epoch 28:	0.02957556  	0.08110461  	0.06567527  
2023-05-30 21:12:08.485: Find a better model.
2023-05-30 21:12:29.572: [iter 29 : loss : 0.6785 = 0.2084 + 0.4668 + 0.0033, time: 21.082042]
2023-05-30 21:12:29.839: epoch 29:	0.02966440  	0.08192719  	0.06603741  
2023-05-30 21:12:29.839: Find a better model.
2023-05-30 21:12:50.974: [iter 30 : loss : 0.6635 = 0.1938 + 0.4662 + 0.0036, time: 21.130549]
2023-05-30 21:12:51.241: epoch 30:	0.02954595  	0.08185458  	0.06606278  
2023-05-30 21:13:12.399: [iter 31 : loss : 0.6504 = 0.1811 + 0.4655 + 0.0038, time: 21.155267]
2023-05-30 21:13:12.668: epoch 31:	0.02959778  	0.08201405  	0.06613743  
2023-05-30 21:13:12.668: Find a better model.
2023-05-30 21:13:33.760: [iter 32 : loss : 0.6394 = 0.1707 + 0.4647 + 0.0040, time: 21.088142]
2023-05-30 21:13:34.028: epoch 32:	0.02966442  	0.08231588  	0.06641627  
2023-05-30 21:13:34.028: Find a better model.
2023-05-30 21:13:55.152: [iter 33 : loss : 0.6295 = 0.1612 + 0.4641 + 0.0042, time: 21.119406]
2023-05-30 21:13:55.432: epoch 33:	0.02978287  	0.08226800  	0.06639765  
2023-05-30 21:14:16.811: [iter 34 : loss : 0.6206 = 0.1527 + 0.4635 + 0.0044, time: 21.375073]
2023-05-30 21:14:17.080: epoch 34:	0.02978287  	0.08239872  	0.06642054  
2023-05-30 21:14:17.080: Find a better model.
2023-05-30 21:14:38.189: [iter 35 : loss : 0.6114 = 0.1441 + 0.4627 + 0.0046, time: 21.105255]
2023-05-30 21:14:38.464: epoch 35:	0.02981247  	0.08236783  	0.06665350  
2023-05-30 21:14:59.738: [iter 36 : loss : 0.6050 = 0.1380 + 0.4622 + 0.0048, time: 21.270393]
2023-05-30 21:15:00.007: epoch 36:	0.02987910  	0.08279063  	0.06687434  
2023-05-30 21:15:00.007: Find a better model.
2023-05-30 21:15:21.376: [iter 37 : loss : 0.5986 = 0.1320 + 0.4616 + 0.0050, time: 21.364716]
2023-05-30 21:15:21.646: epoch 37:	0.02985689  	0.08233834  	0.06671247  
2023-05-30 21:15:42.956: [iter 38 : loss : 0.5919 = 0.1257 + 0.4611 + 0.0052, time: 21.306754]
2023-05-30 21:15:43.223: epoch 38:	0.02984208  	0.08235557  	0.06683059  
2023-05-30 21:16:04.741: [iter 39 : loss : 0.5869 = 0.1210 + 0.4606 + 0.0054, time: 21.514996]
2023-05-30 21:16:05.011: epoch 39:	0.03007899  	0.08286439  	0.06709442  
2023-05-30 21:16:05.011: Find a better model.
2023-05-30 21:16:26.512: [iter 40 : loss : 0.5812 = 0.1156 + 0.4600 + 0.0055, time: 21.498156]
2023-05-30 21:16:26.782: epoch 40:	0.02999756  	0.08270700  	0.06704471  
2023-05-30 21:16:48.171: [iter 41 : loss : 0.5762 = 0.1109 + 0.4597 + 0.0057, time: 21.384075]
2023-05-30 21:16:48.449: epoch 41:	0.02996054  	0.08247616  	0.06703421  
2023-05-30 21:17:09.742: [iter 42 : loss : 0.5720 = 0.1069 + 0.4593 + 0.0059, time: 21.288303]
2023-05-30 21:17:10.009: epoch 42:	0.02990131  	0.08240520  	0.06702408  
2023-05-30 21:17:31.278: [iter 43 : loss : 0.5680 = 0.1031 + 0.4588 + 0.0060, time: 21.265998]
2023-05-30 21:17:31.549: epoch 43:	0.02988650  	0.08186757  	0.06682885  
2023-05-30 21:17:52.665: [iter 44 : loss : 0.5650 = 0.1004 + 0.4584 + 0.0062, time: 21.113181]
2023-05-30 21:17:52.932: epoch 44:	0.02984209  	0.08177335  	0.06673831  
2023-05-30 21:18:14.302: [iter 45 : loss : 0.5609 = 0.0965 + 0.4581 + 0.0063, time: 21.367334]
2023-05-30 21:18:14.573: epoch 45:	0.02977546  	0.08147896  	0.06652578  
2023-05-30 21:18:35.688: [iter 46 : loss : 0.5574 = 0.0933 + 0.4576 + 0.0065, time: 21.112063]
2023-05-30 21:18:35.959: epoch 46:	0.02970882  	0.08112244  	0.06626471  
2023-05-30 21:18:57.052: [iter 47 : loss : 0.5544 = 0.0904 + 0.4574 + 0.0066, time: 21.088063]
2023-05-30 21:18:57.319: epoch 47:	0.02961258  	0.08103374  	0.06614667  
2023-05-30 21:19:18.688: [iter 48 : loss : 0.5519 = 0.0880 + 0.4571 + 0.0068, time: 21.364861]
2023-05-30 21:19:18.958: epoch 48:	0.02960517  	0.08081412  	0.06617085  
2023-05-30 21:19:40.253: [iter 49 : loss : 0.5491 = 0.0855 + 0.4568 + 0.0069, time: 21.290070]
2023-05-30 21:19:40.528: epoch 49:	0.02958295  	0.08063084  	0.06610875  
2023-05-30 21:20:01.472: [iter 50 : loss : 0.5472 = 0.0837 + 0.4565 + 0.0070, time: 20.939920]
2023-05-30 21:20:01.737: epoch 50:	0.02941268  	0.08021217  	0.06585070  
2023-05-30 21:20:22.868: [iter 51 : loss : 0.5441 = 0.0808 + 0.4562 + 0.0072, time: 21.126432]
2023-05-30 21:20:23.139: epoch 51:	0.02946450  	0.08060987  	0.06589765  
2023-05-30 21:20:44.405: [iter 52 : loss : 0.5415 = 0.0784 + 0.4559 + 0.0073, time: 21.263106]
2023-05-30 21:20:44.669: epoch 52:	0.02932384  	0.08036098  	0.06585550  
2023-05-30 21:21:05.624: [iter 53 : loss : 0.5404 = 0.0773 + 0.4557 + 0.0074, time: 20.951957]
2023-05-30 21:21:05.893: epoch 53:	0.02919798  	0.07985856  	0.06552972  
2023-05-30 21:21:27.213: [iter 54 : loss : 0.5383 = 0.0753 + 0.4555 + 0.0076, time: 21.316045]
2023-05-30 21:21:27.485: epoch 54:	0.02912395  	0.07921611  	0.06539550  
2023-05-30 21:21:48.854: [iter 55 : loss : 0.5362 = 0.0733 + 0.4552 + 0.0077, time: 21.364977]
2023-05-30 21:21:49.122: epoch 55:	0.02908693  	0.07900195  	0.06534316  
2023-05-30 21:22:10.398: [iter 56 : loss : 0.5340 = 0.0712 + 0.4550 + 0.0078, time: 21.272116]
2023-05-30 21:22:10.666: epoch 56:	0.02908693  	0.07913595  	0.06526330  
2023-05-30 21:22:31.799: [iter 57 : loss : 0.5320 = 0.0692 + 0.4549 + 0.0079, time: 21.128489]
2023-05-30 21:22:32.069: epoch 57:	0.02905731  	0.07898723  	0.06519233  
2023-05-30 21:22:53.398: [iter 58 : loss : 0.5311 = 0.0683 + 0.4547 + 0.0080, time: 21.326093]
2023-05-30 21:22:53.666: epoch 58:	0.02908691  	0.07886646  	0.06516510  
2023-05-30 21:23:14.780: [iter 59 : loss : 0.5292 = 0.0665 + 0.4545 + 0.0082, time: 21.110594]
2023-05-30 21:23:15.052: epoch 59:	0.02907950  	0.07857859  	0.06498104  
2023-05-30 21:23:36.171: [iter 60 : loss : 0.5280 = 0.0653 + 0.4544 + 0.0083, time: 21.114093]
2023-05-30 21:23:36.445: epoch 60:	0.02901288  	0.07830239  	0.06482487  
2023-05-30 21:23:57.566: [iter 61 : loss : 0.5267 = 0.0641 + 0.4542 + 0.0084, time: 21.116438]
2023-05-30 21:23:57.835: epoch 61:	0.02894625  	0.07797405  	0.06484824  
2023-05-30 21:24:18.997: [iter 62 : loss : 0.5254 = 0.0628 + 0.4540 + 0.0085, time: 21.157913]
2023-05-30 21:24:19.263: epoch 62:	0.02884261  	0.07762864  	0.06458144  
2023-05-30 21:24:40.377: [iter 63 : loss : 0.5240 = 0.0616 + 0.4539 + 0.0086, time: 21.109210]
2023-05-30 21:24:40.643: epoch 63:	0.02879078  	0.07765194  	0.06462392  
2023-05-30 21:25:01.739: [iter 64 : loss : 0.5230 = 0.0605 + 0.4537 + 0.0087, time: 21.092061]
2023-05-30 21:25:02.006: epoch 64:	0.02875377  	0.07755046  	0.06444410  
2023-05-30 21:25:02.006: Early stopping is trigger at epoch: 64
2023-05-30 21:25:02.007: best_result@epoch 39:

2023-05-30 21:25:02.007: 		0.0301      	0.0829      	0.0671      
2023-05-30 21:33:02.362: my pid: 14676
2023-05-30 21:33:02.362: model: model.general_recommender.SGL
2023-05-30 21:33:02.362: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-30 21:33:02.362: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-30 21:33:06.400: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-30 21:33:27.203: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.803708]
2023-05-30 21:33:27.474: epoch 1:	0.00122890  	0.00216991  	0.00187332  
2023-05-30 21:33:27.474: Find a better model.
2023-05-30 21:33:48.588: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 21.110100]
2023-05-30 21:33:48.881: epoch 2:	0.00175452  	0.00344043  	0.00278641  
2023-05-30 21:33:48.881: Find a better model.
2023-05-30 21:34:09.778: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 20.893201]
2023-05-30 21:34:10.081: epoch 3:	0.00194700  	0.00418832  	0.00323903  
2023-05-30 21:34:10.081: Find a better model.
2023-05-30 21:34:30.986: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.900477]
2023-05-30 21:34:31.281: epoch 4:	0.00215428  	0.00438038  	0.00337343  
2023-05-30 21:34:31.281: Find a better model.
2023-05-30 21:34:52.396: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.111969]
2023-05-30 21:34:52.690: epoch 5:	0.00245040  	0.00511253  	0.00405294  
2023-05-30 21:34:52.690: Find a better model.
2023-05-30 21:35:13.577: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.883391]
2023-05-30 21:35:13.869: epoch 6:	0.00315369  	0.00743910  	0.00582560  
2023-05-30 21:35:13.869: Find a better model.
2023-05-30 21:35:34.743: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 20.871775]
2023-05-30 21:35:35.049: epoch 7:	0.00362008  	0.00885998  	0.00723560  
2023-05-30 21:35:35.049: Find a better model.
2023-05-30 21:35:55.793: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 20.738960]
2023-05-30 21:35:56.094: epoch 8:	0.00419011  	0.01078786  	0.00872972  
2023-05-30 21:35:56.094: Find a better model.
2023-05-30 21:36:16.969: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.871547]
2023-05-30 21:36:17.260: epoch 9:	0.00490079  	0.01349663  	0.01042035  
2023-05-30 21:36:17.260: Find a better model.
2023-05-30 21:36:38.176: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.913190]
2023-05-30 21:36:38.466: epoch 10:	0.00579655  	0.01641091  	0.01310133  
2023-05-30 21:36:38.466: Find a better model.
2023-05-30 21:36:59.942: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 21.472481]
2023-05-30 21:37:00.230: epoch 11:	0.00664049  	0.01867322  	0.01510197  
2023-05-30 21:37:00.230: Find a better model.
2023-05-30 21:37:21.750: [iter 12 : loss : 1.1326 = 0.6881 + 0.4445 + 0.0000, time: 21.516606]
2023-05-30 21:37:22.050: epoch 12:	0.00781755  	0.02230028  	0.01828786  
2023-05-30 21:37:22.050: Find a better model.
2023-05-30 21:37:43.328: [iter 13 : loss : 1.1307 = 0.6854 + 0.4452 + 0.0000, time: 21.273314]
2023-05-30 21:37:43.610: epoch 13:	0.00986078  	0.02724789  	0.02334674  
2023-05-30 21:37:43.610: Find a better model.
2023-05-30 21:38:05.108: [iter 14 : loss : 1.1272 = 0.6812 + 0.4460 + 0.0001, time: 21.494202]
2023-05-30 21:38:05.394: epoch 14:	0.01294047  	0.03566554  	0.03046292  
2023-05-30 21:38:05.394: Find a better model.
2023-05-30 21:38:26.485: [iter 15 : loss : 1.1210 = 0.6740 + 0.4469 + 0.0001, time: 21.087557]
2023-05-30 21:38:26.765: epoch 15:	0.01631632  	0.04479185  	0.03874275  
2023-05-30 21:38:26.765: Find a better model.
2023-05-30 21:38:48.085: [iter 16 : loss : 1.1095 = 0.6615 + 0.4479 + 0.0002, time: 21.315060]
2023-05-30 21:38:48.366: epoch 16:	0.01956631  	0.05282305  	0.04595799  
2023-05-30 21:38:48.366: Find a better model.
2023-05-30 21:39:09.641: [iter 17 : loss : 1.0903 = 0.6409 + 0.4491 + 0.0003, time: 21.270255]
2023-05-30 21:39:09.916: epoch 17:	0.02260165  	0.06024959  	0.05222830  
2023-05-30 21:39:09.917: Find a better model.
2023-05-30 21:39:30.819: [iter 18 : loss : 1.0605 = 0.6092 + 0.4509 + 0.0004, time: 20.899030]
2023-05-30 21:39:31.100: epoch 18:	0.02511876  	0.06681243  	0.05737597  
2023-05-30 21:39:31.100: Find a better model.
2023-05-30 21:39:52.202: [iter 19 : loss : 1.0206 = 0.5668 + 0.4532 + 0.0006, time: 21.098282]
2023-05-30 21:39:52.478: epoch 19:	0.02656982  	0.07097889  	0.06069514  
2023-05-30 21:39:52.478: Find a better model.
2023-05-30 21:40:13.628: [iter 20 : loss : 0.9737 = 0.5165 + 0.4563 + 0.0008, time: 21.146027]
2023-05-30 21:40:13.903: epoch 20:	0.02819854  	0.07543724  	0.06364478  
2023-05-30 21:40:13.903: Find a better model.
2023-05-30 21:40:34.985: [iter 21 : loss : 0.9240 = 0.4635 + 0.4594 + 0.0011, time: 21.077104]
2023-05-30 21:40:35.246: epoch 21:	0.02865013  	0.07685841  	0.06467857  
2023-05-30 21:40:35.246: Find a better model.
2023-05-30 21:40:56.188: [iter 22 : loss : 0.8757 = 0.4120 + 0.4624 + 0.0014, time: 20.938051]
2023-05-30 21:40:56.459: epoch 22:	0.02900550  	0.07870359  	0.06562185  
2023-05-30 21:40:56.459: Find a better model.
2023-05-30 21:41:17.565: [iter 23 : loss : 0.8330 = 0.3667 + 0.4647 + 0.0017, time: 21.102621]
2023-05-30 21:41:17.833: epoch 23:	0.02922760  	0.07974552  	0.06607373  
2023-05-30 21:41:17.833: Find a better model.
2023-05-30 21:41:38.782: [iter 24 : loss : 0.7959 = 0.3276 + 0.4662 + 0.0020, time: 20.944031]
2023-05-30 21:41:39.058: epoch 24:	0.02942008  	0.08043703  	0.06639017  
2023-05-30 21:41:39.058: Find a better model.
2023-05-30 21:41:59.947: [iter 25 : loss : 0.7639 = 0.2945 + 0.4671 + 0.0023, time: 20.885040]
2023-05-30 21:42:00.213: epoch 25:	0.02960516  	0.08117715  	0.06682988  
2023-05-30 21:42:00.213: Find a better model.
2023-05-30 21:42:21.355: [iter 26 : loss : 0.7376 = 0.2675 + 0.4675 + 0.0026, time: 21.138005]
2023-05-30 21:42:21.620: epoch 26:	0.02964216  	0.08091934  	0.06673943  
2023-05-30 21:42:42.936: [iter 27 : loss : 0.7146 = 0.2443 + 0.4674 + 0.0028, time: 21.313090]
2023-05-30 21:42:43.201: epoch 27:	0.02972362  	0.08143643  	0.06692794  
2023-05-30 21:42:43.202: Find a better model.
2023-05-30 21:43:04.350: [iter 28 : loss : 0.6952 = 0.2250 + 0.4671 + 0.0031, time: 21.144381]
2023-05-30 21:43:04.612: epoch 28:	0.02990868  	0.08226302  	0.06725652  
2023-05-30 21:43:04.612: Find a better model.
2023-05-30 21:43:25.907: [iter 29 : loss : 0.6788 = 0.2088 + 0.4666 + 0.0033, time: 21.292067]
2023-05-30 21:43:26.173: epoch 29:	0.02987907  	0.08194381  	0.06737227  
2023-05-30 21:43:47.383: [iter 30 : loss : 0.6637 = 0.1941 + 0.4660 + 0.0036, time: 21.206973]
2023-05-30 21:43:47.645: epoch 30:	0.02983466  	0.08139946  	0.06728280  
2023-05-30 21:44:08.900: [iter 31 : loss : 0.6505 = 0.1813 + 0.4654 + 0.0038, time: 21.251448]
2023-05-30 21:44:09.168: epoch 31:	0.02989389  	0.08180796  	0.06741058  
2023-05-30 21:44:30.542: [iter 32 : loss : 0.6396 = 0.1709 + 0.4647 + 0.0040, time: 21.371068]
2023-05-30 21:44:30.804: epoch 32:	0.02994571  	0.08239765  	0.06766073  
2023-05-30 21:44:30.804: Find a better model.
2023-05-30 21:44:52.295: [iter 33 : loss : 0.6297 = 0.1615 + 0.4640 + 0.0042, time: 21.487017]
2023-05-30 21:44:52.553: epoch 33:	0.02993091  	0.08202963  	0.06762061  
2023-05-30 21:45:13.701: [iter 34 : loss : 0.6206 = 0.1528 + 0.4633 + 0.0044, time: 21.144048]
2023-05-30 21:45:13.960: epoch 34:	0.02993831  	0.08199567  	0.06773097  
2023-05-30 21:45:35.279: [iter 35 : loss : 0.6116 = 0.1443 + 0.4627 + 0.0046, time: 21.315008]
2023-05-30 21:45:35.538: epoch 35:	0.02993832  	0.08178481  	0.06768019  
2023-05-30 21:45:56.895: [iter 36 : loss : 0.6048 = 0.1379 + 0.4621 + 0.0048, time: 21.353096]
2023-05-30 21:45:57.162: epoch 36:	0.02992351  	0.08183885  	0.06769843  
2023-05-30 21:46:18.287: [iter 37 : loss : 0.5982 = 0.1316 + 0.4617 + 0.0050, time: 21.122048]
2023-05-30 21:46:18.548: epoch 37:	0.02991611  	0.08160078  	0.06767778  
2023-05-30 21:46:39.880: [iter 38 : loss : 0.5921 = 0.1258 + 0.4611 + 0.0052, time: 21.328980]
2023-05-30 21:46:40.147: epoch 38:	0.02990131  	0.08156717  	0.06787918  
2023-05-30 21:47:01.461: [iter 39 : loss : 0.5870 = 0.1210 + 0.4606 + 0.0054, time: 21.309165]
2023-05-30 21:47:01.720: epoch 39:	0.02988649  	0.08154686  	0.06792636  
2023-05-30 21:47:22.871: [iter 40 : loss : 0.5814 = 0.1158 + 0.4600 + 0.0055, time: 21.147537]
2023-05-30 21:47:23.139: epoch 40:	0.02981987  	0.08088338  	0.06782596  
2023-05-30 21:47:44.428: [iter 41 : loss : 0.5763 = 0.1111 + 0.4595 + 0.0057, time: 21.285035]
2023-05-30 21:47:44.692: epoch 41:	0.02978285  	0.08102991  	0.06777092  
2023-05-30 21:48:05.868: [iter 42 : loss : 0.5720 = 0.1069 + 0.4592 + 0.0059, time: 21.173069]
2023-05-30 21:48:06.134: epoch 42:	0.02974583  	0.08065226  	0.06772378  
2023-05-30 21:48:27.203: [iter 43 : loss : 0.5679 = 0.1031 + 0.4588 + 0.0060, time: 21.065990]
2023-05-30 21:48:27.462: epoch 43:	0.02973101  	0.08015589  	0.06755175  
2023-05-30 21:48:48.619: [iter 44 : loss : 0.5653 = 0.1007 + 0.4584 + 0.0062, time: 21.152020]
2023-05-30 21:48:48.878: epoch 44:	0.02966439  	0.07998882  	0.06763299  
2023-05-30 21:49:10.196: [iter 45 : loss : 0.5611 = 0.0968 + 0.4580 + 0.0063, time: 21.313462]
2023-05-30 21:49:10.456: epoch 45:	0.02958295  	0.07959178  	0.06747286  
2023-05-30 21:49:31.651: [iter 46 : loss : 0.5577 = 0.0935 + 0.4577 + 0.0065, time: 21.190980]
2023-05-30 21:49:31.910: epoch 46:	0.02950891  	0.07944810  	0.06739733  
2023-05-30 21:49:53.173: [iter 47 : loss : 0.5546 = 0.0906 + 0.4574 + 0.0066, time: 21.257986]
2023-05-30 21:49:53.432: epoch 47:	0.02951632  	0.07958933  	0.06727818  
2023-05-30 21:50:14.800: [iter 48 : loss : 0.5520 = 0.0883 + 0.4570 + 0.0068, time: 21.364985]
2023-05-30 21:50:15.069: epoch 48:	0.02947190  	0.07959270  	0.06698452  
2023-05-30 21:50:36.014: [iter 49 : loss : 0.5494 = 0.0858 + 0.4567 + 0.0069, time: 20.940617]
2023-05-30 21:50:36.273: epoch 49:	0.02944229  	0.07950671  	0.06688564  
2023-05-30 21:50:57.196: [iter 50 : loss : 0.5471 = 0.0835 + 0.4565 + 0.0070, time: 20.919630]
2023-05-30 21:50:57.454: epoch 50:	0.02935344  	0.07909476  	0.06661761  
2023-05-30 21:51:18.766: [iter 51 : loss : 0.5442 = 0.0809 + 0.4561 + 0.0072, time: 21.308098]
2023-05-30 21:51:19.038: epoch 51:	0.02927941  	0.07874460  	0.06639161  
2023-05-30 21:51:40.376: [iter 52 : loss : 0.5416 = 0.0785 + 0.4558 + 0.0073, time: 21.333993]
2023-05-30 21:51:40.635: epoch 52:	0.02930901  	0.07886201  	0.06631175  
2023-05-30 21:52:01.952: [iter 53 : loss : 0.5405 = 0.0773 + 0.4557 + 0.0074, time: 21.313704]
2023-05-30 21:52:02.212: epoch 53:	0.02923498  	0.07865895  	0.06615843  
2023-05-30 21:52:23.350: [iter 54 : loss : 0.5385 = 0.0756 + 0.4554 + 0.0076, time: 21.135060]
2023-05-30 21:52:23.609: epoch 54:	0.02922017  	0.07862282  	0.06617373  
2023-05-30 21:52:44.728: [iter 55 : loss : 0.5363 = 0.0734 + 0.4553 + 0.0077, time: 21.116038]
2023-05-30 21:52:44.986: epoch 55:	0.02901288  	0.07794838  	0.06588206  
2023-05-30 21:53:06.146: [iter 56 : loss : 0.5343 = 0.0714 + 0.4551 + 0.0078, time: 21.155212]
2023-05-30 21:53:06.404: epoch 56:	0.02906470  	0.07799028  	0.06562237  
2023-05-30 21:53:27.697: [iter 57 : loss : 0.5325 = 0.0697 + 0.4548 + 0.0079, time: 21.289819]
2023-05-30 21:53:27.954: epoch 57:	0.02909432  	0.07803737  	0.06563424  
2023-05-30 21:53:27.954: Early stopping is trigger at epoch: 57
2023-05-30 21:53:27.954: best_result@epoch 32:

2023-05-30 21:53:27.954: 		0.0299      	0.0824      	0.0677      
2023-05-31 09:13:43.615: my pid: 7700
2023-05-31 09:13:43.615: model: model.general_recommender.SGL
2023-05-31 09:13:43.615: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 09:13:43.615: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 09:13:47.670: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 09:14:08.651: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.981279]
2023-05-31 09:14:08.918: epoch 1:	0.00140657  	0.00299336  	0.00225154  
2023-05-31 09:14:08.918: Find a better model.
2023-05-31 09:14:30.019: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.098037]
2023-05-31 09:14:30.307: epoch 2:	0.00152502  	0.00307791  	0.00236942  
2023-05-31 09:14:30.307: Find a better model.
2023-05-31 09:14:51.405: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.094869]
2023-05-31 09:14:51.696: epoch 3:	0.00199141  	0.00403465  	0.00328491  
2023-05-31 09:14:51.697: Find a better model.
2023-05-31 09:15:12.570: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.868901]
2023-05-31 09:15:12.858: epoch 4:	0.00230234  	0.00499849  	0.00364493  
2023-05-31 09:15:12.858: Find a better model.
2023-05-31 09:15:33.799: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.936244]
2023-05-31 09:15:34.091: epoch 5:	0.00256144  	0.00571936  	0.00456596  
2023-05-31 09:15:34.091: Find a better model.
2023-05-31 09:15:54.975: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.880712]
2023-05-31 09:15:55.263: epoch 6:	0.00323512  	0.00779840  	0.00599702  
2023-05-31 09:15:55.263: Find a better model.
2023-05-31 09:16:16.185: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.918524]
2023-05-31 09:16:16.489: epoch 7:	0.00353124  	0.00879145  	0.00688616  
2023-05-31 09:16:16.489: Find a better model.
2023-05-31 09:16:37.152: [iter 8 : loss : 1.1347 = 0.6920 + 0.4426 + 0.0000, time: 20.658787]
2023-05-31 09:16:37.438: epoch 8:	0.00428635  	0.01190176  	0.00898705  
2023-05-31 09:16:37.438: Find a better model.
2023-05-31 09:16:58.137: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 20.696432]
2023-05-31 09:16:58.423: epoch 9:	0.00533756  	0.01555538  	0.01163929  
2023-05-31 09:16:58.423: Find a better model.
2023-05-31 09:17:19.135: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.708389]
2023-05-31 09:17:19.408: epoch 10:	0.00627033  	0.01755355  	0.01418677  
2023-05-31 09:17:19.408: Find a better model.
2023-05-31 09:17:40.515: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 21.102512]
2023-05-31 09:17:40.797: epoch 11:	0.00724012  	0.02102457  	0.01700146  
2023-05-31 09:17:40.797: Find a better model.
2023-05-31 09:18:01.960: [iter 12 : loss : 1.1328 = 0.6882 + 0.4445 + 0.0000, time: 21.158622]
2023-05-31 09:18:02.242: epoch 12:	0.00856525  	0.02476208  	0.02041757  
2023-05-31 09:18:02.242: Find a better model.
2023-05-31 09:18:23.549: [iter 13 : loss : 1.1308 = 0.6855 + 0.4453 + 0.0000, time: 21.303035]
2023-05-31 09:18:23.833: epoch 13:	0.01076397  	0.03190048  	0.02672864  
2023-05-31 09:18:23.833: Find a better model.
2023-05-31 09:18:44.912: [iter 14 : loss : 1.1273 = 0.6812 + 0.4460 + 0.0001, time: 21.074787]
2023-05-31 09:18:45.189: epoch 14:	0.01355494  	0.03871834  	0.03300727  
2023-05-31 09:18:45.189: Find a better model.
2023-05-31 09:19:06.515: [iter 15 : loss : 1.1209 = 0.6740 + 0.4469 + 0.0001, time: 21.321521]
2023-05-31 09:19:06.795: epoch 15:	0.01696042  	0.04740960  	0.04003080  
2023-05-31 09:19:06.795: Find a better model.
2023-05-31 09:19:27.901: [iter 16 : loss : 1.1093 = 0.6613 + 0.4479 + 0.0002, time: 21.103031]
2023-05-31 09:19:28.184: epoch 16:	0.02044732  	0.05694617  	0.04835758  
2023-05-31 09:19:28.184: Find a better model.
2023-05-31 09:19:49.511: [iter 17 : loss : 1.0899 = 0.6405 + 0.4492 + 0.0003, time: 21.323134]
2023-05-31 09:19:49.776: epoch 17:	0.02368993  	0.06493746  	0.05518183  
2023-05-31 09:19:49.776: Find a better model.
2023-05-31 09:20:10.845: [iter 18 : loss : 1.0599 = 0.6085 + 0.4511 + 0.0004, time: 21.063657]
2023-05-31 09:20:11.124: epoch 18:	0.02615522  	0.07139841  	0.05995711  
2023-05-31 09:20:11.124: Find a better model.
2023-05-31 09:20:32.049: [iter 19 : loss : 1.0197 = 0.5656 + 0.4535 + 0.0006, time: 20.921339]
2023-05-31 09:20:32.321: epoch 19:	0.02769510  	0.07506188  	0.06309916  
2023-05-31 09:20:32.321: Find a better model.
2023-05-31 09:20:53.261: [iter 20 : loss : 0.9724 = 0.5150 + 0.4566 + 0.0008, time: 20.934525]
2023-05-31 09:20:53.539: epoch 20:	0.02852426  	0.07791613  	0.06485896  
2023-05-31 09:20:53.540: Find a better model.
2023-05-31 09:21:14.462: [iter 21 : loss : 0.9226 = 0.4616 + 0.4599 + 0.0011, time: 20.918633]
2023-05-31 09:21:14.729: epoch 21:	0.02897586  	0.07958858  	0.06585360  
2023-05-31 09:21:14.729: Find a better model.
2023-05-31 09:21:35.619: [iter 22 : loss : 0.8740 = 0.4096 + 0.4630 + 0.0014, time: 20.884524]
2023-05-31 09:21:35.887: epoch 22:	0.02922759  	0.08026586  	0.06598965  
2023-05-31 09:21:35.887: Find a better model.
2023-05-31 09:21:56.842: [iter 23 : loss : 0.8315 = 0.3644 + 0.4654 + 0.0017, time: 20.951192]
2023-05-31 09:21:57.108: epoch 23:	0.02937565  	0.08073045  	0.06623369  
2023-05-31 09:21:57.108: Find a better model.
2023-05-31 09:22:18.246: [iter 24 : loss : 0.7945 = 0.3256 + 0.4669 + 0.0020, time: 21.134315]
2023-05-31 09:22:18.527: epoch 24:	0.02945708  	0.08115807  	0.06642232  
2023-05-31 09:22:18.527: Find a better model.
2023-05-31 09:22:39.609: [iter 25 : loss : 0.7630 = 0.2930 + 0.4677 + 0.0023, time: 21.079211]
2023-05-31 09:22:39.876: epoch 25:	0.02959776  	0.08197268  	0.06690863  
2023-05-31 09:22:39.876: Find a better model.
2023-05-31 09:23:01.045: [iter 26 : loss : 0.7365 = 0.2661 + 0.4679 + 0.0026, time: 21.165048]
2023-05-31 09:23:01.315: epoch 26:	0.02968660  	0.08225205  	0.06715214  
2023-05-31 09:23:01.315: Find a better model.
2023-05-31 09:23:22.370: [iter 27 : loss : 0.7138 = 0.2432 + 0.4678 + 0.0028, time: 21.051821]
2023-05-31 09:23:22.640: epoch 27:	0.02973102  	0.08277508  	0.06755724  
2023-05-31 09:23:22.640: Find a better model.
2023-05-31 09:23:43.787: [iter 28 : loss : 0.6946 = 0.2242 + 0.4673 + 0.0031, time: 21.144675]
2023-05-31 09:23:44.054: epoch 28:	0.02993832  	0.08304129  	0.06779850  
2023-05-31 09:23:44.054: Find a better model.
2023-05-31 09:24:05.201: [iter 29 : loss : 0.6779 = 0.2077 + 0.4668 + 0.0033, time: 21.144065]
2023-05-31 09:24:05.479: epoch 29:	0.03007159  	0.08363997  	0.06804865  
2023-05-31 09:24:05.479: Find a better model.
2023-05-31 09:24:26.560: [iter 30 : loss : 0.6630 = 0.1933 + 0.4661 + 0.0036, time: 21.075995]
2023-05-31 09:24:26.824: epoch 30:	0.03001236  	0.08340693  	0.06803063  
2023-05-31 09:24:48.174: [iter 31 : loss : 0.6502 = 0.1809 + 0.4656 + 0.0038, time: 21.346981]
2023-05-31 09:24:48.439: epoch 31:	0.03000495  	0.08354498  	0.06807842  
2023-05-31 09:25:09.563: [iter 32 : loss : 0.6390 = 0.1701 + 0.4649 + 0.0040, time: 21.118069]
2023-05-31 09:25:09.826: epoch 32:	0.03011599  	0.08341572  	0.06829612  
2023-05-31 09:25:30.791: [iter 33 : loss : 0.6296 = 0.1611 + 0.4642 + 0.0042, time: 20.960017]
2023-05-31 09:25:31.057: epoch 33:	0.03023444  	0.08382505  	0.06847742  
2023-05-31 09:25:31.057: Find a better model.
2023-05-31 09:25:52.183: [iter 34 : loss : 0.6206 = 0.1527 + 0.4636 + 0.0044, time: 21.122220]
2023-05-31 09:25:52.451: epoch 34:	0.03019003  	0.08383742  	0.06859714  
2023-05-31 09:25:52.451: Find a better model.
2023-05-31 09:26:13.348: [iter 35 : loss : 0.6115 = 0.1441 + 0.4628 + 0.0046, time: 20.891112]
2023-05-31 09:26:13.619: epoch 35:	0.03032327  	0.08408636  	0.06878069  
2023-05-31 09:26:13.619: Find a better model.
2023-05-31 09:26:34.736: [iter 36 : loss : 0.6045 = 0.1375 + 0.4623 + 0.0048, time: 21.113129]
2023-05-31 09:26:34.999: epoch 36:	0.03029365  	0.08409367  	0.06887601  
2023-05-31 09:26:35.000: Find a better model.
2023-05-31 09:26:56.163: [iter 37 : loss : 0.5985 = 0.1317 + 0.4618 + 0.0050, time: 21.159961]
2023-05-31 09:26:56.431: epoch 37:	0.03026403  	0.08402713  	0.06892131  
2023-05-31 09:27:17.312: [iter 38 : loss : 0.5920 = 0.1256 + 0.4612 + 0.0052, time: 20.877166]
2023-05-31 09:27:17.577: epoch 38:	0.03021961  	0.08363204  	0.06884465  
2023-05-31 09:27:38.726: [iter 39 : loss : 0.5866 = 0.1207 + 0.4606 + 0.0053, time: 21.145032]
2023-05-31 09:27:38.992: epoch 39:	0.03022701  	0.08368649  	0.06896191  
2023-05-31 09:28:00.092: [iter 40 : loss : 0.5811 = 0.1155 + 0.4601 + 0.0055, time: 21.096421]
2023-05-31 09:28:00.346: epoch 40:	0.03036026  	0.08364351  	0.06903152  
2023-05-31 09:28:21.281: [iter 41 : loss : 0.5758 = 0.1106 + 0.4595 + 0.0057, time: 20.932338]
2023-05-31 09:28:21.558: epoch 41:	0.03028623  	0.08320540  	0.06889515  
2023-05-31 09:28:42.492: [iter 42 : loss : 0.5716 = 0.1064 + 0.4593 + 0.0058, time: 20.931279]
2023-05-31 09:28:42.755: epoch 42:	0.03022701  	0.08284627  	0.06875177  
2023-05-31 09:29:03.880: [iter 43 : loss : 0.5676 = 0.1027 + 0.4589 + 0.0060, time: 21.122113]
2023-05-31 09:29:04.151: epoch 43:	0.03022700  	0.08242546  	0.06862360  
2023-05-31 09:29:25.491: [iter 44 : loss : 0.5648 = 0.1002 + 0.4584 + 0.0062, time: 21.335397]
2023-05-31 09:29:25.756: epoch 44:	0.03012336  	0.08212336  	0.06862959  
2023-05-31 09:29:46.906: [iter 45 : loss : 0.5605 = 0.0962 + 0.4581 + 0.0063, time: 21.145818]
2023-05-31 09:29:47.169: epoch 45:	0.03004933  	0.08210850  	0.06850290  
2023-05-31 09:30:08.279: [iter 46 : loss : 0.5575 = 0.0934 + 0.4577 + 0.0065, time: 21.106507]
2023-05-31 09:30:08.550: epoch 46:	0.02994569  	0.08187732  	0.06830578  
2023-05-31 09:30:29.696: [iter 47 : loss : 0.5545 = 0.0904 + 0.4575 + 0.0066, time: 21.141454]
2023-05-31 09:30:29.963: epoch 47:	0.02992348  	0.08170290  	0.06825401  
2023-05-31 09:30:51.083: [iter 48 : loss : 0.5516 = 0.0878 + 0.4570 + 0.0067, time: 21.117423]
2023-05-31 09:30:51.351: epoch 48:	0.02997530  	0.08172625  	0.06818634  
2023-05-31 09:31:12.267: [iter 49 : loss : 0.5488 = 0.0851 + 0.4568 + 0.0069, time: 20.912828]
2023-05-31 09:31:12.541: epoch 49:	0.02996789  	0.08137949  	0.06800641  
2023-05-31 09:31:33.617: [iter 50 : loss : 0.5468 = 0.0833 + 0.4565 + 0.0070, time: 21.073185]
2023-05-31 09:31:33.883: epoch 50:	0.02998270  	0.08141504  	0.06791843  
2023-05-31 09:31:54.818: [iter 51 : loss : 0.5439 = 0.0806 + 0.4561 + 0.0072, time: 20.930663]
2023-05-31 09:31:55.083: epoch 51:	0.02986424  	0.08072118  	0.06775372  
2023-05-31 09:32:16.212: [iter 52 : loss : 0.5412 = 0.0780 + 0.4559 + 0.0073, time: 21.125995]
2023-05-31 09:32:16.494: epoch 52:	0.02990126  	0.08093731  	0.06772429  
2023-05-31 09:32:37.850: [iter 53 : loss : 0.5405 = 0.0773 + 0.4558 + 0.0074, time: 21.353011]
2023-05-31 09:32:38.117: epoch 53:	0.02977541  	0.08078176  	0.06755934  
2023-05-31 09:32:59.212: [iter 54 : loss : 0.5381 = 0.0751 + 0.4554 + 0.0076, time: 21.091702]
2023-05-31 09:32:59.491: epoch 54:	0.02958292  	0.08032797  	0.06748777  
2023-05-31 09:33:20.622: [iter 55 : loss : 0.5358 = 0.0730 + 0.4552 + 0.0077, time: 21.127998]
2023-05-31 09:33:20.887: epoch 55:	0.02964954  	0.08026572  	0.06731434  
2023-05-31 09:33:41.826: [iter 56 : loss : 0.5342 = 0.0714 + 0.4550 + 0.0078, time: 20.934012]
2023-05-31 09:33:42.092: epoch 56:	0.02951629  	0.07966743  	0.06724180  
2023-05-31 09:34:03.195: [iter 57 : loss : 0.5320 = 0.0692 + 0.4548 + 0.0079, time: 21.099069]
2023-05-31 09:34:03.470: epoch 57:	0.02966435  	0.07995594  	0.06735072  
2023-05-31 09:34:24.569: [iter 58 : loss : 0.5303 = 0.0676 + 0.4546 + 0.0080, time: 21.089364]
2023-05-31 09:34:24.839: epoch 58:	0.02964214  	0.07981090  	0.06733558  
2023-05-31 09:34:45.750: [iter 59 : loss : 0.5291 = 0.0665 + 0.4545 + 0.0082, time: 20.907061]
2023-05-31 09:34:46.018: epoch 59:	0.02953109  	0.07920073  	0.06706814  
2023-05-31 09:35:07.159: [iter 60 : loss : 0.5280 = 0.0653 + 0.4544 + 0.0083, time: 21.137498]
2023-05-31 09:35:07.425: epoch 60:	0.02948668  	0.07913578  	0.06698787  
2023-05-31 09:35:28.426: [iter 61 : loss : 0.5263 = 0.0639 + 0.4541 + 0.0084, time: 20.997179]
2023-05-31 09:35:28.694: epoch 61:	0.02939783  	0.07867170  	0.06672981  
2023-05-31 09:35:28.694: Early stopping is trigger at epoch: 61
2023-05-31 09:35:28.694: best_result@epoch 36:

2023-05-31 09:35:28.694: 		0.0303      	0.0841      	0.0689      
2023-05-31 09:36:16.283: my pid: 148
2023-05-31 09:36:16.284: model: model.general_recommender.SGL
2023-05-31 09:36:16.284: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 09:36:16.285: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 09:36:20.269: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 09:36:41.143: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.873893]
2023-05-31 09:36:41.418: epoch 1:	0.00141398  	0.00265888  	0.00216035  
2023-05-31 09:36:41.418: Find a better model.
2023-05-31 09:37:02.312: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 20.889782]
2023-05-31 09:37:02.604: epoch 2:	0.00168789  	0.00344275  	0.00292297  
2023-05-31 09:37:02.604: Find a better model.
2023-05-31 09:37:23.551: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 20.944135]
2023-05-31 09:37:23.850: epoch 3:	0.00179153  	0.00351075  	0.00301484  
2023-05-31 09:37:23.850: Find a better model.
2023-05-31 09:37:44.756: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.903011]
2023-05-31 09:37:45.047: epoch 4:	0.00236157  	0.00523656  	0.00398059  
2023-05-31 09:37:45.047: Find a better model.
2023-05-31 09:38:06.134: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.083093]
2023-05-31 09:38:06.428: epoch 5:	0.00297601  	0.00698261  	0.00514237  
2023-05-31 09:38:06.428: Find a better model.
2023-05-31 09:38:27.299: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 20.868196]
2023-05-31 09:38:27.591: epoch 6:	0.00333136  	0.00794641  	0.00605731  
2023-05-31 09:38:27.591: Find a better model.
2023-05-31 09:38:48.306: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 20.712016]
2023-05-31 09:38:48.593: epoch 7:	0.00398283  	0.00991560  	0.00786426  
2023-05-31 09:38:48.594: Find a better model.
2023-05-31 09:39:09.511: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 20.914299]
2023-05-31 09:39:09.805: epoch 8:	0.00440480  	0.01178193  	0.00904360  
2023-05-31 09:39:09.805: Find a better model.
2023-05-31 09:39:30.510: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.702069]
2023-05-31 09:39:30.806: epoch 9:	0.00523393  	0.01480255  	0.01100769  
2023-05-31 09:39:30.806: Find a better model.
2023-05-31 09:39:51.704: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.894156]
2023-05-31 09:39:51.991: epoch 10:	0.00587058  	0.01625881  	0.01293033  
2023-05-31 09:39:51.991: Find a better model.
2023-05-31 09:40:13.307: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 21.312014]
2023-05-31 09:40:13.587: epoch 11:	0.00686998  	0.01973315  	0.01571809  
2023-05-31 09:40:13.587: Find a better model.
2023-05-31 09:40:34.711: [iter 12 : loss : 1.1327 = 0.6882 + 0.4445 + 0.0000, time: 21.120340]
2023-05-31 09:40:34.991: epoch 12:	0.00842459  	0.02468551  	0.01982011  
2023-05-31 09:40:34.991: Find a better model.
2023-05-31 09:40:56.230: [iter 13 : loss : 1.1308 = 0.6857 + 0.4451 + 0.0000, time: 21.234125]
2023-05-31 09:40:56.509: epoch 13:	0.01078617  	0.03097473  	0.02552205  
2023-05-31 09:40:56.509: Find a better model.
2023-05-31 09:41:17.673: [iter 14 : loss : 1.1274 = 0.6815 + 0.4458 + 0.0001, time: 21.159010]
2023-05-31 09:41:17.948: epoch 14:	0.01357715  	0.03904677  	0.03260737  
2023-05-31 09:41:17.948: Find a better model.
2023-05-31 09:41:39.059: [iter 15 : loss : 1.1214 = 0.6746 + 0.4468 + 0.0001, time: 21.106957]
2023-05-31 09:41:39.335: epoch 15:	0.01690119  	0.04751417  	0.03975364  
2023-05-31 09:41:39.335: Find a better model.
2023-05-31 09:42:00.450: [iter 16 : loss : 1.1103 = 0.6624 + 0.4478 + 0.0002, time: 21.111563]
2023-05-31 09:42:00.723: epoch 16:	0.02002532  	0.05466617  	0.04642837  
2023-05-31 09:42:00.723: Find a better model.
2023-05-31 09:42:21.822: [iter 17 : loss : 1.0917 = 0.6424 + 0.4490 + 0.0002, time: 21.095041]
2023-05-31 09:42:22.098: epoch 17:	0.02300884  	0.06286307  	0.05299702  
2023-05-31 09:42:22.098: Find a better model.
2023-05-31 09:42:43.235: [iter 18 : loss : 1.0625 = 0.6114 + 0.4507 + 0.0004, time: 21.134077]
2023-05-31 09:42:43.512: epoch 18:	0.02543710  	0.06870467  	0.05811894  
2023-05-31 09:42:43.512: Find a better model.
2023-05-31 09:43:04.629: [iter 19 : loss : 1.0233 = 0.5696 + 0.4531 + 0.0006, time: 21.112181]
2023-05-31 09:43:04.907: epoch 19:	0.02717686  	0.07344201  	0.06172520  
2023-05-31 09:43:04.907: Find a better model.
2023-05-31 09:43:25.984: [iter 20 : loss : 0.9765 = 0.5196 + 0.4561 + 0.0008, time: 21.073253]
2023-05-31 09:43:26.267: epoch 20:	0.02818370  	0.07751742  	0.06365160  
2023-05-31 09:43:26.267: Find a better model.
2023-05-31 09:43:47.616: [iter 21 : loss : 0.9269 = 0.4666 + 0.4592 + 0.0011, time: 21.345074]
2023-05-31 09:43:47.895: epoch 21:	0.02881300  	0.07998220  	0.06506552  
2023-05-31 09:43:47.895: Find a better model.
2023-05-31 09:44:08.791: [iter 22 : loss : 0.8781 = 0.4143 + 0.4624 + 0.0014, time: 20.891580]
2023-05-31 09:44:09.065: epoch 22:	0.02905731  	0.08084144  	0.06555137  
2023-05-31 09:44:09.065: Find a better model.
2023-05-31 09:44:30.246: [iter 23 : loss : 0.8349 = 0.3685 + 0.4647 + 0.0017, time: 21.176541]
2023-05-31 09:44:30.521: epoch 23:	0.02916096  	0.08123313  	0.06567159  
2023-05-31 09:44:30.521: Find a better model.
2023-05-31 09:44:51.784: [iter 24 : loss : 0.7973 = 0.3290 + 0.4663 + 0.0020, time: 21.259197]
2023-05-31 09:44:52.055: epoch 24:	0.02931643  	0.08154675  	0.06604449  
2023-05-31 09:44:52.055: Find a better model.
2023-05-31 09:45:13.244: [iter 25 : loss : 0.7652 = 0.2957 + 0.4672 + 0.0023, time: 21.185169]
2023-05-31 09:45:13.517: epoch 25:	0.02951632  	0.08235902  	0.06634240  
2023-05-31 09:45:13.517: Find a better model.
2023-05-31 09:45:34.742: [iter 26 : loss : 0.7382 = 0.2682 + 0.4675 + 0.0025, time: 21.221035]
2023-05-31 09:45:35.013: epoch 26:	0.02963477  	0.08262713  	0.06671617  
2023-05-31 09:45:35.013: Find a better model.
2023-05-31 09:45:56.214: [iter 27 : loss : 0.7154 = 0.2450 + 0.4676 + 0.0028, time: 21.196679]
2023-05-31 09:45:56.488: epoch 27:	0.02955334  	0.08282084  	0.06666624  
2023-05-31 09:45:56.488: Find a better model.
2023-05-31 09:46:17.721: [iter 28 : loss : 0.6959 = 0.2257 + 0.4672 + 0.0031, time: 21.228111]
2023-05-31 09:46:17.991: epoch 28:	0.02970140  	0.08336158  	0.06695395  
2023-05-31 09:46:17.991: Find a better model.
2023-05-31 09:46:39.159: [iter 29 : loss : 0.6791 = 0.2093 + 0.4665 + 0.0033, time: 21.165031]
2023-05-31 09:46:39.428: epoch 29:	0.02967180  	0.08351693  	0.06695338  
2023-05-31 09:46:39.428: Find a better model.
2023-05-31 09:47:00.706: [iter 30 : loss : 0.6642 = 0.1947 + 0.4659 + 0.0036, time: 21.275083]
2023-05-31 09:47:00.965: epoch 30:	0.02968661  	0.08349570  	0.06718882  
2023-05-31 09:47:22.128: [iter 31 : loss : 0.6511 = 0.1820 + 0.4654 + 0.0038, time: 21.159044]
2023-05-31 09:47:22.399: epoch 31:	0.02981986  	0.08395028  	0.06759718  
2023-05-31 09:47:22.399: Find a better model.
2023-05-31 09:47:43.731: [iter 32 : loss : 0.6395 = 0.1709 + 0.4646 + 0.0040, time: 21.329061]
2023-05-31 09:47:43.998: epoch 32:	0.02998273  	0.08393230  	0.06766764  
2023-05-31 09:48:05.314: [iter 33 : loss : 0.6298 = 0.1616 + 0.4640 + 0.0042, time: 21.311144]
2023-05-31 09:48:05.584: epoch 33:	0.02994572  	0.08375543  	0.06779187  
2023-05-31 09:48:26.944: [iter 34 : loss : 0.6207 = 0.1529 + 0.4634 + 0.0044, time: 21.357031]
2023-05-31 09:48:27.212: epoch 34:	0.02996792  	0.08359344  	0.06798668  
2023-05-31 09:48:48.697: [iter 35 : loss : 0.6118 = 0.1445 + 0.4626 + 0.0046, time: 21.481102]
2023-05-31 09:48:48.967: epoch 35:	0.03001975  	0.08371317  	0.06804598  
2023-05-31 09:49:10.695: [iter 36 : loss : 0.6049 = 0.1380 + 0.4621 + 0.0048, time: 21.724135]
2023-05-31 09:49:10.955: epoch 36:	0.02990129  	0.08318791  	0.06799611  
2023-05-31 09:49:32.650: [iter 37 : loss : 0.5981 = 0.1317 + 0.4615 + 0.0050, time: 21.689440]
2023-05-31 09:49:32.914: epoch 37:	0.03001975  	0.08350085  	0.06803222  
2023-05-31 09:49:54.488: [iter 38 : loss : 0.5923 = 0.1261 + 0.4610 + 0.0052, time: 21.571341]
2023-05-31 09:49:54.762: epoch 38:	0.02984208  	0.08300059  	0.06792463  
2023-05-31 09:50:16.704: [iter 39 : loss : 0.5867 = 0.1210 + 0.4603 + 0.0053, time: 21.938456]
2023-05-31 09:50:16.984: epoch 39:	0.02995313  	0.08337416  	0.06800068  
2023-05-31 09:50:44.307: my pid: 4052
2023-05-31 09:50:44.308: model: model.general_recommender.SGL
2023-05-31 09:50:44.308: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 09:50:44.308: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 09:50:48.569: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 09:51:10.025: [iter 1 : loss : 1.1342 = 0.6931 + 0.4412 + 0.0000, time: 21.455965]
2023-05-31 09:51:10.412: epoch 1:	0.00138437  	0.00273892  	0.00218523  
2023-05-31 09:51:10.412: Find a better model.
2023-05-31 09:51:32.828: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 22.411328]
2023-05-31 09:51:33.132: epoch 2:	0.00160646  	0.00285169  	0.00242541  
2023-05-31 09:51:33.132: Find a better model.
2023-05-31 09:51:55.153: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 22.018351]
2023-05-31 09:51:55.456: epoch 3:	0.00158425  	0.00360642  	0.00273285  
2023-05-31 09:51:55.456: Find a better model.
2023-05-31 09:52:17.817: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 22.355933]
2023-05-31 09:52:18.124: epoch 4:	0.00205804  	0.00415252  	0.00338396  
2023-05-31 09:52:18.125: Find a better model.
2023-05-31 09:52:40.186: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 22.058193]
2023-05-31 09:52:40.474: epoch 5:	0.00238377  	0.00520412  	0.00407882  
2023-05-31 09:52:40.474: Find a better model.
2023-05-31 09:53:02.598: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 22.120023]
2023-05-31 09:53:02.897: epoch 6:	0.00291679  	0.00714002  	0.00564088  
2023-05-31 09:53:02.898: Find a better model.
2023-05-31 09:53:24.925: [iter 7 : loss : 1.1346 = 0.6924 + 0.4422 + 0.0000, time: 22.023256]
2023-05-31 09:53:25.227: epoch 7:	0.00348682  	0.00887754  	0.00681287  
2023-05-31 09:53:25.227: Find a better model.
2023-05-31 09:53:47.567: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 22.337645]
2023-05-31 09:53:47.866: epoch 8:	0.00426414  	0.01053921  	0.00812660  
2023-05-31 09:53:47.866: Find a better model.
2023-05-31 09:54:09.983: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 22.112639]
2023-05-31 09:54:10.275: epoch 9:	0.00509327  	0.01305057  	0.01028915  
2023-05-31 09:54:10.275: Find a better model.
2023-05-31 09:54:38.821: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 28.541912]
2023-05-31 09:54:39.210: epoch 10:	0.00602604  	0.01581105  	0.01260961  
2023-05-31 09:54:39.210: Find a better model.
2023-05-31 09:55:03.220: [iter 11 : loss : 1.1340 = 0.6902 + 0.4438 + 0.0000, time: 24.004910]
2023-05-31 09:55:03.522: epoch 11:	0.00698102  	0.01904465  	0.01554440  
2023-05-31 09:55:03.522: Find a better model.
2023-05-31 09:55:25.454: [iter 12 : loss : 1.1329 = 0.6886 + 0.4443 + 0.0000, time: 21.929138]
2023-05-31 09:55:25.757: epoch 12:	0.00816549  	0.02345635  	0.01912296  
2023-05-31 09:55:25.757: Find a better model.
2023-05-31 09:55:47.784: [iter 13 : loss : 1.1311 = 0.6860 + 0.4450 + 0.0000, time: 22.021796]
2023-05-31 09:55:48.080: epoch 13:	0.01054927  	0.03095949  	0.02512703  
2023-05-31 09:55:48.080: Find a better model.
2023-05-31 09:56:09.969: [iter 14 : loss : 1.1278 = 0.6820 + 0.4457 + 0.0001, time: 21.885304]
2023-05-31 09:56:10.271: epoch 14:	0.01328101  	0.03804627  	0.03158770  
2023-05-31 09:56:10.271: Find a better model.
2023-05-31 09:56:32.380: [iter 15 : loss : 1.1219 = 0.6752 + 0.4466 + 0.0001, time: 22.104616]
2023-05-31 09:56:32.689: epoch 15:	0.01658282  	0.04549934  	0.03819494  
2023-05-31 09:56:32.689: Find a better model.
2023-05-31 09:56:54.562: [iter 16 : loss : 1.1112 = 0.6635 + 0.4476 + 0.0002, time: 21.869200]
2023-05-31 09:56:54.858: epoch 16:	0.01989947  	0.05312540  	0.04569653  
2023-05-31 09:56:54.858: Find a better model.
2023-05-31 09:57:16.735: [iter 17 : loss : 1.0932 = 0.6441 + 0.4489 + 0.0002, time: 21.872184]
2023-05-31 09:57:17.034: epoch 17:	0.02283857  	0.06048044  	0.05193749  
2023-05-31 09:57:17.034: Find a better model.
2023-05-31 09:57:38.957: [iter 18 : loss : 1.0647 = 0.6137 + 0.4506 + 0.0004, time: 21.920090]
2023-05-31 09:57:39.260: epoch 18:	0.02533349  	0.06770951  	0.05713506  
2023-05-31 09:57:39.260: Find a better model.
2023-05-31 09:58:00.921: [iter 19 : loss : 1.0259 = 0.5725 + 0.4529 + 0.0006, time: 21.656744]
2023-05-31 09:58:01.216: epoch 19:	0.02730274  	0.07271016  	0.06102700  
2023-05-31 09:58:01.216: Find a better model.
2023-05-31 09:58:23.156: [iter 20 : loss : 0.9793 = 0.5227 + 0.4559 + 0.0008, time: 21.936244]
2023-05-31 09:58:23.446: epoch 20:	0.02831699  	0.07641847  	0.06348456  
2023-05-31 09:58:23.446: Find a better model.
2023-05-31 09:58:45.356: [iter 21 : loss : 0.9294 = 0.4693 + 0.4590 + 0.0011, time: 21.905165]
2023-05-31 09:58:45.664: epoch 21:	0.02886482  	0.07830493  	0.06456929  
2023-05-31 09:58:45.664: Find a better model.
2023-05-31 09:59:07.339: [iter 22 : loss : 0.8804 = 0.4170 + 0.4620 + 0.0014, time: 21.671666]
2023-05-31 09:59:07.645: epoch 22:	0.02916836  	0.07941400  	0.06508563  
2023-05-31 09:59:07.646: Find a better model.
2023-05-31 09:59:29.458: [iter 23 : loss : 0.8368 = 0.3705 + 0.4646 + 0.0017, time: 21.807341]
2023-05-31 09:59:29.746: epoch 23:	0.02927940  	0.07919476  	0.06531298  
2023-05-31 09:59:51.466: [iter 24 : loss : 0.7988 = 0.3306 + 0.4662 + 0.0020, time: 21.715262]
2023-05-31 09:59:51.770: epoch 24:	0.02968659  	0.08075017  	0.06580382  
2023-05-31 09:59:51.770: Find a better model.
2023-05-31 10:00:13.457: [iter 25 : loss : 0.7663 = 0.2971 + 0.4670 + 0.0022, time: 21.683001]
2023-05-31 10:00:13.748: epoch 25:	0.02970879  	0.08123852  	0.06592917  
2023-05-31 10:00:13.749: Find a better model.
2023-05-31 10:00:35.479: [iter 26 : loss : 0.7393 = 0.2693 + 0.4675 + 0.0025, time: 21.726545]
2023-05-31 10:00:35.764: epoch 26:	0.02986425  	0.08192290  	0.06626571  
2023-05-31 10:00:35.764: Find a better model.
2023-05-31 10:00:57.443: [iter 27 : loss : 0.7159 = 0.2458 + 0.4673 + 0.0028, time: 21.673568]
2023-05-31 10:00:57.733: epoch 27:	0.02998271  	0.08211926  	0.06635660  
2023-05-31 10:00:57.733: Find a better model.
2023-05-31 10:01:19.235: [iter 28 : loss : 0.6966 = 0.2264 + 0.4671 + 0.0031, time: 21.498038]
2023-05-31 10:01:19.522: epoch 28:	0.03008635  	0.08250014  	0.06669024  
2023-05-31 10:01:19.522: Find a better model.
2023-05-31 10:01:41.242: [iter 29 : loss : 0.6796 = 0.2099 + 0.4664 + 0.0033, time: 21.715073]
2023-05-31 10:01:41.519: epoch 29:	0.03001973  	0.08223330  	0.06680659  
2023-05-31 10:02:03.061: [iter 30 : loss : 0.6643 = 0.1948 + 0.4660 + 0.0035, time: 21.539143]
2023-05-31 10:02:03.338: epoch 30:	0.03010117  	0.08297070  	0.06695288  
2023-05-31 10:02:03.339: Find a better model.
2023-05-31 10:02:25.036: [iter 31 : loss : 0.6514 = 0.1823 + 0.4653 + 0.0038, time: 21.694248]
2023-05-31 10:02:25.314: epoch 31:	0.03011598  	0.08299546  	0.06709389  
2023-05-31 10:02:25.314: Find a better model.
2023-05-31 10:02:46.907: [iter 32 : loss : 0.6401 = 0.1714 + 0.4646 + 0.0040, time: 21.588139]
2023-05-31 10:02:47.184: epoch 32:	0.03000493  	0.08266321  	0.06701317  
2023-05-31 10:03:09.033: [iter 33 : loss : 0.6304 = 0.1622 + 0.4640 + 0.0042, time: 21.844739]
2023-05-31 10:03:09.312: epoch 33:	0.03005675  	0.08276568  	0.06714053  
2023-05-31 10:03:31.201: [iter 34 : loss : 0.6211 = 0.1534 + 0.4633 + 0.0044, time: 21.884555]
2023-05-31 10:03:31.481: epoch 34:	0.03010117  	0.08242042  	0.06703835  
2023-05-31 10:03:53.423: [iter 35 : loss : 0.6121 = 0.1449 + 0.4626 + 0.0046, time: 21.938181]
2023-05-31 10:03:53.705: epoch 35:	0.03024924  	0.08267638  	0.06703973  
2023-05-31 10:04:15.614: [iter 36 : loss : 0.6048 = 0.1381 + 0.4619 + 0.0048, time: 21.904963]
2023-05-31 10:04:15.893: epoch 36:	0.03020484  	0.08271279  	0.06703743  
2023-05-31 10:04:37.775: [iter 37 : loss : 0.5984 = 0.1320 + 0.4614 + 0.0050, time: 21.878756]
2023-05-31 10:04:38.050: epoch 37:	0.03013080  	0.08187170  	0.06686068  
2023-05-31 10:04:59.790: [iter 38 : loss : 0.5919 = 0.1259 + 0.4609 + 0.0052, time: 21.736130]
2023-05-31 10:05:00.073: epoch 38:	0.03017523  	0.08192589  	0.06694502  
2023-05-31 10:05:21.825: [iter 39 : loss : 0.5869 = 0.1212 + 0.4603 + 0.0053, time: 21.747674]
2023-05-31 10:05:22.100: epoch 39:	0.03019003  	0.08217205  	0.06696502  
2023-05-31 10:05:43.948: [iter 40 : loss : 0.5811 = 0.1158 + 0.4598 + 0.0055, time: 21.842833]
2023-05-31 10:05:44.225: epoch 40:	0.03016041  	0.08220458  	0.06699245  
2023-05-31 10:06:06.153: [iter 41 : loss : 0.5760 = 0.1110 + 0.4594 + 0.0057, time: 21.923132]
2023-05-31 10:06:06.426: epoch 41:	0.03011599  	0.08183264  	0.06677739  
2023-05-31 10:06:28.348: [iter 42 : loss : 0.5718 = 0.1069 + 0.4590 + 0.0058, time: 21.918345]
2023-05-31 10:06:28.640: epoch 42:	0.03015301  	0.08143185  	0.06678198  
2023-05-31 10:06:50.539: [iter 43 : loss : 0.5679 = 0.1033 + 0.4586 + 0.0060, time: 21.894128]
2023-05-31 10:06:50.820: epoch 43:	0.03009378  	0.08144773  	0.06672174  
2023-05-31 10:07:12.529: [iter 44 : loss : 0.5649 = 0.1006 + 0.4582 + 0.0061, time: 21.706083]
2023-05-31 10:07:12.808: epoch 44:	0.02990130  	0.08066871  	0.06634820  
2023-05-31 10:07:34.515: [iter 45 : loss : 0.5609 = 0.0968 + 0.4578 + 0.0063, time: 21.703970]
2023-05-31 10:07:34.799: epoch 45:	0.02984947  	0.08066387  	0.06631106  
2023-05-31 10:07:56.721: [iter 46 : loss : 0.5576 = 0.0936 + 0.4575 + 0.0064, time: 21.917170]
2023-05-31 10:07:56.999: epoch 46:	0.02981246  	0.08009285  	0.06611495  
2023-05-31 10:08:18.899: [iter 47 : loss : 0.5542 = 0.0905 + 0.4571 + 0.0066, time: 21.897048]
2023-05-31 10:08:19.181: epoch 47:	0.02966439  	0.07945512  	0.06588359  
2023-05-31 10:08:41.297: [iter 48 : loss : 0.5516 = 0.0880 + 0.4569 + 0.0067, time: 22.113034]
2023-05-31 10:08:41.574: epoch 48:	0.02964958  	0.07958771  	0.06582893  
2023-05-31 10:09:03.473: [iter 49 : loss : 0.5487 = 0.0853 + 0.4564 + 0.0069, time: 21.896134]
2023-05-31 10:09:03.755: epoch 49:	0.02964959  	0.07930763  	0.06564692  
2023-05-31 10:09:25.702: [iter 50 : loss : 0.5467 = 0.0834 + 0.4563 + 0.0070, time: 21.943049]
2023-05-31 10:09:25.977: epoch 50:	0.02952373  	0.07921549  	0.06557193  
2023-05-31 10:09:47.857: [iter 51 : loss : 0.5439 = 0.0808 + 0.4559 + 0.0072, time: 21.876508]
2023-05-31 10:09:48.137: epoch 51:	0.02937566  	0.07851917  	0.06527054  
2023-05-31 10:10:10.265: [iter 52 : loss : 0.5411 = 0.0782 + 0.4557 + 0.0073, time: 22.124141]
2023-05-31 10:10:10.542: epoch 52:	0.02936085  	0.07830396  	0.06511610  
2023-05-31 10:10:32.810: [iter 53 : loss : 0.5404 = 0.0775 + 0.4555 + 0.0074, time: 22.263214]
2023-05-31 10:10:33.110: epoch 53:	0.02940526  	0.07818180  	0.06500451  
2023-05-31 10:10:55.889: [iter 54 : loss : 0.5382 = 0.0755 + 0.4552 + 0.0075, time: 22.774544]
2023-05-31 10:10:56.198: epoch 54:	0.02930162  	0.07763108  	0.06474331  
2023-05-31 10:11:16.576: my pid: 15492
2023-05-31 10:11:16.577: model: model.general_recommender.SGL
2023-05-31 10:11:16.577: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 10:11:16.577: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 10:11:21.003: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 10:11:43.516: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.513613]
2023-05-31 10:11:43.816: epoch 1:	0.00126592  	0.00232918  	0.00208609  
2023-05-31 10:11:43.816: Find a better model.
2023-05-31 10:12:05.824: [iter 2 : loss : 1.1337 = 0.6930 + 0.4406 + 0.0000, time: 22.004299]
2023-05-31 10:12:06.119: epoch 2:	0.00133254  	0.00323557  	0.00231994  
2023-05-31 10:12:06.119: Find a better model.
2023-05-31 10:12:28.160: [iter 3 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 22.038596]
2023-05-31 10:12:28.454: epoch 3:	0.00158425  	0.00317601  	0.00272251  
2023-05-31 10:12:49.895: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 21.437939]
2023-05-31 10:12:50.188: epoch 4:	0.00205804  	0.00454658  	0.00338503  
2023-05-31 10:12:50.188: Find a better model.
2023-05-31 10:13:11.456: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.265182]
2023-05-31 10:13:11.749: epoch 5:	0.00239858  	0.00566576  	0.00438693  
2023-05-31 10:13:11.749: Find a better model.
2023-05-31 10:13:33.270: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.516663]
2023-05-31 10:13:33.562: epoch 6:	0.00310927  	0.00782910  	0.00586068  
2023-05-31 10:13:33.562: Find a better model.
2023-05-31 10:13:54.653: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 21.086944]
2023-05-31 10:13:54.943: epoch 7:	0.00359787  	0.00897087  	0.00704555  
2023-05-31 10:13:54.943: Find a better model.
2023-05-31 10:14:16.233: [iter 8 : loss : 1.1345 = 0.6921 + 0.4423 + 0.0000, time: 21.287297]
2023-05-31 10:14:16.519: epoch 8:	0.00430856  	0.01105026  	0.00862353  
2023-05-31 10:14:16.519: Find a better model.
2023-05-31 10:14:37.647: [iter 9 : loss : 1.1345 = 0.6917 + 0.4427 + 0.0000, time: 21.124678]
2023-05-31 10:14:37.932: epoch 9:	0.00524133  	0.01516220  	0.01163980  
2023-05-31 10:14:37.932: Find a better model.
2023-05-31 10:14:59.227: [iter 10 : loss : 1.1344 = 0.6912 + 0.4432 + 0.0000, time: 21.292501]
2023-05-31 10:14:59.514: epoch 10:	0.00627034  	0.01731399  	0.01389113  
2023-05-31 10:14:59.514: Find a better model.
2023-05-31 10:15:21.251: [iter 11 : loss : 1.1340 = 0.6904 + 0.4435 + 0.0000, time: 21.733777]
2023-05-31 10:15:21.534: epoch 11:	0.00782495  	0.02176945  	0.01771693  
2023-05-31 10:15:21.534: Find a better model.
2023-05-31 10:15:43.924: [iter 12 : loss : 1.1331 = 0.6890 + 0.4441 + 0.0000, time: 22.385816]
2023-05-31 10:15:44.205: epoch 12:	0.00942399  	0.02621998  	0.02086304  
2023-05-31 10:15:44.205: Find a better model.
2023-05-31 10:16:06.218: [iter 13 : loss : 1.1315 = 0.6867 + 0.4448 + 0.0000, time: 22.010052]
2023-05-31 10:16:06.500: epoch 13:	0.01104529  	0.03071781  	0.02527627  
2023-05-31 10:16:06.500: Find a better model.
2023-05-31 10:16:28.206: [iter 14 : loss : 1.1283 = 0.6828 + 0.4455 + 0.0001, time: 21.701509]
2023-05-31 10:16:28.485: epoch 14:	0.01355495  	0.03803349  	0.03137762  
2023-05-31 10:16:28.485: Find a better model.
2023-05-31 10:16:50.181: [iter 15 : loss : 1.1229 = 0.6765 + 0.4463 + 0.0001, time: 21.692119]
2023-05-31 10:16:50.457: epoch 15:	0.01661246  	0.04574722  	0.03828987  
2023-05-31 10:16:50.458: Find a better model.
2023-05-31 10:17:12.165: [iter 16 : loss : 1.1130 = 0.6655 + 0.4473 + 0.0001, time: 21.702256]
2023-05-31 10:17:12.441: epoch 16:	0.02007714  	0.05408490  	0.04593512  
2023-05-31 10:17:12.441: Find a better model.
2023-05-31 10:17:34.172: [iter 17 : loss : 1.0960 = 0.6472 + 0.4485 + 0.0002, time: 21.726262]
2023-05-31 10:17:34.447: epoch 17:	0.02305325  	0.06220881  	0.05283030  
2023-05-31 10:17:34.447: Find a better model.
2023-05-31 10:17:56.359: [iter 18 : loss : 1.0688 = 0.6182 + 0.4503 + 0.0003, time: 21.907557]
2023-05-31 10:17:56.649: epoch 18:	0.02531865  	0.06887072  	0.05806176  
2023-05-31 10:17:56.649: Find a better model.
2023-05-31 10:18:18.108: [iter 19 : loss : 1.0311 = 0.5781 + 0.4525 + 0.0005, time: 21.455043]
2023-05-31 10:18:18.383: epoch 19:	0.02707321  	0.07309504  	0.06172979  
2023-05-31 10:18:18.383: Find a better model.
2023-05-31 10:18:40.164: [iter 20 : loss : 0.9850 = 0.5288 + 0.4555 + 0.0008, time: 21.778464]
2023-05-31 10:18:40.446: epoch 20:	0.02820591  	0.07662734  	0.06408810  
2023-05-31 10:18:40.446: Find a better model.
2023-05-31 10:19:02.145: [iter 21 : loss : 0.9350 = 0.4752 + 0.4588 + 0.0010, time: 21.695065]
2023-05-31 10:19:02.415: epoch 21:	0.02889442  	0.07832462  	0.06549109  
2023-05-31 10:19:02.415: Find a better model.
2023-05-31 10:19:24.135: [iter 22 : loss : 0.8856 = 0.4225 + 0.4618 + 0.0013, time: 21.715333]
2023-05-31 10:19:24.407: epoch 22:	0.02926458  	0.07894721  	0.06610885  
2023-05-31 10:19:24.407: Find a better model.
2023-05-31 10:19:46.113: [iter 23 : loss : 0.8415 = 0.3753 + 0.4645 + 0.0016, time: 21.703456]
2023-05-31 10:19:46.382: epoch 23:	0.02934604  	0.07937639  	0.06633778  
2023-05-31 10:19:46.382: Find a better model.
2023-05-31 10:20:07.945: [iter 24 : loss : 0.8031 = 0.3349 + 0.4663 + 0.0019, time: 21.559119]
2023-05-31 10:20:08.210: epoch 24:	0.02938306  	0.07985385  	0.06637674  
2023-05-31 10:20:08.210: Find a better model.
2023-05-31 10:20:29.699: [iter 25 : loss : 0.7700 = 0.3006 + 0.4671 + 0.0022, time: 21.484327]
2023-05-31 10:20:29.968: epoch 25:	0.02950151  	0.08032357  	0.06674013  
2023-05-31 10:20:29.968: Find a better model.
2023-05-31 10:20:51.491: [iter 26 : loss : 0.7426 = 0.2726 + 0.4675 + 0.0025, time: 21.519205]
2023-05-31 10:20:51.765: epoch 26:	0.02959774  	0.08129651  	0.06715040  
2023-05-31 10:20:51.765: Find a better model.
2023-05-31 10:21:13.093: [iter 27 : loss : 0.7187 = 0.2485 + 0.4675 + 0.0028, time: 21.324424]
2023-05-31 10:21:13.364: epoch 27:	0.02967177  	0.08123656  	0.06707697  
2023-05-31 10:21:35.102: [iter 28 : loss : 0.6987 = 0.2286 + 0.4670 + 0.0030, time: 21.734399]
2023-05-31 10:21:35.370: epoch 28:	0.02973100  	0.08162600  	0.06711982  
2023-05-31 10:21:35.370: Find a better model.
2023-05-31 10:21:57.116: [iter 29 : loss : 0.6816 = 0.2119 + 0.4665 + 0.0033, time: 21.742879]
2023-05-31 10:21:57.382: epoch 29:	0.02981244  	0.08209871  	0.06753264  
2023-05-31 10:21:57.382: Find a better model.
2023-05-31 10:22:18.874: [iter 30 : loss : 0.6659 = 0.1964 + 0.4660 + 0.0035, time: 21.488672]
2023-05-31 10:22:19.138: epoch 30:	0.02983465  	0.08180094  	0.06752543  
2023-05-31 10:22:40.841: [iter 31 : loss : 0.6530 = 0.1841 + 0.4652 + 0.0037, time: 21.699365]
2023-05-31 10:22:41.108: epoch 31:	0.02989387  	0.08215069  	0.06775490  
2023-05-31 10:22:41.108: Find a better model.
2023-05-31 10:23:02.834: [iter 32 : loss : 0.6415 = 0.1730 + 0.4645 + 0.0040, time: 21.722432]
2023-05-31 10:23:03.099: epoch 32:	0.02993089  	0.08214951  	0.06782818  
2023-05-31 10:23:24.817: [iter 33 : loss : 0.6317 = 0.1637 + 0.4638 + 0.0042, time: 21.714427]
2023-05-31 10:23:25.083: epoch 33:	0.02998271  	0.08222914  	0.06798470  
2023-05-31 10:23:25.083: Find a better model.
2023-05-31 10:23:46.805: [iter 34 : loss : 0.6222 = 0.1546 + 0.4632 + 0.0044, time: 21.719501]
2023-05-31 10:23:47.069: epoch 34:	0.03010117  	0.08243843  	0.06814949  
2023-05-31 10:23:47.069: Find a better model.
2023-05-31 10:24:09.022: [iter 35 : loss : 0.6131 = 0.1460 + 0.4626 + 0.0046, time: 21.949000]
2023-05-31 10:24:09.287: epoch 35:	0.03024923  	0.08299623  	0.06827483  
2023-05-31 10:24:09.287: Find a better model.
2023-05-31 10:24:31.032: [iter 36 : loss : 0.6061 = 0.1394 + 0.4619 + 0.0048, time: 21.741034]
2023-05-31 10:24:31.299: epoch 36:	0.03024182  	0.08290962  	0.06829204  
2023-05-31 10:24:53.209: [iter 37 : loss : 0.5993 = 0.1331 + 0.4613 + 0.0050, time: 21.907037]
2023-05-31 10:24:53.474: epoch 37:	0.03023441  	0.08294199  	0.06819304  
2023-05-31 10:25:15.404: [iter 38 : loss : 0.5930 = 0.1271 + 0.4608 + 0.0051, time: 21.925120]
2023-05-31 10:25:15.682: epoch 38:	0.03011596  	0.08264071  	0.06810308  
2023-05-31 10:25:37.394: [iter 39 : loss : 0.5877 = 0.1222 + 0.4602 + 0.0053, time: 21.709092]
2023-05-31 10:25:37.670: epoch 39:	0.03022701  	0.08241758  	0.06815336  
2023-05-31 10:25:59.580: [iter 40 : loss : 0.5822 = 0.1169 + 0.4598 + 0.0055, time: 21.904196]
2023-05-31 10:25:59.843: epoch 40:	0.03024181  	0.08238837  	0.06816147  
2023-05-31 10:26:21.786: [iter 41 : loss : 0.5769 = 0.1121 + 0.4592 + 0.0056, time: 21.939113]
2023-05-31 10:26:22.054: epoch 41:	0.03018259  	0.08200664  	0.06798505  
2023-05-31 10:26:43.984: [iter 42 : loss : 0.5726 = 0.1079 + 0.4588 + 0.0058, time: 21.925609]
2023-05-31 10:26:44.246: epoch 42:	0.03016777  	0.08179540  	0.06781329  
2023-05-31 10:27:05.979: [iter 43 : loss : 0.5683 = 0.1039 + 0.4585 + 0.0060, time: 21.728533]
2023-05-31 10:27:06.246: epoch 43:	0.03017516  	0.08160597  	0.06793350  
2023-05-31 10:27:28.198: [iter 44 : loss : 0.5653 = 0.1010 + 0.4581 + 0.0061, time: 21.949487]
2023-05-31 10:27:28.462: epoch 44:	0.03018256  	0.08129903  	0.06790780  
2023-05-31 10:27:50.345: [iter 45 : loss : 0.5613 = 0.0974 + 0.4577 + 0.0063, time: 21.877561]
2023-05-31 10:27:50.609: epoch 45:	0.03027140  	0.08147352  	0.06786860  
2023-05-31 10:28:12.355: [iter 46 : loss : 0.5582 = 0.0945 + 0.4573 + 0.0064, time: 21.740133]
2023-05-31 10:28:12.627: epoch 46:	0.03024919  	0.08153559  	0.06785686  
2023-05-31 10:28:34.534: [iter 47 : loss : 0.5549 = 0.0913 + 0.4570 + 0.0066, time: 21.897543]
2023-05-31 10:28:34.801: epoch 47:	0.03020477  	0.08138397  	0.06789581  
2023-05-31 10:28:56.533: [iter 48 : loss : 0.5523 = 0.0889 + 0.4567 + 0.0067, time: 21.727103]
2023-05-31 10:28:56.801: epoch 48:	0.03010114  	0.08069004  	0.06759604  
2023-05-31 10:29:18.501: [iter 49 : loss : 0.5496 = 0.0864 + 0.4563 + 0.0069, time: 21.697154]
2023-05-31 10:29:18.773: epoch 49:	0.02990866  	0.08025630  	0.06726585  
2023-05-31 10:29:40.517: [iter 50 : loss : 0.5470 = 0.0840 + 0.4561 + 0.0070, time: 21.741015]
2023-05-31 10:29:40.787: epoch 50:	0.02976800  	0.07966159  	0.06707917  
2023-05-31 10:30:02.305: [iter 51 : loss : 0.5442 = 0.0813 + 0.4558 + 0.0071, time: 21.515274]
2023-05-31 10:30:02.569: epoch 51:	0.02976800  	0.07967009  	0.06700350  
2023-05-31 10:30:24.257: [iter 52 : loss : 0.5417 = 0.0789 + 0.4555 + 0.0073, time: 21.684181]
2023-05-31 10:30:24.522: epoch 52:	0.02981981  	0.07949955  	0.06676082  
2023-05-31 10:30:46.279: [iter 53 : loss : 0.5409 = 0.0782 + 0.4553 + 0.0074, time: 21.753340]
2023-05-31 10:30:46.543: epoch 53:	0.02973097  	0.07921357  	0.06669294  
2023-05-31 10:31:08.313: [iter 54 : loss : 0.5383 = 0.0758 + 0.4550 + 0.0075, time: 21.766984]
2023-05-31 10:31:08.579: epoch 54:	0.02973097  	0.07906987  	0.06670589  
2023-05-31 10:31:30.087: [iter 55 : loss : 0.5362 = 0.0737 + 0.4549 + 0.0076, time: 21.503324]
2023-05-31 10:31:30.349: epoch 55:	0.02972357  	0.07907254  	0.06663958  
2023-05-31 10:31:52.054: [iter 56 : loss : 0.5343 = 0.0718 + 0.4547 + 0.0078, time: 21.701046]
2023-05-31 10:31:52.318: epoch 56:	0.02964954  	0.07942574  	0.06669386  
2023-05-31 10:32:13.848: [iter 57 : loss : 0.5324 = 0.0701 + 0.4545 + 0.0079, time: 21.526063]
2023-05-31 10:32:14.113: epoch 57:	0.02970877  	0.07934206  	0.06672803  
2023-05-31 10:32:35.873: [iter 58 : loss : 0.5307 = 0.0684 + 0.4543 + 0.0080, time: 21.757270]
2023-05-31 10:32:36.136: epoch 58:	0.02962733  	0.07907914  	0.06659926  
2023-05-31 10:32:57.840: [iter 59 : loss : 0.5293 = 0.0670 + 0.4541 + 0.0081, time: 21.701116]
2023-05-31 10:32:58.103: epoch 59:	0.02960511  	0.07879639  	0.06653097  
2023-05-31 10:33:19.630: [iter 60 : loss : 0.5277 = 0.0655 + 0.4539 + 0.0082, time: 21.522050]
2023-05-31 10:33:19.892: epoch 60:	0.02948667  	0.07847244  	0.06609077  
2023-05-31 10:33:19.893: Early stopping is trigger at epoch: 60
2023-05-31 10:33:19.893: best_result@epoch 35:

2023-05-31 10:33:19.893: 		0.0302      	0.0830      	0.0683      
2023-05-31 10:36:44.979: my pid: 316
2023-05-31 10:36:44.979: model: model.general_recommender.SGL
2023-05-31 10:36:44.979: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 10:36:44.979: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 10:36:48.965: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 10:37:10.851: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.885535]
2023-05-31 10:37:11.125: epoch 1:	0.00114747  	0.00250094  	0.00194138  
2023-05-31 10:37:11.125: Find a better model.
2023-05-31 10:37:33.023: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 21.895648]
2023-05-31 10:37:33.326: epoch 2:	0.00151022  	0.00341402  	0.00239962  
2023-05-31 10:37:33.327: Find a better model.
2023-05-31 10:37:55.006: [iter 3 : loss : 1.1337 = 0.6929 + 0.4408 + 0.0000, time: 21.676049]
2023-05-31 10:37:55.312: epoch 3:	0.00160646  	0.00309484  	0.00270312  
2023-05-31 10:38:17.183: [iter 4 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 21.866264]
2023-05-31 10:38:17.473: epoch 4:	0.00224312  	0.00490755  	0.00383735  
2023-05-31 10:38:17.473: Find a better model.
2023-05-31 10:38:39.373: [iter 5 : loss : 1.1340 = 0.6927 + 0.4413 + 0.0000, time: 21.897274]
2023-05-31 10:38:39.668: epoch 5:	0.00248742  	0.00646294  	0.00492540  
2023-05-31 10:38:39.668: Find a better model.
2023-05-31 10:39:01.557: [iter 6 : loss : 1.1342 = 0.6926 + 0.4416 + 0.0000, time: 21.885207]
2023-05-31 10:39:01.852: epoch 6:	0.00290199  	0.00725112  	0.00600196  
2023-05-31 10:39:01.852: Find a better model.
2023-05-31 10:39:23.735: [iter 7 : loss : 1.1343 = 0.6924 + 0.4419 + 0.0000, time: 21.879410]
2023-05-31 10:39:24.031: epoch 7:	0.00333876  	0.00893669  	0.00689316  
2023-05-31 10:39:24.031: Find a better model.
2023-05-31 10:39:45.771: [iter 8 : loss : 1.1344 = 0.6922 + 0.4422 + 0.0000, time: 21.737746]
2023-05-31 10:39:46.060: epoch 8:	0.00381996  	0.01028028  	0.00825725  
2023-05-31 10:39:46.060: Find a better model.
2023-05-31 10:40:07.982: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 21.917162]
2023-05-31 10:40:08.286: epoch 9:	0.00498963  	0.01433831  	0.01115114  
2023-05-31 10:40:08.286: Find a better model.
2023-05-31 10:40:30.131: [iter 10 : loss : 1.1343 = 0.6913 + 0.4430 + 0.0000, time: 21.840833]
2023-05-31 10:40:30.426: epoch 10:	0.00614449  	0.01689515  	0.01348079  
2023-05-31 10:40:30.426: Find a better model.
2023-05-31 10:40:52.944: [iter 11 : loss : 1.1339 = 0.6906 + 0.4433 + 0.0000, time: 22.515169]
2023-05-31 10:40:53.237: epoch 11:	0.00772871  	0.02244848  	0.01754729  
2023-05-31 10:40:53.237: Find a better model.
2023-05-31 10:41:15.696: [iter 12 : loss : 1.1332 = 0.6894 + 0.4438 + 0.0000, time: 22.455061]
2023-05-31 10:41:15.986: epoch 12:	0.00920930  	0.02608221  	0.02103918  
2023-05-31 10:41:15.986: Find a better model.
2023-05-31 10:41:38.524: [iter 13 : loss : 1.1318 = 0.6874 + 0.4443 + 0.0000, time: 22.533481]
2023-05-31 10:41:38.811: epoch 13:	0.01112672  	0.03059418  	0.02541843  
2023-05-31 10:41:38.811: Find a better model.
2023-05-31 10:42:01.112: [iter 14 : loss : 1.1290 = 0.6838 + 0.4452 + 0.0001, time: 22.296716]
2023-05-31 10:42:01.398: epoch 14:	0.01368080  	0.03748051  	0.03115921  
2023-05-31 10:42:01.398: Find a better model.
2023-05-31 10:42:23.686: [iter 15 : loss : 1.1240 = 0.6779 + 0.4460 + 0.0001, time: 22.284268]
2023-05-31 10:42:23.968: epoch 15:	0.01701962  	0.04626524  	0.03867517  
2023-05-31 10:42:23.968: Find a better model.
2023-05-31 10:42:46.124: [iter 16 : loss : 1.1147 = 0.6677 + 0.4469 + 0.0001, time: 22.153468]
2023-05-31 10:42:46.403: epoch 16:	0.02009194  	0.05504347  	0.04613023  
2023-05-31 10:42:46.403: Find a better model.
2023-05-31 10:43:08.677: [iter 17 : loss : 1.0990 = 0.6506 + 0.4482 + 0.0002, time: 22.270886]
2023-05-31 10:43:08.956: epoch 17:	0.02292739  	0.06163422  	0.05255985  
2023-05-31 10:43:08.956: Find a better model.
2023-05-31 10:43:31.093: [iter 18 : loss : 1.0730 = 0.6229 + 0.4498 + 0.0003, time: 22.132888]
2023-05-31 10:43:31.376: epoch 18:	0.02533345  	0.06675507  	0.05689567  
2023-05-31 10:43:31.376: Find a better model.
2023-05-31 10:43:53.477: [iter 19 : loss : 1.0364 = 0.5838 + 0.4521 + 0.0005, time: 22.098032]
2023-05-31 10:43:53.758: epoch 19:	0.02676230  	0.07133470  	0.06045524  
2023-05-31 10:43:53.758: Find a better model.
2023-05-31 10:44:15.852: [iter 20 : loss : 0.9909 = 0.5351 + 0.4551 + 0.0007, time: 22.090104]
2023-05-31 10:44:16.128: epoch 20:	0.02768028  	0.07408980  	0.06249671  
2023-05-31 10:44:16.128: Find a better model.
2023-05-31 10:44:38.058: [iter 21 : loss : 0.9410 = 0.4816 + 0.4584 + 0.0010, time: 21.926171]
2023-05-31 10:44:38.343: epoch 21:	0.02819114  	0.07554928  	0.06354163  
2023-05-31 10:44:38.343: Find a better model.
2023-05-31 10:45:00.611: [iter 22 : loss : 0.8910 = 0.4281 + 0.4615 + 0.0013, time: 22.263639]
2023-05-31 10:45:00.875: epoch 22:	0.02856871  	0.07724696  	0.06414892  
2023-05-31 10:45:00.875: Find a better model.
2023-05-31 10:45:22.991: [iter 23 : loss : 0.8461 = 0.3804 + 0.4641 + 0.0016, time: 22.111957]
2023-05-31 10:45:23.280: epoch 23:	0.02896847  	0.07853038  	0.06480661  
2023-05-31 10:45:23.280: Find a better model.
2023-05-31 10:45:45.369: [iter 24 : loss : 0.8069 = 0.3390 + 0.4660 + 0.0019, time: 22.084038]
2023-05-31 10:45:45.642: epoch 24:	0.02906471  	0.07891413  	0.06505752  
2023-05-31 10:45:45.642: Find a better model.
2023-05-31 10:46:07.788: [iter 25 : loss : 0.7732 = 0.3042 + 0.4668 + 0.0022, time: 22.142776]
2023-05-31 10:46:08.058: epoch 25:	0.02920537  	0.07954434  	0.06548107  
2023-05-31 10:46:08.058: Find a better model.
2023-05-31 10:46:30.004: [iter 26 : loss : 0.7452 = 0.2756 + 0.4672 + 0.0025, time: 21.943047]
2023-05-31 10:46:30.288: epoch 26:	0.02926459  	0.07984117  	0.06561230  
2023-05-31 10:46:30.289: Find a better model.
2023-05-31 10:46:52.400: [iter 27 : loss : 0.7214 = 0.2515 + 0.4672 + 0.0027, time: 22.108095]
2023-05-31 10:46:52.669: epoch 27:	0.02943486  	0.08031941  	0.06587661  
2023-05-31 10:46:52.669: Find a better model.
2023-05-31 10:47:14.563: [iter 28 : loss : 0.7010 = 0.2313 + 0.4667 + 0.0030, time: 21.890159]
2023-05-31 10:47:14.831: epoch 28:	0.02949410  	0.08049918  	0.06603950  
2023-05-31 10:47:14.831: Find a better model.
2023-05-31 10:47:36.950: [iter 29 : loss : 0.6837 = 0.2141 + 0.4663 + 0.0032, time: 22.115082]
2023-05-31 10:47:37.223: epoch 29:	0.02952371  	0.08031458  	0.06613434  
2023-05-31 10:47:59.358: [iter 30 : loss : 0.6681 = 0.1989 + 0.4657 + 0.0035, time: 22.131152]
2023-05-31 10:47:59.629: epoch 30:	0.02979764  	0.08145136  	0.06656923  
2023-05-31 10:47:59.629: Find a better model.
2023-05-31 10:48:21.558: [iter 31 : loss : 0.6542 = 0.1854 + 0.4651 + 0.0037, time: 21.925039]
2023-05-31 10:48:21.829: epoch 31:	0.02994569  	0.08175518  	0.06674603  
2023-05-31 10:48:21.829: Find a better model.
2023-05-31 10:48:43.952: [iter 32 : loss : 0.6429 = 0.1745 + 0.4644 + 0.0039, time: 22.119192]
2023-05-31 10:48:44.216: epoch 32:	0.03003454  	0.08192360  	0.06679729  
2023-05-31 10:48:44.216: Find a better model.
2023-05-31 10:49:06.175: [iter 33 : loss : 0.6326 = 0.1648 + 0.4636 + 0.0041, time: 21.955140]
2023-05-31 10:49:06.447: epoch 33:	0.03013818  	0.08165629  	0.06673857  
2023-05-31 10:49:28.361: [iter 34 : loss : 0.6231 = 0.1558 + 0.4630 + 0.0044, time: 21.910606]
2023-05-31 10:49:28.627: epoch 34:	0.03008636  	0.08158434  	0.06680146  
2023-05-31 10:49:50.698: [iter 35 : loss : 0.6139 = 0.1471 + 0.4622 + 0.0046, time: 22.067356]
2023-05-31 10:49:50.968: epoch 35:	0.03007155  	0.08113045  	0.06659017  
2023-05-31 10:50:13.081: [iter 36 : loss : 0.6066 = 0.1402 + 0.4616 + 0.0047, time: 22.110112]
2023-05-31 10:50:13.355: epoch 36:	0.03008637  	0.08107579  	0.06655750  
2023-05-31 10:50:35.496: [iter 37 : loss : 0.6002 = 0.1341 + 0.4612 + 0.0049, time: 22.137630]
2023-05-31 10:50:35.763: epoch 37:	0.03016780  	0.08141037  	0.06679024  
2023-05-31 10:50:57.906: [iter 38 : loss : 0.5936 = 0.1280 + 0.4605 + 0.0051, time: 22.139621]
2023-05-31 10:50:58.175: epoch 38:	0.03013818  	0.08146172  	0.06685162  
2023-05-31 10:51:20.292: [iter 39 : loss : 0.5881 = 0.1229 + 0.4599 + 0.0053, time: 22.113225]
2023-05-31 10:51:20.558: epoch 39:	0.03004935  	0.08130049  	0.06677675  
2023-05-31 10:51:42.694: [iter 40 : loss : 0.5825 = 0.1175 + 0.4595 + 0.0055, time: 22.132599]
2023-05-31 10:51:42.961: epoch 40:	0.03010117  	0.08139735  	0.06680302  
2023-05-31 10:52:05.084: [iter 41 : loss : 0.5773 = 0.1127 + 0.4590 + 0.0056, time: 22.120122]
2023-05-31 10:52:05.356: epoch 41:	0.03005674  	0.08116157  	0.06669006  
2023-05-31 10:52:27.487: [iter 42 : loss : 0.5728 = 0.1085 + 0.4586 + 0.0058, time: 22.128644]
2023-05-31 10:52:27.751: epoch 42:	0.03010117  	0.08130211  	0.06671038  
2023-05-31 10:52:49.891: [iter 43 : loss : 0.5686 = 0.1045 + 0.4582 + 0.0060, time: 22.136525]
2023-05-31 10:52:50.157: epoch 43:	0.03008636  	0.08095982  	0.06657182  
2023-05-31 10:53:12.250: [iter 44 : loss : 0.5658 = 0.1018 + 0.4579 + 0.0061, time: 22.089079]
2023-05-31 10:53:12.514: epoch 44:	0.03007896  	0.08071981  	0.06655023  
2023-05-31 10:53:34.848: [iter 45 : loss : 0.5620 = 0.0984 + 0.4574 + 0.0063, time: 22.330079]
2023-05-31 10:53:35.115: epoch 45:	0.03010857  	0.08068041  	0.06667142  
2023-05-31 10:53:57.439: [iter 46 : loss : 0.5584 = 0.0949 + 0.4570 + 0.0064, time: 22.320235]
2023-05-31 10:53:57.708: epoch 46:	0.03002713  	0.07999736  	0.06648222  
2023-05-31 10:54:20.050: [iter 47 : loss : 0.5551 = 0.0919 + 0.4567 + 0.0066, time: 22.338152]
2023-05-31 10:54:20.324: epoch 47:	0.02999752  	0.07998369  	0.06634602  
2023-05-31 10:54:42.969: [iter 48 : loss : 0.5520 = 0.0890 + 0.4563 + 0.0067, time: 22.639450]
2023-05-31 10:54:43.234: epoch 48:	0.02989387  	0.07953385  	0.06626454  
2023-05-31 10:55:06.104: [iter 49 : loss : 0.5495 = 0.0867 + 0.4560 + 0.0068, time: 22.866146]
2023-05-31 10:55:06.374: epoch 49:	0.02970880  	0.07897376  	0.06604297  
2023-05-31 10:56:09.004: my pid: 15940
2023-05-31 10:56:09.004: model: model.general_recommender.SGL
2023-05-31 10:56:09.004: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 10:56:09.004: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 10:56:12.981: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 10:56:33.669: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.686419]
2023-05-31 10:56:33.935: epoch 1:	0.00123630  	0.00226252  	0.00188128  
2023-05-31 10:56:33.935: Find a better model.
2023-05-31 10:56:54.498: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.559327]
2023-05-31 10:56:54.787: epoch 2:	0.00167308  	0.00323940  	0.00268645  
2023-05-31 10:56:54.788: Find a better model.
2023-05-31 10:57:15.279: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 20.488083]
2023-05-31 10:57:15.570: epoch 3:	0.00202843  	0.00428769  	0.00332289  
2023-05-31 10:57:15.570: Find a better model.
2023-05-31 10:57:36.308: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.735083]
2023-05-31 10:57:36.601: epoch 4:	0.00237637  	0.00547376  	0.00414193  
2023-05-31 10:57:36.601: Find a better model.
2023-05-31 10:57:57.262: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.658020]
2023-05-31 10:57:57.556: epoch 5:	0.00248741  	0.00590525  	0.00440310  
2023-05-31 10:57:57.556: Find a better model.
2023-05-31 10:58:18.064: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 20.503429]
2023-05-31 10:58:18.361: epoch 6:	0.00294640  	0.00648913  	0.00513943  
2023-05-31 10:58:18.361: Find a better model.
2023-05-31 10:58:38.862: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 20.498613]
2023-05-31 10:58:39.151: epoch 7:	0.00347202  	0.00874461  	0.00650828  
2023-05-31 10:58:39.151: Find a better model.
2023-05-31 10:58:59.671: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 20.516465]
2023-05-31 10:58:59.957: epoch 8:	0.00417530  	0.01088845  	0.00815719  
2023-05-31 10:58:59.957: Find a better model.
2023-05-31 10:59:20.450: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.487940]
2023-05-31 10:59:20.742: epoch 9:	0.00468611  	0.01246254  	0.01017322  
2023-05-31 10:59:20.742: Find a better model.
2023-05-31 10:59:41.280: [iter 10 : loss : 1.1344 = 0.6909 + 0.4434 + 0.0000, time: 20.535227]
2023-05-31 10:59:41.564: epoch 10:	0.00540419  	0.01486516  	0.01185425  
2023-05-31 10:59:41.564: Find a better model.
2023-05-31 11:00:02.638: [iter 11 : loss : 1.1337 = 0.6898 + 0.4439 + 0.0000, time: 21.070590]
2023-05-31 11:00:02.919: epoch 11:	0.00629255  	0.01704234  	0.01400688  
2023-05-31 11:00:02.919: Find a better model.
2023-05-31 11:00:24.091: [iter 12 : loss : 1.1325 = 0.6880 + 0.4444 + 0.0000, time: 21.169229]
2023-05-31 11:00:24.382: epoch 12:	0.00768430  	0.02144855  	0.01730405  
2023-05-31 11:00:24.382: Find a better model.
2023-05-31 11:00:45.407: [iter 13 : loss : 1.1306 = 0.6854 + 0.4452 + 0.0000, time: 21.021509]
2023-05-31 11:00:45.687: epoch 13:	0.00994222  	0.02780792  	0.02286871  
2023-05-31 11:00:45.687: Find a better model.
2023-05-31 11:01:06.581: [iter 14 : loss : 1.1273 = 0.6813 + 0.4459 + 0.0001, time: 20.890316]
2023-05-31 11:01:06.860: epoch 14:	0.01305893  	0.03695939  	0.03013238  
2023-05-31 11:01:06.860: Find a better model.
2023-05-31 11:01:27.978: [iter 15 : loss : 1.1210 = 0.6741 + 0.4468 + 0.0001, time: 21.115378]
2023-05-31 11:01:28.251: epoch 15:	0.01668648  	0.04573209  	0.03789592  
2023-05-31 11:01:28.251: Find a better model.
2023-05-31 11:01:48.970: [iter 16 : loss : 1.1097 = 0.6618 + 0.4478 + 0.0002, time: 20.716042]
2023-05-31 11:01:49.248: epoch 16:	0.01989207  	0.05319845  	0.04539583  
2023-05-31 11:01:49.248: Find a better model.
2023-05-31 11:02:10.179: [iter 17 : loss : 1.0907 = 0.6414 + 0.4490 + 0.0002, time: 20.926877]
2023-05-31 11:02:10.465: epoch 17:	0.02340863  	0.06285664  	0.05264265  
2023-05-31 11:02:10.465: Find a better model.
2023-05-31 11:02:31.394: [iter 18 : loss : 1.0612 = 0.6100 + 0.4508 + 0.0004, time: 20.926453]
2023-05-31 11:02:31.669: epoch 18:	0.02551117  	0.06860198  	0.05705905  
2023-05-31 11:02:31.669: Find a better model.
2023-05-31 11:02:52.343: [iter 19 : loss : 1.0217 = 0.5679 + 0.4532 + 0.0006, time: 20.670395]
2023-05-31 11:02:52.619: epoch 19:	0.02723613  	0.07278216  	0.06070979  
2023-05-31 11:02:52.619: Find a better model.
2023-05-31 11:03:13.556: [iter 20 : loss : 0.9747 = 0.5176 + 0.4563 + 0.0008, time: 20.934655]
2023-05-31 11:03:13.831: epoch 20:	0.02832441  	0.07617269  	0.06292038  
2023-05-31 11:03:13.831: Find a better model.
2023-05-31 11:03:34.946: [iter 21 : loss : 0.9250 = 0.4642 + 0.4597 + 0.0011, time: 21.110475]
2023-05-31 11:03:35.228: epoch 21:	0.02886484  	0.07782836  	0.06379101  
2023-05-31 11:03:35.229: Find a better model.
2023-05-31 11:03:56.317: [iter 22 : loss : 0.8763 = 0.4122 + 0.4627 + 0.0014, time: 21.085492]
2023-05-31 11:03:56.594: epoch 22:	0.02912396  	0.07863399  	0.06399048  
2023-05-31 11:03:56.594: Find a better model.
2023-05-31 11:04:17.338: [iter 23 : loss : 0.8334 = 0.3668 + 0.4650 + 0.0017, time: 20.739663]
2023-05-31 11:04:17.615: epoch 23:	0.02909435  	0.07855178  	0.06399245  
2023-05-31 11:04:38.551: [iter 24 : loss : 0.7964 = 0.3277 + 0.4667 + 0.0020, time: 20.931278]
2023-05-31 11:04:38.829: epoch 24:	0.02922020  	0.07934912  	0.06439064  
2023-05-31 11:04:38.829: Find a better model.
2023-05-31 11:04:59.744: [iter 25 : loss : 0.7642 = 0.2945 + 0.4674 + 0.0023, time: 20.910704]
2023-05-31 11:05:00.026: epoch 25:	0.02941268  	0.08030004  	0.06475514  
2023-05-31 11:05:00.027: Find a better model.
2023-05-31 11:05:20.954: [iter 26 : loss : 0.7379 = 0.2676 + 0.4678 + 0.0026, time: 20.922419]
2023-05-31 11:05:21.229: epoch 26:	0.02957555  	0.08079889  	0.06508902  
2023-05-31 11:05:21.229: Find a better model.
2023-05-31 11:05:42.068: [iter 27 : loss : 0.7148 = 0.2444 + 0.4677 + 0.0028, time: 20.834392]
2023-05-31 11:05:42.352: epoch 27:	0.02962737  	0.08081049  	0.06512902  
2023-05-31 11:05:42.352: Find a better model.
2023-05-31 11:06:03.296: [iter 28 : loss : 0.6955 = 0.2251 + 0.4673 + 0.0031, time: 20.941138]
2023-05-31 11:06:03.568: epoch 28:	0.02975323  	0.08139844  	0.06539871  
2023-05-31 11:06:03.568: Find a better model.
2023-05-31 11:06:24.454: [iter 29 : loss : 0.6792 = 0.2091 + 0.4667 + 0.0033, time: 20.882309]
2023-05-31 11:06:24.723: epoch 29:	0.02973101  	0.08111106  	0.06537195  
2023-05-31 11:06:45.646: [iter 30 : loss : 0.6638 = 0.1942 + 0.4661 + 0.0036, time: 20.919344]
2023-05-31 11:06:45.915: epoch 30:	0.02974582  	0.08151519  	0.06552164  
2023-05-31 11:06:45.916: Find a better model.
2023-05-31 11:07:07.031: [iter 31 : loss : 0.6508 = 0.1816 + 0.4655 + 0.0038, time: 21.111542]
2023-05-31 11:07:07.306: epoch 31:	0.02981986  	0.08173932  	0.06580660  
2023-05-31 11:07:07.306: Find a better model.
2023-05-31 11:07:28.230: [iter 32 : loss : 0.6396 = 0.1708 + 0.4649 + 0.0040, time: 20.914982]
2023-05-31 11:07:28.508: epoch 32:	0.02980504  	0.08140732  	0.06577989  
2023-05-31 11:07:49.232: [iter 33 : loss : 0.6300 = 0.1617 + 0.4641 + 0.0042, time: 20.721269]
2023-05-31 11:07:49.506: epoch 33:	0.02982725  	0.08154541  	0.06583980  
2023-05-31 11:08:10.284: [iter 34 : loss : 0.6206 = 0.1527 + 0.4635 + 0.0044, time: 20.775239]
2023-05-31 11:08:10.550: epoch 34:	0.02994570  	0.08182988  	0.06629455  
2023-05-31 11:08:10.550: Find a better model.
2023-05-31 11:08:31.405: [iter 35 : loss : 0.6119 = 0.1445 + 0.4628 + 0.0046, time: 20.852173]
2023-05-31 11:08:31.671: epoch 35:	0.02991610  	0.08165485  	0.06628422  
2023-05-31 11:08:52.663: [iter 36 : loss : 0.6050 = 0.1381 + 0.4621 + 0.0048, time: 20.988256]
2023-05-31 11:08:52.928: epoch 36:	0.02994569  	0.08156755  	0.06628925  
2023-05-31 11:09:14.002: [iter 37 : loss : 0.5987 = 0.1321 + 0.4617 + 0.0050, time: 21.069982]
2023-05-31 11:09:14.271: epoch 37:	0.02987906  	0.08087515  	0.06614160  
2023-05-31 11:09:35.009: [iter 38 : loss : 0.5922 = 0.1259 + 0.4612 + 0.0052, time: 20.733997]
2023-05-31 11:09:35.275: epoch 38:	0.02978283  	0.08048194  	0.06602937  
2023-05-31 11:09:56.172: [iter 39 : loss : 0.5869 = 0.1210 + 0.4605 + 0.0053, time: 20.891703]
2023-05-31 11:09:56.440: epoch 39:	0.02973841  	0.08035254  	0.06589113  
2023-05-31 11:10:17.579: [iter 40 : loss : 0.5816 = 0.1160 + 0.4600 + 0.0055, time: 21.135290]
2023-05-31 11:10:17.845: epoch 40:	0.02980504  	0.08071157  	0.06595868  
2023-05-31 11:10:38.969: [iter 41 : loss : 0.5764 = 0.1111 + 0.4596 + 0.0057, time: 21.119673]
2023-05-31 11:10:39.234: epoch 41:	0.02979023  	0.08039132  	0.06602073  
2023-05-31 11:11:00.242: [iter 42 : loss : 0.5719 = 0.1068 + 0.4592 + 0.0058, time: 21.004981]
2023-05-31 11:11:00.501: epoch 42:	0.02985685  	0.08072739  	0.06615114  
2023-05-31 11:11:21.554: [iter 43 : loss : 0.5680 = 0.1033 + 0.4587 + 0.0060, time: 21.049981]
2023-05-31 11:11:21.820: epoch 43:	0.02993088  	0.08043463  	0.06612907  
2023-05-31 11:11:42.980: [iter 44 : loss : 0.5649 = 0.1003 + 0.4584 + 0.0062, time: 21.157050]
2023-05-31 11:11:43.246: epoch 44:	0.02994569  	0.08074681  	0.06618472  
2023-05-31 11:12:04.383: [iter 45 : loss : 0.5611 = 0.0967 + 0.4581 + 0.0063, time: 21.131671]
2023-05-31 11:12:04.647: epoch 45:	0.02982723  	0.08042461  	0.06613922  
2023-05-31 11:12:25.924: [iter 46 : loss : 0.5578 = 0.0936 + 0.4577 + 0.0065, time: 21.274327]
2023-05-31 11:12:26.188: epoch 46:	0.02978281  	0.08034401  	0.06604365  
2023-05-31 11:12:47.314: [iter 47 : loss : 0.5545 = 0.0905 + 0.4574 + 0.0066, time: 21.121415]
2023-05-31 11:12:47.580: epoch 47:	0.02983464  	0.08018917  	0.06597785  
2023-05-31 11:13:08.911: [iter 48 : loss : 0.5519 = 0.0882 + 0.4570 + 0.0067, time: 21.327911]
2023-05-31 11:13:09.180: epoch 48:	0.02972359  	0.07994238  	0.06581265  
2023-05-31 11:13:30.335: [iter 49 : loss : 0.5491 = 0.0854 + 0.4567 + 0.0069, time: 21.151427]
2023-05-31 11:13:30.599: epoch 49:	0.02974581  	0.07996763  	0.06583388  
2023-05-31 11:13:51.928: [iter 50 : loss : 0.5467 = 0.0832 + 0.4565 + 0.0070, time: 21.325038]
2023-05-31 11:13:52.193: epoch 50:	0.02955333  	0.07931362  	0.06557377  
2023-05-31 11:14:13.485: [iter 51 : loss : 0.5443 = 0.0810 + 0.4561 + 0.0072, time: 21.287912]
2023-05-31 11:14:13.749: epoch 51:	0.02952371  	0.07914999  	0.06551126  
2023-05-31 11:14:35.102: [iter 52 : loss : 0.5415 = 0.0783 + 0.4559 + 0.0073, time: 21.349368]
2023-05-31 11:14:35.375: epoch 52:	0.02936083  	0.07855359  	0.06531490  
2023-05-31 11:14:56.675: [iter 53 : loss : 0.5402 = 0.0772 + 0.4556 + 0.0074, time: 21.296035]
2023-05-31 11:14:56.938: epoch 53:	0.02927200  	0.07837569  	0.06525268  
2023-05-31 11:15:18.278: [iter 54 : loss : 0.5383 = 0.0754 + 0.4554 + 0.0076, time: 21.335407]
2023-05-31 11:15:18.545: epoch 54:	0.02927200  	0.07828386  	0.06514543  
2023-05-31 11:15:39.655: [iter 55 : loss : 0.5362 = 0.0733 + 0.4552 + 0.0077, time: 21.107810]
2023-05-31 11:15:39.921: epoch 55:	0.02931641  	0.07834202  	0.06520886  
2023-05-31 11:16:01.291: [iter 56 : loss : 0.5339 = 0.0712 + 0.4549 + 0.0078, time: 21.365660]
2023-05-31 11:16:01.559: epoch 56:	0.02925719  	0.07806259  	0.06507169  
2023-05-31 11:16:23.057: [iter 57 : loss : 0.5320 = 0.0693 + 0.4548 + 0.0079, time: 21.494045]
2023-05-31 11:16:23.338: epoch 57:	0.02919056  	0.07808001  	0.06495551  
2023-05-31 11:16:44.699: [iter 58 : loss : 0.5307 = 0.0680 + 0.4546 + 0.0080, time: 21.358046]
2023-05-31 11:16:44.967: epoch 58:	0.02917575  	0.07773180  	0.06482924  
2023-05-31 11:17:06.273: [iter 59 : loss : 0.5291 = 0.0664 + 0.4545 + 0.0082, time: 21.302110]
2023-05-31 11:17:06.542: epoch 59:	0.02905731  	0.07734305  	0.06460103  
2023-05-31 11:17:06.542: Early stopping is trigger at epoch: 59
2023-05-31 11:17:06.542: best_result@epoch 34:

2023-05-31 11:17:06.542: 		0.0299      	0.0818      	0.0663      
2023-05-31 11:20:29.500: my pid: 13528
2023-05-31 11:20:29.500: model: model.general_recommender.SGL
2023-05-31 11:20:29.500: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 11:20:29.501: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 11:20:33.469: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 11:20:54.201: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.730442]
2023-05-31 11:20:54.477: epoch 1:	0.00135475  	0.00315104  	0.00257486  
2023-05-31 11:20:54.477: Find a better model.
2023-05-31 11:21:15.389: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.908252]
2023-05-31 11:21:15.683: epoch 2:	0.00166568  	0.00303447  	0.00261731  
2023-05-31 11:21:36.971: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.284434]
2023-05-31 11:21:37.269: epoch 3:	0.00168049  	0.00385509  	0.00308182  
2023-05-31 11:21:37.269: Find a better model.
2023-05-31 11:21:58.365: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.091063]
2023-05-31 11:21:58.665: epoch 4:	0.00188777  	0.00393181  	0.00318623  
2023-05-31 11:21:58.665: Find a better model.
2023-05-31 11:22:19.560: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.890062]
2023-05-31 11:22:19.864: epoch 5:	0.00246521  	0.00557645  	0.00421219  
2023-05-31 11:22:19.864: Find a better model.
2023-05-31 11:22:40.940: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.071055]
2023-05-31 11:22:41.241: epoch 6:	0.00276873  	0.00694555  	0.00519676  
2023-05-31 11:22:41.241: Find a better model.
2023-05-31 11:23:02.357: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 21.112023]
2023-05-31 11:23:02.658: epoch 7:	0.00347942  	0.00810003  	0.00642148  
2023-05-31 11:23:02.658: Find a better model.
2023-05-31 11:23:23.735: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 21.072067]
2023-05-31 11:23:24.035: epoch 8:	0.00409387  	0.00974548  	0.00801125  
2023-05-31 11:23:24.036: Find a better model.
2023-05-31 11:23:45.161: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 21.122079]
2023-05-31 11:23:45.455: epoch 9:	0.00478975  	0.01255041  	0.00947768  
2023-05-31 11:23:45.455: Find a better model.
2023-05-31 11:24:06.336: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.877462]
2023-05-31 11:24:06.629: epoch 10:	0.00563368  	0.01484655  	0.01159554  
2023-05-31 11:24:06.629: Find a better model.
2023-05-31 11:24:28.112: [iter 11 : loss : 1.1338 = 0.6899 + 0.4439 + 0.0000, time: 21.478101]
2023-05-31 11:24:28.404: epoch 11:	0.00684777  	0.01897470  	0.01488140  
2023-05-31 11:24:28.404: Find a better model.
2023-05-31 11:24:49.935: [iter 12 : loss : 1.1326 = 0.6881 + 0.4445 + 0.0000, time: 21.526482]
2023-05-31 11:24:50.225: epoch 12:	0.00801743  	0.02241530  	0.01815362  
2023-05-31 11:24:50.225: Find a better model.
2023-05-31 11:25:11.709: [iter 13 : loss : 1.1308 = 0.6856 + 0.4451 + 0.0000, time: 21.479898]
2023-05-31 11:25:12.003: epoch 13:	0.01012730  	0.02781798  	0.02342988  
2023-05-31 11:25:12.003: Find a better model.
2023-05-31 11:25:33.514: [iter 14 : loss : 1.1274 = 0.6814 + 0.4459 + 0.0001, time: 21.507187]
2023-05-31 11:25:33.816: epoch 14:	0.01294048  	0.03612447  	0.03099595  
2023-05-31 11:25:33.816: Find a better model.
2023-05-31 11:25:55.555: [iter 15 : loss : 1.1213 = 0.6745 + 0.4467 + 0.0001, time: 21.734957]
2023-05-31 11:25:55.849: epoch 15:	0.01653102  	0.04577464  	0.03893636  
2023-05-31 11:25:55.849: Find a better model.
2023-05-31 11:26:17.451: [iter 16 : loss : 1.1102 = 0.6623 + 0.4478 + 0.0002, time: 21.596999]
2023-05-31 11:26:17.739: epoch 16:	0.01989207  	0.05473524  	0.04647778  
2023-05-31 11:26:17.739: Find a better model.
2023-05-31 11:26:39.026: [iter 17 : loss : 1.0916 = 0.6423 + 0.4490 + 0.0002, time: 21.282408]
2023-05-31 11:26:39.312: epoch 17:	0.02292741  	0.06159674  	0.05294362  
2023-05-31 11:26:39.312: Find a better model.
2023-05-31 11:27:00.652: [iter 18 : loss : 1.0625 = 0.6113 + 0.4509 + 0.0004, time: 21.335937]
2023-05-31 11:27:00.940: epoch 18:	0.02536311  	0.06776227  	0.05773274  
2023-05-31 11:27:00.940: Find a better model.
2023-05-31 11:27:21.865: [iter 19 : loss : 1.0233 = 0.5695 + 0.4532 + 0.0006, time: 20.921494]
2023-05-31 11:27:22.147: epoch 19:	0.02740639  	0.07305963  	0.06169039  
2023-05-31 11:27:22.147: Find a better model.
2023-05-31 11:27:43.413: [iter 20 : loss : 0.9762 = 0.5193 + 0.4561 + 0.0008, time: 21.262110]
2023-05-31 11:27:43.695: epoch 20:	0.02843543  	0.07572443  	0.06376216  
2023-05-31 11:27:43.695: Find a better model.
2023-05-31 11:28:04.873: [iter 21 : loss : 0.9267 = 0.4660 + 0.4596 + 0.0011, time: 21.174994]
2023-05-31 11:28:05.155: epoch 21:	0.02925719  	0.07775084  	0.06505586  
2023-05-31 11:28:05.155: Find a better model.
2023-05-31 11:28:26.403: [iter 22 : loss : 0.8782 = 0.4142 + 0.4626 + 0.0014, time: 21.245084]
2023-05-31 11:28:26.684: epoch 22:	0.02931642  	0.07793940  	0.06522324  
2023-05-31 11:28:26.684: Find a better model.
2023-05-31 11:28:48.195: [iter 23 : loss : 0.8348 = 0.3682 + 0.4650 + 0.0017, time: 21.507237]
2023-05-31 11:28:48.477: epoch 23:	0.02956813  	0.07837968  	0.06549051  
2023-05-31 11:28:48.477: Find a better model.
2023-05-31 11:29:09.796: [iter 24 : loss : 0.7978 = 0.3292 + 0.4666 + 0.0020, time: 21.315049]
2023-05-31 11:29:10.075: epoch 24:	0.02970139  	0.07931630  	0.06587318  
2023-05-31 11:29:10.075: Find a better model.
2023-05-31 11:29:31.179: [iter 25 : loss : 0.7655 = 0.2960 + 0.4673 + 0.0023, time: 21.099459]
2023-05-31 11:29:31.458: epoch 25:	0.02970880  	0.07982707  	0.06589928  
2023-05-31 11:29:31.458: Find a better model.
2023-05-31 11:29:52.606: [iter 26 : loss : 0.7389 = 0.2686 + 0.4677 + 0.0025, time: 21.142732]
2023-05-31 11:29:52.886: epoch 26:	0.03005674  	0.08121650  	0.06649400  
2023-05-31 11:29:52.886: Find a better model.
2023-05-31 11:30:14.207: [iter 27 : loss : 0.7159 = 0.2455 + 0.4676 + 0.0028, time: 21.317294]
2023-05-31 11:30:14.484: epoch 27:	0.03019000  	0.08178408  	0.06681386  
2023-05-31 11:30:14.484: Find a better model.
2023-05-31 11:30:35.784: [iter 28 : loss : 0.6965 = 0.2262 + 0.4673 + 0.0031, time: 21.295908]
2023-05-31 11:30:36.056: epoch 28:	0.03019740  	0.08205857  	0.06692996  
2023-05-31 11:30:36.056: Find a better model.
2023-05-31 11:30:57.343: [iter 29 : loss : 0.6795 = 0.2096 + 0.4666 + 0.0033, time: 21.282541]
2023-05-31 11:30:57.613: epoch 29:	0.03024182  	0.08215813  	0.06721094  
2023-05-31 11:30:57.613: Find a better model.
2023-05-31 11:31:18.924: [iter 30 : loss : 0.6645 = 0.1948 + 0.4661 + 0.0035, time: 21.307190]
2023-05-31 11:31:19.199: epoch 30:	0.03023441  	0.08239070  	0.06718771  
2023-05-31 11:31:19.200: Find a better model.
2023-05-31 11:31:40.371: [iter 31 : loss : 0.6515 = 0.1822 + 0.4655 + 0.0038, time: 21.167390]
2023-05-31 11:31:40.643: epoch 31:	0.03027144  	0.08253430  	0.06725625  
2023-05-31 11:31:40.643: Find a better model.
2023-05-31 11:32:02.140: [iter 32 : loss : 0.6402 = 0.1714 + 0.4648 + 0.0040, time: 21.493411]
2023-05-31 11:32:02.415: epoch 32:	0.03029365  	0.08207250  	0.06724404  
2023-05-31 11:32:24.161: [iter 33 : loss : 0.6307 = 0.1624 + 0.4641 + 0.0042, time: 21.741611]
2023-05-31 11:32:24.432: epoch 33:	0.03019741  	0.08197343  	0.06732942  
2023-05-31 11:32:45.952: [iter 34 : loss : 0.6212 = 0.1532 + 0.4636 + 0.0044, time: 21.517169]
2023-05-31 11:32:46.222: epoch 34:	0.03036027  	0.08216980  	0.06731154  
2023-05-31 11:33:07.745: [iter 35 : loss : 0.6126 = 0.1451 + 0.4629 + 0.0046, time: 21.519519]
2023-05-31 11:33:08.019: epoch 35:	0.03030104  	0.08190627  	0.06714462  
2023-05-31 11:33:29.511: [iter 36 : loss : 0.6056 = 0.1386 + 0.4622 + 0.0048, time: 21.489619]
2023-05-31 11:33:29.798: epoch 36:	0.03034545  	0.08184811  	0.06712826  
2023-05-31 11:33:51.325: [iter 37 : loss : 0.5989 = 0.1323 + 0.4616 + 0.0050, time: 21.523025]
2023-05-31 11:33:51.598: epoch 37:	0.03030844  	0.08169296  	0.06722306  
2023-05-31 11:34:13.289: [iter 38 : loss : 0.5924 = 0.1261 + 0.4612 + 0.0052, time: 21.686044]
2023-05-31 11:34:13.562: epoch 38:	0.03023441  	0.08158809  	0.06708048  
2023-05-31 11:34:35.311: [iter 39 : loss : 0.5871 = 0.1211 + 0.4606 + 0.0053, time: 21.745013]
2023-05-31 11:34:35.581: epoch 39:	0.03033065  	0.08212081  	0.06715909  
2023-05-31 11:34:57.274: [iter 40 : loss : 0.5818 = 0.1161 + 0.4601 + 0.0055, time: 21.690069]
2023-05-31 11:34:57.548: epoch 40:	0.03024181  	0.08123175  	0.06701530  
2023-05-31 11:35:19.111: [iter 41 : loss : 0.5767 = 0.1114 + 0.4597 + 0.0057, time: 21.559318]
2023-05-31 11:35:19.382: epoch 41:	0.03040468  	0.08175624  	0.06706599  
2023-05-31 11:35:41.067: [iter 42 : loss : 0.5723 = 0.1072 + 0.4592 + 0.0058, time: 21.682125]
2023-05-31 11:35:41.338: epoch 42:	0.03030104  	0.08195276  	0.06706324  
2023-05-31 11:36:02.853: [iter 43 : loss : 0.5683 = 0.1034 + 0.4589 + 0.0060, time: 21.512043]
2023-05-31 11:36:03.123: epoch 43:	0.03028624  	0.08130570  	0.06688199  
2023-05-31 11:36:24.662: [iter 44 : loss : 0.5654 = 0.1007 + 0.4585 + 0.0062, time: 21.533490]
2023-05-31 11:36:24.941: epoch 44:	0.03027143  	0.08184046  	0.06698308  
2023-05-31 11:36:46.479: [iter 45 : loss : 0.5615 = 0.0971 + 0.4580 + 0.0063, time: 21.533107]
2023-05-31 11:36:46.749: epoch 45:	0.03027142  	0.08167012  	0.06712125  
2023-05-31 11:37:08.236: [iter 46 : loss : 0.5580 = 0.0938 + 0.4577 + 0.0065, time: 21.480219]
2023-05-31 11:37:08.508: epoch 46:	0.03023441  	0.08129470  	0.06702931  
2023-05-31 11:37:30.039: [iter 47 : loss : 0.5546 = 0.0907 + 0.4573 + 0.0066, time: 21.526295]
2023-05-31 11:37:30.309: epoch 47:	0.03023442  	0.08098239  	0.06696381  
2023-05-31 11:37:53.553: [iter 48 : loss : 0.5521 = 0.0882 + 0.4572 + 0.0067, time: 23.238980]
2023-05-31 11:37:53.865: epoch 48:	0.03016038  	0.08086614  	0.06697455  
2023-05-31 11:38:02.380: my pid: 7136
2023-05-31 11:38:02.381: model: model.general_recommender.SGL
2023-05-31 11:38:02.381: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 11:38:02.381: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 11:38:06.731: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 11:38:28.174: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.442328]
2023-05-31 11:38:28.440: epoch 1:	0.00126592  	0.00241526  	0.00195244  
2023-05-31 11:38:28.440: Find a better model.
2023-05-31 11:38:49.217: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.774384]
2023-05-31 11:38:49.503: epoch 2:	0.00160646  	0.00335198  	0.00268259  
2023-05-31 11:38:49.503: Find a better model.
2023-05-31 11:39:10.208: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 20.701446]
2023-05-31 11:39:10.496: epoch 3:	0.00177673  	0.00385456  	0.00302449  
2023-05-31 11:39:10.496: Find a better model.
2023-05-31 11:39:31.199: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.698445]
2023-05-31 11:39:31.489: epoch 4:	0.00213207  	0.00403414  	0.00317354  
2023-05-31 11:39:31.489: Find a better model.
2023-05-31 11:39:52.138: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.646475]
2023-05-31 11:39:52.427: epoch 5:	0.00271691  	0.00636107  	0.00447332  
2023-05-31 11:39:52.427: Find a better model.
2023-05-31 11:40:13.153: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.722575]
2023-05-31 11:40:13.441: epoch 6:	0.00287978  	0.00732944  	0.00560023  
2023-05-31 11:40:13.441: Find a better model.
2023-05-31 11:40:34.130: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.685116]
2023-05-31 11:40:34.415: epoch 7:	0.00375333  	0.00897509  	0.00711505  
2023-05-31 11:40:34.415: Find a better model.
2023-05-31 11:40:55.167: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 20.748062]
2023-05-31 11:40:55.453: epoch 8:	0.00447882  	0.01091398  	0.00871053  
2023-05-31 11:40:55.453: Find a better model.
2023-05-31 11:41:16.358: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.900025]
2023-05-31 11:41:16.645: epoch 9:	0.00496001  	0.01278433  	0.01016282  
2023-05-31 11:41:16.645: Find a better model.
2023-05-31 11:41:37.399: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.751070]
2023-05-31 11:41:37.686: epoch 10:	0.00595201  	0.01669158  	0.01297242  
2023-05-31 11:41:37.686: Find a better model.
2023-05-31 11:41:58.820: [iter 11 : loss : 1.1339 = 0.6900 + 0.4438 + 0.0000, time: 21.129453]
2023-05-31 11:41:59.103: epoch 11:	0.00673672  	0.01879862  	0.01535027  
2023-05-31 11:41:59.103: Find a better model.
2023-05-31 11:42:20.159: [iter 12 : loss : 1.1328 = 0.6883 + 0.4444 + 0.0000, time: 21.053010]
2023-05-31 11:42:20.446: epoch 12:	0.00769910  	0.02271479  	0.01827388  
2023-05-31 11:42:20.446: Find a better model.
2023-05-31 11:42:41.965: [iter 13 : loss : 1.1309 = 0.6857 + 0.4452 + 0.0000, time: 21.516780]
2023-05-31 11:42:42.243: epoch 13:	0.01009028  	0.02924871  	0.02384681  
2023-05-31 11:42:42.243: Find a better model.
2023-05-31 11:43:03.499: [iter 14 : loss : 1.1274 = 0.6815 + 0.4458 + 0.0001, time: 21.251853]
2023-05-31 11:43:03.793: epoch 14:	0.01292566  	0.03757470  	0.03074625  
2023-05-31 11:43:03.793: Find a better model.
2023-05-31 11:43:24.923: [iter 15 : loss : 1.1215 = 0.6746 + 0.4467 + 0.0001, time: 21.126094]
2023-05-31 11:43:25.202: epoch 15:	0.01647918  	0.04586282  	0.03883744  
2023-05-31 11:43:25.202: Find a better model.
2023-05-31 11:43:46.540: [iter 16 : loss : 1.1105 = 0.6626 + 0.4477 + 0.0002, time: 21.335153]
2023-05-31 11:43:46.827: epoch 16:	0.01957374  	0.05368399  	0.04599061  
2023-05-31 11:43:46.827: Find a better model.
2023-05-31 11:44:08.273: [iter 17 : loss : 1.0921 = 0.6428 + 0.4491 + 0.0002, time: 21.441920]
2023-05-31 11:44:08.550: epoch 17:	0.02254985  	0.06118917  	0.05233466  
2023-05-31 11:44:08.550: Find a better model.
2023-05-31 11:44:30.126: [iter 18 : loss : 1.0633 = 0.6121 + 0.4508 + 0.0004, time: 21.572155]
2023-05-31 11:44:30.402: epoch 18:	0.02492630  	0.06776533  	0.05685432  
2023-05-31 11:44:30.402: Find a better model.
2023-05-31 11:44:51.651: [iter 19 : loss : 1.0245 = 0.5709 + 0.4531 + 0.0006, time: 21.246840]
2023-05-31 11:44:51.923: epoch 19:	0.02675493  	0.07303341  	0.06061881  
2023-05-31 11:44:51.923: Find a better model.
2023-05-31 11:45:12.843: [iter 20 : loss : 0.9782 = 0.5214 + 0.4560 + 0.0008, time: 20.917833]
2023-05-31 11:45:13.116: epoch 20:	0.02770253  	0.07497185  	0.06267405  
2023-05-31 11:45:13.116: Find a better model.
2023-05-31 11:45:34.067: [iter 21 : loss : 0.9289 = 0.4687 + 0.4592 + 0.0011, time: 20.947501]
2023-05-31 11:45:34.341: epoch 21:	0.02833181  	0.07695684  	0.06378011  
2023-05-31 11:45:34.341: Find a better model.
2023-05-31 11:45:55.486: [iter 22 : loss : 0.8803 = 0.4167 + 0.4622 + 0.0014, time: 21.141210]
2023-05-31 11:45:55.766: epoch 22:	0.02865015  	0.07793628  	0.06436355  
2023-05-31 11:45:55.766: Find a better model.
2023-05-31 11:46:16.881: [iter 23 : loss : 0.8371 = 0.3708 + 0.4646 + 0.0017, time: 21.106084]
2023-05-31 11:46:17.158: epoch 23:	0.02906474  	0.07932274  	0.06489893  
2023-05-31 11:46:17.158: Find a better model.
2023-05-31 11:46:38.250: [iter 24 : loss : 0.7995 = 0.3313 + 0.4662 + 0.0020, time: 21.087998]
2023-05-31 11:46:38.520: epoch 24:	0.02908693  	0.07943369  	0.06507125  
2023-05-31 11:46:38.520: Find a better model.
2023-05-31 11:46:59.677: [iter 25 : loss : 0.7674 = 0.2979 + 0.4672 + 0.0022, time: 21.152606]
2023-05-31 11:46:59.947: epoch 25:	0.02930905  	0.07998076  	0.06520673  
2023-05-31 11:46:59.947: Find a better model.
2023-05-31 11:47:20.994: [iter 26 : loss : 0.7405 = 0.2703 + 0.4676 + 0.0025, time: 21.042100]
2023-05-31 11:47:21.260: epoch 26:	0.02929423  	0.08012601  	0.06550561  
2023-05-31 11:47:21.260: Find a better model.
2023-05-31 11:47:42.423: [iter 27 : loss : 0.7176 = 0.2471 + 0.4677 + 0.0028, time: 21.157935]
2023-05-31 11:47:42.690: epoch 27:	0.02941269  	0.08028583  	0.06567864  
2023-05-31 11:47:42.690: Find a better model.
2023-05-31 11:48:03.786: [iter 28 : loss : 0.6975 = 0.2272 + 0.4672 + 0.0031, time: 21.092156]
2023-05-31 11:48:04.050: epoch 28:	0.02966440  	0.08117210  	0.06585643  
2023-05-31 11:48:04.051: Find a better model.
2023-05-31 11:48:25.403: [iter 29 : loss : 0.6806 = 0.2107 + 0.4667 + 0.0033, time: 21.349390]
2023-05-31 11:48:25.667: epoch 29:	0.02983467  	0.08168950  	0.06615164  
2023-05-31 11:48:25.667: Find a better model.
2023-05-31 11:48:46.994: [iter 30 : loss : 0.6654 = 0.1957 + 0.4661 + 0.0035, time: 21.322745]
2023-05-31 11:48:47.258: epoch 30:	0.02974583  	0.08108440  	0.06604879  
2023-05-31 11:49:08.364: [iter 31 : loss : 0.6523 = 0.1829 + 0.4656 + 0.0038, time: 21.102024]
2023-05-31 11:49:08.626: epoch 31:	0.02982728  	0.08127123  	0.06637735  
2023-05-31 11:49:29.782: [iter 32 : loss : 0.6408 = 0.1720 + 0.4648 + 0.0040, time: 21.151951]
2023-05-31 11:49:30.046: epoch 32:	0.02986429  	0.08175180  	0.06655679  
2023-05-31 11:49:30.046: Find a better model.
2023-05-31 11:49:50.933: [iter 33 : loss : 0.6312 = 0.1628 + 0.4642 + 0.0042, time: 20.883911]
2023-05-31 11:49:51.198: epoch 33:	0.02985688  	0.08149750  	0.06656370  
2023-05-31 11:50:12.345: [iter 34 : loss : 0.6221 = 0.1541 + 0.4636 + 0.0044, time: 21.143482]
2023-05-31 11:50:12.607: epoch 34:	0.02990129  	0.08135158  	0.06655427  
2023-05-31 11:50:33.761: [iter 35 : loss : 0.6130 = 0.1455 + 0.4629 + 0.0046, time: 21.149981]
2023-05-31 11:50:34.023: epoch 35:	0.02989389  	0.08125654  	0.06663488  
2023-05-31 11:50:55.194: [iter 36 : loss : 0.6058 = 0.1387 + 0.4623 + 0.0048, time: 21.167875]
2023-05-31 11:50:55.457: epoch 36:	0.02999754  	0.08208942  	0.06688735  
2023-05-31 11:50:55.457: Find a better model.
2023-05-31 11:51:16.761: [iter 37 : loss : 0.5994 = 0.1327 + 0.4617 + 0.0050, time: 21.301090]
2023-05-31 11:51:17.024: epoch 37:	0.03014560  	0.08207179  	0.06691198  
2023-05-31 11:51:38.309: [iter 38 : loss : 0.5928 = 0.1265 + 0.4611 + 0.0052, time: 21.280988]
2023-05-31 11:51:38.570: epoch 38:	0.03013080  	0.08226466  	0.06687206  
2023-05-31 11:51:38.570: Find a better model.
2023-05-31 11:51:59.744: [iter 39 : loss : 0.5878 = 0.1218 + 0.4607 + 0.0053, time: 21.169989]
2023-05-31 11:52:00.008: epoch 39:	0.03010858  	0.08205005  	0.06676833  
2023-05-31 11:52:21.333: [iter 40 : loss : 0.5823 = 0.1166 + 0.4602 + 0.0055, time: 21.319981]
2023-05-31 11:52:21.594: epoch 40:	0.03016040  	0.08208634  	0.06683789  
2023-05-31 11:52:43.118: [iter 41 : loss : 0.5770 = 0.1116 + 0.4597 + 0.0057, time: 21.519981]
2023-05-31 11:52:43.379: epoch 41:	0.03008637  	0.08206871  	0.06672505  
2023-05-31 11:53:04.720: [iter 42 : loss : 0.5726 = 0.1074 + 0.4593 + 0.0058, time: 21.337996]
2023-05-31 11:53:04.983: epoch 42:	0.03010857  	0.08215773  	0.06670510  
2023-05-31 11:53:26.306: [iter 43 : loss : 0.5686 = 0.1036 + 0.4590 + 0.0060, time: 21.320066]
2023-05-31 11:53:26.570: epoch 43:	0.03010116  	0.08155943  	0.06666628  
2023-05-31 11:53:48.131: [iter 44 : loss : 0.5652 = 0.1006 + 0.4585 + 0.0061, time: 21.557765]
2023-05-31 11:53:48.394: epoch 44:	0.03004934  	0.08151384  	0.06659912  
2023-05-31 11:54:09.855: [iter 45 : loss : 0.5615 = 0.0971 + 0.4581 + 0.0063, time: 21.456221]
2023-05-31 11:54:10.116: epoch 45:	0.03003454  	0.08122925  	0.06670511  
2023-05-31 11:54:31.513: [iter 46 : loss : 0.5582 = 0.0939 + 0.4578 + 0.0064, time: 21.393981]
2023-05-31 11:54:31.783: epoch 46:	0.03004194  	0.08109502  	0.06661302  
2023-05-31 11:54:53.055: [iter 47 : loss : 0.5553 = 0.0913 + 0.4574 + 0.0066, time: 21.264348]
2023-05-31 11:54:53.317: epoch 47:	0.03005675  	0.08112590  	0.06650526  
2023-05-31 11:55:14.639: [iter 48 : loss : 0.5522 = 0.0884 + 0.4571 + 0.0067, time: 21.316980]
2023-05-31 11:55:14.905: epoch 48:	0.03007155  	0.08074693  	0.06644721  
2023-05-31 11:55:36.239: [iter 49 : loss : 0.5494 = 0.0858 + 0.4567 + 0.0069, time: 21.330909]
2023-05-31 11:55:36.502: epoch 49:	0.03005675  	0.08069552  	0.06640612  
2023-05-31 11:55:57.833: [iter 50 : loss : 0.5473 = 0.0837 + 0.4566 + 0.0070, time: 21.328400]
2023-05-31 11:55:58.094: epoch 50:	0.02991608  	0.08020579  	0.06620368  
2023-05-31 11:56:19.241: [iter 51 : loss : 0.5445 = 0.0811 + 0.4562 + 0.0072, time: 21.142461]
2023-05-31 11:56:19.502: epoch 51:	0.02995309  	0.08022762  	0.06622855  
2023-05-31 11:56:40.802: [iter 52 : loss : 0.5417 = 0.0785 + 0.4559 + 0.0073, time: 21.297026]
2023-05-31 11:56:41.063: epoch 52:	0.02988645  	0.07998803  	0.06615733  
2023-05-31 11:57:02.069: [iter 53 : loss : 0.5410 = 0.0779 + 0.4557 + 0.0074, time: 21.002012]
2023-05-31 11:57:02.331: epoch 53:	0.02979021  	0.07976343  	0.06614922  
2023-05-31 11:57:23.601: [iter 54 : loss : 0.5385 = 0.0755 + 0.4555 + 0.0075, time: 21.267580]
2023-05-31 11:57:23.870: epoch 54:	0.02978281  	0.07959523  	0.06598370  
2023-05-31 11:57:45.208: [iter 55 : loss : 0.5360 = 0.0731 + 0.4552 + 0.0077, time: 21.334991]
2023-05-31 11:57:45.468: epoch 55:	0.02984204  	0.07963873  	0.06602991  
2023-05-31 11:58:06.786: [iter 56 : loss : 0.5342 = 0.0714 + 0.4550 + 0.0078, time: 21.314194]
2023-05-31 11:58:07.047: epoch 56:	0.02979761  	0.07935527  	0.06592564  
2023-05-31 11:58:28.184: [iter 57 : loss : 0.5323 = 0.0695 + 0.4549 + 0.0079, time: 21.133182]
2023-05-31 11:58:28.443: epoch 57:	0.02959033  	0.07875625  	0.06550597  
2023-05-31 11:58:49.586: [iter 58 : loss : 0.5311 = 0.0685 + 0.4546 + 0.0080, time: 21.137992]
2023-05-31 11:58:49.852: epoch 58:	0.02959032  	0.07882418  	0.06554785  
2023-05-31 11:59:10.750: [iter 59 : loss : 0.5292 = 0.0666 + 0.4545 + 0.0081, time: 20.894049]
2023-05-31 11:59:11.009: epoch 59:	0.02951628  	0.07862851  	0.06540731  
2023-05-31 11:59:32.168: [iter 60 : loss : 0.5284 = 0.0658 + 0.4544 + 0.0083, time: 21.154034]
2023-05-31 11:59:32.428: epoch 60:	0.02951629  	0.07860326  	0.06530264  
2023-05-31 11:59:53.534: [iter 61 : loss : 0.5267 = 0.0642 + 0.4542 + 0.0084, time: 21.102992]
2023-05-31 11:59:53.807: epoch 61:	0.02940524  	0.07787247  	0.06498218  
2023-05-31 12:00:14.788: [iter 62 : loss : 0.5257 = 0.0632 + 0.4540 + 0.0085, time: 20.977164]
2023-05-31 12:00:15.047: epoch 62:	0.02939784  	0.07772923  	0.06493825  
2023-05-31 12:00:36.129: [iter 63 : loss : 0.5246 = 0.0621 + 0.4539 + 0.0086, time: 21.078191]
2023-05-31 12:00:36.388: epoch 63:	0.02932382  	0.07770992  	0.06488162  
2023-05-31 12:00:36.388: Early stopping is trigger at epoch: 63
2023-05-31 12:00:36.388: best_result@epoch 38:

2023-05-31 12:00:36.388: 		0.0301      	0.0823      	0.0669      
2023-05-31 14:35:40.034: my pid: 8916
2023-05-31 14:35:40.034: model: model.general_recommender.SGL
2023-05-31 14:35:40.034: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 14:35:40.034: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 14:35:44.077: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 14:36:05.225: [iter 1 : loss : 1.1342 = 0.6931 + 0.4412 + 0.0000, time: 21.147336]
2023-05-31 14:36:05.495: epoch 1:	0.00128072  	0.00238771  	0.00214901  
2023-05-31 14:36:05.495: Find a better model.
2023-05-31 14:36:26.620: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.122028]
2023-05-31 14:36:26.909: epoch 2:	0.00125111  	0.00261365  	0.00206717  
2023-05-31 14:36:26.909: Find a better model.
2023-05-31 14:36:48.229: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 21.316194]
2023-05-31 14:36:48.518: epoch 3:	0.00171750  	0.00408689  	0.00288840  
2023-05-31 14:36:48.518: Find a better model.
2023-05-31 14:37:09.424: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.901990]
2023-05-31 14:37:09.727: epoch 4:	0.00191738  	0.00435377  	0.00321155  
2023-05-31 14:37:09.727: Find a better model.
2023-05-31 14:37:30.790: [iter 5 : loss : 1.1343 = 0.6927 + 0.4415 + 0.0000, time: 21.060035]
2023-05-31 14:37:31.078: epoch 5:	0.00248001  	0.00590782  	0.00452354  
2023-05-31 14:37:31.078: Find a better model.
2023-05-31 14:37:52.179: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.096868]
2023-05-31 14:37:52.470: epoch 6:	0.00310187  	0.00718490  	0.00546851  
2023-05-31 14:37:52.470: Find a better model.
2023-05-31 14:38:13.599: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.124396]
2023-05-31 14:38:13.894: epoch 7:	0.00364969  	0.00859733  	0.00683895  
2023-05-31 14:38:13.894: Find a better model.
2023-05-31 14:38:34.996: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 21.099480]
2023-05-31 14:38:35.283: epoch 8:	0.00436038  	0.01138283  	0.00898123  
2023-05-31 14:38:35.283: Find a better model.
2023-05-31 14:38:56.345: [iter 9 : loss : 1.1346 = 0.6917 + 0.4429 + 0.0000, time: 21.058287]
2023-05-31 14:38:56.628: epoch 9:	0.00507846  	0.01413292  	0.01110326  
2023-05-31 14:38:56.628: Find a better model.
2023-05-31 14:39:17.531: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.900384]
2023-05-31 14:39:17.814: epoch 10:	0.00584097  	0.01617777  	0.01273404  
2023-05-31 14:39:17.814: Find a better model.
2023-05-31 14:39:39.513: [iter 11 : loss : 1.1339 = 0.6901 + 0.4438 + 0.0000, time: 21.696155]
2023-05-31 14:39:39.797: epoch 11:	0.00681815  	0.01941040  	0.01484320  
2023-05-31 14:39:39.797: Find a better model.
2023-05-31 14:40:01.301: [iter 12 : loss : 1.1328 = 0.6884 + 0.4444 + 0.0000, time: 21.500723]
2023-05-31 14:40:01.584: epoch 12:	0.00809146  	0.02309757  	0.01799861  
2023-05-31 14:40:01.584: Find a better model.
2023-05-31 14:40:23.136: [iter 13 : loss : 1.1310 = 0.6859 + 0.4451 + 0.0000, time: 21.548375]
2023-05-31 14:40:23.418: epoch 13:	0.01012729  	0.02887463  	0.02357290  
2023-05-31 14:40:23.418: Find a better model.
2023-05-31 14:40:45.104: [iter 14 : loss : 1.1276 = 0.6818 + 0.4458 + 0.0001, time: 21.683115]
2023-05-31 14:40:45.385: epoch 14:	0.01285163  	0.03536681  	0.03004932  
2023-05-31 14:40:45.385: Find a better model.
2023-05-31 14:41:06.879: [iter 15 : loss : 1.1218 = 0.6751 + 0.4467 + 0.0001, time: 21.490288]
2023-05-31 14:41:07.158: epoch 15:	0.01650140  	0.04381556  	0.03805017  
2023-05-31 14:41:07.158: Find a better model.
2023-05-31 14:41:28.713: [iter 16 : loss : 1.1111 = 0.6633 + 0.4476 + 0.0002, time: 21.552084]
2023-05-31 14:41:28.991: epoch 16:	0.01985504  	0.05248029  	0.04568098  
2023-05-31 14:41:28.991: Find a better model.
2023-05-31 14:41:50.487: [iter 17 : loss : 1.0930 = 0.6438 + 0.4490 + 0.0002, time: 21.491580]
2023-05-31 14:41:50.770: epoch 17:	0.02310509  	0.06073512  	0.05301011  
2023-05-31 14:41:50.771: Find a better model.
2023-05-31 14:42:12.285: [iter 18 : loss : 1.0644 = 0.6134 + 0.4507 + 0.0004, time: 21.511657]
2023-05-31 14:42:12.562: epoch 18:	0.02545932  	0.06667186  	0.05779824  
2023-05-31 14:42:12.562: Find a better model.
2023-05-31 14:42:34.091: [iter 19 : loss : 1.0257 = 0.5721 + 0.4530 + 0.0006, time: 21.523221]
2023-05-31 14:42:34.368: epoch 19:	0.02726571  	0.07149423  	0.06142562  
2023-05-31 14:42:34.368: Find a better model.
2023-05-31 14:42:55.855: [iter 20 : loss : 0.9791 = 0.5223 + 0.4560 + 0.0008, time: 21.483787]
2023-05-31 14:42:56.131: epoch 20:	0.02819112  	0.07437913  	0.06362636  
2023-05-31 14:42:56.131: Find a better model.
2023-05-31 14:43:17.464: [iter 21 : loss : 0.9297 = 0.4692 + 0.4594 + 0.0011, time: 21.329713]
2023-05-31 14:43:17.749: epoch 21:	0.02896106  	0.07692184  	0.06448045  
2023-05-31 14:43:17.749: Find a better model.
2023-05-31 14:43:39.036: [iter 22 : loss : 0.8807 = 0.4170 + 0.4623 + 0.0014, time: 21.284346]
2023-05-31 14:43:39.316: epoch 22:	0.02908694  	0.07812581  	0.06535593  
2023-05-31 14:43:39.316: Find a better model.
2023-05-31 14:44:00.816: [iter 23 : loss : 0.8373 = 0.3709 + 0.4647 + 0.0017, time: 21.495447]
2023-05-31 14:44:01.092: epoch 23:	0.02932383  	0.07886447  	0.06590094  
2023-05-31 14:44:01.092: Find a better model.
2023-05-31 14:44:22.433: [iter 24 : loss : 0.7995 = 0.3311 + 0.4664 + 0.0020, time: 21.338351]
2023-05-31 14:44:22.721: epoch 24:	0.02953113  	0.07955070  	0.06626184  
2023-05-31 14:44:22.722: Find a better model.
2023-05-31 14:44:44.053: [iter 25 : loss : 0.7671 = 0.2976 + 0.4672 + 0.0022, time: 21.327455]
2023-05-31 14:44:44.327: epoch 25:	0.02975322  	0.08040502  	0.06667131  
2023-05-31 14:44:44.327: Find a better model.
2023-05-31 14:45:05.813: [iter 26 : loss : 0.7403 = 0.2702 + 0.4676 + 0.0025, time: 21.482267]
2023-05-31 14:45:06.088: epoch 26:	0.02998273  	0.08147418  	0.06692606  
2023-05-31 14:45:06.088: Find a better model.
2023-05-31 14:45:27.618: [iter 27 : loss : 0.7167 = 0.2464 + 0.4675 + 0.0028, time: 21.525495]
2023-05-31 14:45:27.895: epoch 27:	0.03013819  	0.08205295  	0.06726472  
2023-05-31 14:45:27.895: Find a better model.
2023-05-31 14:45:49.409: [iter 28 : loss : 0.6972 = 0.2270 + 0.4672 + 0.0031, time: 21.510927]
2023-05-31 14:45:49.696: epoch 28:	0.03030847  	0.08215789  	0.06744870  
2023-05-31 14:45:49.696: Find a better model.
2023-05-31 14:46:10.775: [iter 29 : loss : 0.6807 = 0.2107 + 0.4668 + 0.0033, time: 21.076846]
2023-05-31 14:46:11.042: epoch 29:	0.03030846  	0.08238185  	0.06758723  
2023-05-31 14:46:11.042: Find a better model.
2023-05-31 14:46:32.353: [iter 30 : loss : 0.6651 = 0.1954 + 0.4661 + 0.0035, time: 21.307916]
2023-05-31 14:46:32.623: epoch 30:	0.03033808  	0.08257104  	0.06769632  
2023-05-31 14:46:32.623: Find a better model.
2023-05-31 14:46:54.003: [iter 31 : loss : 0.6517 = 0.1826 + 0.4654 + 0.0038, time: 21.376090]
2023-05-31 14:46:54.271: epoch 31:	0.03031586  	0.08257204  	0.06778100  
2023-05-31 14:46:54.271: Find a better model.
2023-05-31 14:47:15.566: [iter 32 : loss : 0.6407 = 0.1719 + 0.4648 + 0.0040, time: 21.290198]
2023-05-31 14:47:15.836: epoch 32:	0.03035289  	0.08256245  	0.06791843  
2023-05-31 14:47:37.156: [iter 33 : loss : 0.6309 = 0.1626 + 0.4642 + 0.0042, time: 21.317353]
2023-05-31 14:47:37.420: epoch 33:	0.03032327  	0.08223677  	0.06812498  
2023-05-31 14:47:58.759: [iter 34 : loss : 0.6215 = 0.1537 + 0.4634 + 0.0044, time: 21.335157]
2023-05-31 14:47:59.027: epoch 34:	0.03033808  	0.08219241  	0.06804880  
2023-05-31 14:48:20.372: [iter 35 : loss : 0.6127 = 0.1453 + 0.4628 + 0.0046, time: 21.341031]
2023-05-31 14:48:20.645: epoch 35:	0.03044912  	0.08230121  	0.06813438  
2023-05-31 14:48:42.107: [iter 36 : loss : 0.6054 = 0.1385 + 0.4621 + 0.0048, time: 21.459186]
2023-05-31 14:48:42.376: epoch 36:	0.03041951  	0.08232701  	0.06798540  
2023-05-31 14:49:04.135: [iter 37 : loss : 0.5988 = 0.1323 + 0.4615 + 0.0050, time: 21.754156]
2023-05-31 14:49:04.405: epoch 37:	0.03038990  	0.08215693  	0.06799823  
2023-05-31 14:49:25.727: [iter 38 : loss : 0.5925 = 0.1263 + 0.4610 + 0.0052, time: 21.319625]
2023-05-31 14:49:25.993: epoch 38:	0.03039729  	0.08212365  	0.06798847  
2023-05-31 14:49:47.339: [iter 39 : loss : 0.5873 = 0.1216 + 0.4604 + 0.0053, time: 21.343325]
2023-05-31 14:49:47.606: epoch 39:	0.03046392  	0.08225498  	0.06792028  
2023-05-31 14:50:09.283: [iter 40 : loss : 0.5817 = 0.1163 + 0.4599 + 0.0055, time: 21.672233]
2023-05-31 14:50:09.550: epoch 40:	0.03038249  	0.08180867  	0.06767107  
2023-05-31 14:50:31.527: [iter 41 : loss : 0.5766 = 0.1114 + 0.4595 + 0.0057, time: 21.973678]
2023-05-31 14:50:31.800: epoch 41:	0.03030845  	0.08117925  	0.06740370  
2023-05-31 14:50:53.260: [iter 42 : loss : 0.5723 = 0.1074 + 0.4591 + 0.0058, time: 21.455095]
2023-05-31 14:50:53.526: epoch 42:	0.03024922  	0.08092113  	0.06742714  
2023-05-31 14:51:15.278: [iter 43 : loss : 0.5680 = 0.1033 + 0.4587 + 0.0060, time: 21.749388]
2023-05-31 14:51:15.544: epoch 43:	0.03011597  	0.08066968  	0.06729892  
2023-05-31 14:51:37.080: [iter 44 : loss : 0.5652 = 0.1008 + 0.4583 + 0.0061, time: 21.532034]
2023-05-31 14:51:37.347: epoch 44:	0.03007155  	0.08081236  	0.06735453  
2023-05-31 14:51:59.042: [iter 45 : loss : 0.5612 = 0.0969 + 0.4580 + 0.0063, time: 21.691101]
2023-05-31 14:51:59.305: epoch 45:	0.03003453  	0.08044674  	0.06718377  
2023-05-31 14:52:21.049: [iter 46 : loss : 0.5577 = 0.0937 + 0.4576 + 0.0064, time: 21.740083]
2023-05-31 14:52:21.315: epoch 46:	0.02998272  	0.07979563  	0.06693377  
2023-05-31 14:52:43.022: [iter 47 : loss : 0.5545 = 0.0906 + 0.4573 + 0.0066, time: 21.701608]
2023-05-31 14:52:43.285: epoch 47:	0.02984205  	0.07962816  	0.06676690  
2023-05-31 14:53:05.013: [iter 48 : loss : 0.5517 = 0.0880 + 0.4569 + 0.0067, time: 21.724064]
2023-05-31 14:53:05.279: epoch 48:	0.02998271  	0.07993449  	0.06688391  
2023-05-31 14:53:27.066: [iter 49 : loss : 0.5493 = 0.0858 + 0.4566 + 0.0069, time: 21.783378]
2023-05-31 14:53:27.332: epoch 49:	0.02987907  	0.07953482  	0.06681055  
2023-05-31 14:53:48.836: [iter 50 : loss : 0.5470 = 0.0836 + 0.4563 + 0.0070, time: 21.500140]
2023-05-31 14:53:49.102: epoch 50:	0.02970140  	0.07926400  	0.06664015  
2023-05-31 14:54:10.600: [iter 51 : loss : 0.5443 = 0.0810 + 0.4561 + 0.0072, time: 21.493416]
2023-05-31 14:54:10.873: epoch 51:	0.02976061  	0.07950356  	0.06658357  
2023-05-31 14:54:32.595: [iter 52 : loss : 0.5418 = 0.0787 + 0.4559 + 0.0073, time: 21.719556]
2023-05-31 14:54:32.864: epoch 52:	0.02983465  	0.07953085  	0.06649660  
2023-05-31 14:54:54.211: [iter 53 : loss : 0.5406 = 0.0776 + 0.4556 + 0.0074, time: 21.341329]
2023-05-31 14:54:54.477: epoch 53:	0.02981985  	0.07925323  	0.06654498  
2023-05-31 14:55:16.000: [iter 54 : loss : 0.5383 = 0.0755 + 0.4553 + 0.0075, time: 21.520873]
2023-05-31 14:55:16.267: epoch 54:	0.02975322  	0.07896060  	0.06637207  
2023-05-31 14:55:37.985: [iter 55 : loss : 0.5361 = 0.0733 + 0.4551 + 0.0077, time: 21.714208]
2023-05-31 14:55:38.252: epoch 55:	0.02960515  	0.07840692  	0.06621262  
2023-05-31 14:55:59.592: [iter 56 : loss : 0.5339 = 0.0713 + 0.4549 + 0.0078, time: 21.337300]
2023-05-31 14:55:59.863: epoch 56:	0.02957553  	0.07819767  	0.06604207  
2023-05-31 14:55:59.863: Early stopping is trigger at epoch: 56
2023-05-31 14:55:59.863: best_result@epoch 31:

2023-05-31 14:55:59.863: 		0.0303      	0.0826      	0.0678      
2023-05-31 15:05:23.133: my pid: 13260
2023-05-31 15:05:23.134: model: model.general_recommender.SGL
2023-05-31 15:05:23.134: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 15:05:23.134: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 15:05:27.102: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 15:05:48.004: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.901545]
2023-05-31 15:05:48.271: epoch 1:	0.00113266  	0.00213362  	0.00186242  
2023-05-31 15:05:48.271: Find a better model.
2023-05-31 15:06:09.207: [iter 2 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 20.933095]
2023-05-31 15:06:09.503: epoch 2:	0.00154723  	0.00304111  	0.00247992  
2023-05-31 15:06:09.503: Find a better model.
2023-05-31 15:06:30.787: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.281119]
2023-05-31 15:06:31.079: epoch 3:	0.00181374  	0.00375520  	0.00285481  
2023-05-31 15:06:31.079: Find a better model.
2023-05-31 15:06:52.388: [iter 4 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 21.305105]
2023-05-31 15:06:52.678: epoch 4:	0.00201362  	0.00469538  	0.00384659  
2023-05-31 15:06:52.678: Find a better model.
2023-05-31 15:07:13.963: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.281701]
2023-05-31 15:07:14.258: epoch 5:	0.00263547  	0.00683241  	0.00511113  
2023-05-31 15:07:14.258: Find a better model.
2023-05-31 15:07:35.360: [iter 6 : loss : 1.1343 = 0.6926 + 0.4418 + 0.0000, time: 21.098240]
2023-05-31 15:07:35.648: epoch 6:	0.00278354  	0.00728657  	0.00574621  
2023-05-31 15:07:35.648: Find a better model.
2023-05-31 15:07:56.533: [iter 7 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 20.882147]
2023-05-31 15:07:56.822: epoch 7:	0.00317590  	0.00809054  	0.00654965  
2023-05-31 15:07:56.822: Find a better model.
2023-05-31 15:08:17.975: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 21.148724]
2023-05-31 15:08:18.261: epoch 8:	0.00386438  	0.00981789  	0.00764489  
2023-05-31 15:08:18.261: Find a better model.
2023-05-31 15:08:39.103: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 20.838073]
2023-05-31 15:08:39.396: epoch 9:	0.00468611  	0.01296688  	0.00978497  
2023-05-31 15:08:39.396: Find a better model.
2023-05-31 15:09:00.339: [iter 10 : loss : 1.1344 = 0.6912 + 0.4432 + 0.0000, time: 20.939051]
2023-05-31 15:09:00.607: epoch 10:	0.00573732  	0.01624714  	0.01228981  
2023-05-31 15:09:00.607: Find a better model.
2023-05-31 15:09:22.108: [iter 11 : loss : 1.1340 = 0.6903 + 0.4437 + 0.0000, time: 21.497063]
2023-05-31 15:09:22.397: epoch 11:	0.00703284  	0.02038500  	0.01580170  
2023-05-31 15:09:22.397: Find a better model.
2023-05-31 15:09:43.892: [iter 12 : loss : 1.1331 = 0.6888 + 0.4442 + 0.0000, time: 21.490051]
2023-05-31 15:09:44.171: epoch 12:	0.00801743  	0.02377839  	0.01884044  
2023-05-31 15:09:44.171: Find a better model.
2023-05-31 15:10:05.761: [iter 13 : loss : 1.1313 = 0.6864 + 0.4449 + 0.0000, time: 21.586135]
2023-05-31 15:10:06.043: epoch 13:	0.01016431  	0.02942898  	0.02446290  
2023-05-31 15:10:06.043: Find a better model.
2023-05-31 15:10:27.701: [iter 14 : loss : 1.1281 = 0.6825 + 0.4456 + 0.0001, time: 21.655458]
2023-05-31 15:10:27.977: epoch 14:	0.01271838  	0.03630802  	0.03051856  
2023-05-31 15:10:27.977: Find a better model.
2023-05-31 15:10:49.499: [iter 15 : loss : 1.1226 = 0.6761 + 0.4464 + 0.0001, time: 21.519083]
2023-05-31 15:10:49.775: epoch 15:	0.01627930  	0.04492119  	0.03834815  
2023-05-31 15:10:49.775: Find a better model.
2023-05-31 15:11:11.116: [iter 16 : loss : 1.1125 = 0.6649 + 0.4474 + 0.0001, time: 21.336838]
2023-05-31 15:11:11.402: epoch 16:	0.01978102  	0.05350504  	0.04647322  
2023-05-31 15:11:11.402: Find a better model.
2023-05-31 15:11:32.664: [iter 17 : loss : 1.0953 = 0.6463 + 0.4488 + 0.0002, time: 21.257013]
2023-05-31 15:11:32.941: epoch 17:	0.02307548  	0.06173517  	0.05279037  
2023-05-31 15:11:32.941: Find a better model.
2023-05-31 15:11:54.461: [iter 18 : loss : 1.0675 = 0.6168 + 0.4504 + 0.0004, time: 21.516454]
2023-05-31 15:11:54.739: epoch 18:	0.02531126  	0.06693424  	0.05727451  
2023-05-31 15:11:54.739: Find a better model.
2023-05-31 15:12:16.071: [iter 19 : loss : 1.0297 = 0.5764 + 0.4527 + 0.0005, time: 21.327993]
2023-05-31 15:12:16.364: epoch 19:	0.02702142  	0.07178038  	0.06100136  
2023-05-31 15:12:16.364: Find a better model.
2023-05-31 15:12:37.853: [iter 20 : loss : 0.9835 = 0.5270 + 0.4557 + 0.0008, time: 21.484398]
2023-05-31 15:12:38.129: epoch 20:	0.02821334  	0.07571677  	0.06344529  
2023-05-31 15:12:38.129: Find a better model.
2023-05-31 15:12:59.494: [iter 21 : loss : 0.9339 = 0.4740 + 0.4589 + 0.0010, time: 21.362027]
2023-05-31 15:12:59.772: epoch 21:	0.02865014  	0.07772088  	0.06476027  
2023-05-31 15:12:59.772: Find a better model.
2023-05-31 15:13:21.090: [iter 22 : loss : 0.8848 = 0.4215 + 0.4620 + 0.0013, time: 21.314060]
2023-05-31 15:13:21.376: epoch 22:	0.02896848  	0.07891684  	0.06569176  
2023-05-31 15:13:21.376: Find a better model.
2023-05-31 15:13:42.631: [iter 23 : loss : 0.8409 = 0.3747 + 0.4645 + 0.0016, time: 21.250698]
2023-05-31 15:13:42.901: epoch 23:	0.02927200  	0.08023041  	0.06611838  
2023-05-31 15:13:42.901: Find a better model.
2023-05-31 15:14:04.274: [iter 24 : loss : 0.8024 = 0.3343 + 0.4661 + 0.0019, time: 21.370172]
2023-05-31 15:14:04.547: epoch 24:	0.02942748  	0.08049404  	0.06632135  
2023-05-31 15:14:04.547: Find a better model.
2023-05-31 15:14:25.793: [iter 25 : loss : 0.7694 = 0.3002 + 0.4670 + 0.0022, time: 21.240735]
2023-05-31 15:14:26.065: epoch 25:	0.02967179  	0.08105665  	0.06667440  
2023-05-31 15:14:26.066: Find a better model.
2023-05-31 15:14:47.269: [iter 26 : loss : 0.7422 = 0.2722 + 0.4675 + 0.0025, time: 21.199262]
2023-05-31 15:14:47.540: epoch 26:	0.02964217  	0.08149602  	0.06659702  
2023-05-31 15:14:47.540: Find a better model.
2023-05-31 15:15:08.803: [iter 27 : loss : 0.7185 = 0.2484 + 0.4673 + 0.0028, time: 21.260058]
2023-05-31 15:15:09.072: epoch 27:	0.02973102  	0.08153825  	0.06685047  
2023-05-31 15:15:09.072: Find a better model.
2023-05-31 15:15:30.569: [iter 28 : loss : 0.6990 = 0.2289 + 0.4670 + 0.0030, time: 21.493722]
2023-05-31 15:15:30.838: epoch 28:	0.02982725  	0.08151824  	0.06686646  
2023-05-31 15:15:52.201: [iter 29 : loss : 0.6815 = 0.2119 + 0.4664 + 0.0033, time: 21.360431]
2023-05-31 15:15:52.467: epoch 29:	0.03007897  	0.08264915  	0.06733162  
2023-05-31 15:15:52.468: Find a better model.
2023-05-31 15:16:13.996: [iter 30 : loss : 0.6664 = 0.1969 + 0.4660 + 0.0035, time: 21.524594]
2023-05-31 15:16:14.263: epoch 30:	0.03016780  	0.08283897  	0.06752106  
2023-05-31 15:16:14.263: Find a better model.
2023-05-31 15:16:35.758: [iter 31 : loss : 0.6529 = 0.1839 + 0.4653 + 0.0038, time: 21.490549]
2023-05-31 15:16:36.025: epoch 31:	0.03016039  	0.08260866  	0.06766616  
2023-05-31 15:16:57.412: [iter 32 : loss : 0.6413 = 0.1727 + 0.4646 + 0.0040, time: 21.384032]
2023-05-31 15:16:57.679: epoch 32:	0.03014559  	0.08248105  	0.06765680  
2023-05-31 15:17:18.938: [iter 33 : loss : 0.6317 = 0.1636 + 0.4640 + 0.0042, time: 21.255066]
2023-05-31 15:17:19.209: epoch 33:	0.03024184  	0.08269580  	0.06782193  
2023-05-31 15:17:40.600: [iter 34 : loss : 0.6221 = 0.1545 + 0.4633 + 0.0044, time: 21.386486]
2023-05-31 15:17:40.867: epoch 34:	0.03036029  	0.08287787  	0.06792480  
2023-05-31 15:17:40.867: Find a better model.
2023-05-31 15:18:02.199: [iter 35 : loss : 0.6130 = 0.1459 + 0.4626 + 0.0046, time: 21.329084]
2023-05-31 15:18:02.476: epoch 35:	0.03030107  	0.08303151  	0.06792479  
2023-05-31 15:18:02.476: Find a better model.
2023-05-31 15:18:23.947: [iter 36 : loss : 0.6059 = 0.1392 + 0.4620 + 0.0048, time: 21.467567]
2023-05-31 15:18:24.214: epoch 36:	0.03024924  	0.08263732  	0.06773396  
2023-05-31 15:18:45.751: [iter 37 : loss : 0.5995 = 0.1331 + 0.4615 + 0.0050, time: 21.534048]
2023-05-31 15:18:46.021: epoch 37:	0.03035288  	0.08263955  	0.06772111  
2023-05-31 15:19:07.638: [iter 38 : loss : 0.5927 = 0.1268 + 0.4608 + 0.0051, time: 21.614553]
2023-05-31 15:19:07.906: epoch 38:	0.03041210  	0.08273400  	0.06776581  
2023-05-31 15:19:29.498: [iter 39 : loss : 0.5876 = 0.1219 + 0.4604 + 0.0053, time: 21.587578]
2023-05-31 15:19:29.764: epoch 39:	0.03041950  	0.08233234  	0.06772243  
2023-05-31 15:19:51.311: [iter 40 : loss : 0.5821 = 0.1167 + 0.4598 + 0.0055, time: 21.543287]
2023-05-31 15:19:51.575: epoch 40:	0.03037509  	0.08259204  	0.06785370  
2023-05-31 15:20:13.273: [iter 41 : loss : 0.5769 = 0.1118 + 0.4594 + 0.0057, time: 21.695181]
2023-05-31 15:20:13.541: epoch 41:	0.03026404  	0.08192209  	0.06757048  
2023-05-31 15:20:35.091: [iter 42 : loss : 0.5728 = 0.1080 + 0.4590 + 0.0058, time: 21.546896]
2023-05-31 15:20:35.367: epoch 42:	0.03033067  	0.08218043  	0.06761368  
2023-05-31 15:20:57.075: [iter 43 : loss : 0.5683 = 0.1038 + 0.4586 + 0.0060, time: 21.704327]
2023-05-31 15:20:57.357: epoch 43:	0.03030846  	0.08160788  	0.06744844  
2023-05-31 15:21:19.093: [iter 44 : loss : 0.5653 = 0.1011 + 0.4581 + 0.0061, time: 21.732123]
2023-05-31 15:21:19.369: epoch 44:	0.03021222  	0.08146775  	0.06734965  
2023-05-31 15:21:41.089: [iter 45 : loss : 0.5613 = 0.0973 + 0.4577 + 0.0063, time: 21.717370]
2023-05-31 15:21:41.370: epoch 45:	0.03019741  	0.08174881  	0.06717603  
2023-05-31 15:22:03.102: [iter 46 : loss : 0.5579 = 0.0940 + 0.4575 + 0.0064, time: 21.727353]
2023-05-31 15:22:03.382: epoch 46:	0.03010858  	0.08167296  	0.06716111  
2023-05-31 15:22:25.064: [iter 47 : loss : 0.5544 = 0.0907 + 0.4571 + 0.0066, time: 21.679542]
2023-05-31 15:22:25.342: epoch 47:	0.03017521  	0.08140831  	0.06704347  
2023-05-31 15:22:47.060: [iter 48 : loss : 0.5521 = 0.0886 + 0.4568 + 0.0067, time: 21.712720]
2023-05-31 15:22:47.334: epoch 48:	0.03014559  	0.08124381  	0.06691571  
2023-05-31 15:23:09.059: [iter 49 : loss : 0.5489 = 0.0856 + 0.4564 + 0.0069, time: 21.717926]
2023-05-31 15:23:09.330: epoch 49:	0.03006415  	0.08095725  	0.06687192  
2023-05-31 15:23:31.018: [iter 50 : loss : 0.5472 = 0.0839 + 0.4562 + 0.0070, time: 21.677783]
2023-05-31 15:23:31.284: epoch 50:	0.03004194  	0.08077515  	0.06666325  
2023-05-31 15:23:53.034: [iter 51 : loss : 0.5440 = 0.0810 + 0.4559 + 0.0071, time: 21.746294]
2023-05-31 15:23:53.296: epoch 51:	0.03001233  	0.08029477  	0.06657434  
2023-05-31 15:24:15.216: [iter 52 : loss : 0.5416 = 0.0786 + 0.4557 + 0.0073, time: 21.916518]
2023-05-31 15:24:15.487: epoch 52:	0.02997530  	0.07989293  	0.06642085  
2023-05-31 15:24:37.231: [iter 53 : loss : 0.5406 = 0.0777 + 0.4554 + 0.0074, time: 21.740997]
2023-05-31 15:24:37.506: epoch 53:	0.02999011  	0.07982992  	0.06639597  
2023-05-31 15:24:59.215: [iter 54 : loss : 0.5384 = 0.0757 + 0.4552 + 0.0075, time: 21.703663]
2023-05-31 15:24:59.483: epoch 54:	0.02988646  	0.07942842  	0.06619838  
2023-05-31 15:25:21.186: [iter 55 : loss : 0.5362 = 0.0735 + 0.4550 + 0.0077, time: 21.700050]
2023-05-31 15:25:21.459: epoch 55:	0.02976061  	0.07936927  	0.06621303  
2023-05-31 15:25:42.969: [iter 56 : loss : 0.5339 = 0.0714 + 0.4547 + 0.0078, time: 21.505132]
2023-05-31 15:25:43.231: epoch 56:	0.02973839  	0.07929985  	0.06614716  
2023-05-31 15:26:04.987: [iter 57 : loss : 0.5321 = 0.0696 + 0.4546 + 0.0079, time: 21.751037]
2023-05-31 15:26:05.252: epoch 57:	0.02963476  	0.07898573  	0.06585113  
2023-05-31 15:26:26.969: [iter 58 : loss : 0.5309 = 0.0685 + 0.4543 + 0.0080, time: 21.713007]
2023-05-31 15:26:27.234: epoch 58:	0.02961995  	0.07911581  	0.06589302  
2023-05-31 15:26:48.955: [iter 59 : loss : 0.5289 = 0.0666 + 0.4542 + 0.0081, time: 21.716152]
2023-05-31 15:26:49.219: epoch 59:	0.02956072  	0.07870142  	0.06568490  
2023-05-31 15:27:11.956: [iter 60 : loss : 0.5279 = 0.0656 + 0.4540 + 0.0082, time: 22.732814]
2023-05-31 15:27:12.260: epoch 60:	0.02949409  	0.07856651  	0.06567976  
2023-05-31 15:27:12.261: Early stopping is trigger at epoch: 60
2023-05-31 15:27:12.261: best_result@epoch 35:

2023-05-31 15:27:12.261: 		0.0303      	0.0830      	0.0679      
2023-05-31 15:27:47.536: my pid: 7448
2023-05-31 15:27:47.537: model: model.general_recommender.SGL
2023-05-31 15:27:47.537: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 15:27:47.537: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 15:27:51.636: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 15:28:13.201: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.564983]
2023-05-31 15:28:13.488: epoch 1:	0.00116968  	0.00211116  	0.00186115  
2023-05-31 15:28:13.488: Find a better model.
2023-05-31 15:28:35.193: [iter 2 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 21.700727]
2023-05-31 15:28:35.494: epoch 2:	0.00148801  	0.00284876  	0.00223544  
2023-05-31 15:28:35.494: Find a better model.
2023-05-31 15:28:57.148: [iter 3 : loss : 1.1338 = 0.6929 + 0.4408 + 0.0000, time: 21.650130]
2023-05-31 15:28:57.456: epoch 3:	0.00161386  	0.00351824  	0.00296637  
2023-05-31 15:28:57.456: Find a better model.
2023-05-31 15:29:18.984: [iter 4 : loss : 1.1339 = 0.6928 + 0.4411 + 0.0000, time: 21.525214]
2023-05-31 15:29:19.283: epoch 4:	0.00193959  	0.00443980  	0.00338408  
2023-05-31 15:29:19.283: Find a better model.
2023-05-31 15:29:40.745: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.457680]
2023-05-31 15:29:41.040: epoch 5:	0.00204324  	0.00479836  	0.00364902  
2023-05-31 15:29:41.040: Find a better model.
2023-05-31 15:30:02.397: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.353438]
2023-05-31 15:30:02.694: epoch 6:	0.00259106  	0.00576945  	0.00457500  
2023-05-31 15:30:02.694: Find a better model.
2023-05-31 15:30:24.355: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 21.657426]
2023-05-31 15:30:24.651: epoch 7:	0.00313888  	0.00706951  	0.00575526  
2023-05-31 15:30:24.652: Find a better model.
2023-05-31 15:30:46.195: [iter 8 : loss : 1.1344 = 0.6921 + 0.4423 + 0.0000, time: 21.539216]
2023-05-31 15:30:46.494: epoch 8:	0.00391620  	0.00959060  	0.00750814  
2023-05-31 15:30:46.494: Find a better model.
2023-05-31 15:31:08.104: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 21.605280]
2023-05-31 15:31:08.407: epoch 9:	0.00469351  	0.01158156  	0.00943621  
2023-05-31 15:31:08.407: Find a better model.
2023-05-31 15:31:29.902: [iter 10 : loss : 1.1343 = 0.6912 + 0.4430 + 0.0000, time: 21.491730]
2023-05-31 15:31:30.191: epoch 10:	0.00593721  	0.01594747  	0.01249987  
2023-05-31 15:31:30.192: Find a better model.
2023-05-31 15:31:52.303: [iter 11 : loss : 1.1340 = 0.6904 + 0.4435 + 0.0000, time: 22.108452]
2023-05-31 15:31:52.587: epoch 11:	0.00743260  	0.02128467  	0.01628592  
2023-05-31 15:31:52.587: Find a better model.
2023-05-31 15:32:14.489: [iter 12 : loss : 1.1332 = 0.6891 + 0.4440 + 0.0000, time: 21.899160]
2023-05-31 15:32:14.775: epoch 12:	0.00862447  	0.02515171  	0.01978617  
2023-05-31 15:32:14.775: Find a better model.
2023-05-31 15:32:36.848: [iter 13 : loss : 1.1314 = 0.6869 + 0.4445 + 0.0000, time: 22.069091]
2023-05-31 15:32:37.131: epoch 13:	0.01011989  	0.02855748  	0.02384804  
2023-05-31 15:32:37.131: Find a better model.
2023-05-31 15:32:59.063: [iter 14 : loss : 1.1285 = 0.6831 + 0.4454 + 0.0001, time: 21.928086]
2023-05-31 15:32:59.345: epoch 14:	0.01268876  	0.03613390  	0.03033272  
2023-05-31 15:32:59.345: Find a better model.
2023-05-31 15:33:21.260: [iter 15 : loss : 1.1232 = 0.6769 + 0.4462 + 0.0001, time: 21.910033]
2023-05-31 15:33:21.542: epoch 15:	0.01601278  	0.04390473  	0.03740834  
2023-05-31 15:33:21.542: Find a better model.
2023-05-31 15:33:43.288: [iter 16 : loss : 1.1136 = 0.6662 + 0.4472 + 0.0001, time: 21.740046]
2023-05-31 15:33:43.572: epoch 16:	0.01983285  	0.05413976  	0.04505742  
2023-05-31 15:33:43.572: Find a better model.
2023-05-31 15:34:05.282: [iter 17 : loss : 1.0970 = 0.6484 + 0.4484 + 0.0002, time: 21.705601]
2023-05-31 15:34:05.567: epoch 17:	0.02280156  	0.06123780  	0.05136466  
2023-05-31 15:34:05.567: Find a better model.
2023-05-31 15:34:27.269: [iter 18 : loss : 1.0703 = 0.6199 + 0.4501 + 0.0003, time: 21.698035]
2023-05-31 15:34:27.546: epoch 18:	0.02493369  	0.06696434  	0.05625791  
2023-05-31 15:34:27.547: Find a better model.
2023-05-31 15:34:49.262: [iter 19 : loss : 1.0331 = 0.5802 + 0.4524 + 0.0005, time: 21.711097]
2023-05-31 15:34:49.547: epoch 19:	0.02671047  	0.07186594  	0.05965579  
2023-05-31 15:34:49.547: Find a better model.
2023-05-31 15:35:11.413: [iter 20 : loss : 0.9872 = 0.5312 + 0.4553 + 0.0007, time: 21.860975]
2023-05-31 15:35:11.693: epoch 20:	0.02805787  	0.07536855  	0.06231995  
2023-05-31 15:35:11.693: Find a better model.
2023-05-31 15:35:33.421: [iter 21 : loss : 0.9378 = 0.4781 + 0.4588 + 0.0010, time: 21.723349]
2023-05-31 15:35:33.701: epoch 21:	0.02858351  	0.07709213  	0.06345257  
2023-05-31 15:35:33.701: Find a better model.
2023-05-31 15:35:55.409: [iter 22 : loss : 0.8879 = 0.4248 + 0.4618 + 0.0013, time: 21.703783]
2023-05-31 15:35:55.692: epoch 22:	0.02907212  	0.07769211  	0.06422453  
2023-05-31 15:35:55.692: Find a better model.
2023-05-31 15:36:17.365: [iter 23 : loss : 0.8434 = 0.3775 + 0.4643 + 0.0016, time: 21.669347]
2023-05-31 15:36:17.646: epoch 23:	0.02912395  	0.07853564  	0.06442342  
2023-05-31 15:36:17.646: Find a better model.
2023-05-31 15:36:39.400: [iter 24 : loss : 0.8047 = 0.3367 + 0.4661 + 0.0019, time: 21.750475]
2023-05-31 15:36:39.679: epoch 24:	0.02913135  	0.07924943  	0.06489509  
2023-05-31 15:36:39.679: Find a better model.
2023-05-31 15:37:01.195: [iter 25 : loss : 0.7718 = 0.3024 + 0.4671 + 0.0022, time: 21.511142]
2023-05-31 15:37:01.475: epoch 25:	0.02933123  	0.07956409  	0.06517693  
2023-05-31 15:37:01.475: Find a better model.
2023-05-31 15:37:23.185: [iter 26 : loss : 0.7438 = 0.2739 + 0.4674 + 0.0025, time: 21.705127]
2023-05-31 15:37:23.465: epoch 26:	0.02958294  	0.08043754  	0.06550644  
2023-05-31 15:37:23.466: Find a better model.
2023-05-31 15:37:45.359: [iter 27 : loss : 0.7199 = 0.2499 + 0.4672 + 0.0028, time: 21.888602]
2023-05-31 15:37:45.635: epoch 27:	0.02959035  	0.08055250  	0.06551664  
2023-05-31 15:37:45.635: Find a better model.
2023-05-31 15:38:07.352: [iter 28 : loss : 0.6999 = 0.2298 + 0.4671 + 0.0030, time: 21.711640]
2023-05-31 15:38:07.625: epoch 28:	0.02967919  	0.08090664  	0.06565669  
2023-05-31 15:38:07.625: Find a better model.
2023-05-31 15:38:29.348: [iter 29 : loss : 0.6825 = 0.2130 + 0.4663 + 0.0033, time: 21.718176]
2023-05-31 15:38:29.625: epoch 29:	0.02976804  	0.08105534  	0.06592025  
2023-05-31 15:38:29.625: Find a better model.
2023-05-31 15:38:51.357: [iter 30 : loss : 0.6670 = 0.1978 + 0.4657 + 0.0035, time: 21.729067]
2023-05-31 15:38:51.636: epoch 30:	0.02983466  	0.08156365  	0.06615473  
2023-05-31 15:38:51.636: Find a better model.
2023-05-31 15:39:13.521: [iter 31 : loss : 0.6537 = 0.1849 + 0.4651 + 0.0037, time: 21.882039]
2023-05-31 15:39:13.792: epoch 31:	0.02993090  	0.08180989  	0.06616673  
2023-05-31 15:39:13.792: Find a better model.
2023-05-31 15:39:35.909: [iter 32 : loss : 0.6421 = 0.1737 + 0.4645 + 0.0040, time: 22.112054]
2023-05-31 15:39:36.183: epoch 32:	0.02993831  	0.08156805  	0.06614456  
2023-05-31 15:39:58.127: [iter 33 : loss : 0.6322 = 0.1643 + 0.4638 + 0.0042, time: 21.940987]
2023-05-31 15:39:58.413: epoch 33:	0.03008638  	0.08175888  	0.06642324  
2023-05-31 15:40:20.321: [iter 34 : loss : 0.6225 = 0.1550 + 0.4631 + 0.0044, time: 21.904545]
2023-05-31 15:40:20.596: epoch 34:	0.02997533  	0.08162223  	0.06641452  
2023-05-31 15:40:42.503: [iter 35 : loss : 0.6135 = 0.1466 + 0.4623 + 0.0046, time: 21.903341]
2023-05-31 15:40:42.772: epoch 35:	0.02998273  	0.08164462  	0.06657670  
2023-05-31 15:41:04.913: [iter 36 : loss : 0.6065 = 0.1399 + 0.4619 + 0.0048, time: 22.135074]
2023-05-31 15:41:05.183: epoch 36:	0.03007896  	0.08169279  	0.06664776  
2023-05-31 15:41:27.488: [iter 37 : loss : 0.5997 = 0.1335 + 0.4613 + 0.0049, time: 22.301907]
2023-05-31 15:41:27.761: epoch 37:	0.02998273  	0.08151101  	0.06665675  
2023-05-31 15:41:50.096: [iter 38 : loss : 0.5933 = 0.1275 + 0.4607 + 0.0051, time: 22.331216]
2023-05-31 15:41:50.383: epoch 38:	0.02999013  	0.08185467  	0.06664820  
2023-05-31 15:41:50.383: Find a better model.
2023-05-31 15:42:12.513: [iter 39 : loss : 0.5880 = 0.1225 + 0.4602 + 0.0053, time: 22.123633]
2023-05-31 15:42:12.782: epoch 39:	0.02999753  	0.08170634  	0.06677823  
2023-05-31 15:42:34.847: [iter 40 : loss : 0.5823 = 0.1171 + 0.4597 + 0.0055, time: 22.061074]
2023-05-31 15:42:35.119: epoch 40:	0.02990869  	0.08145978  	0.06668565  
2023-05-31 15:42:57.256: [iter 41 : loss : 0.5771 = 0.1123 + 0.4592 + 0.0056, time: 22.132070]
2023-05-31 15:42:57.528: epoch 41:	0.02993090  	0.08130969  	0.06676593  
2023-05-31 15:43:19.643: [iter 42 : loss : 0.5726 = 0.1081 + 0.4588 + 0.0058, time: 22.110444]
2023-05-31 15:43:19.915: epoch 42:	0.03006415  	0.08188877  	0.06687858  
2023-05-31 15:43:19.915: Find a better model.
2023-05-31 15:43:42.158: [iter 43 : loss : 0.5684 = 0.1041 + 0.4583 + 0.0060, time: 22.239058]
2023-05-31 15:43:42.446: epoch 43:	0.02995310  	0.08135340  	0.06676102  
2023-05-31 15:44:04.874: [iter 44 : loss : 0.5654 = 0.1013 + 0.4580 + 0.0061, time: 22.424089]
2023-05-31 15:44:05.144: epoch 44:	0.02993089  	0.08146191  	0.06684450  
2023-05-31 15:44:27.206: [iter 45 : loss : 0.5612 = 0.0974 + 0.4576 + 0.0063, time: 22.057910]
2023-05-31 15:44:27.480: epoch 45:	0.02987168  	0.08135653  	0.06683798  
2023-05-31 15:44:49.409: [iter 46 : loss : 0.5582 = 0.0946 + 0.4571 + 0.0064, time: 21.924626]
2023-05-31 15:44:49.679: epoch 46:	0.02979763  	0.08121547  	0.06663779  
2023-05-31 15:45:11.791: [iter 47 : loss : 0.5551 = 0.0916 + 0.4569 + 0.0066, time: 22.108951]
2023-05-31 15:45:12.062: epoch 47:	0.02986426  	0.08126058  	0.06670782  
2023-05-31 15:45:34.211: [iter 48 : loss : 0.5520 = 0.0888 + 0.4566 + 0.0067, time: 22.145278]
2023-05-31 15:45:34.489: epoch 48:	0.02979763  	0.08068258  	0.06645935  
2023-05-31 15:45:56.191: [iter 49 : loss : 0.5491 = 0.0860 + 0.4562 + 0.0069, time: 21.698069]
2023-05-31 15:45:56.464: epoch 49:	0.02976802  	0.08088946  	0.06639975  
2023-05-31 15:46:18.375: [iter 50 : loss : 0.5470 = 0.0839 + 0.4561 + 0.0070, time: 21.907130]
2023-05-31 15:46:18.649: epoch 50:	0.02973100  	0.08098546  	0.06626345  
2023-05-31 15:46:40.765: [iter 51 : loss : 0.5445 = 0.0816 + 0.4557 + 0.0071, time: 22.111731]
2023-05-31 15:46:41.037: epoch 51:	0.02963476  	0.08057418  	0.06617045  
2023-05-31 15:47:02.992: [iter 52 : loss : 0.5412 = 0.0786 + 0.4553 + 0.0073, time: 21.951051]
2023-05-31 15:47:03.264: epoch 52:	0.02953111  	0.08057510  	0.06599712  
2023-05-31 15:47:25.320: [iter 53 : loss : 0.5405 = 0.0779 + 0.4552 + 0.0074, time: 22.051076]
2023-05-31 15:47:25.599: epoch 53:	0.02956072  	0.08013687  	0.06583481  
2023-05-31 15:47:48.061: [iter 54 : loss : 0.5381 = 0.0757 + 0.4549 + 0.0075, time: 22.458352]
2023-05-31 15:47:48.341: epoch 54:	0.02949410  	0.08002032  	0.06575435  
2023-05-31 15:48:11.224: [iter 55 : loss : 0.5361 = 0.0737 + 0.4547 + 0.0076, time: 22.877941]
2023-05-31 15:48:11.514: epoch 55:	0.02939044  	0.07964580  	0.06553362  
2023-05-31 15:48:23.852: my pid: 14620
2023-05-31 15:48:23.852: model: model.general_recommender.SGL
2023-05-31 15:48:23.852: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 15:48:23.852: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 15:48:28.254: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 15:48:50.777: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.520298]
2023-05-31 15:48:51.062: epoch 1:	0.00107344  	0.00233274  	0.00181751  
2023-05-31 15:48:51.062: Find a better model.
2023-05-31 15:49:13.373: [iter 2 : loss : 1.1336 = 0.6930 + 0.4405 + 0.0000, time: 22.307776]
2023-05-31 15:49:13.680: epoch 2:	0.00117708  	0.00273357  	0.00203754  
2023-05-31 15:49:13.680: Find a better model.
2023-05-31 15:49:35.562: [iter 3 : loss : 1.1337 = 0.6929 + 0.4408 + 0.0000, time: 21.878110]
2023-05-31 15:49:35.859: epoch 3:	0.00135475  	0.00258493  	0.00210086  
2023-05-31 15:49:57.965: [iter 4 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 22.101789]
2023-05-31 15:49:58.258: epoch 4:	0.00183595  	0.00404673  	0.00321487  
2023-05-31 15:49:58.258: Find a better model.
2023-05-31 15:50:20.179: [iter 5 : loss : 1.1340 = 0.6927 + 0.4413 + 0.0000, time: 21.916362]
2023-05-31 15:50:20.478: epoch 5:	0.00214688  	0.00488018  	0.00369264  
2023-05-31 15:50:20.478: Find a better model.
2023-05-31 15:50:42.140: [iter 6 : loss : 1.1342 = 0.6926 + 0.4416 + 0.0000, time: 21.656587]
2023-05-31 15:50:42.442: epoch 6:	0.00266509  	0.00628097  	0.00460321  
2023-05-31 15:50:42.443: Find a better model.
2023-05-31 15:51:04.344: [iter 7 : loss : 1.1343 = 0.6924 + 0.4419 + 0.0000, time: 21.898533]
2023-05-31 15:51:04.637: epoch 7:	0.00322772  	0.00712372  	0.00573672  
2023-05-31 15:51:04.637: Find a better model.
2023-05-31 15:51:26.334: [iter 8 : loss : 1.1344 = 0.6922 + 0.4422 + 0.0000, time: 21.692569]
2023-05-31 15:51:26.629: epoch 8:	0.00387918  	0.00902703  	0.00734470  
2023-05-31 15:51:26.629: Find a better model.
2023-05-31 15:51:48.333: [iter 9 : loss : 1.1344 = 0.6919 + 0.4425 + 0.0000, time: 21.700340]
2023-05-31 15:51:48.615: epoch 9:	0.00453064  	0.01140852  	0.00886916  
2023-05-31 15:51:48.615: Find a better model.
2023-05-31 15:52:10.329: [iter 10 : loss : 1.1343 = 0.6914 + 0.4429 + 0.0000, time: 21.711209]
2023-05-31 15:52:10.623: epoch 10:	0.00589279  	0.01510043  	0.01213616  
2023-05-31 15:52:10.623: Find a better model.
2023-05-31 15:52:33.129: [iter 11 : loss : 1.1339 = 0.6907 + 0.4432 + 0.0000, time: 22.502189]
2023-05-31 15:52:33.432: epoch 11:	0.00758066  	0.02055426  	0.01710617  
2023-05-31 15:52:33.432: Find a better model.
2023-05-31 15:52:55.918: [iter 12 : loss : 1.1333 = 0.6896 + 0.4437 + 0.0000, time: 22.482373]
2023-05-31 15:52:56.209: epoch 12:	0.00915748  	0.02521113  	0.02124023  
2023-05-31 15:52:56.209: Find a better model.
2023-05-31 15:53:18.737: [iter 13 : loss : 1.1321 = 0.6878 + 0.4443 + 0.0000, time: 22.524598]
2023-05-31 15:53:19.024: epoch 13:	0.01118594  	0.03101099  	0.02601547  
2023-05-31 15:53:19.024: Find a better model.
2023-05-31 15:53:41.491: [iter 14 : loss : 1.1295 = 0.6845 + 0.4449 + 0.0001, time: 22.462097]
2023-05-31 15:53:41.778: epoch 14:	0.01319958  	0.03624231  	0.03112323  
2023-05-31 15:53:41.778: Find a better model.
2023-05-31 15:54:04.084: [iter 15 : loss : 1.1248 = 0.6789 + 0.4458 + 0.0001, time: 22.303669]
2023-05-31 15:54:04.385: epoch 15:	0.01588695  	0.04298055  	0.03720170  
2023-05-31 15:54:04.386: Find a better model.
2023-05-31 15:54:26.683: [iter 16 : loss : 1.1162 = 0.6694 + 0.4467 + 0.0001, time: 22.291623]
2023-05-31 15:54:26.965: epoch 16:	0.01872235  	0.05015664  	0.04348698  
2023-05-31 15:54:26.965: Find a better model.
2023-05-31 15:54:49.096: [iter 17 : loss : 1.1015 = 0.6534 + 0.4479 + 0.0002, time: 22.127882]
2023-05-31 15:54:49.394: epoch 17:	0.02206124  	0.05769537  	0.05011910  
2023-05-31 15:54:49.394: Find a better model.
2023-05-31 15:55:11.479: [iter 18 : loss : 1.0771 = 0.6272 + 0.4495 + 0.0003, time: 22.081458]
2023-05-31 15:55:11.762: epoch 18:	0.02474121  	0.06491593  	0.05539053  
2023-05-31 15:55:11.762: Find a better model.
2023-05-31 15:55:34.046: [iter 19 : loss : 1.0420 = 0.5899 + 0.4516 + 0.0005, time: 22.279831]
2023-05-31 15:55:34.330: epoch 19:	0.02642915  	0.06938185  	0.05924189  
2023-05-31 15:55:34.330: Find a better model.
2023-05-31 15:55:56.478: [iter 20 : loss : 0.9978 = 0.5425 + 0.4545 + 0.0007, time: 22.144059]
2023-05-31 15:55:56.760: epoch 20:	0.02756185  	0.07289799  	0.06144794  
2023-05-31 15:55:56.760: Find a better model.
2023-05-31 15:56:19.044: [iter 21 : loss : 0.9483 = 0.4897 + 0.4577 + 0.0010, time: 22.280411]
2023-05-31 15:56:19.326: epoch 21:	0.02796163  	0.07399721  	0.06242413  
2023-05-31 15:56:19.326: Find a better model.
2023-05-31 15:56:41.611: [iter 22 : loss : 0.8982 = 0.4360 + 0.4610 + 0.0012, time: 22.281073]
2023-05-31 15:56:41.897: epoch 22:	0.02871676  	0.07608690  	0.06340830  
2023-05-31 15:56:41.897: Find a better model.
2023-05-31 15:57:04.041: [iter 23 : loss : 0.8526 = 0.3875 + 0.4636 + 0.0015, time: 22.141231]
2023-05-31 15:57:04.324: epoch 23:	0.02897588  	0.07685383  	0.06378593  
2023-05-31 15:57:04.324: Find a better model.
2023-05-31 15:57:26.407: [iter 24 : loss : 0.8128 = 0.3452 + 0.4657 + 0.0018, time: 22.079046]
2023-05-31 15:57:26.679: epoch 24:	0.02902030  	0.07709775  	0.06409363  
2023-05-31 15:57:26.679: Find a better model.
2023-05-31 15:57:48.794: [iter 25 : loss : 0.7781 = 0.3093 + 0.4666 + 0.0021, time: 22.111069]
2023-05-31 15:57:49.068: epoch 25:	0.02914616  	0.07772816  	0.06428643  
2023-05-31 15:57:49.068: Find a better model.
2023-05-31 15:58:11.192: [iter 26 : loss : 0.7496 = 0.2800 + 0.4672 + 0.0024, time: 22.119979]
2023-05-31 15:58:11.472: epoch 26:	0.02933864  	0.07840440  	0.06475317  
2023-05-31 15:58:11.472: Find a better model.
2023-05-31 15:58:33.794: [iter 27 : loss : 0.7249 = 0.2550 + 0.4672 + 0.0027, time: 22.317452]
2023-05-31 15:58:34.064: epoch 27:	0.02958295  	0.07906529  	0.06521544  
2023-05-31 15:58:34.064: Find a better model.
2023-05-31 15:58:56.367: [iter 28 : loss : 0.7042 = 0.2343 + 0.4669 + 0.0030, time: 22.298051]
2023-05-31 15:58:56.639: epoch 28:	0.02954593  	0.07938486  	0.06552656  
2023-05-31 15:58:56.639: Find a better model.
2023-05-31 15:59:18.565: [iter 29 : loss : 0.6859 = 0.2165 + 0.4662 + 0.0032, time: 21.922044]
2023-05-31 15:59:18.835: epoch 29:	0.02968660  	0.07966315  	0.06579909  
2023-05-31 15:59:18.835: Find a better model.
2023-05-31 15:59:40.995: [iter 30 : loss : 0.6698 = 0.2007 + 0.4656 + 0.0035, time: 22.155054]
2023-05-31 15:59:41.267: epoch 30:	0.02961257  	0.07968881  	0.06555507  
2023-05-31 15:59:41.267: Find a better model.
2023-05-31 16:00:03.557: [iter 31 : loss : 0.6561 = 0.1874 + 0.4650 + 0.0037, time: 22.287235]
2023-05-31 16:00:03.829: epoch 31:	0.02970139  	0.07991097  	0.06563473  
2023-05-31 16:00:03.829: Find a better model.
2023-05-31 16:00:26.153: [iter 32 : loss : 0.6443 = 0.1760 + 0.4644 + 0.0039, time: 22.319047]
2023-05-31 16:00:26.432: epoch 32:	0.02973841  	0.07977253  	0.06568598  
2023-05-31 16:00:48.768: [iter 33 : loss : 0.6342 = 0.1664 + 0.4637 + 0.0041, time: 22.331378]
2023-05-31 16:00:49.038: epoch 33:	0.02965698  	0.07973429  	0.06561601  
2023-05-31 16:01:11.533: [iter 34 : loss : 0.6244 = 0.1571 + 0.4629 + 0.0043, time: 22.491432]
2023-05-31 16:01:11.804: epoch 34:	0.02965698  	0.07976889  	0.06570152  
2023-05-31 16:01:34.124: [iter 35 : loss : 0.6151 = 0.1483 + 0.4623 + 0.0045, time: 22.315094]
2023-05-31 16:01:34.408: epoch 35:	0.02971620  	0.07988968  	0.06579125  
2023-05-31 16:01:56.745: [iter 36 : loss : 0.6075 = 0.1411 + 0.4617 + 0.0047, time: 22.331924]
2023-05-31 16:01:57.019: epoch 36:	0.02970881  	0.08020929  	0.06591509  
2023-05-31 16:01:57.019: Find a better model.
2023-05-31 16:02:19.510: [iter 37 : loss : 0.6010 = 0.1350 + 0.4611 + 0.0049, time: 22.487369]
2023-05-31 16:02:19.784: epoch 37:	0.02989389  	0.08023077  	0.06604583  
2023-05-31 16:02:19.784: Find a better model.
2023-05-31 16:02:42.313: [iter 38 : loss : 0.5943 = 0.1287 + 0.4605 + 0.0051, time: 22.523994]
2023-05-31 16:02:42.589: epoch 38:	0.02982727  	0.07992695  	0.06609111  
2023-05-31 16:03:05.086: [iter 39 : loss : 0.5889 = 0.1238 + 0.4599 + 0.0053, time: 22.491858]
2023-05-31 16:03:05.359: epoch 39:	0.02976063  	0.07958487  	0.06604007  
2023-05-31 16:03:27.932: [iter 40 : loss : 0.5833 = 0.1183 + 0.4596 + 0.0054, time: 22.566099]
2023-05-31 16:03:28.207: epoch 40:	0.02971622  	0.07936778  	0.06611213  
2023-05-31 16:03:50.877: [iter 41 : loss : 0.5778 = 0.1133 + 0.4589 + 0.0056, time: 22.665996]
2023-05-31 16:03:51.154: epoch 41:	0.02972362  	0.07939371  	0.06604425  
2023-05-31 16:04:13.874: [iter 42 : loss : 0.5735 = 0.1091 + 0.4587 + 0.0058, time: 22.716463]
2023-05-31 16:04:14.149: epoch 42:	0.02979025  	0.07970291  	0.06632061  
2023-05-31 16:04:37.062: [iter 43 : loss : 0.5692 = 0.1051 + 0.4581 + 0.0059, time: 22.907109]
2023-05-31 16:04:37.337: epoch 43:	0.02973103  	0.07951882  	0.06614722  
2023-05-31 16:05:00.057: [iter 44 : loss : 0.5660 = 0.1022 + 0.4578 + 0.0061, time: 22.716050]
2023-05-31 16:05:00.320: epoch 44:	0.02988650  	0.07981986  	0.06623957  
2023-05-31 16:05:23.047: [iter 45 : loss : 0.5621 = 0.0984 + 0.4574 + 0.0062, time: 22.724002]
2023-05-31 16:05:23.316: epoch 45:	0.02985688  	0.07974467  	0.06622931  
2023-05-31 16:05:45.632: [iter 46 : loss : 0.5588 = 0.0953 + 0.4571 + 0.0064, time: 22.313078]
2023-05-31 16:05:45.903: epoch 46:	0.02987170  	0.07973474  	0.06629453  
2023-05-31 16:06:08.434: [iter 47 : loss : 0.5556 = 0.0924 + 0.4567 + 0.0065, time: 22.526776]
2023-05-31 16:06:08.706: epoch 47:	0.02980507  	0.07952405  	0.06613628  
2023-05-31 16:06:31.213: [iter 48 : loss : 0.5526 = 0.0896 + 0.4564 + 0.0067, time: 22.503391]
2023-05-31 16:06:31.490: epoch 48:	0.02982727  	0.07946429  	0.06616125  
2023-05-31 16:06:54.013: [iter 49 : loss : 0.5494 = 0.0866 + 0.4560 + 0.0068, time: 22.519336]
2023-05-31 16:06:54.285: epoch 49:	0.02977544  	0.07925861  	0.06598508  
2023-05-31 16:07:17.015: [iter 50 : loss : 0.5475 = 0.0848 + 0.4558 + 0.0070, time: 22.724373]
2023-05-31 16:07:17.286: epoch 50:	0.02970881  	0.07905432  	0.06593965  
2023-05-31 16:07:39.798: [iter 51 : loss : 0.5447 = 0.0821 + 0.4555 + 0.0071, time: 22.507627]
2023-05-31 16:07:40.068: epoch 51:	0.02973102  	0.07931298  	0.06596348  
2023-05-31 16:08:02.399: [iter 52 : loss : 0.5419 = 0.0795 + 0.4552 + 0.0072, time: 22.327210]
2023-05-31 16:08:02.672: epoch 52:	0.02979024  	0.07919516  	0.06580353  
2023-05-31 16:08:25.184: [iter 53 : loss : 0.5409 = 0.0786 + 0.4549 + 0.0074, time: 22.509105]
2023-05-31 16:08:25.463: epoch 53:	0.02972361  	0.07893609  	0.06568407  
2023-05-31 16:08:47.588: [iter 54 : loss : 0.5385 = 0.0762 + 0.4548 + 0.0075, time: 22.121462]
2023-05-31 16:08:47.861: epoch 54:	0.02959776  	0.07858261  	0.06544178  
2023-05-31 16:09:10.147: [iter 55 : loss : 0.5366 = 0.0745 + 0.4545 + 0.0076, time: 22.281151]
2023-05-31 16:09:10.430: epoch 55:	0.02945709  	0.07818344  	0.06531385  
2023-05-31 16:09:32.795: [iter 56 : loss : 0.5344 = 0.0724 + 0.4543 + 0.0077, time: 22.361080]
2023-05-31 16:09:33.066: epoch 56:	0.02931643  	0.07797055  	0.06514467  
2023-05-31 16:09:55.540: [iter 57 : loss : 0.5323 = 0.0704 + 0.4541 + 0.0079, time: 22.470104]
2023-05-31 16:09:55.810: epoch 57:	0.02933124  	0.07797147  	0.06514447  
2023-05-31 16:10:18.181: [iter 58 : loss : 0.5310 = 0.0691 + 0.4539 + 0.0080, time: 22.367439]
2023-05-31 16:10:18.459: epoch 58:	0.02913135  	0.07729698  	0.06483040  
2023-05-31 16:10:40.954: [iter 59 : loss : 0.5292 = 0.0674 + 0.4537 + 0.0081, time: 22.489589]
2023-05-31 16:10:41.224: epoch 59:	0.02914616  	0.07732663  	0.06485163  
2023-05-31 16:11:03.541: [iter 60 : loss : 0.5279 = 0.0662 + 0.4535 + 0.0082, time: 22.313399]
2023-05-31 16:11:03.819: epoch 60:	0.02906472  	0.07695556  	0.06452170  
2023-05-31 16:11:26.115: [iter 61 : loss : 0.5266 = 0.0648 + 0.4534 + 0.0083, time: 22.293354]
2023-05-31 16:11:26.401: epoch 61:	0.02902031  	0.07647057  	0.06431362  
2023-05-31 16:11:48.880: [iter 62 : loss : 0.5254 = 0.0637 + 0.4533 + 0.0084, time: 22.476114]
2023-05-31 16:11:49.153: epoch 62:	0.02897588  	0.07631494  	0.06422706  
2023-05-31 16:11:49.153: Early stopping is trigger at epoch: 62
2023-05-31 16:11:49.153: best_result@epoch 37:

2023-05-31 16:11:49.153: 		0.0299      	0.0802      	0.0660      
2023-05-31 16:37:47.465: my pid: 6528
2023-05-31 16:37:47.466: model: model.general_recommender.SGL
2023-05-31 16:37:47.466: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 16:37:47.466: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 16:37:51.429: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 16:38:12.374: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.944126]
2023-05-31 16:38:12.640: epoch 1:	0.00122150  	0.00223070  	0.00185498  
2023-05-31 16:38:12.640: Find a better model.
2023-05-31 16:38:33.330: [iter 2 : loss : 1.1337 = 0.6930 + 0.4406 + 0.0000, time: 20.686144]
2023-05-31 16:38:33.615: epoch 2:	0.00159905  	0.00356930  	0.00257623  
2023-05-31 16:38:33.615: Find a better model.
2023-05-31 16:38:54.314: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 20.695569]
2023-05-31 16:38:54.600: epoch 3:	0.00173971  	0.00365814  	0.00294933  
2023-05-31 16:38:54.600: Find a better model.
2023-05-31 16:39:15.283: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 20.679415]
2023-05-31 16:39:15.569: epoch 4:	0.00219130  	0.00469530  	0.00351481  
2023-05-31 16:39:15.569: Find a better model.
2023-05-31 16:39:36.235: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 20.662645]
2023-05-31 16:39:36.521: epoch 5:	0.00240598  	0.00538795  	0.00440829  
2023-05-31 16:39:36.521: Find a better model.
2023-05-31 16:39:57.076: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 20.551163]
2023-05-31 16:39:57.358: epoch 6:	0.00289458  	0.00698509  	0.00520155  
2023-05-31 16:39:57.358: Find a better model.
2023-05-31 16:40:18.013: [iter 7 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 20.651090]
2023-05-31 16:40:18.294: epoch 7:	0.00336837  	0.00818032  	0.00638746  
2023-05-31 16:40:18.294: Find a better model.
2023-05-31 16:40:39.224: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 20.926161]
2023-05-31 16:40:39.508: epoch 8:	0.00411608  	0.01080271  	0.00819676  
2023-05-31 16:40:39.508: Find a better model.
2023-05-31 16:41:00.247: [iter 9 : loss : 1.1346 = 0.6917 + 0.4429 + 0.0000, time: 20.734219]
2023-05-31 16:41:00.514: epoch 9:	0.00490079  	0.01371041  	0.01052701  
2023-05-31 16:41:00.514: Find a better model.
2023-05-31 16:41:20.994: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 20.475118]
2023-05-31 16:41:21.275: epoch 10:	0.00557446  	0.01608307  	0.01251219  
2023-05-31 16:41:21.275: Find a better model.
2023-05-31 16:41:42.448: [iter 11 : loss : 1.1340 = 0.6902 + 0.4438 + 0.0000, time: 21.168366]
2023-05-31 16:41:42.716: epoch 11:	0.00645541  	0.01932563  	0.01482568  
2023-05-31 16:41:42.716: Find a better model.
2023-05-31 16:42:03.626: [iter 12 : loss : 1.1329 = 0.6886 + 0.4443 + 0.0000, time: 20.905245]
2023-05-31 16:42:03.919: epoch 12:	0.00775833  	0.02256034  	0.01821440  
2023-05-31 16:42:03.919: Find a better model.
2023-05-31 16:42:24.856: [iter 13 : loss : 1.1311 = 0.6860 + 0.4450 + 0.0000, time: 20.932724]
2023-05-31 16:42:25.131: epoch 13:	0.00962388  	0.02810919  	0.02279008  
2023-05-31 16:42:25.131: Find a better model.
2023-05-31 16:42:45.996: [iter 14 : loss : 1.1279 = 0.6821 + 0.4458 + 0.0001, time: 20.861057]
2023-05-31 16:42:46.271: epoch 14:	0.01270357  	0.03634737  	0.03057587  
2023-05-31 16:42:46.271: Find a better model.
2023-05-31 16:43:07.208: [iter 15 : loss : 1.1222 = 0.6755 + 0.4466 + 0.0001, time: 20.933439]
2023-05-31 16:43:07.482: epoch 15:	0.01604241  	0.04481344  	0.03795537  
2023-05-31 16:43:07.482: Find a better model.
2023-05-31 16:43:28.570: [iter 16 : loss : 1.1118 = 0.6640 + 0.4476 + 0.0001, time: 21.084102]
2023-05-31 16:43:28.857: epoch 16:	0.01947009  	0.05337302  	0.04558465  
2023-05-31 16:43:28.857: Find a better model.
2023-05-31 16:43:49.782: [iter 17 : loss : 1.0940 = 0.6450 + 0.4488 + 0.0002, time: 20.919070]
2023-05-31 16:43:50.058: epoch 17:	0.02280156  	0.06233505  	0.05268378  
2023-05-31 16:43:50.058: Find a better model.
2023-05-31 16:44:10.957: [iter 18 : loss : 1.0660 = 0.6151 + 0.4505 + 0.0004, time: 20.895133]
2023-05-31 16:44:11.231: epoch 18:	0.02542232  	0.06875949  	0.05779912  
2023-05-31 16:44:11.231: Find a better model.
2023-05-31 16:44:32.161: [iter 19 : loss : 1.0277 = 0.5744 + 0.4528 + 0.0005, time: 20.924465]
2023-05-31 16:44:32.435: epoch 19:	0.02721388  	0.07394323  	0.06181275  
2023-05-31 16:44:32.435: Find a better model.
2023-05-31 16:44:53.506: [iter 20 : loss : 0.9816 = 0.5250 + 0.4558 + 0.0008, time: 21.066734]
2023-05-31 16:44:53.775: epoch 20:	0.02838359  	0.07735167  	0.06431521  
2023-05-31 16:44:53.775: Find a better model.
2023-05-31 16:45:14.706: [iter 21 : loss : 0.9322 = 0.4722 + 0.4590 + 0.0010, time: 20.928083]
2023-05-31 16:45:14.984: epoch 21:	0.02882040  	0.07855625  	0.06539436  
2023-05-31 16:45:14.984: Find a better model.
2023-05-31 16:45:35.909: [iter 22 : loss : 0.8833 = 0.4199 + 0.4621 + 0.0013, time: 20.922392]
2023-05-31 16:45:36.175: epoch 22:	0.02922019  	0.08040878  	0.06620308  
2023-05-31 16:45:36.175: Find a better model.
2023-05-31 16:45:56.745: [iter 23 : loss : 0.8396 = 0.3733 + 0.4646 + 0.0016, time: 20.564936]
2023-05-31 16:45:57.016: epoch 23:	0.02938306  	0.08078165  	0.06644775  
2023-05-31 16:45:57.016: Find a better model.
2023-05-31 16:46:17.707: [iter 24 : loss : 0.8017 = 0.3334 + 0.4664 + 0.0019, time: 20.688021]
2023-05-31 16:46:17.982: epoch 24:	0.02947931  	0.08136848  	0.06671137  
2023-05-31 16:46:17.982: Find a better model.
2023-05-31 16:46:38.763: [iter 25 : loss : 0.7690 = 0.2995 + 0.4673 + 0.0022, time: 20.776537]
2023-05-31 16:46:39.037: epoch 25:	0.02969399  	0.08184750  	0.06703878  
2023-05-31 16:46:39.038: Find a better model.
2023-05-31 16:47:00.074: [iter 26 : loss : 0.7419 = 0.2718 + 0.4676 + 0.0025, time: 21.032039]
2023-05-31 16:47:00.328: epoch 26:	0.02978284  	0.08213012  	0.06728456  
2023-05-31 16:47:00.328: Find a better model.
2023-05-31 16:47:21.277: [iter 27 : loss : 0.7186 = 0.2481 + 0.4677 + 0.0028, time: 20.944923]
2023-05-31 16:47:21.544: epoch 27:	0.02987168  	0.08260642  	0.06746940  
2023-05-31 16:47:21.544: Find a better model.
2023-05-31 16:47:42.903: [iter 28 : loss : 0.6988 = 0.2284 + 0.4673 + 0.0030, time: 21.356027]
2023-05-31 16:47:43.174: epoch 28:	0.03004195  	0.08271442  	0.06763873  
2023-05-31 16:47:43.174: Find a better model.
2023-05-31 16:48:04.260: [iter 29 : loss : 0.6818 = 0.2117 + 0.4668 + 0.0033, time: 21.083469]
2023-05-31 16:48:04.526: epoch 29:	0.03011598  	0.08306104  	0.06784152  
2023-05-31 16:48:04.526: Find a better model.
2023-05-31 16:48:25.646: [iter 30 : loss : 0.6665 = 0.1967 + 0.4662 + 0.0035, time: 21.116420]
2023-05-31 16:48:25.918: epoch 30:	0.03004935  	0.08263887  	0.06781727  
2023-05-31 16:48:46.898: [iter 31 : loss : 0.6530 = 0.1836 + 0.4656 + 0.0038, time: 20.975075]
2023-05-31 16:48:47.166: epoch 31:	0.03006415  	0.08250163  	0.06779432  
2023-05-31 16:49:08.234: [iter 32 : loss : 0.6417 = 0.1728 + 0.4650 + 0.0040, time: 21.064591]
2023-05-31 16:49:08.498: epoch 32:	0.03016780  	0.08259061  	0.06791870  
2023-05-31 16:49:29.672: [iter 33 : loss : 0.6320 = 0.1635 + 0.4643 + 0.0042, time: 21.171031]
2023-05-31 16:49:29.945: epoch 33:	0.03019001  	0.08254331  	0.06776670  
2023-05-31 16:49:51.033: [iter 34 : loss : 0.6226 = 0.1546 + 0.4636 + 0.0044, time: 21.083342]
2023-05-31 16:49:51.298: epoch 34:	0.03009376  	0.08213454  	0.06757861  
2023-05-31 16:50:12.469: [iter 35 : loss : 0.6136 = 0.1461 + 0.4629 + 0.0046, time: 21.168160]
2023-05-31 16:50:12.734: epoch 35:	0.03009376  	0.08251648  	0.06755605  
2023-05-31 16:50:33.855: [iter 36 : loss : 0.6066 = 0.1395 + 0.4623 + 0.0048, time: 21.117244]
2023-05-31 16:50:34.120: epoch 36:	0.03012339  	0.08253856  	0.06768046  
2023-05-31 16:50:55.399: [iter 37 : loss : 0.6000 = 0.1333 + 0.4617 + 0.0050, time: 21.276049]
2023-05-31 16:50:55.665: epoch 37:	0.03007157  	0.08249185  	0.06773469  
2023-05-31 16:51:16.800: [iter 38 : loss : 0.5936 = 0.1272 + 0.4613 + 0.0051, time: 21.132526]
2023-05-31 16:51:17.066: epoch 38:	0.03010118  	0.08229412  	0.06766295  
2023-05-31 16:51:38.207: [iter 39 : loss : 0.5882 = 0.1222 + 0.4607 + 0.0053, time: 21.137029]
2023-05-31 16:51:38.472: epoch 39:	0.03003455  	0.08219887  	0.06777208  
2023-05-31 16:51:59.779: [iter 40 : loss : 0.5825 = 0.1169 + 0.4602 + 0.0055, time: 21.304505]
2023-05-31 16:52:00.051: epoch 40:	0.03001976  	0.08219537  	0.06773788  
2023-05-31 16:52:21.173: [iter 41 : loss : 0.5773 = 0.1120 + 0.4597 + 0.0056, time: 21.119123]
2023-05-31 16:52:21.437: epoch 41:	0.02999754  	0.08187504  	0.06771695  
2023-05-31 16:52:42.754: [iter 42 : loss : 0.5733 = 0.1081 + 0.4593 + 0.0058, time: 21.314120]
2023-05-31 16:52:43.021: epoch 42:	0.02988648  	0.08155734  	0.06757041  
2023-05-31 16:53:04.351: [iter 43 : loss : 0.5688 = 0.1039 + 0.4589 + 0.0060, time: 21.325134]
2023-05-31 16:53:04.614: epoch 43:	0.02992350  	0.08148731  	0.06750606  
2023-05-31 16:53:25.947: [iter 44 : loss : 0.5660 = 0.1013 + 0.4585 + 0.0061, time: 21.327636]
2023-05-31 16:53:26.215: epoch 44:	0.02985686  	0.08139830  	0.06738716  
2023-05-31 16:53:47.559: [iter 45 : loss : 0.5621 = 0.0976 + 0.4582 + 0.0063, time: 21.340359]
2023-05-31 16:53:47.826: epoch 45:	0.02972360  	0.08098219  	0.06729893  
2023-05-31 16:54:09.118: [iter 46 : loss : 0.5586 = 0.0944 + 0.4578 + 0.0064, time: 21.286148]
2023-05-31 16:54:09.380: epoch 46:	0.02987167  	0.08142786  	0.06746476  
2023-05-31 16:54:30.748: [iter 47 : loss : 0.5551 = 0.0912 + 0.4574 + 0.0066, time: 21.363061]
2023-05-31 16:54:31.019: epoch 47:	0.02979023  	0.08134763  	0.06731588  
2023-05-31 16:54:52.527: [iter 48 : loss : 0.5527 = 0.0889 + 0.4571 + 0.0067, time: 21.504759]
2023-05-31 16:54:52.791: epoch 48:	0.02978283  	0.08101165  	0.06717245  
2023-05-31 16:55:14.128: [iter 49 : loss : 0.5498 = 0.0862 + 0.4567 + 0.0069, time: 21.333212]
2023-05-31 16:55:14.396: epoch 49:	0.02970880  	0.08066853  	0.06694062  
2023-05-31 16:55:35.521: [iter 50 : loss : 0.5474 = 0.0839 + 0.4565 + 0.0070, time: 21.122471]
2023-05-31 16:55:35.784: epoch 50:	0.02965698  	0.08027282  	0.06683173  
2023-05-31 16:55:56.889: [iter 51 : loss : 0.5449 = 0.0814 + 0.4563 + 0.0071, time: 21.099924]
2023-05-31 16:55:57.153: epoch 51:	0.02964218  	0.08045228  	0.06695062  
2023-05-31 16:56:18.491: [iter 52 : loss : 0.5422 = 0.0790 + 0.4559 + 0.0073, time: 21.334277]
2023-05-31 16:56:18.753: epoch 52:	0.02951632  	0.08014757  	0.06666422  
2023-05-31 16:56:39.931: [iter 53 : loss : 0.5414 = 0.0781 + 0.4558 + 0.0074, time: 21.174027]
2023-05-31 16:56:40.202: epoch 53:	0.02953852  	0.08007758  	0.06664553  
2023-05-31 16:57:01.461: [iter 54 : loss : 0.5390 = 0.0759 + 0.4555 + 0.0075, time: 21.254933]
2023-05-31 16:57:01.726: epoch 54:	0.02952371  	0.08012126  	0.06652266  
2023-05-31 16:57:01.726: Early stopping is trigger at epoch: 54
2023-05-31 16:57:01.726: best_result@epoch 29:

2023-05-31 16:57:01.726: 		0.0301      	0.0831      	0.0678      
2023-05-31 18:21:08.006: my pid: 12556
2023-05-31 18:21:08.006: model: model.general_recommender.SGL
2023-05-31 18:21:08.006: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 18:21:08.006: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 18:21:12.032: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 18:21:32.737: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.703674]
2023-05-31 18:21:33.011: epoch 1:	0.00132514  	0.00220931  	0.00190248  
2023-05-31 18:21:33.011: Find a better model.
2023-05-31 18:21:53.751: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.737370]
2023-05-31 18:21:54.042: epoch 2:	0.00138437  	0.00304541  	0.00233618  
2023-05-31 18:21:54.042: Find a better model.
2023-05-31 18:22:14.760: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 20.713346]
2023-05-31 18:22:15.050: epoch 3:	0.00173231  	0.00341066  	0.00260709  
2023-05-31 18:22:15.050: Find a better model.
2023-05-31 18:22:35.753: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 20.698130]
2023-05-31 18:22:36.043: epoch 4:	0.00216168  	0.00437498  	0.00344094  
2023-05-31 18:22:36.043: Find a better model.
2023-05-31 18:22:56.714: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 20.668045]
2023-05-31 18:22:57.007: epoch 5:	0.00191738  	0.00469214  	0.00334237  
2023-05-31 18:22:57.008: Find a better model.
2023-05-31 18:23:17.692: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 20.681231]
2023-05-31 18:23:17.987: epoch 6:	0.00289458  	0.00645280  	0.00498628  
2023-05-31 18:23:17.988: Find a better model.
2023-05-31 18:23:38.506: [iter 7 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 20.514052]
2023-05-31 18:23:38.792: epoch 7:	0.00330175  	0.00746888  	0.00607795  
2023-05-31 18:23:38.792: Find a better model.
2023-05-31 18:23:59.466: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 20.669535]
2023-05-31 18:23:59.754: epoch 8:	0.00384957  	0.00977004  	0.00759289  
2023-05-31 18:23:59.754: Find a better model.
2023-05-31 18:24:20.465: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 20.708045]
2023-05-31 18:24:20.747: epoch 9:	0.00461948  	0.01255072  	0.00977280  
2023-05-31 18:24:20.747: Find a better model.
2023-05-31 18:24:41.264: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 20.513055]
2023-05-31 18:24:41.546: epoch 10:	0.00530795  	0.01453005  	0.01166583  
2023-05-31 18:24:41.546: Find a better model.
2023-05-31 18:25:02.880: [iter 11 : loss : 1.1339 = 0.6901 + 0.4437 + 0.0000, time: 21.330431]
2023-05-31 18:25:03.164: epoch 11:	0.00631475  	0.01805317  	0.01412539  
2023-05-31 18:25:03.164: Find a better model.
2023-05-31 18:25:24.256: [iter 12 : loss : 1.1328 = 0.6885 + 0.4444 + 0.0000, time: 21.088222]
2023-05-31 18:25:24.533: epoch 12:	0.00792860  	0.02296505  	0.01820767  
2023-05-31 18:25:24.533: Find a better model.
2023-05-31 18:25:45.639: [iter 13 : loss : 1.1310 = 0.6860 + 0.4450 + 0.0000, time: 21.102055]
2023-05-31 18:25:45.930: epoch 13:	0.01011990  	0.02980614  	0.02433211  
2023-05-31 18:25:45.930: Find a better model.
2023-05-31 18:26:06.867: [iter 14 : loss : 1.1280 = 0.6822 + 0.4457 + 0.0001, time: 20.931067]
2023-05-31 18:26:07.145: epoch 14:	0.01267396  	0.03630389  	0.03065672  
2023-05-31 18:26:07.145: Find a better model.
2023-05-31 18:26:28.217: [iter 15 : loss : 1.1224 = 0.6758 + 0.4465 + 0.0001, time: 21.068410]
2023-05-31 18:26:28.488: epoch 15:	0.01608682  	0.04518286  	0.03792847  
2023-05-31 18:26:28.488: Find a better model.
2023-05-31 18:26:49.418: [iter 16 : loss : 1.1122 = 0.6646 + 0.4475 + 0.0001, time: 20.924180]
2023-05-31 18:26:49.693: epoch 16:	0.01932941  	0.05274946  	0.04515354  
2023-05-31 18:26:49.693: Find a better model.
2023-05-31 18:27:10.600: [iter 17 : loss : 1.0950 = 0.6460 + 0.4487 + 0.0002, time: 20.903041]
2023-05-31 18:27:10.875: epoch 17:	0.02241657  	0.06040753  	0.05101660  
2023-05-31 18:27:10.875: Find a better model.
2023-05-31 18:27:31.788: [iter 18 : loss : 1.0676 = 0.6168 + 0.4505 + 0.0004, time: 20.909179]
2023-05-31 18:27:32.057: epoch 18:	0.02497811  	0.06677409  	0.05605337  
2023-05-31 18:27:32.058: Find a better model.
2023-05-31 18:27:52.810: [iter 19 : loss : 1.0299 = 0.5767 + 0.4527 + 0.0005, time: 20.749016]
2023-05-31 18:27:53.082: epoch 19:	0.02653280  	0.07055335  	0.05960010  
2023-05-31 18:27:53.082: Find a better model.
2023-05-31 18:28:14.011: [iter 20 : loss : 0.9840 = 0.5277 + 0.4556 + 0.0008, time: 20.925553]
2023-05-31 18:28:14.279: epoch 20:	0.02777655  	0.07412294  	0.06208027  
2023-05-31 18:28:14.279: Find a better model.
2023-05-31 18:28:35.198: [iter 21 : loss : 0.9346 = 0.4746 + 0.4590 + 0.0010, time: 20.914111]
2023-05-31 18:28:35.463: epoch 21:	0.02848726  	0.07650683  	0.06322637  
2023-05-31 18:28:35.463: Find a better model.
2023-05-31 18:28:56.563: [iter 22 : loss : 0.8855 = 0.4222 + 0.4620 + 0.0013, time: 21.096260]
2023-05-31 18:28:56.829: epoch 22:	0.02891666  	0.07795828  	0.06412819  
2023-05-31 18:28:56.829: Find a better model.
2023-05-31 18:29:17.945: [iter 23 : loss : 0.8417 = 0.3754 + 0.4647 + 0.0016, time: 21.113651]
2023-05-31 18:29:18.210: epoch 23:	0.02918317  	0.07902510  	0.06452211  
2023-05-31 18:29:18.210: Find a better model.
2023-05-31 18:29:39.352: [iter 24 : loss : 0.8031 = 0.3349 + 0.4663 + 0.0019, time: 21.136471]
2023-05-31 18:29:39.616: epoch 24:	0.02919058  	0.07902551  	0.06476302  
2023-05-31 18:29:39.616: Find a better model.
2023-05-31 18:30:00.966: [iter 25 : loss : 0.7701 = 0.3006 + 0.4673 + 0.0022, time: 21.346003]
2023-05-31 18:30:01.237: epoch 25:	0.02942747  	0.07974097  	0.06496655  
2023-05-31 18:30:01.237: Find a better model.
2023-05-31 18:30:22.593: [iter 26 : loss : 0.7431 = 0.2728 + 0.4678 + 0.0025, time: 21.351391]
2023-05-31 18:30:22.860: epoch 26:	0.02944968  	0.07959363  	0.06508864  
2023-05-31 18:30:44.162: [iter 27 : loss : 0.7195 = 0.2490 + 0.4677 + 0.0028, time: 21.297210]
2023-05-31 18:30:44.428: epoch 27:	0.02954593  	0.07956187  	0.06501202  
2023-05-31 18:31:05.756: [iter 28 : loss : 0.6994 = 0.2291 + 0.4673 + 0.0030, time: 21.324252]
2023-05-31 18:31:06.024: epoch 28:	0.02965698  	0.07962091  	0.06511600  
2023-05-31 18:31:27.318: [iter 29 : loss : 0.6825 = 0.2124 + 0.4668 + 0.0033, time: 21.290023]
2023-05-31 18:31:27.580: epoch 29:	0.02958294  	0.07933071  	0.06514572  
2023-05-31 18:31:48.762: [iter 30 : loss : 0.6669 = 0.1972 + 0.4662 + 0.0035, time: 21.177688]
2023-05-31 18:31:49.035: epoch 30:	0.02973101  	0.07970891  	0.06543445  
2023-05-31 18:32:09.915: [iter 31 : loss : 0.6532 = 0.1838 + 0.4657 + 0.0037, time: 20.875917]
2023-05-31 18:32:10.179: epoch 31:	0.02972360  	0.07972427  	0.06550558  
2023-05-31 18:32:31.502: [iter 32 : loss : 0.6421 = 0.1732 + 0.4650 + 0.0040, time: 21.319098]
2023-05-31 18:32:31.765: epoch 32:	0.02993829  	0.08018614  	0.06586789  
2023-05-31 18:32:31.765: Find a better model.
2023-05-31 18:32:52.908: [iter 33 : loss : 0.6322 = 0.1639 + 0.4642 + 0.0042, time: 21.138229]
2023-05-31 18:32:53.170: epoch 33:	0.03005674  	0.08063254  	0.06597628  
2023-05-31 18:32:53.170: Find a better model.
2023-05-31 18:33:14.322: [iter 34 : loss : 0.6227 = 0.1547 + 0.4636 + 0.0044, time: 21.148531]
2023-05-31 18:33:14.588: epoch 34:	0.03013077  	0.08074897  	0.06615301  
2023-05-31 18:33:14.588: Find a better model.
2023-05-31 18:33:35.897: [iter 35 : loss : 0.6135 = 0.1461 + 0.4628 + 0.0046, time: 21.305394]
2023-05-31 18:33:36.160: epoch 35:	0.03005674  	0.08069479  	0.06611519  
2023-05-31 18:33:57.039: [iter 36 : loss : 0.6067 = 0.1397 + 0.4623 + 0.0048, time: 20.874045]
2023-05-31 18:33:57.300: epoch 36:	0.03004194  	0.08050937  	0.06618591  
2023-05-31 18:34:18.243: [iter 37 : loss : 0.6000 = 0.1333 + 0.4617 + 0.0049, time: 20.938101]
2023-05-31 18:34:18.504: epoch 37:	0.03001233  	0.08023605  	0.06601891  
2023-05-31 18:34:39.664: [iter 38 : loss : 0.5933 = 0.1270 + 0.4611 + 0.0051, time: 21.155033]
2023-05-31 18:34:39.939: epoch 38:	0.03012337  	0.08103444  	0.06636305  
2023-05-31 18:34:39.939: Find a better model.
2023-05-31 18:35:00.861: [iter 39 : loss : 0.5882 = 0.1223 + 0.4606 + 0.0053, time: 20.918603]
2023-05-31 18:35:01.130: epoch 39:	0.03007896  	0.08058897  	0.06625123  
2023-05-31 18:35:22.056: [iter 40 : loss : 0.5826 = 0.1170 + 0.4602 + 0.0055, time: 20.921057]
2023-05-31 18:35:22.318: epoch 40:	0.03019740  	0.08069063  	0.06643660  
2023-05-31 18:35:43.636: [iter 41 : loss : 0.5776 = 0.1123 + 0.4596 + 0.0056, time: 21.313029]
2023-05-31 18:35:43.898: epoch 41:	0.03004933  	0.08016073  	0.06631507  
2023-05-31 18:36:05.037: [iter 42 : loss : 0.5728 = 0.1078 + 0.4592 + 0.0058, time: 21.136126]
2023-05-31 18:36:05.298: epoch 42:	0.03007154  	0.08027893  	0.06651037  
2023-05-31 18:36:26.240: [iter 43 : loss : 0.5688 = 0.1040 + 0.4589 + 0.0060, time: 20.938486]
2023-05-31 18:36:26.502: epoch 43:	0.02996048  	0.07957081  	0.06619328  
2023-05-31 18:36:47.788: [iter 44 : loss : 0.5660 = 0.1013 + 0.4585 + 0.0061, time: 21.281622]
2023-05-31 18:36:48.054: epoch 44:	0.02997529  	0.07981306  	0.06626716  
2023-05-31 18:37:08.822: [iter 45 : loss : 0.5619 = 0.0975 + 0.4581 + 0.0063, time: 20.764074]
2023-05-31 18:37:09.088: epoch 45:	0.02999009  	0.07950446  	0.06612065  
2023-05-31 18:37:29.977: [iter 46 : loss : 0.5585 = 0.0944 + 0.4576 + 0.0064, time: 20.885031]
2023-05-31 18:37:30.239: epoch 46:	0.02996047  	0.07939298  	0.06602868  
2023-05-31 18:37:51.202: [iter 47 : loss : 0.5551 = 0.0912 + 0.4573 + 0.0066, time: 20.959342]
2023-05-31 18:37:51.464: epoch 47:	0.02993827  	0.07924767  	0.06611799  
2023-05-31 18:38:12.385: [iter 48 : loss : 0.5525 = 0.0887 + 0.4571 + 0.0067, time: 20.917039]
2023-05-31 18:38:12.644: epoch 48:	0.02975320  	0.07891991  	0.06589094  
2023-05-31 18:38:33.751: [iter 49 : loss : 0.5500 = 0.0864 + 0.4567 + 0.0069, time: 21.103009]
2023-05-31 18:38:34.022: epoch 49:	0.02963475  	0.07877135  	0.06581037  
2023-05-31 18:38:54.974: [iter 50 : loss : 0.5477 = 0.0842 + 0.4565 + 0.0070, time: 20.946261]
2023-05-31 18:38:55.239: epoch 50:	0.02956811  	0.07879109  	0.06564805  
2023-05-31 18:39:15.953: [iter 51 : loss : 0.5444 = 0.0812 + 0.4561 + 0.0071, time: 20.710066]
2023-05-31 18:39:16.215: epoch 51:	0.02960513  	0.07897104  	0.06576381  
2023-05-31 18:39:36.991: [iter 52 : loss : 0.5424 = 0.0792 + 0.4560 + 0.0073, time: 20.773138]
2023-05-31 18:39:37.253: epoch 52:	0.02956072  	0.07852512  	0.06549180  
2023-05-31 18:39:58.181: [iter 53 : loss : 0.5410 = 0.0780 + 0.4556 + 0.0074, time: 20.923108]
2023-05-31 18:39:58.443: epoch 53:	0.02964215  	0.07891904  	0.06567009  
2023-05-31 18:40:19.514: [iter 54 : loss : 0.5386 = 0.0756 + 0.4554 + 0.0075, time: 21.067077]
2023-05-31 18:40:19.775: epoch 54:	0.02951629  	0.07872701  	0.06556661  
2023-05-31 18:40:40.706: [iter 55 : loss : 0.5368 = 0.0739 + 0.4553 + 0.0076, time: 20.927029]
2023-05-31 18:40:40.975: epoch 55:	0.02955331  	0.07870635  	0.06543902  
2023-05-31 18:41:01.965: [iter 56 : loss : 0.5346 = 0.0718 + 0.4550 + 0.0078, time: 20.985057]
2023-05-31 18:41:02.231: epoch 56:	0.02944967  	0.07843688  	0.06509063  
2023-05-31 18:41:22.948: [iter 57 : loss : 0.5328 = 0.0701 + 0.4547 + 0.0079, time: 20.713073]
2023-05-31 18:41:23.209: epoch 57:	0.02950149  	0.07823648  	0.06507277  
2023-05-31 18:41:44.320: [iter 58 : loss : 0.5313 = 0.0686 + 0.4546 + 0.0080, time: 21.105225]
2023-05-31 18:41:44.582: epoch 58:	0.02941264  	0.07814039  	0.06487276  
2023-05-31 18:42:05.495: [iter 59 : loss : 0.5299 = 0.0672 + 0.4545 + 0.0081, time: 20.910058]
2023-05-31 18:42:05.757: epoch 59:	0.02942745  	0.07797474  	0.06485844  
2023-05-31 18:42:26.712: [iter 60 : loss : 0.5284 = 0.0659 + 0.4543 + 0.0082, time: 20.951184]
2023-05-31 18:42:26.983: epoch 60:	0.02942005  	0.07770451  	0.06467122  
2023-05-31 18:42:47.941: [iter 61 : loss : 0.5266 = 0.0642 + 0.4541 + 0.0084, time: 20.954034]
2023-05-31 18:42:48.204: epoch 61:	0.02926457  	0.07748082  	0.06463252  
2023-05-31 18:43:09.296: [iter 62 : loss : 0.5254 = 0.0629 + 0.4540 + 0.0085, time: 21.089263]
2023-05-31 18:43:09.556: epoch 62:	0.02924237  	0.07718121  	0.06460941  
2023-05-31 18:43:30.503: [iter 63 : loss : 0.5245 = 0.0621 + 0.4538 + 0.0086, time: 20.943037]
2023-05-31 18:43:30.764: epoch 63:	0.02923497  	0.07727934  	0.06460842  
2023-05-31 18:43:30.764: Early stopping is trigger at epoch: 63
2023-05-31 18:43:30.764: best_result@epoch 38:

2023-05-31 18:43:30.764: 		0.0301      	0.0810      	0.0664      
2023-05-31 18:44:43.081: my pid: 8116
2023-05-31 18:44:43.081: model: model.general_recommender.SGL
2023-05-31 18:44:43.081: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 18:44:43.081: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 18:44:47.252: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 18:45:08.434: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.180533]
2023-05-31 18:45:08.731: epoch 1:	0.00114007  	0.00253674  	0.00192808  
2023-05-31 18:45:08.731: Find a better model.
2023-05-31 18:45:30.222: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.488390]
2023-05-31 18:45:30.519: epoch 2:	0.00163607  	0.00328688  	0.00276508  
2023-05-31 18:45:30.520: Find a better model.
2023-05-31 18:45:52.023: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.499064]
2023-05-31 18:45:52.328: epoch 3:	0.00195440  	0.00434528  	0.00343333  
2023-05-31 18:45:52.328: Find a better model.
2023-05-31 18:46:13.788: [iter 4 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 21.457056]
2023-05-31 18:46:14.099: epoch 4:	0.00209506  	0.00390160  	0.00341314  
2023-05-31 18:46:35.548: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.441409]
2023-05-31 18:46:35.851: epoch 5:	0.00262807  	0.00586432  	0.00438831  
2023-05-31 18:46:35.851: Find a better model.
2023-05-31 18:46:57.340: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 21.486446]
2023-05-31 18:46:57.641: epoch 6:	0.00287977  	0.00623934  	0.00495474  
2023-05-31 18:46:57.641: Find a better model.
2023-05-31 18:47:19.154: [iter 7 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 21.507974]
2023-05-31 18:47:19.454: epoch 7:	0.00321291  	0.00742359  	0.00613102  
2023-05-31 18:47:19.454: Find a better model.
2023-05-31 18:47:40.924: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 21.464330]
2023-05-31 18:47:41.234: epoch 8:	0.00393841  	0.00990502  	0.00780682  
2023-05-31 18:47:41.234: Find a better model.
2023-05-31 18:48:02.735: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 21.497563]
2023-05-31 18:48:03.032: epoch 9:	0.00453064  	0.01236951  	0.00959009  
2023-05-31 18:48:03.033: Find a better model.
2023-05-31 18:48:24.351: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 21.314641]
2023-05-31 18:48:24.651: epoch 10:	0.00514509  	0.01463679  	0.01166331  
2023-05-31 18:48:24.651: Find a better model.
2023-05-31 18:48:46.933: [iter 11 : loss : 1.1339 = 0.6901 + 0.4438 + 0.0000, time: 22.277218]
2023-05-31 18:48:47.247: epoch 11:	0.00591500  	0.01660811  	0.01355108  
2023-05-31 18:48:47.247: Find a better model.
2023-05-31 18:49:09.164: [iter 12 : loss : 1.1328 = 0.6884 + 0.4443 + 0.0000, time: 21.913120]
2023-05-31 18:49:09.460: epoch 12:	0.00716610  	0.02064946  	0.01682328  
2023-05-31 18:49:09.460: Find a better model.
2023-05-31 18:49:31.312: [iter 13 : loss : 1.1310 = 0.6860 + 0.4450 + 0.0000, time: 21.847045]
2023-05-31 18:49:31.626: epoch 13:	0.00923892  	0.02728382  	0.02196369  
2023-05-31 18:49:31.626: Find a better model.
2023-05-31 18:49:53.641: [iter 14 : loss : 1.1280 = 0.6822 + 0.4457 + 0.0001, time: 22.011215]
2023-05-31 18:49:53.931: epoch 14:	0.01233341  	0.03604100  	0.02923168  
2023-05-31 18:49:53.931: Find a better model.
2023-05-31 18:50:15.876: [iter 15 : loss : 1.1224 = 0.6758 + 0.4465 + 0.0001, time: 21.942503]
2023-05-31 18:50:16.180: epoch 15:	0.01555380  	0.04401445  	0.03729434  
2023-05-31 18:50:16.180: Find a better model.
2023-05-31 18:50:38.226: [iter 16 : loss : 1.1122 = 0.6645 + 0.4475 + 0.0001, time: 22.042049]
2023-05-31 18:50:38.513: epoch 16:	0.01937385  	0.05417392  	0.04569301  
2023-05-31 18:50:38.513: Find a better model.
2023-05-31 18:51:00.313: [iter 17 : loss : 1.0949 = 0.6459 + 0.4488 + 0.0002, time: 21.796102]
2023-05-31 18:51:00.597: epoch 17:	0.02246099  	0.06146212  	0.05219069  
2023-05-31 18:51:00.597: Find a better model.
2023-05-31 18:51:22.603: [iter 18 : loss : 1.0673 = 0.6165 + 0.4505 + 0.0004, time: 22.002065]
2023-05-31 18:51:22.892: epoch 18:	0.02525201  	0.06870688  	0.05790744  
2023-05-31 18:51:22.892: Find a better model.
2023-05-31 18:51:44.851: [iter 19 : loss : 1.0295 = 0.5762 + 0.4527 + 0.0005, time: 21.956353]
2023-05-31 18:51:45.168: epoch 19:	0.02711764  	0.07274044  	0.06141866  
2023-05-31 18:51:45.168: Find a better model.
2023-05-31 18:52:06.998: [iter 20 : loss : 0.9837 = 0.5272 + 0.4557 + 0.0008, time: 21.826095]
2023-05-31 18:52:07.282: epoch 20:	0.02824294  	0.07568318  	0.06381398  
2023-05-31 18:52:07.282: Find a better model.
2023-05-31 18:52:29.009: [iter 21 : loss : 0.9343 = 0.4743 + 0.4590 + 0.0010, time: 21.723038]
2023-05-31 18:52:29.296: epoch 21:	0.02867233  	0.07704658  	0.06498335  
2023-05-31 18:52:29.296: Find a better model.
2023-05-31 18:52:51.033: [iter 22 : loss : 0.8852 = 0.4219 + 0.4620 + 0.0013, time: 21.733189]
2023-05-31 18:52:51.314: epoch 22:	0.02911651  	0.07813437  	0.06563921  
2023-05-31 18:52:51.315: Find a better model.
2023-05-31 18:53:13.050: [iter 23 : loss : 0.8414 = 0.3752 + 0.4645 + 0.0016, time: 21.731028]
2023-05-31 18:53:13.337: epoch 23:	0.02934602  	0.07889275  	0.06596968  
2023-05-31 18:53:13.338: Find a better model.
2023-05-31 18:53:35.010: [iter 24 : loss : 0.8029 = 0.3347 + 0.4663 + 0.0019, time: 21.669017]
2023-05-31 18:53:35.293: epoch 24:	0.02961253  	0.08048297  	0.06642736  
2023-05-31 18:53:35.294: Find a better model.
2023-05-31 18:53:56.998: [iter 25 : loss : 0.7703 = 0.3009 + 0.4672 + 0.0022, time: 21.700557]
2023-05-31 18:53:57.275: epoch 25:	0.02975320  	0.08077374  	0.06677377  
2023-05-31 18:53:57.275: Find a better model.
2023-05-31 18:54:18.794: [iter 26 : loss : 0.7429 = 0.2729 + 0.4675 + 0.0025, time: 21.516032]
2023-05-31 18:54:19.066: epoch 26:	0.03007894  	0.08204776  	0.06719508  
2023-05-31 18:54:19.066: Find a better model.
2023-05-31 18:54:40.743: [iter 27 : loss : 0.7193 = 0.2490 + 0.4675 + 0.0028, time: 21.674356]
2023-05-31 18:54:41.015: epoch 27:	0.03007893  	0.08221470  	0.06716754  
2023-05-31 18:54:41.015: Find a better model.
2023-05-31 18:55:02.761: [iter 28 : loss : 0.6993 = 0.2291 + 0.4672 + 0.0030, time: 21.740433]
2023-05-31 18:55:03.031: epoch 28:	0.03014556  	0.08271141  	0.06736065  
2023-05-31 18:55:03.031: Find a better model.
2023-05-31 18:55:24.771: [iter 29 : loss : 0.6825 = 0.2125 + 0.4667 + 0.0033, time: 21.736049]
2023-05-31 18:55:25.045: epoch 29:	0.03037506  	0.08353441  	0.06781197  
2023-05-31 18:55:25.045: Find a better model.
2023-05-31 18:55:46.949: [iter 30 : loss : 0.6666 = 0.1970 + 0.4661 + 0.0035, time: 21.899044]
2023-05-31 18:55:47.232: epoch 30:	0.03018999  	0.08279238  	0.06776751  
2023-05-31 18:56:08.912: [iter 31 : loss : 0.6531 = 0.1840 + 0.4654 + 0.0037, time: 21.676055]
2023-05-31 18:56:09.194: epoch 31:	0.03024922  	0.08343490  	0.06815880  
2023-05-31 18:56:30.917: [iter 32 : loss : 0.6419 = 0.1732 + 0.4647 + 0.0040, time: 21.718395]
2023-05-31 18:56:31.213: epoch 32:	0.03036027  	0.08367292  	0.06813732  
2023-05-31 18:56:31.213: Find a better model.
2023-05-31 18:56:53.110: [iter 33 : loss : 0.6319 = 0.1636 + 0.4641 + 0.0042, time: 21.893003]
2023-05-31 18:56:53.379: epoch 33:	0.03047870  	0.08382455  	0.06846387  
2023-05-31 18:56:53.379: Find a better model.
2023-05-31 18:57:15.112: [iter 34 : loss : 0.6226 = 0.1548 + 0.4634 + 0.0044, time: 21.729257]
2023-05-31 18:57:15.383: epoch 34:	0.03039727  	0.08304060  	0.06850212  
2023-05-31 18:57:37.110: [iter 35 : loss : 0.6136 = 0.1462 + 0.4628 + 0.0046, time: 21.723025]
2023-05-31 18:57:37.384: epoch 35:	0.03044910  	0.08320695  	0.06848880  
2023-05-31 18:57:59.121: [iter 36 : loss : 0.6067 = 0.1398 + 0.4621 + 0.0048, time: 21.734024]
2023-05-31 18:57:59.393: epoch 36:	0.03053053  	0.08351375  	0.06869140  
2023-05-31 18:58:21.327: [iter 37 : loss : 0.5998 = 0.1332 + 0.4617 + 0.0050, time: 21.929382]
2023-05-31 18:58:21.599: epoch 37:	0.03061937  	0.08378199  	0.06876526  
2023-05-31 18:58:43.696: [iter 38 : loss : 0.5934 = 0.1272 + 0.4611 + 0.0051, time: 22.092942]
2023-05-31 18:58:43.966: epoch 38:	0.03068600  	0.08352898  	0.06883604  
2023-05-31 18:59:05.874: [iter 39 : loss : 0.5880 = 0.1221 + 0.4606 + 0.0053, time: 21.904579]
2023-05-31 18:59:06.161: epoch 39:	0.03068600  	0.08366372  	0.06887167  
2023-05-31 18:59:27.835: [iter 40 : loss : 0.5821 = 0.1167 + 0.4600 + 0.0055, time: 21.669043]
2023-05-31 18:59:28.125: epoch 40:	0.03068600  	0.08329226  	0.06878716  
2023-05-31 18:59:50.057: [iter 41 : loss : 0.5774 = 0.1121 + 0.4596 + 0.0057, time: 21.928020]
2023-05-31 18:59:50.333: epoch 41:	0.03058235  	0.08289477  	0.06872132  
2023-05-31 19:00:12.313: [iter 42 : loss : 0.5727 = 0.1077 + 0.4592 + 0.0058, time: 21.976965]
2023-05-31 19:00:12.582: epoch 42:	0.03057493  	0.08242973  	0.06846251  
2023-05-31 19:00:35.146: [iter 43 : loss : 0.5686 = 0.1039 + 0.4588 + 0.0060, time: 22.560785]
2023-05-31 19:00:35.431: epoch 43:	0.03056013  	0.08252840  	0.06855766  
2023-05-31 19:00:51.592: my pid: 9780
2023-05-31 19:00:51.592: model: model.general_recommender.SGL
2023-05-31 19:00:51.592: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 19:00:51.592: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 19:00:55.841: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 19:01:17.621: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.779467]
2023-05-31 19:01:17.889: epoch 1:	0.00101421  	0.00200373  	0.00167393  
2023-05-31 19:01:17.889: Find a better model.
2023-05-31 19:01:39.664: [iter 2 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 21.772709]
2023-05-31 19:01:39.955: epoch 2:	0.00148801  	0.00351155  	0.00262133  
2023-05-31 19:01:39.955: Find a better model.
2023-05-31 19:02:01.629: [iter 3 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 21.670065]
2023-05-31 19:02:01.918: epoch 3:	0.00175452  	0.00359900  	0.00273670  
2023-05-31 19:02:01.918: Find a better model.
2023-05-31 19:02:23.426: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 21.503728]
2023-05-31 19:02:23.718: epoch 4:	0.00232455  	0.00483472  	0.00364930  
2023-05-31 19:02:23.718: Find a better model.
2023-05-31 19:02:45.245: [iter 5 : loss : 1.1342 = 0.6927 + 0.4414 + 0.0000, time: 21.522378]
2023-05-31 19:02:45.535: epoch 5:	0.00267249  	0.00587327  	0.00440416  
2023-05-31 19:02:45.535: Find a better model.
2023-05-31 19:03:07.027: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.488137]
2023-05-31 19:03:07.327: epoch 6:	0.00277613  	0.00649199  	0.00480900  
2023-05-31 19:03:07.328: Find a better model.
2023-05-31 19:03:28.990: [iter 7 : loss : 1.1344 = 0.6924 + 0.4421 + 0.0000, time: 21.659559]
2023-05-31 19:03:29.284: epoch 7:	0.00313888  	0.00771995  	0.00583086  
2023-05-31 19:03:29.284: Find a better model.
2023-05-31 19:03:50.796: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 21.508469]
2023-05-31 19:03:51.097: epoch 8:	0.00413089  	0.01111408  	0.00844033  
2023-05-31 19:03:51.097: Find a better model.
2023-05-31 19:04:12.584: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 21.479021]
2023-05-31 19:04:12.871: epoch 9:	0.00478975  	0.01340736  	0.01023522  
2023-05-31 19:04:12.871: Find a better model.
2023-05-31 19:04:34.385: [iter 10 : loss : 1.1343 = 0.6911 + 0.4432 + 0.0000, time: 21.509948]
2023-05-31 19:04:34.669: epoch 10:	0.00577434  	0.01660805  	0.01247420  
2023-05-31 19:04:34.669: Find a better model.
2023-05-31 19:04:56.793: [iter 11 : loss : 1.1340 = 0.6902 + 0.4437 + 0.0000, time: 22.118505]
2023-05-31 19:04:57.083: epoch 11:	0.00705505  	0.02003365  	0.01553973  
2023-05-31 19:04:57.084: Find a better model.
2023-05-31 19:05:19.172: [iter 12 : loss : 1.1329 = 0.6887 + 0.4442 + 0.0000, time: 22.079452]
2023-05-31 19:05:19.452: epoch 12:	0.00817289  	0.02287808  	0.01828095  
2023-05-31 19:05:19.452: Find a better model.
2023-05-31 19:05:41.600: [iter 13 : loss : 1.1311 = 0.6862 + 0.4449 + 0.0000, time: 22.143170]
2023-05-31 19:05:41.899: epoch 13:	0.00992741  	0.02764118  	0.02263745  
2023-05-31 19:05:41.899: Find a better model.
2023-05-31 19:06:03.922: [iter 14 : loss : 1.1280 = 0.6823 + 0.4456 + 0.0001, time: 22.018524]
2023-05-31 19:06:04.215: epoch 14:	0.01277020  	0.03627783  	0.02927626  
2023-05-31 19:06:04.215: Find a better model.
2023-05-31 19:06:25.956: [iter 15 : loss : 1.1225 = 0.6759 + 0.4465 + 0.0001, time: 21.736195]
2023-05-31 19:06:26.248: epoch 15:	0.01587954  	0.04476847  	0.03706026  
2023-05-31 19:06:26.248: Find a better model.
2023-05-31 19:06:48.128: [iter 16 : loss : 1.1122 = 0.6646 + 0.4474 + 0.0001, time: 21.877006]
2023-05-31 19:06:48.401: epoch 16:	0.01969959  	0.05438260  	0.04599759  
2023-05-31 19:06:48.401: Find a better model.
2023-05-31 19:07:10.115: [iter 17 : loss : 1.0949 = 0.6460 + 0.4487 + 0.0002, time: 21.708963]
2023-05-31 19:07:10.387: epoch 17:	0.02257946  	0.06147938  	0.05216123  
2023-05-31 19:07:10.387: Find a better model.
2023-05-31 19:07:32.106: [iter 18 : loss : 1.0672 = 0.6165 + 0.4504 + 0.0004, time: 21.714986]
2023-05-31 19:07:32.378: epoch 18:	0.02491148  	0.06677689  	0.05660939  
2023-05-31 19:07:32.378: Find a better model.
2023-05-31 19:07:54.296: [iter 19 : loss : 1.0292 = 0.5761 + 0.4526 + 0.0005, time: 21.914519]
2023-05-31 19:07:54.566: epoch 19:	0.02687334  	0.07078624  	0.06037479  
2023-05-31 19:07:54.566: Find a better model.
2023-05-31 19:08:16.307: [iter 20 : loss : 0.9832 = 0.5267 + 0.4557 + 0.0008, time: 21.737092]
2023-05-31 19:08:16.584: epoch 20:	0.02793202  	0.07415756  	0.06271003  
2023-05-31 19:08:16.584: Find a better model.
2023-05-31 19:08:38.329: [iter 21 : loss : 0.9337 = 0.4736 + 0.4592 + 0.0010, time: 21.741324]
2023-05-31 19:08:38.624: epoch 21:	0.02876120  	0.07622930  	0.06375732  
2023-05-31 19:08:38.624: Find a better model.
2023-05-31 19:09:00.686: [iter 22 : loss : 0.8844 = 0.4209 + 0.4622 + 0.0013, time: 22.056365]
2023-05-31 19:09:00.957: epoch 22:	0.02899810  	0.07754548  	0.06455010  
2023-05-31 19:09:00.957: Find a better model.
2023-05-31 19:09:22.841: [iter 23 : loss : 0.8406 = 0.3743 + 0.4647 + 0.0016, time: 21.879904]
2023-05-31 19:09:23.134: epoch 23:	0.02933865  	0.07861561  	0.06507985  
2023-05-31 19:09:23.134: Find a better model.
2023-05-31 19:09:44.843: [iter 24 : loss : 0.8025 = 0.3342 + 0.4664 + 0.0019, time: 21.706066]
2023-05-31 19:09:45.131: epoch 24:	0.02959035  	0.07945033  	0.06545498  
2023-05-31 19:09:45.131: Find a better model.
2023-05-31 19:10:07.061: [iter 25 : loss : 0.7698 = 0.3002 + 0.4674 + 0.0022, time: 21.926107]
2023-05-31 19:10:07.332: epoch 25:	0.02970140  	0.08020898  	0.06567178  
2023-05-31 19:10:07.332: Find a better model.
2023-05-31 19:10:29.225: [iter 26 : loss : 0.7425 = 0.2723 + 0.4677 + 0.0025, time: 21.889025]
2023-05-31 19:10:29.493: epoch 26:	0.02979765  	0.08072563  	0.06581512  
2023-05-31 19:10:29.493: Find a better model.
2023-05-31 19:10:51.448: [iter 27 : loss : 0.7189 = 0.2485 + 0.4676 + 0.0028, time: 21.950043]
2023-05-31 19:10:51.717: epoch 27:	0.02993091  	0.08103661  	0.06583060  
2023-05-31 19:10:51.717: Find a better model.
2023-05-31 19:11:13.639: [iter 28 : loss : 0.6992 = 0.2289 + 0.4673 + 0.0030, time: 21.919059]
2023-05-31 19:11:13.927: epoch 28:	0.02993091  	0.08107799  	0.06593972  
2023-05-31 19:11:13.927: Find a better model.
2023-05-31 19:11:35.785: [iter 29 : loss : 0.6822 = 0.2122 + 0.4667 + 0.0033, time: 21.855041]
2023-05-31 19:11:36.050: epoch 29:	0.03007898  	0.08147345  	0.06622931  
2023-05-31 19:11:36.050: Find a better model.
2023-05-31 19:11:57.803: [iter 30 : loss : 0.6667 = 0.1970 + 0.4662 + 0.0035, time: 21.749017]
2023-05-31 19:11:58.068: epoch 30:	0.03013079  	0.08172986  	0.06628211  
2023-05-31 19:11:58.069: Find a better model.
2023-05-31 19:12:19.806: [iter 31 : loss : 0.6534 = 0.1841 + 0.4656 + 0.0037, time: 21.733987]
2023-05-31 19:12:20.071: epoch 31:	0.03014561  	0.08184769  	0.06627820  
2023-05-31 19:12:20.071: Find a better model.
2023-05-31 19:12:41.992: [iter 32 : loss : 0.6419 = 0.1730 + 0.4649 + 0.0040, time: 21.916260]
2023-05-31 19:12:42.267: epoch 32:	0.03019001  	0.08191155  	0.06654730  
2023-05-31 19:12:42.267: Find a better model.
2023-05-31 19:13:04.165: [iter 33 : loss : 0.6319 = 0.1637 + 0.4641 + 0.0042, time: 21.893780]
2023-05-31 19:13:04.433: epoch 33:	0.03011599  	0.08161604  	0.06631634  
2023-05-31 19:13:26.223: [iter 34 : loss : 0.6225 = 0.1547 + 0.4634 + 0.0044, time: 21.787214]
2023-05-31 19:13:26.488: epoch 34:	0.03019742  	0.08140402  	0.06613620  
2023-05-31 19:13:48.359: [iter 35 : loss : 0.6136 = 0.1462 + 0.4629 + 0.0046, time: 21.866317]
2023-05-31 19:13:48.625: epoch 35:	0.03023443  	0.08152358  	0.06613465  
2023-05-31 19:14:10.782: [iter 36 : loss : 0.6064 = 0.1394 + 0.4622 + 0.0048, time: 22.152818]
2023-05-31 19:14:11.048: epoch 36:	0.03016780  	0.08124370  	0.06600512  
2023-05-31 19:14:32.971: [iter 37 : loss : 0.5998 = 0.1333 + 0.4616 + 0.0050, time: 21.920026]
2023-05-31 19:14:33.239: epoch 37:	0.03014559  	0.08165854  	0.06614454  
2023-05-31 19:14:55.178: [iter 38 : loss : 0.5936 = 0.1273 + 0.4612 + 0.0051, time: 21.933466]
2023-05-31 19:14:55.444: epoch 38:	0.03014560  	0.08142978  	0.06603054  
2023-05-31 19:15:17.525: [iter 39 : loss : 0.5882 = 0.1223 + 0.4606 + 0.0053, time: 22.078070]
2023-05-31 19:15:17.792: epoch 39:	0.03023442  	0.08168890  	0.06631196  
2023-05-31 19:15:40.199: [iter 40 : loss : 0.5823 = 0.1167 + 0.4601 + 0.0055, time: 22.401483]
2023-05-31 19:15:40.496: epoch 40:	0.03007155  	0.08100013  	0.06599890  
2023-05-31 19:15:51.312: my pid: 14100
2023-05-31 19:15:51.312: model: model.general_recommender.SGL
2023-05-31 19:15:51.312: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 19:15:51.312: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 19:15:55.544: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 19:16:17.465: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.921034]
2023-05-31 19:16:17.744: epoch 1:	0.00113266  	0.00234664  	0.00186531  
2023-05-31 19:16:17.744: Find a better model.
2023-05-31 19:16:39.560: [iter 2 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 21.812497]
2023-05-31 19:16:39.845: epoch 2:	0.00158425  	0.00375613  	0.00250507  
2023-05-31 19:16:39.846: Find a better model.
2023-05-31 19:17:01.413: [iter 3 : loss : 1.1338 = 0.6929 + 0.4408 + 0.0000, time: 21.563984]
2023-05-31 19:17:01.700: epoch 3:	0.00132514  	0.00270808  	0.00230611  
2023-05-31 19:17:23.378: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 21.675598]
2023-05-31 19:17:23.667: epoch 4:	0.00191738  	0.00391069  	0.00297978  
2023-05-31 19:17:23.667: Find a better model.
2023-05-31 19:17:45.150: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.479202]
2023-05-31 19:17:45.436: epoch 5:	0.00202103  	0.00496100  	0.00394978  
2023-05-31 19:17:45.436: Find a better model.
2023-05-31 19:18:06.929: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.488347]
2023-05-31 19:18:07.223: epoch 6:	0.00260586  	0.00723163  	0.00558020  
2023-05-31 19:18:07.223: Find a better model.
2023-05-31 19:18:28.783: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 21.557381]
2023-05-31 19:18:29.066: epoch 7:	0.00312408  	0.00803127  	0.00604879  
2023-05-31 19:18:29.066: Find a better model.
2023-05-31 19:18:50.510: [iter 8 : loss : 1.1344 = 0.6921 + 0.4423 + 0.0000, time: 21.441050]
2023-05-31 19:18:50.813: epoch 8:	0.00373112  	0.00996962  	0.00776596  
2023-05-31 19:18:50.813: Find a better model.
2023-05-31 19:19:12.238: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 21.422319]
2023-05-31 19:19:12.525: epoch 9:	0.00460467  	0.01304535  	0.01031070  
2023-05-31 19:19:12.525: Find a better model.
2023-05-31 19:19:33.739: [iter 10 : loss : 1.1343 = 0.6912 + 0.4431 + 0.0000, time: 21.210497]
2023-05-31 19:19:34.018: epoch 10:	0.00584096  	0.01679680  	0.01342445  
2023-05-31 19:19:34.018: Find a better model.
2023-05-31 19:19:55.941: [iter 11 : loss : 1.1340 = 0.6905 + 0.4435 + 0.0000, time: 21.919088]
2023-05-31 19:19:56.232: epoch 11:	0.00703283  	0.02116533  	0.01669580  
2023-05-31 19:19:56.232: Find a better model.
2023-05-31 19:20:18.135: [iter 12 : loss : 1.1331 = 0.6891 + 0.4440 + 0.0000, time: 21.899408]
2023-05-31 19:20:18.410: epoch 12:	0.00806185  	0.02333749  	0.01943889  
2023-05-31 19:20:18.410: Find a better model.
2023-05-31 19:20:40.256: [iter 13 : loss : 1.1315 = 0.6868 + 0.4447 + 0.0000, time: 21.842111]
2023-05-31 19:20:40.547: epoch 13:	0.00992742  	0.02756016  	0.02304470  
2023-05-31 19:20:40.547: Find a better model.
2023-05-31 19:21:02.290: [iter 14 : loss : 1.1286 = 0.6830 + 0.4455 + 0.0001, time: 21.739135]
2023-05-31 19:21:02.565: epoch 14:	0.01236302  	0.03458560  	0.02965995  
2023-05-31 19:21:02.566: Find a better model.
2023-05-31 19:21:24.251: [iter 15 : loss : 1.1233 = 0.6769 + 0.4463 + 0.0001, time: 21.681606]
2023-05-31 19:21:24.546: epoch 15:	0.01508739  	0.04130898  	0.03613789  
2023-05-31 19:21:24.546: Find a better model.
2023-05-31 19:21:46.251: [iter 16 : loss : 1.1137 = 0.6663 + 0.4472 + 0.0001, time: 21.699864]
2023-05-31 19:21:46.522: epoch 16:	0.01835959  	0.04990512  	0.04332231  
2023-05-31 19:21:46.522: Find a better model.
2023-05-31 19:22:08.233: [iter 17 : loss : 1.0973 = 0.6486 + 0.4485 + 0.0002, time: 21.706316]
2023-05-31 19:22:08.523: epoch 17:	0.02178730  	0.05746156  	0.05000683  
2023-05-31 19:22:08.523: Find a better model.
2023-05-31 19:22:30.396: [iter 18 : loss : 1.0708 = 0.6203 + 0.4501 + 0.0003, time: 21.868164]
2023-05-31 19:22:30.667: epoch 18:	0.02439325  	0.06478095  	0.05539229  
2023-05-31 19:22:30.667: Find a better model.
2023-05-31 19:22:52.410: [iter 19 : loss : 1.0337 = 0.5810 + 0.4523 + 0.0005, time: 21.739177]
2023-05-31 19:22:52.681: epoch 19:	0.02654017  	0.07074840  	0.05961984  
2023-05-31 19:22:52.681: Find a better model.
2023-05-31 19:23:14.401: [iter 20 : loss : 0.9882 = 0.5323 + 0.4551 + 0.0007, time: 21.715800]
2023-05-31 19:23:14.672: epoch 20:	0.02744338  	0.07354386  	0.06192980  
2023-05-31 19:23:14.672: Find a better model.
2023-05-31 19:23:36.291: [iter 21 : loss : 0.9389 = 0.4794 + 0.4584 + 0.0010, time: 21.615041]
2023-05-31 19:23:36.564: epoch 21:	0.02832437  	0.07557856  	0.06332475  
2023-05-31 19:23:36.564: Find a better model.
2023-05-31 19:23:58.164: [iter 22 : loss : 0.8893 = 0.4265 + 0.4615 + 0.0013, time: 21.594946]
2023-05-31 19:23:58.431: epoch 22:	0.02870195  	0.07688247  	0.06405494  
2023-05-31 19:23:58.431: Find a better model.
2023-05-31 19:24:20.017: [iter 23 : loss : 0.8449 = 0.3792 + 0.4641 + 0.0016, time: 21.583473]
2023-05-31 19:24:20.288: epoch 23:	0.02910173  	0.07792953  	0.06457206  
2023-05-31 19:24:20.288: Find a better model.
2023-05-31 19:24:42.011: [iter 24 : loss : 0.8059 = 0.3382 + 0.4658 + 0.0019, time: 21.719024]
2023-05-31 19:24:42.282: epoch 24:	0.02915355  	0.07820103  	0.06479342  
2023-05-31 19:24:42.283: Find a better model.
2023-05-31 19:25:03.969: [iter 25 : loss : 0.7725 = 0.3034 + 0.4669 + 0.0022, time: 21.683028]
2023-05-31 19:25:04.240: epoch 25:	0.02924238  	0.07903807  	0.06529404  
2023-05-31 19:25:04.240: Find a better model.
2023-05-31 19:25:25.933: [iter 26 : loss : 0.7446 = 0.2748 + 0.4673 + 0.0025, time: 21.688464]
2023-05-31 19:25:26.206: epoch 26:	0.02927200  	0.07974664  	0.06549048  
2023-05-31 19:25:26.206: Find a better model.
2023-05-31 19:25:47.981: [iter 27 : loss : 0.7206 = 0.2506 + 0.4672 + 0.0028, time: 21.772046]
2023-05-31 19:25:48.250: epoch 27:	0.02940527  	0.07998300  	0.06564584  
2023-05-31 19:25:48.250: Find a better model.
2023-05-31 19:26:09.945: [iter 28 : loss : 0.7005 = 0.2306 + 0.4669 + 0.0030, time: 21.691108]
2023-05-31 19:26:10.220: epoch 28:	0.02965699  	0.08053316  	0.06607078  
2023-05-31 19:26:10.220: Find a better model.
2023-05-31 19:26:32.155: [iter 29 : loss : 0.6829 = 0.2133 + 0.4663 + 0.0033, time: 21.931499]
2023-05-31 19:26:32.421: epoch 29:	0.02966440  	0.08016085  	0.06608626  
2023-05-31 19:26:54.116: [iter 30 : loss : 0.6674 = 0.1981 + 0.4658 + 0.0035, time: 21.692058]
2023-05-31 19:26:54.379: epoch 30:	0.02961997  	0.07966790  	0.06594613  
2023-05-31 19:27:16.160: [iter 31 : loss : 0.6540 = 0.1851 + 0.4652 + 0.0037, time: 21.777062]
2023-05-31 19:27:16.422: epoch 31:	0.02970140  	0.07997718  	0.06619126  
2023-05-31 19:27:38.115: [iter 32 : loss : 0.6424 = 0.1739 + 0.4646 + 0.0039, time: 21.689011]
2023-05-31 19:27:38.378: epoch 32:	0.02988649  	0.08061466  	0.06644946  
2023-05-31 19:27:38.378: Find a better model.
2023-05-31 19:28:00.116: [iter 33 : loss : 0.6324 = 0.1644 + 0.4638 + 0.0042, time: 21.735186]
2023-05-31 19:28:00.380: epoch 33:	0.02990869  	0.08088754  	0.06669226  
2023-05-31 19:28:00.380: Find a better model.
2023-05-31 19:28:22.151: [iter 34 : loss : 0.6231 = 0.1556 + 0.4631 + 0.0044, time: 21.766396]
2023-05-31 19:28:22.433: epoch 34:	0.03003454  	0.08125190  	0.06675658  
2023-05-31 19:28:22.433: Find a better model.
2023-05-31 19:28:44.122: [iter 35 : loss : 0.6137 = 0.1467 + 0.4624 + 0.0046, time: 21.684439]
2023-05-31 19:28:44.388: epoch 35:	0.03000493  	0.08107132  	0.06663001  
2023-05-31 19:29:06.285: [iter 36 : loss : 0.6066 = 0.1399 + 0.4619 + 0.0048, time: 21.892662]
2023-05-31 19:29:06.549: epoch 36:	0.02987907  	0.08069713  	0.06643113  
2023-05-31 19:29:28.519: [iter 37 : loss : 0.6000 = 0.1337 + 0.4614 + 0.0049, time: 21.967210]
2023-05-31 19:29:28.780: epoch 37:	0.02977543  	0.08049065  	0.06644041  
2023-05-31 19:29:50.658: [iter 38 : loss : 0.5933 = 0.1275 + 0.4607 + 0.0051, time: 21.874113]
2023-05-31 19:29:50.920: epoch 38:	0.02976802  	0.08064261  	0.06653725  
2023-05-31 19:30:13.096: [iter 39 : loss : 0.5876 = 0.1221 + 0.4602 + 0.0053, time: 22.172408]
2023-05-31 19:30:13.366: epoch 39:	0.02972360  	0.08061276  	0.06658126  
2023-05-31 19:30:35.950: [iter 40 : loss : 0.5823 = 0.1172 + 0.4597 + 0.0055, time: 22.580082]
2023-05-31 19:30:36.220: epoch 40:	0.02967178  	0.08041801  	0.06646504  
2023-05-31 19:30:42.331: my pid: 13652
2023-05-31 19:30:42.331: model: model.general_recommender.SGL
2023-05-31 19:30:42.331: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 19:30:42.332: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 19:30:44.774: my pid: 11980
2023-05-31 19:30:44.774: model: model.general_recommender.SGL
2023-05-31 19:30:44.774: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 19:30:44.774: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 19:30:49.075: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 19:31:11.456: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.381113]
2023-05-31 19:31:11.736: epoch 1:	0.00134735  	0.00317163  	0.00234620  
2023-05-31 19:31:11.736: Find a better model.
2023-05-31 19:31:33.477: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 21.737812]
2023-05-31 19:31:33.764: epoch 2:	0.00142138  	0.00279951  	0.00238017  
2023-05-31 19:31:55.302: [iter 3 : loss : 1.1337 = 0.6929 + 0.4407 + 0.0000, time: 21.533421]
2023-05-31 19:31:55.590: epoch 3:	0.00173231  	0.00356023  	0.00281795  
2023-05-31 19:31:55.590: Find a better model.
2023-05-31 19:32:17.258: [iter 4 : loss : 1.1338 = 0.6929 + 0.4410 + 0.0000, time: 21.664079]
2023-05-31 19:32:17.540: epoch 4:	0.00182855  	0.00362390  	0.00295197  
2023-05-31 19:32:17.540: Find a better model.
2023-05-31 19:32:39.095: [iter 5 : loss : 1.1340 = 0.6927 + 0.4413 + 0.0000, time: 21.552596]
2023-05-31 19:32:39.380: epoch 5:	0.00199141  	0.00449783  	0.00328260  
2023-05-31 19:32:39.380: Find a better model.
2023-05-31 19:33:00.886: [iter 6 : loss : 1.1341 = 0.6926 + 0.4415 + 0.0000, time: 21.501552]
2023-05-31 19:33:01.182: epoch 6:	0.00239858  	0.00615499  	0.00431900  
2023-05-31 19:33:01.182: Find a better model.
2023-05-31 19:33:22.656: [iter 7 : loss : 1.1343 = 0.6924 + 0.4418 + 0.0000, time: 21.470519]
2023-05-31 19:33:22.934: epoch 7:	0.00302043  	0.00756864  	0.00573666  
2023-05-31 19:33:22.934: Find a better model.
2023-05-31 19:33:44.491: [iter 8 : loss : 1.1343 = 0.6922 + 0.4421 + 0.0000, time: 21.553052]
2023-05-31 19:33:44.788: epoch 8:	0.00394581  	0.01067907  	0.00784181  
2023-05-31 19:33:44.788: Find a better model.
2023-05-31 19:34:06.230: [iter 9 : loss : 1.1344 = 0.6918 + 0.4425 + 0.0000, time: 21.437060]
2023-05-31 19:34:06.511: epoch 9:	0.00450843  	0.01236634  	0.00946433  
2023-05-31 19:34:06.511: Find a better model.
2023-05-31 19:34:27.846: [iter 10 : loss : 1.1343 = 0.6913 + 0.4429 + 0.0000, time: 21.331140]
2023-05-31 19:34:28.139: epoch 10:	0.00521171  	0.01450150  	0.01103851  
2023-05-31 19:34:28.139: Find a better model.
2023-05-31 19:34:50.018: [iter 11 : loss : 1.1339 = 0.6906 + 0.4432 + 0.0000, time: 21.875771]
2023-05-31 19:34:50.292: epoch 11:	0.00644801  	0.01760346  	0.01395863  
2023-05-31 19:34:50.292: Find a better model.
2023-05-31 19:35:12.237: [iter 12 : loss : 1.1332 = 0.6894 + 0.4438 + 0.0000, time: 21.940020]
2023-05-31 19:35:12.508: epoch 12:	0.00778053  	0.02207202  	0.01719127  
2023-05-31 19:35:12.508: Find a better model.
2023-05-31 19:35:34.392: [iter 13 : loss : 1.1318 = 0.6873 + 0.4444 + 0.0000, time: 21.879395]
2023-05-31 19:35:34.682: epoch 13:	0.00917229  	0.02613906  	0.02081179  
2023-05-31 19:35:34.682: Find a better model.
2023-05-31 19:35:56.426: [iter 14 : loss : 1.1290 = 0.6837 + 0.4452 + 0.0001, time: 21.740113]
2023-05-31 19:35:56.693: epoch 14:	0.01188922  	0.03388873  	0.02773102  
2023-05-31 19:35:56.693: Find a better model.
2023-05-31 19:36:18.785: [iter 15 : loss : 1.1241 = 0.6781 + 0.4460 + 0.0001, time: 22.087078]
2023-05-31 19:36:19.054: epoch 15:	0.01550937  	0.04309976  	0.03640496  
2023-05-31 19:36:19.054: Find a better model.
2023-05-31 19:36:40.985: [iter 16 : loss : 1.1154 = 0.6683 + 0.4470 + 0.0001, time: 21.927006]
2023-05-31 19:36:41.271: epoch 16:	0.01854468  	0.05015667  	0.04335863  
2023-05-31 19:36:41.272: Find a better model.
2023-05-31 19:37:03.166: [iter 17 : loss : 1.1001 = 0.6518 + 0.4481 + 0.0002, time: 21.890503]
2023-05-31 19:37:03.452: epoch 17:	0.02177250  	0.05877539  	0.05052062  
2023-05-31 19:37:03.452: Find a better model.
2023-05-31 19:37:24.993: [iter 18 : loss : 1.0748 = 0.6249 + 0.4496 + 0.0003, time: 21.537045]
2023-05-31 19:37:25.273: epoch 18:	0.02416378  	0.06504958  	0.05550298  
2023-05-31 19:37:25.273: Find a better model.
2023-05-31 19:37:47.145: [iter 19 : loss : 1.0391 = 0.5868 + 0.4518 + 0.0005, time: 21.867108]
2023-05-31 19:37:47.413: epoch 19:	0.02615522  	0.07000383  	0.05908686  
2023-05-31 19:37:47.413: Find a better model.
2023-05-31 19:38:09.158: [iter 20 : loss : 0.9943 = 0.5389 + 0.4546 + 0.0007, time: 21.741052]
2023-05-31 19:38:09.423: epoch 20:	0.02750261  	0.07314172  	0.06179041  
2023-05-31 19:38:09.423: Find a better model.
2023-05-31 19:38:31.330: [iter 21 : loss : 0.9447 = 0.4857 + 0.4580 + 0.0010, time: 21.903437]
2023-05-31 19:38:31.596: epoch 21:	0.02871675  	0.07662674  	0.06387217  
2023-05-31 19:38:31.596: Find a better model.
2023-05-31 19:38:53.525: [iter 22 : loss : 0.8945 = 0.4321 + 0.4612 + 0.0013, time: 21.926232]
2023-05-31 19:38:53.789: epoch 22:	0.02906470  	0.07753353  	0.06448168  
2023-05-31 19:38:53.789: Find a better model.
2023-05-31 19:39:15.704: [iter 23 : loss : 0.8491 = 0.3837 + 0.4639 + 0.0016, time: 21.910086]
2023-05-31 19:39:15.966: epoch 23:	0.02925719  	0.07844820  	0.06474008  
2023-05-31 19:39:15.966: Find a better model.
2023-05-31 19:39:37.712: [iter 24 : loss : 0.8096 = 0.3419 + 0.4658 + 0.0019, time: 21.741371]
2023-05-31 19:39:37.978: epoch 24:	0.02937564  	0.07891697  	0.06530546  
2023-05-31 19:39:37.978: Find a better model.
2023-05-31 19:39:59.923: [iter 25 : loss : 0.7755 = 0.3065 + 0.4668 + 0.0022, time: 21.941648]
2023-05-31 19:40:00.201: epoch 25:	0.02954593  	0.07987846  	0.06564900  
2023-05-31 19:40:00.201: Find a better model.
2023-05-31 19:40:21.919: [iter 26 : loss : 0.7470 = 0.2774 + 0.4672 + 0.0024, time: 21.713661]
2023-05-31 19:40:22.192: epoch 26:	0.02975322  	0.08087726  	0.06620002  
2023-05-31 19:40:22.192: Find a better model.
2023-05-31 19:40:44.119: [iter 27 : loss : 0.7226 = 0.2526 + 0.4673 + 0.0027, time: 21.921547]
2023-05-31 19:40:44.379: epoch 27:	0.02989388  	0.08126263  	0.06643115  
2023-05-31 19:40:44.379: Find a better model.
2023-05-31 19:41:06.287: [iter 28 : loss : 0.7023 = 0.2323 + 0.4670 + 0.0030, time: 21.902980]
2023-05-31 19:41:06.548: epoch 28:	0.03002714  	0.08109422  	0.06662241  
2023-05-31 19:41:28.634: [iter 29 : loss : 0.6844 = 0.2148 + 0.4664 + 0.0032, time: 22.083025]
2023-05-31 19:41:28.895: epoch 29:	0.03007156  	0.08135448  	0.06702422  
2023-05-31 19:41:28.895: Find a better model.
2023-05-31 19:41:51.114: [iter 30 : loss : 0.6687 = 0.1993 + 0.4659 + 0.0035, time: 22.215038]
2023-05-31 19:41:51.376: epoch 30:	0.03005677  	0.08139829  	0.06706426  
2023-05-31 19:41:51.376: Find a better model.
2023-05-31 19:42:13.631: [iter 31 : loss : 0.6551 = 0.1862 + 0.4652 + 0.0037, time: 22.251446]
2023-05-31 19:42:13.891: epoch 31:	0.03017520  	0.08144581  	0.06716408  
2023-05-31 19:42:13.891: Find a better model.
2023-05-31 19:42:36.030: [iter 32 : loss : 0.6433 = 0.1749 + 0.4645 + 0.0039, time: 22.135344]
2023-05-31 19:42:36.296: epoch 32:	0.03020481  	0.08139247  	0.06722123  
2023-05-31 19:42:58.256: [iter 33 : loss : 0.6336 = 0.1656 + 0.4638 + 0.0041, time: 21.956015]
2023-05-31 19:42:58.536: epoch 33:	0.03026403  	0.08161911  	0.06734663  
2023-05-31 19:42:58.536: Find a better model.
2023-05-31 19:43:20.822: [iter 34 : loss : 0.6239 = 0.1564 + 0.4632 + 0.0043, time: 22.283400]
2023-05-31 19:43:21.083: epoch 34:	0.03021961  	0.08157258  	0.06738152  
2023-05-31 19:43:43.220: [iter 35 : loss : 0.6144 = 0.1474 + 0.4624 + 0.0045, time: 22.132819]
2023-05-31 19:43:43.483: epoch 35:	0.03019740  	0.08140264  	0.06737110  
2023-05-31 19:44:05.794: [iter 36 : loss : 0.6070 = 0.1404 + 0.4619 + 0.0047, time: 22.306529]
2023-05-31 19:44:06.055: epoch 36:	0.03019741  	0.08128107  	0.06750111  
2023-05-31 19:44:28.218: [iter 37 : loss : 0.6002 = 0.1340 + 0.4613 + 0.0049, time: 22.160036]
2023-05-31 19:44:28.476: epoch 37:	0.03026405  	0.08151099  	0.06759953  
2023-05-31 19:44:50.797: [iter 38 : loss : 0.5939 = 0.1282 + 0.4606 + 0.0051, time: 22.317159]
2023-05-31 19:44:51.056: epoch 38:	0.03036029  	0.08166677  	0.06755043  
2023-05-31 19:44:51.056: Find a better model.
2023-05-31 19:45:13.415: [iter 39 : loss : 0.5886 = 0.1230 + 0.4603 + 0.0053, time: 22.355981]
2023-05-31 19:45:13.673: epoch 39:	0.03040471  	0.08172221  	0.06750544  
2023-05-31 19:45:13.673: Find a better model.
2023-05-31 19:45:36.179: [iter 40 : loss : 0.5829 = 0.1177 + 0.4597 + 0.0055, time: 22.502043]
2023-05-31 19:45:36.438: epoch 40:	0.03043432  	0.08167647  	0.06763086  
2023-05-31 19:45:58.573: [iter 41 : loss : 0.5777 = 0.1129 + 0.4592 + 0.0056, time: 22.129540]
2023-05-31 19:45:58.833: epoch 41:	0.03035289  	0.08134500  	0.06752775  
2023-05-31 19:46:21.165: [iter 42 : loss : 0.5730 = 0.1084 + 0.4589 + 0.0058, time: 22.328469]
2023-05-31 19:46:21.424: epoch 42:	0.03038250  	0.08120745  	0.06758093  
2023-05-31 19:46:43.717: [iter 43 : loss : 0.5687 = 0.1043 + 0.4584 + 0.0059, time: 22.288071]
2023-05-31 19:46:43.994: epoch 43:	0.03014559  	0.08041619  	0.06726503  
2023-05-31 19:47:06.326: [iter 44 : loss : 0.5662 = 0.1021 + 0.4580 + 0.0061, time: 22.329065]
2023-05-31 19:47:06.607: epoch 44:	0.03016780  	0.08068288  	0.06731198  
2023-05-31 19:47:28.946: [iter 45 : loss : 0.5620 = 0.0982 + 0.4576 + 0.0063, time: 22.333434]
2023-05-31 19:47:29.226: epoch 45:	0.03007155  	0.08048951  	0.06723259  
2023-05-31 19:47:51.752: [iter 46 : loss : 0.5585 = 0.0949 + 0.4572 + 0.0064, time: 22.523005]
2023-05-31 19:47:52.014: epoch 46:	0.02979765  	0.07983704  	0.06680071  
2023-05-31 19:48:14.543: [iter 47 : loss : 0.5551 = 0.0917 + 0.4569 + 0.0066, time: 22.525633]
2023-05-31 19:48:14.806: epoch 47:	0.02978283  	0.07976141  	0.06674398  
2023-05-31 19:48:37.311: [iter 48 : loss : 0.5524 = 0.0891 + 0.4566 + 0.0067, time: 22.501701]
2023-05-31 19:48:37.572: epoch 48:	0.02982725  	0.07974999  	0.06668258  
2023-05-31 19:49:00.105: [iter 49 : loss : 0.5497 = 0.0866 + 0.4563 + 0.0068, time: 22.530233]
2023-05-31 19:49:00.364: epoch 49:	0.02974581  	0.07948743  	0.06640189  
2023-05-31 19:49:22.724: [iter 50 : loss : 0.5475 = 0.0845 + 0.4560 + 0.0070, time: 22.356500]
2023-05-31 19:49:22.988: epoch 50:	0.02976063  	0.07945280  	0.06630087  
2023-05-31 19:49:45.477: [iter 51 : loss : 0.5446 = 0.0818 + 0.4557 + 0.0071, time: 22.485090]
2023-05-31 19:49:45.740: epoch 51:	0.02961255  	0.07907106  	0.06598644  
2023-05-31 19:50:08.291: [iter 52 : loss : 0.5418 = 0.0791 + 0.4554 + 0.0072, time: 22.546960]
2023-05-31 19:50:08.553: epoch 52:	0.02964958  	0.07942145  	0.06625282  
2023-05-31 19:50:31.068: [iter 53 : loss : 0.5408 = 0.0783 + 0.4552 + 0.0074, time: 22.511003]
2023-05-31 19:50:31.333: epoch 53:	0.02953112  	0.07878490  	0.06589837  
2023-05-31 19:50:53.877: [iter 54 : loss : 0.5385 = 0.0761 + 0.4548 + 0.0075, time: 22.539219]
2023-05-31 19:50:54.151: epoch 54:	0.02940525  	0.07843103  	0.06584701  
2023-05-31 19:51:16.469: [iter 55 : loss : 0.5364 = 0.0741 + 0.4547 + 0.0076, time: 22.314053]
2023-05-31 19:51:16.729: epoch 55:	0.02942746  	0.07856107  	0.06583619  
2023-05-31 19:51:39.071: [iter 56 : loss : 0.5341 = 0.0719 + 0.4544 + 0.0078, time: 22.336998]
2023-05-31 19:51:39.332: epoch 56:	0.02939044  	0.07808990  	0.06558653  
2023-05-31 19:52:01.842: [iter 57 : loss : 0.5323 = 0.0701 + 0.4544 + 0.0079, time: 22.507034]
2023-05-31 19:52:02.113: epoch 57:	0.02929420  	0.07777349  	0.06536628  
2023-05-31 19:52:24.836: [iter 58 : loss : 0.5308 = 0.0687 + 0.4541 + 0.0080, time: 22.714564]
2023-05-31 19:52:25.131: epoch 58:	0.02924238  	0.07740152  	0.06524912  
2023-05-31 19:52:47.281: [iter 59 : loss : 0.5291 = 0.0670 + 0.4540 + 0.0081, time: 22.145890]
2023-05-31 19:52:47.545: epoch 59:	0.02923498  	0.07747005  	0.06515440  
2023-05-31 19:53:10.502: [iter 60 : loss : 0.5279 = 0.0659 + 0.4538 + 0.0082, time: 22.954451]
2023-05-31 19:53:10.771: epoch 60:	0.02910912  	0.07719005  	0.06501907  
2023-05-31 19:53:17.916: my pid: 3364
2023-05-31 19:53:17.916: model: model.general_recommender.SGL
2023-05-31 19:53:17.916: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 19:53:17.916: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 19:53:21.970: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 19:53:44.565: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.594349]
2023-05-31 19:53:44.832: epoch 1:	0.00108084  	0.00250235  	0.00185243  
2023-05-31 19:53:44.833: Find a better model.
2023-05-31 19:54:07.454: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 22.617937]
2023-05-31 19:54:07.740: epoch 2:	0.00130293  	0.00283148  	0.00229384  
2023-05-31 19:54:07.740: Find a better model.
2023-05-31 19:54:30.072: [iter 3 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 22.327189]
2023-05-31 19:54:30.361: epoch 3:	0.00155464  	0.00335119  	0.00264216  
2023-05-31 19:54:30.361: Find a better model.
2023-05-31 19:54:52.652: [iter 4 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 22.287241]
2023-05-31 19:54:52.940: epoch 4:	0.00178413  	0.00383510  	0.00321281  
2023-05-31 19:54:52.940: Find a better model.
2023-05-31 19:55:15.231: [iter 5 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 22.286406]
2023-05-31 19:55:15.522: epoch 5:	0.00213207  	0.00466393  	0.00373668  
2023-05-31 19:55:15.522: Find a better model.
2023-05-31 19:55:37.823: [iter 6 : loss : 1.1341 = 0.6926 + 0.4414 + 0.0000, time: 22.297084]
2023-05-31 19:55:38.128: epoch 6:	0.00236897  	0.00543498  	0.00427514  
2023-05-31 19:55:38.129: Find a better model.
2023-05-31 19:56:00.288: [iter 7 : loss : 1.1342 = 0.6925 + 0.4417 + 0.0000, time: 22.154519]
2023-05-31 19:56:00.580: epoch 7:	0.00310187  	0.00785033  	0.00607974  
2023-05-31 19:56:00.580: Find a better model.
2023-05-31 19:56:22.829: [iter 8 : loss : 1.1342 = 0.6922 + 0.4420 + 0.0000, time: 22.245589]
2023-05-31 19:56:23.133: epoch 8:	0.00366450  	0.00911707  	0.00727763  
2023-05-31 19:56:23.133: Find a better model.
2023-05-31 19:56:45.217: [iter 9 : loss : 1.1342 = 0.6919 + 0.4423 + 0.0000, time: 22.080041]
2023-05-31 19:56:45.506: epoch 9:	0.00459727  	0.01177415  	0.00961250  
2023-05-31 19:56:45.506: Find a better model.
2023-05-31 19:57:07.452: [iter 10 : loss : 1.1342 = 0.6915 + 0.4427 + 0.0000, time: 21.941643]
2023-05-31 19:57:07.737: epoch 10:	0.00544861  	0.01436802  	0.01146526  
2023-05-31 19:57:07.737: Find a better model.
2023-05-31 19:57:30.218: [iter 11 : loss : 1.1339 = 0.6908 + 0.4431 + 0.0000, time: 22.476620]
2023-05-31 19:57:30.506: epoch 11:	0.00695881  	0.01962628  	0.01558838  
2023-05-31 19:57:30.507: Find a better model.
2023-05-31 19:57:52.995: [iter 12 : loss : 1.1333 = 0.6898 + 0.4435 + 0.0000, time: 22.485306]
2023-05-31 19:57:53.287: epoch 12:	0.00811367  	0.02343210  	0.01859096  
2023-05-31 19:57:53.287: Find a better model.
2023-05-31 19:58:15.969: [iter 13 : loss : 1.1322 = 0.6881 + 0.4441 + 0.0000, time: 22.677332]
2023-05-31 19:58:16.261: epoch 13:	0.01008287  	0.02833700  	0.02363391  
2023-05-31 19:58:16.261: Find a better model.
2023-05-31 19:58:38.599: [iter 14 : loss : 1.1298 = 0.6850 + 0.4447 + 0.0001, time: 22.335433]
2023-05-31 19:58:38.874: epoch 14:	0.01182260  	0.03337791  	0.02769164  
2023-05-31 19:58:38.874: Find a better model.
2023-05-31 19:59:01.359: [iter 15 : loss : 1.1255 = 0.6799 + 0.4455 + 0.0001, time: 22.482051]
2023-05-31 19:59:01.633: epoch 15:	0.01480607  	0.04057644  	0.03409467  
2023-05-31 19:59:01.633: Find a better model.
2023-05-31 19:59:24.162: [iter 16 : loss : 1.1178 = 0.6711 + 0.4465 + 0.0001, time: 22.524585]
2023-05-31 19:59:24.452: epoch 16:	0.01795242  	0.04849519  	0.04090418  
2023-05-31 19:59:24.452: Find a better model.
2023-05-31 19:59:46.764: [iter 17 : loss : 1.1042 = 0.6564 + 0.4476 + 0.0002, time: 22.307405]
2023-05-31 19:59:47.064: epoch 17:	0.02126909  	0.05660921  	0.04832945  
2023-05-31 19:59:47.065: Find a better model.
2023-05-31 20:00:09.565: [iter 18 : loss : 1.0813 = 0.6319 + 0.4492 + 0.0003, time: 22.496074]
2023-05-31 20:00:09.843: epoch 18:	0.02396386  	0.06390032  	0.05430456  
2023-05-31 20:00:09.843: Find a better model.
2023-05-31 20:00:32.519: [iter 19 : loss : 1.0478 = 0.5961 + 0.4512 + 0.0004, time: 22.671172]
2023-05-31 20:00:32.795: epoch 19:	0.02599978  	0.06901937  	0.05859879  
2023-05-31 20:00:32.795: Find a better model.
2023-05-31 20:00:55.321: [iter 20 : loss : 1.0045 = 0.5499 + 0.4540 + 0.0007, time: 22.521135]
2023-05-31 20:00:55.593: epoch 20:	0.02723614  	0.07236022  	0.06166380  
2023-05-31 20:00:55.593: Find a better model.
2023-05-31 20:01:17.932: [iter 21 : loss : 0.9554 = 0.4973 + 0.4572 + 0.0009, time: 22.336143]
2023-05-31 20:01:18.212: epoch 21:	0.02832439  	0.07574686  	0.06382473  
2023-05-31 20:01:18.212: Find a better model.
2023-05-31 20:01:40.510: [iter 22 : loss : 0.9047 = 0.4431 + 0.4604 + 0.0012, time: 22.293919]
2023-05-31 20:01:40.774: epoch 22:	0.02876859  	0.07720149  	0.06481691  
2023-05-31 20:01:40.774: Find a better model.
2023-05-31 20:02:02.918: [iter 23 : loss : 0.8583 = 0.3934 + 0.4634 + 0.0015, time: 22.140140]
2023-05-31 20:02:03.194: epoch 23:	0.02910172  	0.07757772  	0.06527526  
2023-05-31 20:02:03.194: Find a better model.
2023-05-31 20:02:25.662: [iter 24 : loss : 0.8173 = 0.3500 + 0.4655 + 0.0018, time: 22.464466]
2023-05-31 20:02:25.929: epoch 24:	0.02924979  	0.07848199  	0.06579541  
2023-05-31 20:02:25.929: Find a better model.
2023-05-31 20:02:48.429: [iter 25 : loss : 0.7821 = 0.3135 + 0.4665 + 0.0021, time: 22.496068]
2023-05-31 20:02:48.697: epoch 25:	0.02929419  	0.07910579  	0.06582121  
2023-05-31 20:02:48.697: Find a better model.
2023-05-31 20:03:11.105: [iter 26 : loss : 0.7527 = 0.2832 + 0.4671 + 0.0024, time: 22.405095]
2023-05-31 20:03:11.388: epoch 26:	0.02947928  	0.08022833  	0.06626888  
2023-05-31 20:03:11.388: Find a better model.
2023-05-31 20:03:33.874: [iter 27 : loss : 0.7274 = 0.2577 + 0.4671 + 0.0027, time: 22.480443]
2023-05-31 20:03:34.157: epoch 27:	0.02964216  	0.08072341  	0.06663355  
2023-05-31 20:03:34.157: Find a better model.
2023-05-31 20:03:56.852: [iter 28 : loss : 0.7066 = 0.2367 + 0.4669 + 0.0029, time: 22.690911]
2023-05-31 20:03:57.136: epoch 28:	0.02985685  	0.08180308  	0.06692588  
2023-05-31 20:03:57.136: Find a better model.
2023-05-31 20:04:19.642: [iter 29 : loss : 0.6883 = 0.2187 + 0.4664 + 0.0032, time: 22.501508]
2023-05-31 20:04:19.907: epoch 29:	0.02990867  	0.08177589  	0.06703748  
2023-05-31 20:04:42.599: [iter 30 : loss : 0.6720 = 0.2028 + 0.4658 + 0.0034, time: 22.688151]
2023-05-31 20:04:42.866: epoch 30:	0.02986426  	0.08182883  	0.06722443  
2023-05-31 20:04:42.866: Find a better model.
2023-05-31 20:05:05.440: [iter 31 : loss : 0.6578 = 0.1891 + 0.4650 + 0.0037, time: 22.570026]
2023-05-31 20:05:05.702: epoch 31:	0.02987907  	0.08200868  	0.06729273  
2023-05-31 20:05:05.702: Find a better model.
2023-05-31 20:05:28.225: [iter 32 : loss : 0.6457 = 0.1775 + 0.4643 + 0.0039, time: 22.519025]
2023-05-31 20:05:28.490: epoch 32:	0.03007896  	0.08270621  	0.06760738  
2023-05-31 20:05:28.490: Find a better model.
2023-05-31 20:05:51.234: [iter 33 : loss : 0.6355 = 0.1678 + 0.4637 + 0.0041, time: 22.739980]
2023-05-31 20:05:51.499: epoch 33:	0.03011597  	0.08244369  	0.06768617  
2023-05-31 20:06:13.990: [iter 34 : loss : 0.6257 = 0.1583 + 0.4631 + 0.0043, time: 22.488128]
2023-05-31 20:06:14.262: epoch 34:	0.03010857  	0.08224371  	0.06766715  
2023-05-31 20:06:36.789: [iter 35 : loss : 0.6164 = 0.1496 + 0.4624 + 0.0045, time: 22.523141]
2023-05-31 20:06:37.053: epoch 35:	0.03009377  	0.08250781  	0.06790024  
2023-05-31 20:06:59.762: [iter 36 : loss : 0.6087 = 0.1424 + 0.4616 + 0.0047, time: 22.705007]
2023-05-31 20:07:00.027: epoch 36:	0.03013819  	0.08252496  	0.06794250  
2023-05-31 20:07:22.754: [iter 37 : loss : 0.6023 = 0.1362 + 0.4612 + 0.0049, time: 22.721659]
2023-05-31 20:07:23.036: epoch 37:	0.03006415  	0.08264937  	0.06799421  
2023-05-31 20:07:45.766: [iter 38 : loss : 0.5958 = 0.1301 + 0.4606 + 0.0051, time: 22.726436]
2023-05-31 20:07:46.033: epoch 38:	0.03000492  	0.08229074  	0.06773154  
2023-05-31 20:08:08.965: [iter 39 : loss : 0.5900 = 0.1248 + 0.4600 + 0.0052, time: 22.927991]
2023-05-31 20:08:09.242: epoch 39:	0.03004195  	0.08259989  	0.06775066  
2023-05-31 20:08:31.955: [iter 40 : loss : 0.5840 = 0.1191 + 0.4595 + 0.0054, time: 22.707325]
2023-05-31 20:08:32.236: epoch 40:	0.02996791  	0.08206188  	0.06765611  
2023-05-31 20:08:54.940: [iter 41 : loss : 0.5786 = 0.1140 + 0.4590 + 0.0056, time: 22.700545]
2023-05-31 20:08:55.217: epoch 41:	0.02989387  	0.08157330  	0.06743086  
2023-05-31 20:09:17.916: [iter 42 : loss : 0.5742 = 0.1099 + 0.4586 + 0.0058, time: 22.693473]
2023-05-31 20:09:18.199: epoch 42:	0.02981244  	0.08126252  	0.06739726  
2023-05-31 20:09:40.890: [iter 43 : loss : 0.5698 = 0.1056 + 0.4582 + 0.0059, time: 22.687407]
2023-05-31 20:09:41.172: epoch 43:	0.02979764  	0.08169285  	0.06760362  
2023-05-31 20:10:03.984: [iter 44 : loss : 0.5668 = 0.1029 + 0.4578 + 0.0061, time: 22.807940]
2023-05-31 20:10:04.271: epoch 44:	0.02971619  	0.08125347  	0.06726290  
2023-05-31 20:11:40.184: my pid: 9492
2023-05-31 20:11:40.184: model: model.general_recommender.SGL
2023-05-31 20:11:40.184: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 20:11:40.184: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 20:11:44.373: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 20:12:04.992: [iter 1 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 20.618771]
2023-05-31 20:12:05.274: epoch 1:	0.00171010  	0.00312880  	0.00282151  
2023-05-31 20:12:05.274: Find a better model.
2023-05-31 20:12:25.988: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.710209]
2023-05-31 20:12:26.293: epoch 2:	0.00214688  	0.00362482  	0.00319392  
2023-05-31 20:12:26.293: Find a better model.
2023-05-31 20:12:47.135: [iter 3 : loss : 1.1343 = 0.6929 + 0.4414 + 0.0000, time: 20.839120]
2023-05-31 20:12:47.428: epoch 3:	0.00240598  	0.00466314  	0.00393167  
2023-05-31 20:12:47.428: Find a better model.
2023-05-31 20:13:08.133: [iter 4 : loss : 1.1345 = 0.6928 + 0.4417 + 0.0000, time: 20.700490]
2023-05-31 20:13:08.429: epoch 4:	0.00302043  	0.00655564  	0.00544582  
2023-05-31 20:13:08.429: Find a better model.
2023-05-31 20:13:29.148: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.715268]
2023-05-31 20:13:29.437: epoch 5:	0.00389399  	0.00857605  	0.00729151  
2023-05-31 20:13:29.437: Find a better model.
2023-05-31 20:13:49.939: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.498688]
2023-05-31 20:13:50.237: epoch 6:	0.00434557  	0.01010339  	0.00794818  
2023-05-31 20:13:50.237: Find a better model.
2023-05-31 20:14:10.946: [iter 7 : loss : 1.1348 = 0.6921 + 0.4427 + 0.0000, time: 20.705249]
2023-05-31 20:14:11.254: epoch 7:	0.00495261  	0.01209625  	0.00996008  
2023-05-31 20:14:11.254: Find a better model.
2023-05-31 20:14:31.954: [iter 8 : loss : 1.1348 = 0.6917 + 0.4431 + 0.0000, time: 20.696994]
2023-05-31 20:14:32.270: epoch 8:	0.00575213  	0.01463003  	0.01205241  
2023-05-31 20:14:32.270: Find a better model.
2023-05-31 20:14:52.884: [iter 9 : loss : 1.1346 = 0.6911 + 0.4435 + 0.0000, time: 20.610902]
2023-05-31 20:14:53.189: epoch 9:	0.00663308  	0.01719261  	0.01462771  
2023-05-31 20:14:53.189: Find a better model.
2023-05-31 20:15:13.872: [iter 10 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 20.679033]
2023-05-31 20:15:14.175: epoch 10:	0.00729935  	0.01860256  	0.01568677  
2023-05-31 20:15:14.175: Find a better model.
2023-05-31 20:15:34.897: [iter 11 : loss : 1.1331 = 0.6883 + 0.4447 + 0.0000, time: 20.716974]
2023-05-31 20:15:35.198: epoch 11:	0.00902424  	0.02287667  	0.01939091  
2023-05-31 20:15:35.198: Find a better model.
2023-05-31 20:15:55.868: [iter 12 : loss : 1.1314 = 0.6860 + 0.4454 + 0.0000, time: 20.665362]
2023-05-31 20:15:56.172: epoch 12:	0.01148946  	0.03011250  	0.02540468  
2023-05-31 20:15:56.172: Find a better model.
2023-05-31 20:16:16.639: [iter 13 : loss : 1.1285 = 0.6823 + 0.4461 + 0.0001, time: 20.463536]
2023-05-31 20:16:16.943: epoch 13:	0.01437669  	0.03818003  	0.03283093  
2023-05-31 20:16:16.943: Find a better model.
2023-05-31 20:16:37.298: [iter 14 : loss : 1.1230 = 0.6760 + 0.4469 + 0.0001, time: 20.350089]
2023-05-31 20:16:37.576: epoch 14:	0.01731576  	0.04537597  	0.03992269  
2023-05-31 20:16:37.576: Find a better model.
2023-05-31 20:16:57.864: [iter 15 : loss : 1.1130 = 0.6650 + 0.4479 + 0.0001, time: 20.282370]
2023-05-31 20:16:58.156: epoch 15:	0.02018080  	0.05377185  	0.04647345  
2023-05-31 20:16:58.156: Find a better model.
2023-05-31 20:17:18.855: [iter 16 : loss : 1.0957 = 0.6465 + 0.4491 + 0.0002, time: 20.691137]
2023-05-31 20:17:19.129: epoch 16:	0.02298663  	0.06131544  	0.05313241  
2023-05-31 20:17:19.129: Find a better model.
2023-05-31 20:17:39.601: [iter 17 : loss : 1.0690 = 0.6179 + 0.4507 + 0.0003, time: 20.469144]
2023-05-31 20:17:39.875: epoch 17:	0.02554077  	0.06796683  	0.05823699  
2023-05-31 20:17:39.875: Find a better model.
2023-05-31 20:18:00.206: [iter 18 : loss : 1.0310 = 0.5776 + 0.4528 + 0.0005, time: 20.327031]
2023-05-31 20:18:00.483: epoch 18:	0.02725832  	0.07266431  	0.06150838  
2023-05-31 20:18:00.483: Find a better model.
2023-05-31 20:18:21.007: [iter 19 : loss : 0.9850 = 0.5286 + 0.4556 + 0.0008, time: 20.519458]
2023-05-31 20:18:21.295: epoch 19:	0.02839842  	0.07553479  	0.06376372  
2023-05-31 20:18:21.295: Find a better model.
2023-05-31 20:18:41.822: [iter 20 : loss : 0.9352 = 0.4754 + 0.4587 + 0.0010, time: 20.522872]
2023-05-31 20:18:42.123: epoch 20:	0.02912393  	0.07824759  	0.06510340  
2023-05-31 20:18:42.123: Find a better model.
2023-05-31 20:19:03.171: [iter 21 : loss : 0.8863 = 0.4233 + 0.4617 + 0.0013, time: 21.044060]
2023-05-31 20:19:03.451: epoch 21:	0.02945709  	0.07926656  	0.06577770  
2023-05-31 20:19:03.451: Find a better model.
2023-05-31 20:19:24.633: [iter 22 : loss : 0.8411 = 0.3752 + 0.4643 + 0.0016, time: 21.176334]
2023-05-31 20:19:24.918: epoch 22:	0.02963478  	0.08079620  	0.06633050  
2023-05-31 20:19:24.918: Find a better model.
2023-05-31 20:19:46.033: [iter 23 : loss : 0.8025 = 0.3345 + 0.4661 + 0.0019, time: 21.111022]
2023-05-31 20:19:46.314: epoch 23:	0.02963478  	0.08129723  	0.06638318  
2023-05-31 20:19:46.314: Find a better model.
2023-05-31 20:20:07.166: [iter 24 : loss : 0.7699 = 0.3006 + 0.4670 + 0.0022, time: 20.848054]
2023-05-31 20:20:07.436: epoch 24:	0.02996792  	0.08199582  	0.06674862  
2023-05-31 20:20:07.436: Find a better model.
2023-05-31 20:20:28.380: [iter 25 : loss : 0.7416 = 0.2718 + 0.4673 + 0.0025, time: 20.941019]
2023-05-31 20:20:28.652: epoch 25:	0.03007898  	0.08235221  	0.06709621  
2023-05-31 20:20:28.652: Find a better model.
2023-05-31 20:20:49.793: [iter 26 : loss : 0.7185 = 0.2482 + 0.4675 + 0.0028, time: 21.138029]
2023-05-31 20:20:50.081: epoch 26:	0.03005677  	0.08262505  	0.06719888  
2023-05-31 20:20:50.081: Find a better model.
2023-05-31 20:21:11.134: [iter 27 : loss : 0.6976 = 0.2276 + 0.4670 + 0.0030, time: 21.048059]
2023-05-31 20:21:11.405: epoch 27:	0.02999755  	0.08244593  	0.06707890  
2023-05-31 20:21:32.564: [iter 28 : loss : 0.6808 = 0.2109 + 0.4666 + 0.0033, time: 21.154135]
2023-05-31 20:21:32.832: epoch 28:	0.03010119  	0.08286738  	0.06736483  
2023-05-31 20:21:32.832: Find a better model.
2023-05-31 20:21:53.961: [iter 29 : loss : 0.6659 = 0.1964 + 0.4659 + 0.0035, time: 21.125349]
2023-05-31 20:21:54.243: epoch 29:	0.03009378  	0.08287016  	0.06743027  
2023-05-31 20:21:54.243: Find a better model.
2023-05-31 20:22:15.501: [iter 30 : loss : 0.6523 = 0.1831 + 0.4654 + 0.0037, time: 21.254386]
2023-05-31 20:22:15.768: epoch 30:	0.03016782  	0.08326776  	0.06754231  
2023-05-31 20:22:15.768: Find a better model.
2023-05-31 20:22:37.152: [iter 31 : loss : 0.6404 = 0.1717 + 0.4647 + 0.0040, time: 21.380309]
2023-05-31 20:22:37.440: epoch 31:	0.03007157  	0.08300689  	0.06750729  
2023-05-31 20:22:58.703: [iter 32 : loss : 0.6304 = 0.1623 + 0.4639 + 0.0042, time: 21.259091]
2023-05-31 20:22:58.973: epoch 32:	0.03009379  	0.08293974  	0.06754348  
2023-05-31 20:23:20.296: [iter 33 : loss : 0.6219 = 0.1541 + 0.4634 + 0.0044, time: 21.319278]
2023-05-31 20:23:20.567: epoch 33:	0.03015300  	0.08306847  	0.06788623  
2023-05-31 20:23:41.898: [iter 34 : loss : 0.6131 = 0.1457 + 0.4628 + 0.0046, time: 21.327507]
2023-05-31 20:23:42.185: epoch 34:	0.03016782  	0.08343757  	0.06805233  
2023-05-31 20:23:42.185: Find a better model.
2023-05-31 20:24:03.262: [iter 35 : loss : 0.6052 = 0.1383 + 0.4621 + 0.0048, time: 21.072548]
2023-05-31 20:24:03.530: epoch 35:	0.03019742  	0.08333329  	0.06810766  
2023-05-31 20:24:24.706: [iter 36 : loss : 0.5987 = 0.1322 + 0.4616 + 0.0050, time: 21.171003]
2023-05-31 20:24:24.991: epoch 36:	0.03012339  	0.08332022  	0.06814944  
2023-05-31 20:24:46.248: [iter 37 : loss : 0.5928 = 0.1267 + 0.4610 + 0.0051, time: 21.252043]
2023-05-31 20:24:46.518: epoch 37:	0.03015301  	0.08357060  	0.06820170  
2023-05-31 20:24:46.518: Find a better model.
2023-05-31 20:25:07.660: [iter 38 : loss : 0.5866 = 0.1208 + 0.4605 + 0.0053, time: 21.138059]
2023-05-31 20:25:07.933: epoch 38:	0.03016041  	0.08349629  	0.06814570  
2023-05-31 20:25:29.069: [iter 39 : loss : 0.5820 = 0.1166 + 0.4599 + 0.0055, time: 21.132074]
2023-05-31 20:25:29.349: epoch 39:	0.03008639  	0.08309076  	0.06820305  
2023-05-31 20:25:50.422: [iter 40 : loss : 0.5769 = 0.1116 + 0.4596 + 0.0057, time: 21.068089]
2023-05-31 20:25:50.691: epoch 40:	0.03001975  	0.08273019  	0.06806399  
2023-05-31 20:26:11.848: [iter 41 : loss : 0.5721 = 0.1071 + 0.4591 + 0.0058, time: 21.153038]
2023-05-31 20:26:12.119: epoch 41:	0.02997532  	0.08259860  	0.06800582  
2023-05-31 20:26:33.214: [iter 42 : loss : 0.5683 = 0.1036 + 0.4587 + 0.0060, time: 21.090990]
2023-05-31 20:26:33.486: epoch 42:	0.02993831  	0.08241411  	0.06788960  
2023-05-31 20:26:54.655: [iter 43 : loss : 0.5644 = 0.0999 + 0.4584 + 0.0061, time: 21.165077]
2023-05-31 20:26:54.932: epoch 43:	0.02993830  	0.08251271  	0.06785109  
2023-05-31 20:27:15.994: [iter 44 : loss : 0.5617 = 0.0973 + 0.4581 + 0.0063, time: 21.057250]
2023-05-31 20:27:16.269: epoch 44:	0.02992349  	0.08236338  	0.06785077  
2023-05-31 20:27:37.390: [iter 45 : loss : 0.5579 = 0.0938 + 0.4576 + 0.0064, time: 21.116599]
2023-05-31 20:27:37.656: epoch 45:	0.02999012  	0.08251757  	0.06787052  
2023-05-31 20:27:58.809: [iter 46 : loss : 0.5550 = 0.0911 + 0.4573 + 0.0066, time: 21.149614]
2023-05-31 20:27:59.079: epoch 46:	0.02991608  	0.08196717  	0.06770438  
2023-05-31 20:28:20.022: [iter 47 : loss : 0.5517 = 0.0880 + 0.4570 + 0.0067, time: 20.940103]
2023-05-31 20:28:20.297: epoch 47:	0.02992349  	0.08218767  	0.06778643  
2023-05-31 20:28:41.249: [iter 48 : loss : 0.5494 = 0.0859 + 0.4567 + 0.0069, time: 20.946019]
2023-05-31 20:28:41.536: epoch 48:	0.03002713  	0.08262945  	0.06799250  
2023-05-31 20:29:02.536: [iter 49 : loss : 0.5468 = 0.0835 + 0.4563 + 0.0070, time: 20.997025]
2023-05-31 20:29:02.806: epoch 49:	0.02997531  	0.08223562  	0.06792129  
2023-05-31 20:29:23.800: [iter 50 : loss : 0.5445 = 0.0813 + 0.4561 + 0.0071, time: 20.989480]
2023-05-31 20:29:24.068: epoch 50:	0.02977543  	0.08207467  	0.06766301  
2023-05-31 20:29:45.031: [iter 51 : loss : 0.5421 = 0.0790 + 0.4558 + 0.0073, time: 20.960075]
2023-05-31 20:29:45.310: epoch 51:	0.02978283  	0.08202183  	0.06763681  
2023-05-31 20:30:06.215: [iter 52 : loss : 0.5393 = 0.0763 + 0.4556 + 0.0074, time: 20.899581]
2023-05-31 20:30:06.485: epoch 52:	0.02974581  	0.08145561  	0.06743970  
2023-05-31 20:30:27.183: [iter 53 : loss : 0.5386 = 0.0756 + 0.4554 + 0.0075, time: 20.693610]
2023-05-31 20:30:27.451: epoch 53:	0.02962736  	0.08126136  	0.06735344  
2023-05-31 20:30:48.313: [iter 54 : loss : 0.5364 = 0.0736 + 0.4552 + 0.0077, time: 20.857980]
2023-05-31 20:30:48.586: epoch 54:	0.02956072  	0.08085579  	0.06720898  
2023-05-31 20:31:09.571: [iter 55 : loss : 0.5347 = 0.0719 + 0.4550 + 0.0078, time: 20.982048]
2023-05-31 20:31:09.846: epoch 55:	0.02959774  	0.08085347  	0.06705824  
2023-05-31 20:31:30.954: [iter 56 : loss : 0.5323 = 0.0697 + 0.4548 + 0.0079, time: 21.103286]
2023-05-31 20:31:31.241: epoch 56:	0.02949409  	0.08001354  	0.06656834  
2023-05-31 20:31:52.323: [iter 57 : loss : 0.5307 = 0.0681 + 0.4546 + 0.0080, time: 21.078745]
2023-05-31 20:31:52.595: epoch 57:	0.02945707  	0.07961849  	0.06649055  
2023-05-31 20:32:13.703: [iter 58 : loss : 0.5294 = 0.0669 + 0.4544 + 0.0081, time: 21.104029]
2023-05-31 20:32:13.976: epoch 58:	0.02943487  	0.07947846  	0.06640696  
2023-05-31 20:32:35.142: [iter 59 : loss : 0.5280 = 0.0655 + 0.4543 + 0.0082, time: 21.162080]
2023-05-31 20:32:35.414: epoch 59:	0.02935342  	0.07924163  	0.06637169  
2023-05-31 20:32:56.529: [iter 60 : loss : 0.5267 = 0.0642 + 0.4541 + 0.0083, time: 21.111019]
2023-05-31 20:32:56.799: epoch 60:	0.02922758  	0.07878455  	0.06606721  
2023-05-31 20:33:17.904: [iter 61 : loss : 0.5252 = 0.0629 + 0.4538 + 0.0085, time: 21.099470]
2023-05-31 20:33:18.191: epoch 61:	0.02921277  	0.07868586  	0.06606673  
2023-05-31 20:33:39.081: [iter 62 : loss : 0.5246 = 0.0622 + 0.4538 + 0.0086, time: 20.885209]
2023-05-31 20:33:39.352: epoch 62:	0.02918315  	0.07834267  	0.06582820  
2023-05-31 20:33:39.352: Early stopping is trigger at epoch: 62
2023-05-31 20:33:39.352: best_result@epoch 37:

2023-05-31 20:33:39.352: 		0.0302      	0.0836      	0.0682      
2023-05-31 20:51:57.965: my pid: 12148
2023-05-31 20:51:57.965: model: model.general_recommender.SGL
2023-05-31 20:51:57.965: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 20:51:57.965: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 20:52:02.032: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 20:52:23.038: [iter 1 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 21.004818]
2023-05-31 20:52:23.318: epoch 1:	0.00175452  	0.00317001  	0.00272148  
2023-05-31 20:52:23.318: Find a better model.
2023-05-31 20:52:44.220: [iter 2 : loss : 1.1330 = 0.6930 + 0.4400 + 0.0000, time: 20.897609]
2023-05-31 20:52:44.504: epoch 2:	0.00210986  	0.00347976  	0.00293910  
2023-05-31 20:52:44.504: Find a better model.
2023-05-31 20:53:05.382: [iter 3 : loss : 1.1330 = 0.6929 + 0.4401 + 0.0000, time: 20.874073]
2023-05-31 20:53:05.665: epoch 3:	0.00260586  	0.00506644  	0.00418690  
2023-05-31 20:53:05.665: Find a better model.
2023-05-31 20:53:26.419: [iter 4 : loss : 1.1330 = 0.6928 + 0.4402 + 0.0000, time: 20.751294]
2023-05-31 20:53:26.703: epoch 4:	0.00290199  	0.00631561  	0.00512954  
2023-05-31 20:53:26.703: Find a better model.
2023-05-31 20:53:47.577: [iter 5 : loss : 1.1331 = 0.6927 + 0.4404 + 0.0000, time: 20.871063]
2023-05-31 20:53:47.859: epoch 5:	0.00350163  	0.00728927  	0.00639274  
2023-05-31 20:53:47.859: Find a better model.
2023-05-31 20:54:08.745: [iter 6 : loss : 1.1331 = 0.6925 + 0.4406 + 0.0000, time: 20.883220]
2023-05-31 20:54:09.026: epoch 6:	0.00474533  	0.01060843  	0.00888134  
2023-05-31 20:54:09.026: Find a better model.
2023-05-31 20:54:29.795: [iter 7 : loss : 1.1331 = 0.6923 + 0.4408 + 0.0000, time: 20.766031]
2023-05-31 20:54:30.096: epoch 7:	0.00542640  	0.01297154  	0.01093096  
2023-05-31 20:54:30.096: Find a better model.
2023-05-31 20:54:50.780: [iter 8 : loss : 1.1331 = 0.6920 + 0.4411 + 0.0000, time: 20.679029]
2023-05-31 20:54:51.055: epoch 8:	0.00640359  	0.01573626  	0.01315203  
2023-05-31 20:54:51.056: Find a better model.
2023-05-31 20:55:11.728: [iter 9 : loss : 1.1328 = 0.6915 + 0.4414 + 0.0000, time: 20.669196]
2023-05-31 20:55:12.002: epoch 9:	0.00749182  	0.01907090  	0.01564321  
2023-05-31 20:55:12.002: Find a better model.
2023-05-31 20:55:32.734: [iter 10 : loss : 1.1323 = 0.6905 + 0.4418 + 0.0000, time: 20.728475]
2023-05-31 20:55:33.029: epoch 10:	0.00770651  	0.02010431  	0.01688659  
2023-05-31 20:55:33.029: Find a better model.
2023-05-31 20:55:53.724: [iter 11 : loss : 1.1312 = 0.6890 + 0.4422 + 0.0000, time: 20.690346]
2023-05-31 20:55:53.995: epoch 11:	0.00923152  	0.02357617  	0.01971319  
2023-05-31 20:55:53.995: Find a better model.
2023-05-31 20:56:14.714: [iter 12 : loss : 1.1294 = 0.6866 + 0.4428 + 0.0000, time: 20.713074]
2023-05-31 20:56:14.986: epoch 12:	0.01117114  	0.02837961  	0.02471817  
2023-05-31 20:56:14.986: Find a better model.
2023-05-31 20:56:35.569: [iter 13 : loss : 1.1263 = 0.6829 + 0.4433 + 0.0001, time: 20.580365]
2023-05-31 20:56:35.840: epoch 13:	0.01374743  	0.03481825  	0.03131257  
2023-05-31 20:56:35.841: Find a better model.
2023-05-31 20:56:56.521: [iter 14 : loss : 1.1204 = 0.6763 + 0.4440 + 0.0001, time: 20.676107]
2023-05-31 20:56:56.808: epoch 14:	0.01671611  	0.04336845  	0.03829406  
2023-05-31 20:56:56.809: Find a better model.
2023-05-31 20:57:17.307: [iter 15 : loss : 1.1094 = 0.6645 + 0.4447 + 0.0001, time: 20.492059]
2023-05-31 20:57:17.575: epoch 15:	0.01992909  	0.05192187  	0.04540236  
2023-05-31 20:57:17.575: Find a better model.
2023-05-31 20:57:38.285: [iter 16 : loss : 1.0905 = 0.6444 + 0.4458 + 0.0002, time: 20.707036]
2023-05-31 20:57:38.552: epoch 16:	0.02292741  	0.06062699  	0.05240293  
2023-05-31 20:57:38.553: Find a better model.
2023-05-31 20:57:59.114: [iter 17 : loss : 1.0611 = 0.6134 + 0.4473 + 0.0004, time: 20.557032]
2023-05-31 20:57:59.394: epoch 17:	0.02513358  	0.06693874  	0.05759210  
2023-05-31 20:57:59.395: Find a better model.
2023-05-31 20:58:20.081: [iter 18 : loss : 1.0203 = 0.5703 + 0.4494 + 0.0006, time: 20.682073]
2023-05-31 20:58:20.374: epoch 18:	0.02699920  	0.07125603  	0.06107689  
2023-05-31 20:58:20.375: Find a better model.
2023-05-31 20:58:40.919: [iter 19 : loss : 0.9721 = 0.5192 + 0.4522 + 0.0008, time: 20.540482]
2023-05-31 20:58:41.186: epoch 19:	0.02827995  	0.07519469  	0.06359500  
2023-05-31 20:58:41.186: Find a better model.
2023-05-31 20:59:01.696: [iter 20 : loss : 0.9210 = 0.4647 + 0.4553 + 0.0011, time: 20.505127]
2023-05-31 20:59:01.965: epoch 20:	0.02887961  	0.07721736  	0.06485623  
2023-05-31 20:59:01.965: Find a better model.
2023-05-31 20:59:23.199: [iter 21 : loss : 0.8722 = 0.4127 + 0.4581 + 0.0014, time: 21.230310]
2023-05-31 20:59:23.475: epoch 21:	0.02940527  	0.07907397  	0.06576218  
2023-05-31 20:59:23.475: Find a better model.
2023-05-31 20:59:44.448: [iter 22 : loss : 0.8276 = 0.3654 + 0.4606 + 0.0017, time: 20.968023]
2023-05-31 20:59:44.715: epoch 22:	0.02952373  	0.07995940  	0.06592066  
2023-05-31 20:59:44.715: Find a better model.
2023-05-31 21:00:05.843: [iter 23 : loss : 0.7901 = 0.3258 + 0.4623 + 0.0020, time: 21.124058]
2023-05-31 21:00:06.117: epoch 23:	0.02964958  	0.08088093  	0.06629203  
2023-05-31 21:00:06.117: Find a better model.
2023-05-31 21:00:27.210: [iter 24 : loss : 0.7583 = 0.2928 + 0.4632 + 0.0023, time: 21.089018]
2023-05-31 21:00:27.485: epoch 24:	0.02981985  	0.08143537  	0.06651677  
2023-05-31 21:00:27.485: Find a better model.
2023-05-31 21:00:48.440: [iter 25 : loss : 0.7311 = 0.2652 + 0.4634 + 0.0026, time: 20.950160]
2023-05-31 21:00:48.703: epoch 25:	0.02984205  	0.08163999  	0.06650092  
2023-05-31 21:00:48.703: Find a better model.
2023-05-31 21:01:09.628: [iter 26 : loss : 0.7088 = 0.2425 + 0.4635 + 0.0028, time: 20.920238]
2023-05-31 21:01:09.891: epoch 26:	0.02984946  	0.08179785  	0.06665862  
2023-05-31 21:01:09.891: Find a better model.
2023-05-31 21:01:31.189: [iter 27 : loss : 0.6889 = 0.2229 + 0.4630 + 0.0031, time: 21.293082]
2023-05-31 21:01:31.455: epoch 27:	0.02990127  	0.08158265  	0.06648562  
2023-05-31 21:01:52.598: [iter 28 : loss : 0.6725 = 0.2065 + 0.4627 + 0.0033, time: 21.139039]
2023-05-31 21:01:52.861: epoch 28:	0.02979022  	0.08173767  	0.06660713  
2023-05-31 21:02:14.163: [iter 29 : loss : 0.6583 = 0.1927 + 0.4620 + 0.0036, time: 21.299399]
2023-05-31 21:02:14.436: epoch 29:	0.02979022  	0.08120788  	0.06658956  
2023-05-31 21:02:35.566: [iter 30 : loss : 0.6450 = 0.1798 + 0.4614 + 0.0038, time: 21.126418]
2023-05-31 21:02:35.826: epoch 30:	0.02979022  	0.08143502  	0.06682432  
2023-05-31 21:02:57.176: [iter 31 : loss : 0.6335 = 0.1686 + 0.4609 + 0.0040, time: 21.344764]
2023-05-31 21:02:57.447: epoch 31:	0.02981245  	0.08170505  	0.06693861  
2023-05-31 21:03:18.540: [iter 32 : loss : 0.6238 = 0.1594 + 0.4601 + 0.0042, time: 21.088187]
2023-05-31 21:03:18.800: epoch 32:	0.02975322  	0.08171933  	0.06697199  
2023-05-31 21:03:39.959: [iter 33 : loss : 0.6156 = 0.1516 + 0.4596 + 0.0044, time: 21.154222]
2023-05-31 21:03:40.223: epoch 33:	0.02987908  	0.08208171  	0.06714152  
2023-05-31 21:03:40.223: Find a better model.
2023-05-31 21:04:01.540: [iter 34 : loss : 0.6074 = 0.1437 + 0.4590 + 0.0046, time: 21.313244]
2023-05-31 21:04:01.800: epoch 34:	0.02990869  	0.08194834  	0.06732313  
2023-05-31 21:04:23.136: [iter 35 : loss : 0.5992 = 0.1360 + 0.4584 + 0.0048, time: 21.333045]
2023-05-31 21:04:23.415: epoch 35:	0.02981244  	0.08163035  	0.06737372  
2023-05-31 21:04:44.752: [iter 36 : loss : 0.5929 = 0.1300 + 0.4579 + 0.0050, time: 21.332452]
2023-05-31 21:04:45.027: epoch 36:	0.02987906  	0.08156067  	0.06755743  
2023-05-31 21:05:06.356: [iter 37 : loss : 0.5876 = 0.1251 + 0.4573 + 0.0052, time: 21.325179]
2023-05-31 21:05:06.615: epoch 37:	0.02996790  	0.08184607  	0.06765413  
2023-05-31 21:05:28.107: [iter 38 : loss : 0.5816 = 0.1194 + 0.4568 + 0.0054, time: 21.486895]
2023-05-31 21:05:28.379: epoch 38:	0.02990128  	0.08149869  	0.06751625  
2023-05-31 21:05:49.712: [iter 39 : loss : 0.5772 = 0.1153 + 0.4563 + 0.0055, time: 21.329041]
2023-05-31 21:05:49.972: epoch 39:	0.02985686  	0.08167183  	0.06751616  
2023-05-31 21:06:11.289: [iter 40 : loss : 0.5717 = 0.1100 + 0.4559 + 0.0057, time: 21.312984]
2023-05-31 21:06:11.551: epoch 40:	0.02985686  	0.08141690  	0.06764138  
2023-05-31 21:06:32.905: [iter 41 : loss : 0.5673 = 0.1060 + 0.4554 + 0.0059, time: 21.350500]
2023-05-31 21:06:33.187: epoch 41:	0.02977543  	0.08130552  	0.06761903  
2023-05-31 21:06:54.653: [iter 42 : loss : 0.5633 = 0.1022 + 0.4551 + 0.0060, time: 21.461502]
2023-05-31 21:06:54.915: epoch 42:	0.02970880  	0.08119041  	0.06745977  
2023-05-31 21:07:16.264: [iter 43 : loss : 0.5594 = 0.0985 + 0.4547 + 0.0062, time: 21.345054]
2023-05-31 21:07:16.526: epoch 43:	0.02959035  	0.08084831  	0.06733661  
2023-05-31 21:07:37.872: [iter 44 : loss : 0.5571 = 0.0964 + 0.4544 + 0.0063, time: 21.342513]
2023-05-31 21:07:38.134: epoch 44:	0.02956814  	0.08058415  	0.06737041  
2023-05-31 21:07:59.640: [iter 45 : loss : 0.5532 = 0.0927 + 0.4540 + 0.0065, time: 21.503210]
2023-05-31 21:07:59.897: epoch 45:	0.02957554  	0.08039621  	0.06734902  
2023-05-31 21:08:21.263: [iter 46 : loss : 0.5502 = 0.0899 + 0.4537 + 0.0066, time: 21.359981]
2023-05-31 21:08:21.523: epoch 46:	0.02953852  	0.08025867  	0.06725255  
2023-05-31 21:08:42.828: [iter 47 : loss : 0.5473 = 0.0872 + 0.4534 + 0.0068, time: 21.300066]
2023-05-31 21:08:43.090: epoch 47:	0.02951632  	0.08002974  	0.06721168  
2023-05-31 21:09:04.436: [iter 48 : loss : 0.5447 = 0.0848 + 0.4531 + 0.0069, time: 21.341733]
2023-05-31 21:09:04.698: epoch 48:	0.02947929  	0.07982755  	0.06695124  
2023-05-31 21:09:26.024: [iter 49 : loss : 0.5422 = 0.0824 + 0.4527 + 0.0071, time: 21.322011]
2023-05-31 21:09:26.305: epoch 49:	0.02934603  	0.07950531  	0.06684103  
2023-05-31 21:09:47.663: [iter 50 : loss : 0.5398 = 0.0801 + 0.4525 + 0.0072, time: 21.354058]
2023-05-31 21:09:47.927: epoch 50:	0.02920537  	0.07930224  	0.06658089  
2023-05-31 21:10:09.421: [iter 51 : loss : 0.5376 = 0.0780 + 0.4523 + 0.0073, time: 21.490956]
2023-05-31 21:10:09.682: epoch 51:	0.02925720  	0.07941234  	0.06653459  
2023-05-31 21:10:31.002: [iter 52 : loss : 0.5352 = 0.0757 + 0.4520 + 0.0074, time: 21.317300]
2023-05-31 21:10:31.265: epoch 52:	0.02918317  	0.07913718  	0.06639433  
2023-05-31 21:10:52.583: [iter 53 : loss : 0.5341 = 0.0747 + 0.4518 + 0.0076, time: 21.309138]
2023-05-31 21:10:52.843: epoch 53:	0.02910174  	0.07861415  	0.06612235  
2023-05-31 21:11:14.194: [iter 54 : loss : 0.5322 = 0.0729 + 0.4516 + 0.0077, time: 21.346319]
2023-05-31 21:11:14.459: epoch 54:	0.02904249  	0.07833565  	0.06603558  
2023-05-31 21:11:35.605: [iter 55 : loss : 0.5305 = 0.0713 + 0.4514 + 0.0078, time: 21.142054]
2023-05-31 21:11:35.864: epoch 55:	0.02901290  	0.07813812  	0.06581288  
2023-05-31 21:11:57.234: [iter 56 : loss : 0.5281 = 0.0690 + 0.4512 + 0.0080, time: 21.366046]
2023-05-31 21:11:57.498: epoch 56:	0.02891665  	0.07778764  	0.06559689  
2023-05-31 21:12:18.951: [iter 57 : loss : 0.5266 = 0.0676 + 0.4510 + 0.0081, time: 21.448130]
2023-05-31 21:12:19.214: epoch 57:	0.02906471  	0.07815728  	0.06579223  
2023-05-31 21:12:40.621: [iter 58 : loss : 0.5252 = 0.0662 + 0.4508 + 0.0082, time: 21.401822]
2023-05-31 21:12:40.883: epoch 58:	0.02891665  	0.07759521  	0.06549122  
2023-05-31 21:12:40.884: Early stopping is trigger at epoch: 58
2023-05-31 21:12:40.884: best_result@epoch 33:

2023-05-31 21:12:40.884: 		0.0299      	0.0821      	0.0671      
2023-05-31 21:13:15.955: my pid: 5624
2023-05-31 21:13:15.955: model: model.general_recommender.SGL
2023-05-31 21:13:15.955: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 21:13:15.955: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 21:13:19.983: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 21:13:39.934: [iter 1 : loss : 1.1360 = 0.6931 + 0.4429 + 0.0000, time: 19.951277]
2023-05-31 21:13:40.211: epoch 1:	0.00171750  	0.00340853  	0.00276330  
2023-05-31 21:13:40.211: Find a better model.
2023-05-31 21:14:00.294: [iter 2 : loss : 1.1362 = 0.6930 + 0.4432 + 0.0000, time: 20.079355]
2023-05-31 21:14:00.588: epoch 2:	0.00220610  	0.00360176  	0.00329479  
2023-05-31 21:14:00.588: Find a better model.
2023-05-31 21:14:20.895: [iter 3 : loss : 1.1366 = 0.6929 + 0.4437 + 0.0000, time: 20.303470]
2023-05-31 21:14:21.190: epoch 3:	0.00225792  	0.00402329  	0.00348002  
2023-05-31 21:14:21.190: Find a better model.
2023-05-31 21:14:41.189: [iter 4 : loss : 1.1370 = 0.6927 + 0.4443 + 0.0000, time: 19.994305]
2023-05-31 21:14:41.507: epoch 4:	0.00309446  	0.00689642  	0.00522526  
2023-05-31 21:14:41.507: Find a better model.
2023-05-31 21:15:01.514: [iter 5 : loss : 1.1373 = 0.6925 + 0.4448 + 0.0000, time: 20.004246]
2023-05-31 21:15:01.809: epoch 5:	0.00368671  	0.00824480  	0.00679670  
2023-05-31 21:15:01.809: Find a better model.
2023-05-31 21:15:21.907: [iter 6 : loss : 1.1377 = 0.6923 + 0.4454 + 0.0000, time: 20.093951]
2023-05-31 21:15:22.201: epoch 6:	0.00453064  	0.01096300  	0.00864665  
2023-05-31 21:15:22.201: Find a better model.
2023-05-31 21:15:42.269: [iter 7 : loss : 1.1379 = 0.6920 + 0.4459 + 0.0000, time: 20.064035]
2023-05-31 21:15:42.566: epoch 7:	0.00514509  	0.01312741  	0.01042816  
2023-05-31 21:15:42.566: Find a better model.
2023-05-31 21:16:02.653: [iter 8 : loss : 1.1380 = 0.6915 + 0.4465 + 0.0000, time: 20.083097]
2023-05-31 21:16:02.946: epoch 8:	0.00583357  	0.01432780  	0.01200892  
2023-05-31 21:16:02.946: Find a better model.
2023-05-31 21:16:22.886: [iter 9 : loss : 1.1378 = 0.6907 + 0.4471 + 0.0000, time: 19.936829]
2023-05-31 21:16:23.182: epoch 9:	0.00618150  	0.01489354  	0.01221266  
2023-05-31 21:16:23.183: Find a better model.
2023-05-31 21:16:43.066: [iter 10 : loss : 1.1373 = 0.6895 + 0.4478 + 0.0000, time: 19.879345]
2023-05-31 21:16:43.366: epoch 10:	0.00735117  	0.01819280  	0.01496571  
2023-05-31 21:16:43.366: Find a better model.
2023-05-31 21:17:03.267: [iter 11 : loss : 1.1369 = 0.6882 + 0.4487 + 0.0000, time: 19.895183]
2023-05-31 21:17:03.558: epoch 11:	0.00880215  	0.02098198  	0.01865691  
2023-05-31 21:17:03.558: Find a better model.
2023-05-31 21:17:23.226: [iter 12 : loss : 1.1357 = 0.6862 + 0.4494 + 0.0000, time: 19.663119]
2023-05-31 21:17:23.519: epoch 12:	0.01110450  	0.02799027  	0.02419552  
2023-05-31 21:17:23.519: Find a better model.
2023-05-31 21:17:43.277: [iter 13 : loss : 1.1338 = 0.6833 + 0.4504 + 0.0001, time: 19.755138]
2023-05-31 21:17:43.576: epoch 13:	0.01369560  	0.03485844  	0.03044786  
2023-05-31 21:17:43.576: Find a better model.
2023-05-31 21:18:03.240: [iter 14 : loss : 1.1300 = 0.6786 + 0.4512 + 0.0001, time: 19.659389]
2023-05-31 21:18:03.530: epoch 14:	0.01686416  	0.04307925  	0.03756637  
2023-05-31 21:18:03.530: Find a better model.
2023-05-31 21:18:23.225: [iter 15 : loss : 1.1235 = 0.6711 + 0.4523 + 0.0001, time: 19.691311]
2023-05-31 21:18:23.516: epoch 15:	0.02008456  	0.05111683  	0.04444515  
2023-05-31 21:18:23.517: Find a better model.
2023-05-31 21:18:43.234: [iter 16 : loss : 1.1123 = 0.6586 + 0.4535 + 0.0002, time: 19.712029]
2023-05-31 21:18:43.522: epoch 16:	0.02296441  	0.05879711  	0.05118415  
2023-05-31 21:18:43.522: Find a better model.
2023-05-31 21:19:03.194: [iter 17 : loss : 1.0943 = 0.6391 + 0.4550 + 0.0003, time: 19.667148]
2023-05-31 21:19:03.487: epoch 17:	0.02525202  	0.06504311  	0.05643283  
2023-05-31 21:19:03.487: Find a better model.
2023-05-31 21:19:23.223: [iter 18 : loss : 1.0669 = 0.6096 + 0.4569 + 0.0004, time: 19.732046]
2023-05-31 21:19:23.519: epoch 18:	0.02753964  	0.07111503  	0.06048620  
2023-05-31 21:19:23.519: Find a better model.
2023-05-31 21:19:43.206: [iter 19 : loss : 1.0300 = 0.5699 + 0.4596 + 0.0006, time: 19.684005]
2023-05-31 21:19:43.501: epoch 19:	0.02853907  	0.07473336  	0.06273469  
2023-05-31 21:19:43.501: Find a better model.
2023-05-31 21:20:03.204: [iter 20 : loss : 0.9852 = 0.5217 + 0.4627 + 0.0008, time: 19.699252]
2023-05-31 21:20:03.491: epoch 20:	0.02915354  	0.07661883  	0.06403290  
2023-05-31 21:20:03.491: Find a better model.
2023-05-31 21:20:23.595: [iter 21 : loss : 0.9369 = 0.4698 + 0.4660 + 0.0011, time: 20.100000]
2023-05-31 21:20:23.875: epoch 21:	0.02963475  	0.07895850  	0.06487632  
2023-05-31 21:20:23.875: Find a better model.
2023-05-31 21:20:44.134: [iter 22 : loss : 0.8891 = 0.4184 + 0.4694 + 0.0013, time: 20.256033]
2023-05-31 21:20:44.420: epoch 22:	0.02971619  	0.07975013  	0.06508041  
2023-05-31 21:20:44.420: Find a better model.
2023-05-31 21:21:04.557: [iter 23 : loss : 0.8461 = 0.3725 + 0.4719 + 0.0016, time: 20.131897]
2023-05-31 21:21:04.832: epoch 23:	0.02985685  	0.08035776  	0.06524926  
2023-05-31 21:21:04.832: Find a better model.
2023-05-31 21:21:25.172: [iter 24 : loss : 0.8084 = 0.3331 + 0.4734 + 0.0019, time: 20.336727]
2023-05-31 21:21:25.458: epoch 24:	0.03001232  	0.08153778  	0.06562079  
2023-05-31 21:21:25.458: Find a better model.
2023-05-31 21:21:45.749: [iter 25 : loss : 0.7760 = 0.2997 + 0.4740 + 0.0022, time: 20.287137]
2023-05-31 21:21:46.023: epoch 25:	0.03009376  	0.08178291  	0.06562956  
2023-05-31 21:21:46.023: Find a better model.
2023-05-31 21:22:06.550: [iter 26 : loss : 0.7489 = 0.2721 + 0.4743 + 0.0025, time: 20.522595]
2023-05-31 21:22:06.820: epoch 26:	0.03015299  	0.08167139  	0.06567132  
2023-05-31 21:22:27.122: [iter 27 : loss : 0.7252 = 0.2484 + 0.4740 + 0.0028, time: 20.298687]
2023-05-31 21:22:27.410: epoch 27:	0.03017520  	0.08176968  	0.06577379  
2023-05-31 21:22:47.902: [iter 28 : loss : 0.7055 = 0.2288 + 0.4737 + 0.0030, time: 20.488268]
2023-05-31 21:22:48.179: epoch 28:	0.03015299  	0.08204875  	0.06571164  
2023-05-31 21:22:48.180: Find a better model.
2023-05-31 21:23:08.352: [iter 29 : loss : 0.6884 = 0.2121 + 0.4731 + 0.0033, time: 20.169089]
2023-05-31 21:23:08.626: epoch 29:	0.03027144  	0.08271982  	0.06621892  
2023-05-31 21:23:08.626: Find a better model.
2023-05-31 21:23:28.771: [iter 30 : loss : 0.6729 = 0.1970 + 0.4724 + 0.0035, time: 20.142194]
2023-05-31 21:23:29.048: epoch 30:	0.03019741  	0.08243553  	0.06606052  
2023-05-31 21:23:49.330: [iter 31 : loss : 0.6594 = 0.1841 + 0.4716 + 0.0037, time: 20.279034]
2023-05-31 21:23:49.610: epoch 31:	0.03032327  	0.08253490  	0.06631023  
2023-05-31 21:24:09.861: [iter 32 : loss : 0.6478 = 0.1730 + 0.4709 + 0.0039, time: 20.245263]
2023-05-31 21:24:10.131: epoch 32:	0.03033808  	0.08260420  	0.06633476  
2023-05-31 21:24:30.667: [iter 33 : loss : 0.6379 = 0.1636 + 0.4702 + 0.0042, time: 20.531122]
2023-05-31 21:24:30.943: epoch 33:	0.03039730  	0.08297042  	0.06665288  
2023-05-31 21:24:30.943: Find a better model.
2023-05-31 21:24:51.325: [iter 34 : loss : 0.6286 = 0.1548 + 0.4694 + 0.0044, time: 20.379035]
2023-05-31 21:24:51.604: epoch 34:	0.03036770  	0.08275101  	0.06666006  
2023-05-31 21:25:11.872: [iter 35 : loss : 0.6197 = 0.1464 + 0.4688 + 0.0046, time: 20.265719]
2023-05-31 21:25:12.144: epoch 35:	0.03045655  	0.08297287  	0.06695218  
2023-05-31 21:25:12.144: Find a better model.
2023-05-31 21:25:32.331: [iter 36 : loss : 0.6122 = 0.1393 + 0.4681 + 0.0047, time: 20.183196]
2023-05-31 21:25:32.612: epoch 36:	0.03047875  	0.08326243  	0.06705191  
2023-05-31 21:25:32.612: Find a better model.
2023-05-31 21:25:52.838: [iter 37 : loss : 0.6058 = 0.1333 + 0.4675 + 0.0049, time: 20.223621]
2023-05-31 21:25:53.113: epoch 37:	0.03038251  	0.08286481  	0.06718361  
2023-05-31 21:26:13.224: [iter 38 : loss : 0.5990 = 0.1271 + 0.4668 + 0.0051, time: 20.107529]
2023-05-31 21:26:13.501: epoch 38:	0.03028627  	0.08228000  	0.06704008  
2023-05-31 21:26:33.673: [iter 39 : loss : 0.5940 = 0.1223 + 0.4664 + 0.0053, time: 20.167981]
2023-05-31 21:26:33.947: epoch 39:	0.03033808  	0.08279369  	0.06728252  
2023-05-31 21:26:54.249: [iter 40 : loss : 0.5882 = 0.1168 + 0.4660 + 0.0055, time: 20.296987]
2023-05-31 21:26:54.528: epoch 40:	0.03032327  	0.08249157  	0.06733439  
2023-05-31 21:27:14.471: [iter 41 : loss : 0.5830 = 0.1119 + 0.4655 + 0.0056, time: 19.938005]
2023-05-31 21:27:14.743: epoch 41:	0.03030846  	0.08247567  	0.06714134  
2023-05-31 21:27:34.816: [iter 42 : loss : 0.5789 = 0.1082 + 0.4650 + 0.0058, time: 20.067995]
2023-05-31 21:27:35.087: epoch 42:	0.03036769  	0.08243266  	0.06712713  
2023-05-31 21:27:55.263: [iter 43 : loss : 0.5745 = 0.1039 + 0.4646 + 0.0059, time: 20.172057]
2023-05-31 21:27:55.551: epoch 43:	0.03033067  	0.08235921  	0.06713749  
2023-05-31 21:28:15.808: [iter 44 : loss : 0.5716 = 0.1012 + 0.4643 + 0.0061, time: 20.253031]
2023-05-31 21:28:16.081: epoch 44:	0.03032328  	0.08219979  	0.06708582  
2023-05-31 21:28:36.208: [iter 45 : loss : 0.5674 = 0.0974 + 0.4638 + 0.0062, time: 20.124541]
2023-05-31 21:28:36.487: epoch 45:	0.03028626  	0.08187869  	0.06703009  
2023-05-31 21:28:56.815: [iter 46 : loss : 0.5640 = 0.0942 + 0.4635 + 0.0064, time: 20.324136]
2023-05-31 21:28:57.090: epoch 46:	0.03016781  	0.08184160  	0.06708843  
2023-05-31 21:29:17.164: [iter 47 : loss : 0.5609 = 0.0910 + 0.4633 + 0.0065, time: 20.070053]
2023-05-31 21:29:17.451: epoch 47:	0.03017522  	0.08164574  	0.06695520  
2023-05-31 21:29:37.567: [iter 48 : loss : 0.5583 = 0.0887 + 0.4629 + 0.0067, time: 20.111000]
2023-05-31 21:29:37.840: epoch 48:	0.03024185  	0.08157510  	0.06709762  
2023-05-31 21:29:58.014: [iter 49 : loss : 0.5556 = 0.0863 + 0.4624 + 0.0068, time: 20.170022]
2023-05-31 21:29:58.288: epoch 49:	0.03008638  	0.08141853  	0.06703033  
2023-05-31 21:30:18.392: [iter 50 : loss : 0.5530 = 0.0839 + 0.4622 + 0.0070, time: 20.100053]
2023-05-31 21:30:18.665: epoch 50:	0.03007157  	0.08138651  	0.06701146  
2023-05-31 21:30:38.824: [iter 51 : loss : 0.5501 = 0.0813 + 0.4618 + 0.0071, time: 20.155055]
2023-05-31 21:30:39.097: epoch 51:	0.03013819  	0.08125244  	0.06698494  
2023-05-31 21:30:59.320: [iter 52 : loss : 0.5475 = 0.0786 + 0.4616 + 0.0072, time: 20.218044]
2023-05-31 21:30:59.595: epoch 52:	0.03003454  	0.08068809  	0.06673323  
2023-05-31 21:31:19.571: [iter 53 : loss : 0.5466 = 0.0779 + 0.4614 + 0.0074, time: 19.973086]
2023-05-31 21:31:19.844: epoch 53:	0.02996792  	0.08034039  	0.06655586  
2023-05-31 21:31:40.130: [iter 54 : loss : 0.5445 = 0.0758 + 0.4611 + 0.0075, time: 20.282039]
2023-05-31 21:31:40.417: epoch 54:	0.02984207  	0.08012685  	0.06639075  
2023-05-31 21:32:00.551: [iter 55 : loss : 0.5425 = 0.0738 + 0.4611 + 0.0076, time: 20.130033]
2023-05-31 21:32:00.823: epoch 55:	0.02982727  	0.08009212  	0.06631013  
2023-05-31 21:32:20.970: [iter 56 : loss : 0.5401 = 0.0716 + 0.4608 + 0.0077, time: 20.143023]
2023-05-31 21:32:21.241: epoch 56:	0.02975324  	0.07953084  	0.06601811  
2023-05-31 21:32:41.303: [iter 57 : loss : 0.5386 = 0.0702 + 0.4605 + 0.0078, time: 20.058061]
2023-05-31 21:32:41.576: epoch 57:	0.02969401  	0.07939073  	0.06575376  
2023-05-31 21:33:01.944: [iter 58 : loss : 0.5370 = 0.0687 + 0.4604 + 0.0080, time: 20.364142]
2023-05-31 21:33:02.237: epoch 58:	0.02975324  	0.07954329  	0.06580852  
2023-05-31 21:34:30.916: my pid: 8
2023-05-31 21:34:30.916: model: model.general_recommender.SGL
2023-05-31 21:34:30.916: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-05-31 21:34:30.916: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-05-31 21:34:35.017: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-05-31 21:34:55.694: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.676038]
2023-05-31 21:34:55.961: epoch 1:	0.00147320  	0.00311625  	0.00246160  
2023-05-31 21:34:55.961: Find a better model.
2023-05-31 21:35:16.909: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.944066]
2023-05-31 21:35:17.193: epoch 2:	0.00186556  	0.00367127  	0.00304520  
2023-05-31 21:35:17.193: Find a better model.
2023-05-31 21:35:38.126: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 20.929591]
2023-05-31 21:35:38.428: epoch 3:	0.00208025  	0.00367878  	0.00312150  
2023-05-31 21:35:38.428: Find a better model.
2023-05-31 21:35:59.307: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.873351]
2023-05-31 21:35:59.596: epoch 4:	0.00270951  	0.00538028  	0.00431047  
2023-05-31 21:35:59.596: Find a better model.
2023-05-31 21:36:20.533: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 20.933365]
2023-05-31 21:36:20.817: epoch 5:	0.00339059  	0.00700443  	0.00559875  
2023-05-31 21:36:20.817: Find a better model.
2023-05-31 21:36:41.710: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 20.887257]
2023-05-31 21:36:41.998: epoch 6:	0.00381996  	0.00861051  	0.00682860  
2023-05-31 21:36:41.998: Find a better model.
2023-05-31 21:37:02.886: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.884840]
2023-05-31 21:37:03.170: epoch 7:	0.00472312  	0.01130587  	0.00911435  
2023-05-31 21:37:03.170: Find a better model.
2023-05-31 21:37:23.866: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 20.692997]
2023-05-31 21:37:24.150: epoch 8:	0.00569291  	0.01371649  	0.01111854  
2023-05-31 21:37:24.150: Find a better model.
2023-05-31 21:37:44.840: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 20.687542]
2023-05-31 21:37:45.143: epoch 9:	0.00649983  	0.01589953  	0.01310365  
2023-05-31 21:37:45.143: Find a better model.
2023-05-31 21:38:05.862: [iter 10 : loss : 1.1343 = 0.6904 + 0.4439 + 0.0000, time: 20.715044]
2023-05-31 21:38:06.140: epoch 10:	0.00720311  	0.01824263  	0.01473298  
2023-05-31 21:38:06.140: Find a better model.
2023-05-31 21:38:27.020: [iter 11 : loss : 1.1335 = 0.6889 + 0.4446 + 0.0000, time: 20.872532]
2023-05-31 21:38:27.332: epoch 11:	0.00821731  	0.02103022  	0.01777499  
2023-05-31 21:38:27.332: Find a better model.
2023-05-31 21:38:47.881: [iter 12 : loss : 1.1320 = 0.6867 + 0.4453 + 0.0000, time: 20.545158]
2023-05-31 21:38:48.157: epoch 12:	0.01017171  	0.02679923  	0.02266460  
2023-05-31 21:38:48.157: Find a better model.
2023-05-31 21:39:08.834: [iter 13 : loss : 1.1295 = 0.6835 + 0.4459 + 0.0001, time: 20.672868]
2023-05-31 21:39:09.125: epoch 13:	0.01262214  	0.03344887  	0.02884988  
2023-05-31 21:39:09.125: Find a better model.
2023-05-31 21:39:29.600: [iter 14 : loss : 1.1248 = 0.6779 + 0.4467 + 0.0001, time: 20.471717]
2023-05-31 21:39:29.869: epoch 14:	0.01533172  	0.04076338  	0.03491249  
2023-05-31 21:39:29.869: Find a better model.
2023-05-31 21:39:50.407: [iter 15 : loss : 1.1162 = 0.6684 + 0.4477 + 0.0001, time: 20.534156]
2023-05-31 21:39:50.676: epoch 15:	0.01913693  	0.05036114  	0.04303724  
2023-05-31 21:39:50.677: Find a better model.
2023-05-31 21:40:11.202: [iter 16 : loss : 1.1012 = 0.6522 + 0.4488 + 0.0002, time: 20.522557]
2023-05-31 21:40:11.485: epoch 16:	0.02227593  	0.05936190  	0.05093386  
2023-05-31 21:40:11.486: Find a better model.
2023-05-31 21:40:31.776: [iter 17 : loss : 1.0773 = 0.6267 + 0.4503 + 0.0003, time: 20.285660]
2023-05-31 21:40:32.047: epoch 17:	0.02469679  	0.06581390  	0.05671369  
2023-05-31 21:40:32.047: Find a better model.
2023-05-31 21:40:52.428: [iter 18 : loss : 1.0421 = 0.5893 + 0.4523 + 0.0005, time: 20.377489]
2023-05-31 21:40:52.697: epoch 18:	0.02657720  	0.07027114  	0.06000552  
2023-05-31 21:40:52.697: Find a better model.
2023-05-31 21:41:13.198: [iter 19 : loss : 0.9981 = 0.5424 + 0.4550 + 0.0007, time: 20.496381]
2023-05-31 21:41:13.481: epoch 19:	0.02820593  	0.07526424  	0.06356076  
2023-05-31 21:41:13.481: Find a better model.
2023-05-31 21:41:34.011: [iter 20 : loss : 0.9486 = 0.4896 + 0.4580 + 0.0009, time: 20.526054]
2023-05-31 21:41:34.299: epoch 20:	0.02890184  	0.07748393  	0.06518577  
2023-05-31 21:41:34.299: Find a better model.
2023-05-31 21:41:55.541: [iter 21 : loss : 0.8991 = 0.4367 + 0.4612 + 0.0012, time: 21.238169]
2023-05-31 21:41:55.812: epoch 21:	0.02940526  	0.07886424  	0.06586211  
2023-05-31 21:41:55.813: Find a better model.
2023-05-31 21:42:16.914: [iter 22 : loss : 0.8522 = 0.3866 + 0.4640 + 0.0015, time: 21.098468]
2023-05-31 21:42:17.179: epoch 22:	0.02964215  	0.07963707  	0.06619304  
2023-05-31 21:42:17.179: Find a better model.
2023-05-31 21:42:38.446: [iter 23 : loss : 0.8123 = 0.3443 + 0.4661 + 0.0018, time: 21.261834]
2023-05-31 21:42:38.717: epoch 23:	0.02979762  	0.08015758  	0.06631237  
2023-05-31 21:42:38.717: Find a better model.
2023-05-31 21:43:00.130: [iter 24 : loss : 0.7777 = 0.3084 + 0.4671 + 0.0021, time: 21.409098]
2023-05-31 21:43:00.406: epoch 24:	0.03002713  	0.08105719  	0.06668311  
2023-05-31 21:43:00.407: Find a better model.
2023-05-31 21:43:21.512: [iter 25 : loss : 0.7483 = 0.2782 + 0.4676 + 0.0024, time: 21.094471]
2023-05-31 21:43:21.774: epoch 25:	0.03013818  	0.08176267  	0.06680363  
2023-05-31 21:43:21.774: Find a better model.
2023-05-31 21:43:42.908: [iter 26 : loss : 0.7238 = 0.2533 + 0.4678 + 0.0027, time: 21.130054]
2023-05-31 21:43:43.173: epoch 26:	0.03032324  	0.08219951  	0.06715274  
2023-05-31 21:43:43.173: Find a better model.
2023-05-31 21:44:04.516: [iter 27 : loss : 0.7027 = 0.2324 + 0.4674 + 0.0030, time: 21.339427]
2023-05-31 21:44:04.779: epoch 27:	0.03029364  	0.08220284  	0.06697103  
2023-05-31 21:44:04.779: Find a better model.
2023-05-31 21:44:25.910: [iter 28 : loss : 0.6847 = 0.2145 + 0.4670 + 0.0032, time: 21.128279]
2023-05-31 21:44:26.171: epoch 28:	0.03020481  	0.08175605  	0.06691720  
2023-05-31 21:44:47.461: [iter 29 : loss : 0.6694 = 0.1997 + 0.4663 + 0.0034, time: 21.285237]
2023-05-31 21:44:47.743: epoch 29:	0.03037508  	0.08251782  	0.06739847  
2023-05-31 21:44:47.744: Find a better model.
2023-05-31 21:45:08.843: [iter 30 : loss : 0.6551 = 0.1858 + 0.4657 + 0.0037, time: 21.096632]
2023-05-31 21:45:09.106: epoch 30:	0.03033807  	0.08234454  	0.06738220  
2023-05-31 21:45:30.269: [iter 31 : loss : 0.6432 = 0.1741 + 0.4651 + 0.0039, time: 21.159473]
2023-05-31 21:45:30.541: epoch 31:	0.03040469  	0.08248837  	0.06743175  
2023-05-31 21:45:51.874: [iter 32 : loss : 0.6330 = 0.1644 + 0.4645 + 0.0041, time: 21.328367]
2023-05-31 21:45:52.134: epoch 32:	0.03034545  	0.08228856  	0.06742987  
2023-05-31 21:46:13.666: [iter 33 : loss : 0.6239 = 0.1559 + 0.4636 + 0.0043, time: 21.527875]
2023-05-31 21:46:13.930: epoch 33:	0.03035285  	0.08273276  	0.06754492  
2023-05-31 21:46:13.930: Find a better model.
2023-05-31 21:46:35.265: [iter 34 : loss : 0.6153 = 0.1476 + 0.4632 + 0.0045, time: 21.330193]
2023-05-31 21:46:35.538: epoch 34:	0.03029363  	0.08229297  	0.06747939  
2023-05-31 21:46:57.054: [iter 35 : loss : 0.6071 = 0.1399 + 0.4625 + 0.0047, time: 21.512123]
2023-05-31 21:46:57.316: epoch 35:	0.03038248  	0.08261393  	0.06763095  
2023-05-31 21:47:18.843: [iter 36 : loss : 0.6003 = 0.1334 + 0.4619 + 0.0049, time: 21.524081]
2023-05-31 21:47:19.107: epoch 36:	0.03045651  	0.08265522  	0.06766582  
2023-05-31 21:47:40.645: [iter 37 : loss : 0.5941 = 0.1276 + 0.4614 + 0.0051, time: 21.534028]
2023-05-31 21:47:40.906: epoch 37:	0.03045652  	0.08244592  	0.06752502  
2023-05-31 21:48:02.410: [iter 38 : loss : 0.5882 = 0.1221 + 0.4609 + 0.0053, time: 21.499016]
2023-05-31 21:48:02.670: epoch 38:	0.03047131  	0.08226954  	0.06773991  
2023-05-31 21:48:24.226: [iter 39 : loss : 0.5833 = 0.1174 + 0.4604 + 0.0054, time: 21.552091]
2023-05-31 21:48:24.499: epoch 39:	0.03046391  	0.08198939  	0.06757374  
2023-05-31 21:48:46.000: [iter 40 : loss : 0.5782 = 0.1127 + 0.4599 + 0.0056, time: 21.498052]
2023-05-31 21:48:46.260: epoch 40:	0.03041948  	0.08187671  	0.06752735  
2023-05-31 21:49:07.805: [iter 41 : loss : 0.5732 = 0.1081 + 0.4593 + 0.0058, time: 21.541086]
2023-05-31 21:49:08.064: epoch 41:	0.03037507  	0.08184680  	0.06753155  
2023-05-31 21:49:29.805: [iter 42 : loss : 0.5690 = 0.1040 + 0.4590 + 0.0059, time: 21.736350]
2023-05-31 21:49:30.065: epoch 42:	0.03027884  	0.08141249  	0.06738851  
2023-05-31 21:49:51.559: [iter 43 : loss : 0.5650 = 0.1004 + 0.4586 + 0.0061, time: 21.490048]
2023-05-31 21:49:51.821: epoch 43:	0.03019740  	0.08108747  	0.06707761  
2023-05-31 21:50:13.350: [iter 44 : loss : 0.5623 = 0.0979 + 0.4582 + 0.0062, time: 21.525023]
2023-05-31 21:50:13.614: epoch 44:	0.03013077  	0.08098068  	0.06678923  
2023-05-31 21:50:35.177: [iter 45 : loss : 0.5585 = 0.0943 + 0.4578 + 0.0064, time: 21.560032]
2023-05-31 21:50:35.468: epoch 45:	0.03017520  	0.08118921  	0.06684867  
2023-05-31 21:50:56.953: [iter 46 : loss : 0.5556 = 0.0916 + 0.4575 + 0.0065, time: 21.479973]
2023-05-31 21:50:57.214: epoch 46:	0.03016779  	0.08135180  	0.06691896  
2023-05-31 21:51:18.765: [iter 47 : loss : 0.5524 = 0.0885 + 0.4572 + 0.0067, time: 21.548047]
2023-05-31 21:51:19.024: epoch 47:	0.03014558  	0.08122990  	0.06667910  
2023-05-31 21:51:40.571: [iter 48 : loss : 0.5498 = 0.0861 + 0.4569 + 0.0068, time: 21.543028]
2023-05-31 21:51:40.853: epoch 48:	0.03000491  	0.08092565  	0.06649656  
2023-05-31 21:52:02.353: [iter 49 : loss : 0.5472 = 0.0837 + 0.4565 + 0.0070, time: 21.497371]
2023-05-31 21:52:02.621: epoch 49:	0.02996790  	0.08073810  	0.06651606  
2023-05-31 21:52:24.145: [iter 50 : loss : 0.5451 = 0.0817 + 0.4563 + 0.0071, time: 21.520049]
2023-05-31 21:52:24.437: epoch 50:	0.03003452  	0.08070974  	0.06660679  
2023-05-31 21:52:46.115: [iter 51 : loss : 0.5421 = 0.0788 + 0.4560 + 0.0072, time: 21.674017]
2023-05-31 21:52:46.406: epoch 51:	0.02988645  	0.08044195  	0.06638393  
2023-05-31 21:53:07.529: [iter 52 : loss : 0.5398 = 0.0766 + 0.4558 + 0.0074, time: 21.113544]
2023-05-31 21:53:07.790: epoch 52:	0.02996788  	0.08031549  	0.06645107  
2023-05-31 21:53:29.297: [iter 53 : loss : 0.5388 = 0.0758 + 0.4556 + 0.0075, time: 21.504093]
2023-05-31 21:53:29.567: epoch 53:	0.02990125  	0.08017652  	0.06632292  
2023-05-31 21:53:51.099: [iter 54 : loss : 0.5368 = 0.0738 + 0.4553 + 0.0076, time: 21.528017]
2023-05-31 21:53:51.359: epoch 54:	0.02978280  	0.07950860  	0.06600270  
2023-05-31 21:54:12.874: [iter 55 : loss : 0.5349 = 0.0721 + 0.4551 + 0.0077, time: 21.510068]
2023-05-31 21:54:13.132: epoch 55:	0.02966434  	0.07920216  	0.06602705  
2023-05-31 21:54:34.560: [iter 56 : loss : 0.5329 = 0.0702 + 0.4549 + 0.0079, time: 21.422884]
2023-05-31 21:54:34.835: epoch 56:	0.02961252  	0.07906339  	0.06577478  
2023-05-31 21:54:56.337: [iter 57 : loss : 0.5310 = 0.0684 + 0.4547 + 0.0080, time: 21.498087]
2023-05-31 21:54:56.619: epoch 57:	0.02954591  	0.07909048  	0.06579385  
2023-05-31 21:55:18.046: [iter 58 : loss : 0.5297 = 0.0670 + 0.4546 + 0.0081, time: 21.424439]
2023-05-31 21:55:18.306: epoch 58:	0.02956811  	0.07925954  	0.06587468  
2023-05-31 21:55:18.306: Early stopping is trigger at epoch: 58
2023-05-31 21:55:18.307: best_result@epoch 33:

2023-05-31 21:55:18.307: 		0.0304      	0.0827      	0.0675      
2023-06-01 09:13:23.886: my pid: 9036
2023-06-01 09:13:23.886: model: model.general_recommender.SGL
2023-06-01 09:13:23.886: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 09:13:23.886: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 09:13:27.972: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 09:13:48.967: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.993131]
2023-06-01 09:13:49.233: epoch 1:	0.00136956  	0.00279948  	0.00234637  
2023-06-01 09:13:49.233: Find a better model.
2023-06-01 09:14:10.029: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 20.793106]
2023-06-01 09:14:10.303: epoch 2:	0.00205064  	0.00342005  	0.00296493  
2023-06-01 09:14:10.303: Find a better model.
2023-06-01 09:14:31.174: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 20.866322]
2023-06-01 09:14:31.472: epoch 3:	0.00248742  	0.00431755  	0.00366544  
2023-06-01 09:14:31.473: Find a better model.
2023-06-01 09:14:52.346: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.869630]
2023-06-01 09:14:52.638: epoch 4:	0.00286497  	0.00544243  	0.00436115  
2023-06-01 09:14:52.639: Find a better model.
2023-06-01 09:15:13.552: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.910562]
2023-06-01 09:15:13.839: epoch 5:	0.00343500  	0.00765780  	0.00567347  
2023-06-01 09:15:13.839: Find a better model.
2023-06-01 09:15:34.563: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.720174]
2023-06-01 09:15:34.851: epoch 6:	0.00409387  	0.01006373  	0.00777498  
2023-06-01 09:15:34.851: Find a better model.
2023-06-01 09:15:55.556: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.701027]
2023-06-01 09:15:55.844: epoch 7:	0.00484157  	0.01193137  	0.00984523  
2023-06-01 09:15:55.844: Find a better model.
2023-06-01 09:16:16.540: [iter 8 : loss : 1.1349 = 0.6918 + 0.4430 + 0.0000, time: 20.690275]
2023-06-01 09:16:16.823: epoch 8:	0.00564849  	0.01441304  	0.01142721  
2023-06-01 09:16:16.823: Find a better model.
2023-06-01 09:16:37.544: [iter 9 : loss : 1.1347 = 0.6912 + 0.4435 + 0.0000, time: 20.717103]
2023-06-01 09:16:37.828: epoch 9:	0.00621852  	0.01618110  	0.01271930  
2023-06-01 09:16:37.828: Find a better model.
2023-06-01 09:16:58.524: [iter 10 : loss : 1.1342 = 0.6901 + 0.4440 + 0.0000, time: 20.693050]
2023-06-01 09:16:58.808: epoch 10:	0.00678854  	0.01731339  	0.01429276  
2023-06-01 09:16:58.808: Find a better model.
2023-06-01 09:17:19.519: [iter 11 : loss : 1.1334 = 0.6887 + 0.4447 + 0.0000, time: 20.707526]
2023-06-01 09:17:19.803: epoch 11:	0.00862447  	0.02326103  	0.01873187  
2023-06-01 09:17:19.804: Find a better model.
2023-06-01 09:17:40.508: [iter 12 : loss : 1.1319 = 0.6866 + 0.4453 + 0.0000, time: 20.700029]
2023-06-01 09:17:40.784: epoch 12:	0.01039380  	0.02776038  	0.02334057  
2023-06-01 09:17:40.784: Find a better model.
2023-06-01 09:18:01.128: [iter 13 : loss : 1.1294 = 0.6833 + 0.4460 + 0.0001, time: 20.340076]
2023-06-01 09:18:01.422: epoch 13:	0.01324400  	0.03506248  	0.02949741  
2023-06-01 09:18:01.422: Find a better model.
2023-06-01 09:18:21.967: [iter 14 : loss : 1.1246 = 0.6778 + 0.4467 + 0.0001, time: 20.541008]
2023-06-01 09:18:22.241: epoch 14:	0.01598319  	0.04250266  	0.03636566  
2023-06-01 09:18:22.241: Find a better model.
2023-06-01 09:18:42.903: [iter 15 : loss : 1.1161 = 0.6683 + 0.4477 + 0.0001, time: 20.657041]
2023-06-01 09:18:43.176: epoch 15:	0.01941825  	0.05131000  	0.04419443  
2023-06-01 09:18:43.176: Find a better model.
2023-06-01 09:19:03.498: [iter 16 : loss : 1.1010 = 0.6520 + 0.4488 + 0.0002, time: 20.316073]
2023-06-01 09:19:03.770: epoch 16:	0.02255724  	0.05934464  	0.05141217  
2023-06-01 09:19:03.770: Find a better model.
2023-06-01 09:19:24.292: [iter 17 : loss : 1.0772 = 0.6267 + 0.4502 + 0.0003, time: 20.518118]
2023-06-01 09:19:24.570: epoch 17:	0.02520760  	0.06648590  	0.05682095  
2023-06-01 09:19:24.570: Find a better model.
2023-06-01 09:19:45.097: [iter 18 : loss : 1.0422 = 0.5895 + 0.4522 + 0.0005, time: 20.522164]
2023-06-01 09:19:45.369: epoch 18:	0.02733974  	0.07238484  	0.06130173  
2023-06-01 09:19:45.369: Find a better model.
2023-06-01 09:20:05.894: [iter 19 : loss : 0.9985 = 0.5430 + 0.4549 + 0.0007, time: 20.515528]
2023-06-01 09:20:06.174: epoch 19:	0.02847244  	0.07648432  	0.06368905  
2023-06-01 09:20:06.174: Find a better model.
2023-06-01 09:20:26.661: [iter 20 : loss : 0.9493 = 0.4903 + 0.4581 + 0.0009, time: 20.481285]
2023-06-01 09:20:26.933: epoch 20:	0.02935343  	0.07885832  	0.06496760  
2023-06-01 09:20:26.933: Find a better model.
2023-06-01 09:20:48.036: [iter 21 : loss : 0.8997 = 0.4372 + 0.4612 + 0.0012, time: 21.098876]
2023-06-01 09:20:48.304: epoch 21:	0.02970138  	0.07963442  	0.06570598  
2023-06-01 09:20:48.304: Find a better model.
2023-06-01 09:21:09.217: [iter 22 : loss : 0.8530 = 0.3875 + 0.4640 + 0.0015, time: 20.907835]
2023-06-01 09:21:09.490: epoch 22:	0.02991608  	0.08029871  	0.06629738  
2023-06-01 09:21:09.490: Find a better model.
2023-06-01 09:21:30.464: [iter 23 : loss : 0.8129 = 0.3450 + 0.4661 + 0.0018, time: 20.970047]
2023-06-01 09:21:30.733: epoch 23:	0.02998270  	0.08098866  	0.06634092  
2023-06-01 09:21:30.733: Find a better model.
2023-06-01 09:21:51.632: [iter 24 : loss : 0.7784 = 0.3091 + 0.4672 + 0.0021, time: 20.895698]
2023-06-01 09:21:51.894: epoch 24:	0.02989387  	0.08103499  	0.06657594  
2023-06-01 09:21:51.894: Find a better model.
2023-06-01 09:22:13.192: [iter 25 : loss : 0.7491 = 0.2790 + 0.4677 + 0.0024, time: 21.292988]
2023-06-01 09:22:13.483: epoch 25:	0.03001232  	0.08150883  	0.06671742  
2023-06-01 09:22:13.483: Find a better model.
2023-06-01 09:22:34.593: [iter 26 : loss : 0.7247 = 0.2543 + 0.4677 + 0.0027, time: 21.105458]
2023-06-01 09:22:34.859: epoch 26:	0.03010857  	0.08205710  	0.06708378  
2023-06-01 09:22:34.859: Find a better model.
2023-06-01 09:22:55.984: [iter 27 : loss : 0.7034 = 0.2330 + 0.4674 + 0.0030, time: 21.121213]
2023-06-01 09:22:56.251: epoch 27:	0.03026403  	0.08280556  	0.06739238  
2023-06-01 09:22:56.251: Find a better model.
2023-06-01 09:23:17.401: [iter 28 : loss : 0.6857 = 0.2155 + 0.4670 + 0.0032, time: 21.146021]
2023-06-01 09:23:17.683: epoch 28:	0.03033066  	0.08287277  	0.06751357  
2023-06-01 09:23:17.683: Find a better model.
2023-06-01 09:23:38.755: [iter 29 : loss : 0.6704 = 0.2006 + 0.4663 + 0.0034, time: 21.069061]
2023-06-01 09:23:39.022: epoch 29:	0.03060459  	0.08353881  	0.06792603  
2023-06-01 09:23:39.022: Find a better model.
2023-06-01 09:24:00.149: [iter 30 : loss : 0.6560 = 0.1866 + 0.4658 + 0.0037, time: 21.122770]
2023-06-01 09:24:00.429: epoch 30:	0.03051574  	0.08353838  	0.06802282  
2023-06-01 09:24:21.541: [iter 31 : loss : 0.6441 = 0.1750 + 0.4652 + 0.0039, time: 21.107061]
2023-06-01 09:24:21.808: epoch 31:	0.03040469  	0.08307997  	0.06786063  
2023-06-01 09:24:43.149: [iter 32 : loss : 0.6333 = 0.1647 + 0.4644 + 0.0041, time: 21.337561]
2023-06-01 09:24:43.428: epoch 32:	0.03042691  	0.08317262  	0.06805786  
2023-06-01 09:25:04.566: [iter 33 : loss : 0.6244 = 0.1564 + 0.4637 + 0.0043, time: 21.133701]
2023-06-01 09:25:04.833: epoch 33:	0.03052315  	0.08357974  	0.06827288  
2023-06-01 09:25:04.833: Find a better model.
2023-06-01 09:25:26.133: [iter 34 : loss : 0.6158 = 0.1482 + 0.4631 + 0.0045, time: 21.296169]
2023-06-01 09:25:26.414: epoch 34:	0.03053056  	0.08312727  	0.06817978  
2023-06-01 09:25:47.536: [iter 35 : loss : 0.6076 = 0.1404 + 0.4625 + 0.0047, time: 21.117517]
2023-06-01 09:25:47.801: epoch 35:	0.03053796  	0.08350338  	0.06829132  
2023-06-01 09:26:08.924: [iter 36 : loss : 0.6009 = 0.1342 + 0.4619 + 0.0049, time: 21.119065]
2023-06-01 09:26:09.188: epoch 36:	0.03063419  	0.08380372  	0.06837501  
2023-06-01 09:26:09.188: Find a better model.
2023-06-01 09:26:30.320: [iter 37 : loss : 0.5948 = 0.1282 + 0.4614 + 0.0051, time: 21.129474]
2023-06-01 09:26:30.588: epoch 37:	0.03063418  	0.08338808  	0.06832821  
2023-06-01 09:26:51.902: [iter 38 : loss : 0.5887 = 0.1227 + 0.4608 + 0.0053, time: 21.309985]
2023-06-01 09:26:52.166: epoch 38:	0.03062678  	0.08388156  	0.06846527  
2023-06-01 09:26:52.166: Find a better model.
2023-06-01 09:27:13.305: [iter 39 : loss : 0.5837 = 0.1180 + 0.4602 + 0.0054, time: 21.133981]
2023-06-01 09:27:13.573: epoch 39:	0.03053054  	0.08361605  	0.06851520  
2023-06-01 09:27:34.488: [iter 40 : loss : 0.5786 = 0.1131 + 0.4599 + 0.0056, time: 20.910252]
2023-06-01 09:27:34.755: epoch 40:	0.03053054  	0.08364309  	0.06849363  
2023-06-01 09:27:55.878: [iter 41 : loss : 0.5737 = 0.1085 + 0.4594 + 0.0058, time: 21.119119]
2023-06-01 09:27:56.144: epoch 41:	0.03050092  	0.08317398  	0.06845491  
2023-06-01 09:28:17.076: [iter 42 : loss : 0.5696 = 0.1047 + 0.4590 + 0.0059, time: 20.927649]
2023-06-01 09:28:17.338: epoch 42:	0.03047871  	0.08292998  	0.06845394  
2023-06-01 09:28:38.324: [iter 43 : loss : 0.5656 = 0.1008 + 0.4587 + 0.0061, time: 20.980929]
2023-06-01 09:28:38.590: epoch 43:	0.03046391  	0.08283027  	0.06852314  
2023-06-01 09:28:59.663: [iter 44 : loss : 0.5626 = 0.0981 + 0.4582 + 0.0062, time: 21.069194]
2023-06-01 09:28:59.927: epoch 44:	0.03038988  	0.08278231  	0.06840719  
2023-06-01 09:29:20.875: [iter 45 : loss : 0.5590 = 0.0947 + 0.4579 + 0.0064, time: 20.944494]
2023-06-01 09:29:21.141: epoch 45:	0.03039729  	0.08264357  	0.06826614  
2023-06-01 09:29:42.055: [iter 46 : loss : 0.5557 = 0.0916 + 0.4576 + 0.0065, time: 20.910038]
2023-06-01 09:29:42.320: epoch 46:	0.03024922  	0.08245034  	0.06817470  
2023-06-01 09:30:03.287: [iter 47 : loss : 0.5527 = 0.0889 + 0.4572 + 0.0067, time: 20.962150]
2023-06-01 09:30:03.557: epoch 47:	0.03028624  	0.08223324  	0.06813616  
2023-06-01 09:30:24.427: [iter 48 : loss : 0.5501 = 0.0864 + 0.4569 + 0.0068, time: 20.866760]
2023-06-01 09:30:24.690: epoch 48:	0.03020480  	0.08168411  	0.06797249  
2023-06-01 09:30:45.683: [iter 49 : loss : 0.5476 = 0.0840 + 0.4566 + 0.0070, time: 20.989075]
2023-06-01 09:30:45.965: epoch 49:	0.03018259  	0.08133169  	0.06801261  
2023-06-01 09:31:07.231: [iter 50 : loss : 0.5457 = 0.0822 + 0.4564 + 0.0071, time: 21.262056]
2023-06-01 09:31:07.507: epoch 50:	0.03010114  	0.08112412  	0.06791776  
2023-06-01 09:31:28.913: [iter 51 : loss : 0.5428 = 0.0796 + 0.4560 + 0.0072, time: 21.401489]
2023-06-01 09:31:29.185: epoch 51:	0.02996048  	0.08059239  	0.06752118  
2023-06-01 09:31:43.657: my pid: 14372
2023-06-01 09:31:43.657: model: model.general_recommender.SGL
2023-06-01 09:31:43.657: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 09:31:43.657: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 09:31:47.862: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 09:32:09.320: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.457198]
2023-06-01 09:32:09.684: epoch 1:	0.00142878  	0.00313628  	0.00255931  
2023-06-01 09:32:09.684: Find a better model.
2023-06-01 09:32:30.835: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.148291]
2023-06-01 09:32:31.121: epoch 2:	0.00211727  	0.00401552  	0.00340936  
2023-06-01 09:32:31.121: Find a better model.
2023-06-01 09:32:52.184: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.058972]
2023-06-01 09:32:52.480: epoch 3:	0.00255404  	0.00485833  	0.00420555  
2023-06-01 09:32:52.480: Find a better model.
2023-06-01 09:33:13.404: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.919487]
2023-06-01 09:33:13.692: epoch 4:	0.00307225  	0.00639382  	0.00543158  
2023-06-01 09:33:13.692: Find a better model.
2023-06-01 09:33:34.556: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 20.859434]
2023-06-01 09:33:34.847: epoch 5:	0.00341279  	0.00738436  	0.00584296  
2023-06-01 09:33:34.847: Find a better model.
2023-06-01 09:33:55.548: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 20.695787]
2023-06-01 09:33:55.833: epoch 6:	0.00398283  	0.00975071  	0.00760551  
2023-06-01 09:33:55.833: Find a better model.
2023-06-01 09:34:16.346: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 20.509820]
2023-06-01 09:34:16.632: epoch 7:	0.00467130  	0.01121791  	0.00899793  
2023-06-01 09:34:16.632: Find a better model.
2023-06-01 09:34:37.180: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 20.544058]
2023-06-01 09:34:37.479: epoch 8:	0.00550043  	0.01284051  	0.01032016  
2023-06-01 09:34:37.479: Find a better model.
2023-06-01 09:34:58.360: [iter 9 : loss : 1.1347 = 0.6913 + 0.4435 + 0.0000, time: 20.877137]
2023-06-01 09:34:58.651: epoch 9:	0.00588538  	0.01554470  	0.01223183  
2023-06-01 09:34:58.651: Find a better model.
2023-06-01 09:35:19.147: [iter 10 : loss : 1.1342 = 0.6902 + 0.4440 + 0.0000, time: 20.492408]
2023-06-01 09:35:19.441: epoch 10:	0.00652944  	0.01614631  	0.01308230  
2023-06-01 09:35:19.441: Find a better model.
2023-06-01 09:35:39.959: [iter 11 : loss : 1.1334 = 0.6888 + 0.4446 + 0.0000, time: 20.512463]
2023-06-01 09:35:40.254: epoch 11:	0.00795081  	0.02045380  	0.01694816  
2023-06-01 09:35:40.254: Find a better model.
2023-06-01 09:36:00.728: [iter 12 : loss : 1.1320 = 0.6867 + 0.4453 + 0.0000, time: 20.469974]
2023-06-01 09:36:01.005: epoch 12:	0.01042342  	0.02658043  	0.02321867  
2023-06-01 09:36:01.005: Find a better model.
2023-06-01 09:36:21.485: [iter 13 : loss : 1.1295 = 0.6835 + 0.4459 + 0.0001, time: 20.477042]
2023-06-01 09:36:21.760: epoch 13:	0.01285904  	0.03316090  	0.02981612  
2023-06-01 09:36:21.760: Find a better model.
2023-06-01 09:36:42.293: [iter 14 : loss : 1.1248 = 0.6781 + 0.4467 + 0.0001, time: 20.529661]
2023-06-01 09:36:42.574: epoch 14:	0.01630151  	0.04332626  	0.03774878  
2023-06-01 09:36:42.574: Find a better model.
2023-06-01 09:37:02.938: [iter 15 : loss : 1.1165 = 0.6687 + 0.4476 + 0.0001, time: 20.361208]
2023-06-01 09:37:03.208: epoch 15:	0.01961073  	0.05216521  	0.04517951  
2023-06-01 09:37:03.208: Find a better model.
2023-06-01 09:37:23.508: [iter 16 : loss : 1.1017 = 0.6528 + 0.4487 + 0.0002, time: 20.295552]
2023-06-01 09:37:23.779: epoch 16:	0.02248320  	0.06044105  	0.05175781  
2023-06-01 09:37:23.779: Find a better model.
2023-06-01 09:37:44.257: [iter 17 : loss : 1.0781 = 0.6276 + 0.4502 + 0.0003, time: 20.473188]
2023-06-01 09:37:44.549: epoch 17:	0.02514098  	0.06870001  	0.05748072  
2023-06-01 09:37:44.549: Find a better model.
2023-06-01 09:38:05.078: [iter 18 : loss : 1.0435 = 0.5908 + 0.4522 + 0.0005, time: 20.523992]
2023-06-01 09:38:05.369: epoch 18:	0.02682152  	0.07306028  	0.06108351  
2023-06-01 09:38:05.369: Find a better model.
2023-06-01 09:38:25.720: [iter 19 : loss : 0.9999 = 0.5445 + 0.4548 + 0.0007, time: 20.344059]
2023-06-01 09:38:25.992: epoch 19:	0.02818370  	0.07716746  	0.06393463  
2023-06-01 09:38:25.992: Find a better model.
2023-06-01 09:38:46.299: [iter 20 : loss : 0.9509 = 0.4921 + 0.4579 + 0.0009, time: 20.304042]
2023-06-01 09:38:46.576: epoch 20:	0.02916094  	0.08013467  	0.06565475  
2023-06-01 09:38:46.576: Find a better model.
2023-06-01 09:39:07.448: [iter 21 : loss : 0.9015 = 0.4392 + 0.4611 + 0.0012, time: 20.867316]
2023-06-01 09:39:07.715: epoch 21:	0.02968658  	0.08211814  	0.06654242  
2023-06-01 09:39:07.715: Find a better model.
2023-06-01 09:39:28.649: [iter 22 : loss : 0.8545 = 0.3891 + 0.4638 + 0.0015, time: 20.930364]
2023-06-01 09:39:28.917: epoch 22:	0.02995309  	0.08325786  	0.06699125  
2023-06-01 09:39:28.917: Find a better model.
2023-06-01 09:39:50.253: [iter 23 : loss : 0.8142 = 0.3465 + 0.4659 + 0.0018, time: 21.330340]
2023-06-01 09:39:50.531: epoch 23:	0.03011597  	0.08422124  	0.06737902  
2023-06-01 09:39:50.531: Find a better model.
2023-06-01 09:40:11.614: [iter 24 : loss : 0.7795 = 0.3101 + 0.4672 + 0.0021, time: 21.078109]
2023-06-01 09:40:11.880: epoch 24:	0.03019740  	0.08419592  	0.06748395  
2023-06-01 09:40:33.190: [iter 25 : loss : 0.7497 = 0.2797 + 0.4676 + 0.0024, time: 21.306374]
2023-06-01 09:40:33.467: epoch 25:	0.03034547  	0.08463504  	0.06781326  
2023-06-01 09:40:33.467: Find a better model.
2023-06-01 09:40:54.784: [iter 26 : loss : 0.7250 = 0.2547 + 0.4676 + 0.0027, time: 21.312515]
2023-06-01 09:40:55.051: epoch 26:	0.03033807  	0.08496737  	0.06794430  
2023-06-01 09:40:55.051: Find a better model.
2023-06-01 09:41:16.182: [iter 27 : loss : 0.7040 = 0.2335 + 0.4675 + 0.0029, time: 21.127295]
2023-06-01 09:41:16.461: epoch 27:	0.03031586  	0.08487146  	0.06790587  
2023-06-01 09:41:37.568: [iter 28 : loss : 0.6861 = 0.2157 + 0.4672 + 0.0032, time: 21.103107]
2023-06-01 09:41:37.836: epoch 28:	0.03024183  	0.08453684  	0.06794717  
2023-06-01 09:41:59.159: [iter 29 : loss : 0.6706 = 0.2007 + 0.4664 + 0.0034, time: 21.318063]
2023-06-01 09:41:59.441: epoch 29:	0.03035287  	0.08503693  	0.06829146  
2023-06-01 09:41:59.441: Find a better model.
2023-06-01 09:42:20.377: [iter 30 : loss : 0.6562 = 0.1867 + 0.4658 + 0.0037, time: 20.932482]
2023-06-01 09:42:20.639: epoch 30:	0.03033806  	0.08472833  	0.06827459  
2023-06-01 09:42:41.804: [iter 31 : loss : 0.6441 = 0.1751 + 0.4652 + 0.0039, time: 21.160350]
2023-06-01 09:42:42.065: epoch 31:	0.03034548  	0.08475085  	0.06835999  
2023-06-01 09:43:03.177: [iter 32 : loss : 0.6337 = 0.1651 + 0.4645 + 0.0041, time: 21.106557]
2023-06-01 09:43:03.454: epoch 32:	0.03036768  	0.08480050  	0.06848069  
2023-06-01 09:43:24.558: [iter 33 : loss : 0.6244 = 0.1564 + 0.4638 + 0.0043, time: 21.100026]
2023-06-01 09:43:24.823: epoch 33:	0.03037508  	0.08445936  	0.06854134  
2023-06-01 09:43:45.953: [iter 34 : loss : 0.6159 = 0.1482 + 0.4632 + 0.0045, time: 21.126807]
2023-06-01 09:43:46.215: epoch 34:	0.03030845  	0.08420575  	0.06849296  
2023-06-01 09:44:07.370: [iter 35 : loss : 0.6073 = 0.1400 + 0.4626 + 0.0047, time: 21.150164]
2023-06-01 09:44:07.635: epoch 35:	0.03032326  	0.08421192  	0.06860991  
2023-06-01 09:44:28.544: [iter 36 : loss : 0.6007 = 0.1339 + 0.4619 + 0.0049, time: 20.905690]
2023-06-01 09:44:28.808: epoch 36:	0.03037507  	0.08401506  	0.06866648  
2023-06-01 09:44:49.898: [iter 37 : loss : 0.5948 = 0.1284 + 0.4613 + 0.0051, time: 21.086506]
2023-06-01 09:44:50.162: epoch 37:	0.03037507  	0.08425666  	0.06881853  
2023-06-01 09:45:11.330: [iter 38 : loss : 0.5885 = 0.1225 + 0.4607 + 0.0053, time: 21.164478]
2023-06-01 09:45:11.603: epoch 38:	0.03041210  	0.08454745  	0.06904256  
2023-06-01 09:45:32.522: [iter 39 : loss : 0.5834 = 0.1176 + 0.4604 + 0.0054, time: 20.914425]
2023-06-01 09:45:32.786: epoch 39:	0.03037508  	0.08431480  	0.06911685  
2023-06-01 09:45:53.931: [iter 40 : loss : 0.5785 = 0.1129 + 0.4600 + 0.0056, time: 21.141414]
2023-06-01 09:45:54.213: epoch 40:	0.03050093  	0.08469634  	0.06921250  
2023-06-01 09:46:15.275: [iter 41 : loss : 0.5735 = 0.1083 + 0.4594 + 0.0058, time: 21.058186]
2023-06-01 09:46:15.542: epoch 41:	0.03045652  	0.08475494  	0.06929597  
2023-06-01 09:46:36.479: [iter 42 : loss : 0.5694 = 0.1045 + 0.4590 + 0.0059, time: 20.931364]
2023-06-01 09:46:36.745: epoch 42:	0.03047132  	0.08444179  	0.06920918  
2023-06-01 09:46:57.513: [iter 43 : loss : 0.5654 = 0.1007 + 0.4586 + 0.0061, time: 20.764927]
2023-06-01 09:46:57.776: epoch 43:	0.03047132  	0.08454765  	0.06918725  
2023-06-01 09:47:18.886: [iter 44 : loss : 0.5631 = 0.0986 + 0.4583 + 0.0062, time: 21.106261]
2023-06-01 09:47:19.151: epoch 44:	0.03044912  	0.08429569  	0.06907953  
2023-06-01 09:47:40.241: [iter 45 : loss : 0.5589 = 0.0946 + 0.4580 + 0.0064, time: 21.087050]
2023-06-01 09:47:40.515: epoch 45:	0.03035286  	0.08416690  	0.06895748  
2023-06-01 09:48:01.444: [iter 46 : loss : 0.5559 = 0.0918 + 0.4575 + 0.0065, time: 20.925081]
2023-06-01 09:48:01.708: epoch 46:	0.03037507  	0.08407863  	0.06887159  
2023-06-01 09:48:22.881: [iter 47 : loss : 0.5531 = 0.0892 + 0.4573 + 0.0067, time: 21.169387]
2023-06-01 09:48:23.146: epoch 47:	0.03030103  	0.08380278  	0.06879617  
2023-06-01 09:48:44.249: [iter 48 : loss : 0.5500 = 0.0863 + 0.4569 + 0.0068, time: 21.099033]
2023-06-01 09:48:44.519: epoch 48:	0.03031583  	0.08390196  	0.06884371  
2023-06-01 09:49:05.614: [iter 49 : loss : 0.5473 = 0.0838 + 0.4565 + 0.0070, time: 21.092048]
2023-06-01 09:49:05.878: epoch 49:	0.03014557  	0.08373556  	0.06859119  
2023-06-01 09:49:27.044: [iter 50 : loss : 0.5451 = 0.0817 + 0.4564 + 0.0071, time: 21.163053]
2023-06-01 09:49:27.311: epoch 50:	0.03016039  	0.08388388  	0.06872156  
2023-06-01 09:49:48.233: [iter 51 : loss : 0.5425 = 0.0793 + 0.4560 + 0.0072, time: 20.916097]
2023-06-01 09:49:48.505: epoch 51:	0.03001972  	0.08346035  	0.06848895  
2023-06-01 09:50:09.421: [iter 52 : loss : 0.5403 = 0.0771 + 0.4558 + 0.0073, time: 20.912033]
2023-06-01 09:50:09.688: epoch 52:	0.03008635  	0.08339139  	0.06847833  
2023-06-01 09:50:30.814: [iter 53 : loss : 0.5391 = 0.0760 + 0.4556 + 0.0075, time: 21.121034]
2023-06-01 09:50:31.079: epoch 53:	0.02998271  	0.08291128  	0.06819451  
2023-06-01 09:50:51.998: [iter 54 : loss : 0.5369 = 0.0740 + 0.4553 + 0.0076, time: 20.914049]
2023-06-01 09:50:52.262: epoch 54:	0.02980503  	0.08214760  	0.06795818  
2023-06-01 09:50:52.262: Early stopping is trigger at epoch: 54
2023-06-01 09:50:52.262: best_result@epoch 29:

2023-06-01 09:50:52.262: 		0.0304      	0.0850      	0.0683      
2023-06-01 09:56:04.834: my pid: 14420
2023-06-01 09:56:04.835: model: model.general_recommender.SGL
2023-06-01 09:56:04.835: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 09:56:04.835: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 09:56:08.937: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 09:56:29.925: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.987965]
2023-06-01 09:56:30.210: epoch 1:	0.00122890  	0.00226294  	0.00197152  
2023-06-01 09:56:30.210: Find a better model.
2023-06-01 09:56:51.182: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.967045]
2023-06-01 09:56:51.469: epoch 2:	0.00175452  	0.00322440  	0.00273182  
2023-06-01 09:56:51.469: Find a better model.
2023-06-01 09:57:12.683: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.211478]
2023-06-01 09:57:12.973: epoch 3:	0.00218389  	0.00455071  	0.00341320  
2023-06-01 09:57:12.973: Find a better model.
2023-06-01 09:57:34.087: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.110094]
2023-06-01 09:57:34.382: epoch 4:	0.00265028  	0.00528285  	0.00432979  
2023-06-01 09:57:34.382: Find a better model.
2023-06-01 09:57:55.654: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 21.269256]
2023-06-01 09:57:55.943: epoch 5:	0.00306485  	0.00618714  	0.00509432  
2023-06-01 09:57:55.943: Find a better model.
2023-06-01 09:58:17.046: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.098438]
2023-06-01 09:58:17.342: epoch 6:	0.00401244  	0.00876116  	0.00706466  
2023-06-01 09:58:17.342: Find a better model.
2023-06-01 09:58:38.629: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.282009]
2023-06-01 09:58:38.915: epoch 7:	0.00496742  	0.01205179  	0.00947663  
2023-06-01 09:58:38.915: Find a better model.
2023-06-01 09:58:59.864: [iter 8 : loss : 1.1348 = 0.6919 + 0.4430 + 0.0000, time: 20.945049]
2023-06-01 09:59:00.171: epoch 8:	0.00581135  	0.01444685  	0.01162993  
2023-06-01 09:59:00.171: Find a better model.
2023-06-01 09:59:21.057: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 20.882140]
2023-06-01 09:59:21.349: epoch 9:	0.00647762  	0.01728873  	0.01412234  
2023-06-01 09:59:21.349: Find a better model.
2023-06-01 09:59:42.462: [iter 10 : loss : 1.1344 = 0.6904 + 0.4440 + 0.0000, time: 21.108058]
2023-06-01 09:59:42.763: epoch 10:	0.00704024  	0.01832657  	0.01527409  
2023-06-01 09:59:42.764: Find a better model.
2023-06-01 10:00:03.653: [iter 11 : loss : 1.1335 = 0.6890 + 0.4445 + 0.0000, time: 20.885536]
2023-06-01 10:00:03.938: epoch 11:	0.00879474  	0.02308093  	0.01905508  
2023-06-01 10:00:03.938: Find a better model.
2023-06-01 10:00:24.651: [iter 12 : loss : 1.1320 = 0.6869 + 0.4451 + 0.0000, time: 20.709156]
2023-06-01 10:00:24.933: epoch 12:	0.01074175  	0.02801635  	0.02335261  
2023-06-01 10:00:24.933: Find a better model.
2023-06-01 10:00:45.786: [iter 13 : loss : 1.1297 = 0.6836 + 0.4460 + 0.0001, time: 20.849115]
2023-06-01 10:00:46.063: epoch 13:	0.01338466  	0.03574116  	0.03019944  
2023-06-01 10:00:46.063: Find a better model.
2023-06-01 10:01:06.980: [iter 14 : loss : 1.1249 = 0.6782 + 0.4467 + 0.0001, time: 20.914131]
2023-06-01 10:01:07.269: epoch 14:	0.01658283  	0.04371455  	0.03777208  
2023-06-01 10:01:07.269: Find a better model.
2023-06-01 10:01:28.032: [iter 15 : loss : 1.1165 = 0.6688 + 0.4476 + 0.0001, time: 20.757031]
2023-06-01 10:01:28.314: epoch 15:	0.01966254  	0.05163598  	0.04459338  
2023-06-01 10:01:28.315: Find a better model.
2023-06-01 10:01:49.198: [iter 16 : loss : 1.1016 = 0.6528 + 0.4486 + 0.0002, time: 20.879065]
2023-06-01 10:01:49.472: epoch 16:	0.02260164  	0.05927834  	0.05100800  
2023-06-01 10:01:49.472: Find a better model.
2023-06-01 10:02:10.194: [iter 17 : loss : 1.0780 = 0.6276 + 0.4502 + 0.0003, time: 20.717531]
2023-06-01 10:02:10.472: epoch 17:	0.02519279  	0.06643504  	0.05674829  
2023-06-01 10:02:10.473: Find a better model.
2023-06-01 10:02:31.179: [iter 18 : loss : 1.0432 = 0.5906 + 0.4522 + 0.0005, time: 20.701357]
2023-06-01 10:02:31.456: epoch 18:	0.02669566  	0.07126512  	0.06027513  
2023-06-01 10:02:31.456: Find a better model.
2023-06-01 10:02:52.158: [iter 19 : loss : 0.9994 = 0.5440 + 0.4547 + 0.0007, time: 20.696583]
2023-06-01 10:02:52.433: epoch 19:	0.02831698  	0.07678983  	0.06348528  
2023-06-01 10:02:52.433: Find a better model.
2023-06-01 10:03:13.182: [iter 20 : loss : 0.9501 = 0.4913 + 0.4579 + 0.0009, time: 20.745141]
2023-06-01 10:03:13.457: epoch 20:	0.02917574  	0.07894716  	0.06525359  
2023-06-01 10:03:13.457: Find a better model.
2023-06-01 10:03:34.932: [iter 21 : loss : 0.9006 = 0.4384 + 0.4610 + 0.0012, time: 21.472041]
2023-06-01 10:03:35.234: epoch 21:	0.02961253  	0.08075069  	0.06616062  
2023-06-01 10:03:35.234: Find a better model.
2023-06-01 10:03:56.712: [iter 22 : loss : 0.8537 = 0.3884 + 0.4637 + 0.0015, time: 21.473312]
2023-06-01 10:03:56.983: epoch 22:	0.02971620  	0.08163128  	0.06643543  
2023-06-01 10:03:56.983: Find a better model.
2023-06-01 10:04:18.496: [iter 23 : loss : 0.8134 = 0.3458 + 0.4658 + 0.0018, time: 21.509055]
2023-06-01 10:04:18.765: epoch 23:	0.02983466  	0.08201680  	0.06668330  
2023-06-01 10:04:18.765: Find a better model.
2023-06-01 10:04:40.288: [iter 24 : loss : 0.7790 = 0.3098 + 0.4670 + 0.0021, time: 21.519423]
2023-06-01 10:04:40.556: epoch 24:	0.03012340  	0.08292297  	0.06710307  
2023-06-01 10:04:40.556: Find a better model.
2023-06-01 10:05:01.887: [iter 25 : loss : 0.7492 = 0.2793 + 0.4674 + 0.0024, time: 21.326043]
2023-06-01 10:05:02.175: epoch 25:	0.03012340  	0.08316064  	0.06716807  
2023-06-01 10:05:02.175: Find a better model.
2023-06-01 10:05:23.698: [iter 26 : loss : 0.7246 = 0.2544 + 0.4675 + 0.0027, time: 21.517906]
2023-06-01 10:05:23.968: epoch 26:	0.03018264  	0.08331960  	0.06731605  
2023-06-01 10:05:23.968: Find a better model.
2023-06-01 10:05:45.467: [iter 27 : loss : 0.7033 = 0.2331 + 0.4673 + 0.0029, time: 21.494046]
2023-06-01 10:05:45.735: epoch 27:	0.03016043  	0.08353316  	0.06734387  
2023-06-01 10:05:45.735: Find a better model.
2023-06-01 10:06:07.293: [iter 28 : loss : 0.6853 = 0.2153 + 0.4668 + 0.0032, time: 21.554050]
2023-06-01 10:06:07.561: epoch 28:	0.03023446  	0.08386896  	0.06753299  
2023-06-01 10:06:07.561: Find a better model.
2023-06-01 10:06:29.126: [iter 29 : loss : 0.6700 = 0.2004 + 0.4662 + 0.0034, time: 21.560910]
2023-06-01 10:06:29.399: epoch 29:	0.03030109  	0.08415988  	0.06770387  
2023-06-01 10:06:29.399: Find a better model.
2023-06-01 10:06:51.058: [iter 30 : loss : 0.6558 = 0.1866 + 0.4656 + 0.0037, time: 21.655030]
2023-06-01 10:06:51.331: epoch 30:	0.03036773  	0.08433353  	0.06763493  
2023-06-01 10:06:51.331: Find a better model.
2023-06-01 10:07:12.865: [iter 31 : loss : 0.6434 = 0.1746 + 0.4650 + 0.0039, time: 21.530055]
2023-06-01 10:07:13.131: epoch 31:	0.03056762  	0.08511072  	0.06801191  
2023-06-01 10:07:13.131: Find a better model.
2023-06-01 10:07:34.838: [iter 32 : loss : 0.6333 = 0.1649 + 0.4643 + 0.0041, time: 21.702395]
2023-06-01 10:07:35.106: epoch 32:	0.03055281  	0.08518638  	0.06817337  
2023-06-01 10:07:35.106: Find a better model.
2023-06-01 10:07:56.617: [iter 33 : loss : 0.6242 = 0.1562 + 0.4637 + 0.0043, time: 21.506202]
2023-06-01 10:07:56.883: epoch 33:	0.03063424  	0.08550256  	0.06828158  
2023-06-01 10:07:56.883: Find a better model.
2023-06-01 10:08:18.406: [iter 34 : loss : 0.6158 = 0.1483 + 0.4630 + 0.0045, time: 21.520049]
2023-06-01 10:08:18.674: epoch 34:	0.03059722  	0.08511797  	0.06815075  
2023-06-01 10:08:40.203: [iter 35 : loss : 0.6070 = 0.1399 + 0.4624 + 0.0047, time: 21.524114]
2023-06-01 10:08:40.468: epoch 35:	0.03069346  	0.08555064  	0.06847708  
2023-06-01 10:08:40.468: Find a better model.
2023-06-01 10:09:01.996: [iter 36 : loss : 0.6002 = 0.1336 + 0.4618 + 0.0049, time: 21.523496]
2023-06-01 10:09:02.271: epoch 36:	0.03071566  	0.08564542  	0.06859430  
2023-06-01 10:09:02.271: Find a better model.
2023-06-01 10:09:23.608: [iter 37 : loss : 0.5943 = 0.1280 + 0.4613 + 0.0051, time: 21.333337]
2023-06-01 10:09:23.878: epoch 37:	0.03068604  	0.08551880  	0.06857660  
2023-06-01 10:09:45.398: [iter 38 : loss : 0.5882 = 0.1223 + 0.4606 + 0.0053, time: 21.516186]
2023-06-01 10:09:45.668: epoch 38:	0.03069345  	0.08541726  	0.06848169  
2023-06-01 10:10:07.221: [iter 39 : loss : 0.5835 = 0.1178 + 0.4602 + 0.0054, time: 21.548789]
2023-06-01 10:10:07.488: epoch 39:	0.03063422  	0.08507638  	0.06843463  
2023-06-01 10:10:28.762: [iter 40 : loss : 0.5781 = 0.1128 + 0.4597 + 0.0056, time: 21.269667]
2023-06-01 10:10:29.027: epoch 40:	0.03055280  	0.08514019  	0.06839337  
2023-06-01 10:10:50.588: [iter 41 : loss : 0.5732 = 0.1081 + 0.4593 + 0.0058, time: 21.555150]
2023-06-01 10:10:50.857: epoch 41:	0.03055280  	0.08524136  	0.06849867  
2023-06-01 10:11:12.399: [iter 42 : loss : 0.5692 = 0.1043 + 0.4590 + 0.0059, time: 21.539049]
2023-06-01 10:11:12.667: epoch 42:	0.03048616  	0.08508869  	0.06841297  
2023-06-01 10:11:34.163: [iter 43 : loss : 0.5653 = 0.1007 + 0.4585 + 0.0061, time: 21.491093]
2023-06-01 10:11:34.427: epoch 43:	0.03039732  	0.08473486  	0.06826561  
2023-06-01 10:11:55.957: [iter 44 : loss : 0.5624 = 0.0981 + 0.4581 + 0.0062, time: 21.525099]
2023-06-01 10:11:56.240: epoch 44:	0.03039733  	0.08467489  	0.06819204  
2023-06-01 10:12:17.773: [iter 45 : loss : 0.5589 = 0.0946 + 0.4579 + 0.0064, time: 21.528417]
2023-06-01 10:12:18.039: epoch 45:	0.03033809  	0.08415722  	0.06816205  
2023-06-01 10:12:39.383: [iter 46 : loss : 0.5556 = 0.0916 + 0.4575 + 0.0065, time: 21.338187]
2023-06-01 10:12:39.669: epoch 46:	0.03025666  	0.08350184  	0.06792145  
2023-06-01 10:13:01.315: [iter 47 : loss : 0.5525 = 0.0887 + 0.4572 + 0.0067, time: 21.642193]
2023-06-01 10:13:01.584: epoch 47:	0.03024926  	0.08345824  	0.06782544  
2023-06-01 10:13:22.962: [iter 48 : loss : 0.5498 = 0.0861 + 0.4568 + 0.0068, time: 21.375199]
2023-06-01 10:13:23.244: epoch 48:	0.03017523  	0.08307864  	0.06784703  
2023-06-01 10:13:44.506: [iter 49 : loss : 0.5473 = 0.0839 + 0.4565 + 0.0070, time: 21.257989]
2023-06-01 10:13:44.774: epoch 49:	0.03009379  	0.08287318  	0.06774443  
2023-06-01 10:14:06.138: [iter 50 : loss : 0.5452 = 0.0818 + 0.4563 + 0.0071, time: 21.360172]
2023-06-01 10:14:06.406: epoch 50:	0.02995313  	0.08258124  	0.06757871  
2023-06-01 10:14:27.867: [iter 51 : loss : 0.5425 = 0.0792 + 0.4561 + 0.0072, time: 21.456251]
2023-06-01 10:14:28.135: epoch 51:	0.02979766  	0.08181644  	0.06722333  
2023-06-01 10:14:49.289: [iter 52 : loss : 0.5399 = 0.0769 + 0.4557 + 0.0073, time: 21.146368]
2023-06-01 10:14:49.556: epoch 52:	0.02977545  	0.08153537  	0.06728701  
2023-06-01 10:15:10.872: [iter 53 : loss : 0.5392 = 0.0762 + 0.4555 + 0.0075, time: 21.312001]
2023-06-01 10:15:11.137: epoch 53:	0.02972362  	0.08132596  	0.06716735  
2023-06-01 10:15:32.662: [iter 54 : loss : 0.5367 = 0.0739 + 0.4553 + 0.0076, time: 21.514156]
2023-06-01 10:15:32.931: epoch 54:	0.02962737  	0.08092244  	0.06695510  
2023-06-01 10:15:54.265: [iter 55 : loss : 0.5350 = 0.0722 + 0.4551 + 0.0077, time: 21.329171]
2023-06-01 10:15:54.535: epoch 55:	0.02960516  	0.08082210  	0.06679280  
2023-06-01 10:16:16.057: [iter 56 : loss : 0.5327 = 0.0700 + 0.4548 + 0.0078, time: 21.518070]
2023-06-01 10:16:16.330: epoch 56:	0.02949412  	0.08041169  	0.06656518  
2023-06-01 10:16:37.860: [iter 57 : loss : 0.5309 = 0.0682 + 0.4546 + 0.0080, time: 21.526588]
2023-06-01 10:16:38.129: epoch 57:	0.02947191  	0.07994066  	0.06647202  
2023-06-01 10:16:59.295: [iter 58 : loss : 0.5295 = 0.0669 + 0.4545 + 0.0081, time: 21.163478]
2023-06-01 10:16:59.561: epoch 58:	0.02946451  	0.08007707  	0.06645557  
2023-06-01 10:17:20.864: [iter 59 : loss : 0.5278 = 0.0653 + 0.4543 + 0.0082, time: 21.300406]
2023-06-01 10:17:21.131: epoch 59:	0.02947931  	0.07972190  	0.06633370  
2023-06-01 10:17:42.240: [iter 60 : loss : 0.5269 = 0.0644 + 0.4542 + 0.0083, time: 21.104308]
2023-06-01 10:17:42.507: epoch 60:	0.02944230  	0.07978046  	0.06623650  
2023-06-01 10:18:03.844: [iter 61 : loss : 0.5256 = 0.0631 + 0.4540 + 0.0084, time: 21.331981]
2023-06-01 10:18:04.113: epoch 61:	0.02950153  	0.07957187  	0.06617730  
2023-06-01 10:18:04.113: Early stopping is trigger at epoch: 61
2023-06-01 10:18:04.113: best_result@epoch 36:

2023-06-01 10:18:04.113: 		0.0307      	0.0856      	0.0686      
2023-06-01 10:25:53.448: my pid: 2928
2023-06-01 10:25:53.448: model: model.general_recommender.SGL
2023-06-01 10:25:53.448: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 10:25:53.448: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 10:25:57.694: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 10:26:18.976: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.282006]
2023-06-01 10:26:19.258: epoch 1:	0.00117708  	0.00238847  	0.00195853  
2023-06-01 10:26:19.258: Find a better model.
2023-06-01 10:26:40.712: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.449927]
2023-06-01 10:26:40.996: epoch 2:	0.00173231  	0.00331134  	0.00281544  
2023-06-01 10:26:40.996: Find a better model.
2023-06-01 10:27:02.478: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.479071]
2023-06-01 10:27:02.769: epoch 3:	0.00208025  	0.00380434  	0.00316227  
2023-06-01 10:27:02.770: Find a better model.
2023-06-01 10:27:24.322: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 21.548569]
2023-06-01 10:27:24.610: epoch 4:	0.00262067  	0.00586857  	0.00470534  
2023-06-01 10:27:24.610: Find a better model.
2023-06-01 10:27:46.057: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.444000]
2023-06-01 10:27:46.353: epoch 5:	0.00330175  	0.00698588  	0.00595037  
2023-06-01 10:27:46.353: Find a better model.
2023-06-01 10:28:07.712: [iter 6 : loss : 1.1346 = 0.6925 + 0.4422 + 0.0000, time: 21.355227]
2023-06-01 10:28:08.016: epoch 6:	0.00409387  	0.00935021  	0.00745241  
2023-06-01 10:28:08.016: Find a better model.
2023-06-01 10:28:29.269: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 21.250215]
2023-06-01 10:28:29.555: epoch 7:	0.00459727  	0.01091589  	0.00877803  
2023-06-01 10:28:29.555: Find a better model.
2023-06-01 10:28:50.688: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 21.130049]
2023-06-01 10:28:50.970: epoch 8:	0.00533756  	0.01303778  	0.01070128  
2023-06-01 10:28:50.970: Find a better model.
2023-06-01 10:29:12.060: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 21.085730]
2023-06-01 10:29:12.353: epoch 9:	0.00662568  	0.01718501  	0.01418958  
2023-06-01 10:29:12.353: Find a better model.
2023-06-01 10:29:33.648: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 21.290522]
2023-06-01 10:29:33.930: epoch 10:	0.00767690  	0.02035583  	0.01690010  
2023-06-01 10:29:33.930: Find a better model.
2023-06-01 10:29:55.043: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 21.107716]
2023-06-01 10:29:55.331: epoch 11:	0.00903164  	0.02334693  	0.02031161  
2023-06-01 10:29:55.331: Find a better model.
2023-06-01 10:30:16.455: [iter 12 : loss : 1.1322 = 0.6872 + 0.4450 + 0.0000, time: 21.119472]
2023-06-01 10:30:16.737: epoch 12:	0.01079358  	0.02894541  	0.02456461  
2023-06-01 10:30:16.737: Find a better model.
2023-06-01 10:30:37.839: [iter 13 : loss : 1.1298 = 0.6841 + 0.4457 + 0.0001, time: 21.098290]
2023-06-01 10:30:38.114: epoch 13:	0.01296269  	0.03447208  	0.03030593  
2023-06-01 10:30:38.114: Find a better model.
2023-06-01 10:30:59.048: [iter 14 : loss : 1.1255 = 0.6789 + 0.4465 + 0.0001, time: 20.929088]
2023-06-01 10:30:59.337: epoch 14:	0.01613865  	0.04324150  	0.03754946  
2023-06-01 10:30:59.338: Find a better model.
2023-06-01 10:31:20.415: [iter 15 : loss : 1.1176 = 0.6701 + 0.4473 + 0.0001, time: 21.072501]
2023-06-01 10:31:20.691: epoch 15:	0.01949971  	0.05120222  	0.04482744  
2023-06-01 10:31:20.691: Find a better model.
2023-06-01 10:31:41.638: [iter 16 : loss : 1.1036 = 0.6549 + 0.4485 + 0.0002, time: 20.943213]
2023-06-01 10:31:41.916: epoch 16:	0.02216489  	0.05849852  	0.05089123  
2023-06-01 10:31:41.916: Find a better model.
2023-06-01 10:32:02.970: [iter 17 : loss : 1.0809 = 0.6307 + 0.4499 + 0.0003, time: 21.049130]
2023-06-01 10:32:03.257: epoch 17:	0.02471159  	0.06544933  	0.05601059  
2023-06-01 10:32:03.257: Find a better model.
2023-06-01 10:32:24.216: [iter 18 : loss : 1.0471 = 0.5947 + 0.4519 + 0.0005, time: 20.954981]
2023-06-01 10:32:24.491: epoch 18:	0.02685114  	0.07137330  	0.06042627  
2023-06-01 10:32:24.491: Find a better model.
2023-06-01 10:32:45.595: [iter 19 : loss : 1.0037 = 0.5486 + 0.4544 + 0.0007, time: 21.100437]
2023-06-01 10:32:45.869: epoch 19:	0.02796902  	0.07450838  	0.06308263  
2023-06-01 10:32:45.869: Find a better model.
2023-06-01 10:33:06.997: [iter 20 : loss : 0.9546 = 0.4961 + 0.4576 + 0.0009, time: 21.124090]
2023-06-01 10:33:07.284: epoch 20:	0.02888702  	0.07732386  	0.06508829  
2023-06-01 10:33:07.284: Find a better model.
2023-06-01 10:33:28.967: [iter 21 : loss : 0.9046 = 0.4426 + 0.4608 + 0.0012, time: 21.678719]
2023-06-01 10:33:29.252: epoch 21:	0.02939785  	0.07934419  	0.06618673  
2023-06-01 10:33:29.252: Find a better model.
2023-06-01 10:33:50.951: [iter 22 : loss : 0.8572 = 0.3920 + 0.4637 + 0.0015, time: 21.694984]
2023-06-01 10:33:51.239: epoch 22:	0.02959036  	0.08050397  	0.06640498  
2023-06-01 10:33:51.239: Find a better model.
2023-06-01 10:34:12.777: [iter 23 : loss : 0.8162 = 0.3486 + 0.4658 + 0.0018, time: 21.532408]
2023-06-01 10:34:13.063: epoch 23:	0.02965697  	0.08103555  	0.06637581  
2023-06-01 10:34:13.064: Find a better model.
2023-06-01 10:34:34.727: [iter 24 : loss : 0.7811 = 0.3120 + 0.4670 + 0.0021, time: 21.657922]
2023-06-01 10:34:34.995: epoch 24:	0.02970139  	0.08142659  	0.06655166  
2023-06-01 10:34:34.995: Find a better model.
2023-06-01 10:34:56.732: [iter 25 : loss : 0.7512 = 0.2813 + 0.4675 + 0.0024, time: 21.732929]
2023-06-01 10:34:57.003: epoch 25:	0.02979763  	0.08154045  	0.06669555  
2023-06-01 10:34:57.003: Find a better model.
2023-06-01 10:35:18.507: [iter 26 : loss : 0.7263 = 0.2561 + 0.4675 + 0.0027, time: 21.500162]
2023-06-01 10:35:18.774: epoch 26:	0.02993089  	0.08194088  	0.06679457  
2023-06-01 10:35:18.774: Find a better model.
2023-06-01 10:35:40.288: [iter 27 : loss : 0.7049 = 0.2347 + 0.4672 + 0.0029, time: 21.510030]
2023-06-01 10:35:40.554: epoch 27:	0.02990127  	0.08157757  	0.06683581  
2023-06-01 10:36:02.125: [iter 28 : loss : 0.6865 = 0.2164 + 0.4669 + 0.0032, time: 21.566980]
2023-06-01 10:36:02.394: epoch 28:	0.02997530  	0.08205875  	0.06698770  
2023-06-01 10:36:02.394: Find a better model.
2023-06-01 10:36:24.080: [iter 29 : loss : 0.6711 = 0.2013 + 0.4663 + 0.0034, time: 21.681573]
2023-06-01 10:36:24.353: epoch 29:	0.03021961  	0.08267160  	0.06733427  
2023-06-01 10:36:24.353: Find a better model.
2023-06-01 10:36:45.919: [iter 30 : loss : 0.6567 = 0.1874 + 0.4656 + 0.0037, time: 21.561904]
2023-06-01 10:36:46.204: epoch 30:	0.03022702  	0.08275068  	0.06730504  
2023-06-01 10:36:46.204: Find a better model.
2023-06-01 10:37:07.894: [iter 31 : loss : 0.6445 = 0.1756 + 0.4650 + 0.0039, time: 21.686533]
2023-06-01 10:37:08.175: epoch 31:	0.03029364  	0.08270859  	0.06733117  
2023-06-01 10:37:29.883: [iter 32 : loss : 0.6341 = 0.1657 + 0.4643 + 0.0041, time: 21.700539]
2023-06-01 10:37:30.152: epoch 32:	0.03038989  	0.08318288  	0.06759311  
2023-06-01 10:37:30.152: Find a better model.
2023-06-01 10:37:52.110: [iter 33 : loss : 0.6247 = 0.1568 + 0.4636 + 0.0043, time: 21.949737]
2023-06-01 10:37:52.385: epoch 33:	0.03047130  	0.08283594  	0.06758505  
2023-06-01 10:38:14.256: [iter 34 : loss : 0.6160 = 0.1486 + 0.4629 + 0.0045, time: 21.867572]
2023-06-01 10:38:14.521: epoch 34:	0.03047871  	0.08276664  	0.06766503  
2023-06-01 10:38:36.280: [iter 35 : loss : 0.6075 = 0.1405 + 0.4623 + 0.0047, time: 21.755012]
2023-06-01 10:38:36.546: epoch 35:	0.03045650  	0.08287922  	0.06779867  
2023-06-01 10:38:58.299: [iter 36 : loss : 0.6012 = 0.1347 + 0.4616 + 0.0049, time: 21.749484]
2023-06-01 10:38:58.583: epoch 36:	0.03038988  	0.08262018  	0.06763595  
2023-06-01 10:39:20.214: [iter 37 : loss : 0.5949 = 0.1287 + 0.4612 + 0.0051, time: 21.628057]
2023-06-01 10:39:20.480: epoch 37:	0.03042690  	0.08250603  	0.06770704  
2023-06-01 10:39:42.203: [iter 38 : loss : 0.5890 = 0.1231 + 0.4606 + 0.0052, time: 21.719004]
2023-06-01 10:39:42.468: epoch 38:	0.03050833  	0.08247156  	0.06761958  
2023-06-01 10:40:04.173: [iter 39 : loss : 0.5835 = 0.1180 + 0.4601 + 0.0054, time: 21.702092]
2023-06-01 10:40:04.439: epoch 39:	0.03055276  	0.08270887  	0.06780819  
2023-06-01 10:40:26.039: [iter 40 : loss : 0.5786 = 0.1133 + 0.4598 + 0.0056, time: 21.595063]
2023-06-01 10:40:26.311: epoch 40:	0.03050093  	0.08294947  	0.06794690  
2023-06-01 10:40:48.174: [iter 41 : loss : 0.5737 = 0.1087 + 0.4592 + 0.0057, time: 21.858639]
2023-06-01 10:40:48.454: epoch 41:	0.03042690  	0.08282027  	0.06788350  
2023-06-01 10:41:10.003: [iter 42 : loss : 0.5695 = 0.1048 + 0.4588 + 0.0059, time: 21.546016]
2023-06-01 10:41:10.274: epoch 42:	0.03041210  	0.08270174  	0.06784569  
2023-06-01 10:41:31.959: [iter 43 : loss : 0.5654 = 0.1010 + 0.4584 + 0.0061, time: 21.679421]
2023-06-01 10:41:32.238: epoch 43:	0.03041209  	0.08271120  	0.06796353  
2023-06-01 10:41:53.970: [iter 44 : loss : 0.5629 = 0.0986 + 0.4581 + 0.0062, time: 21.728030]
2023-06-01 10:41:54.250: epoch 44:	0.03047132  	0.08268490  	0.06800932  
2023-06-01 10:42:15.953: [iter 45 : loss : 0.5590 = 0.0949 + 0.4577 + 0.0064, time: 21.699001]
2023-06-01 10:42:16.234: epoch 45:	0.03041951  	0.08269639  	0.06808715  
2023-06-01 10:42:37.958: [iter 46 : loss : 0.5558 = 0.0919 + 0.4573 + 0.0065, time: 21.718461]
2023-06-01 10:42:38.237: epoch 46:	0.03048613  	0.08251929  	0.06811506  
2023-06-01 10:42:59.951: [iter 47 : loss : 0.5526 = 0.0890 + 0.4570 + 0.0067, time: 21.710043]
2023-06-01 10:43:00.235: epoch 47:	0.03038989  	0.08233856  	0.06798467  
2023-06-01 10:43:21.934: [iter 48 : loss : 0.5500 = 0.0865 + 0.4567 + 0.0068, time: 21.695081]
2023-06-01 10:43:22.214: epoch 48:	0.03042690  	0.08252102  	0.06797661  
2023-06-01 10:43:43.931: [iter 49 : loss : 0.5477 = 0.0843 + 0.4564 + 0.0069, time: 21.712998]
2023-06-01 10:43:44.214: epoch 49:	0.03033805  	0.08234715  	0.06792533  
2023-06-01 10:44:05.924: [iter 50 : loss : 0.5451 = 0.0819 + 0.4561 + 0.0071, time: 21.705987]
2023-06-01 10:44:06.206: epoch 50:	0.03031584  	0.08186048  	0.06750714  
2023-06-01 10:44:27.889: [iter 51 : loss : 0.5427 = 0.0796 + 0.4559 + 0.0072, time: 21.679471]
2023-06-01 10:44:28.157: epoch 51:	0.03018258  	0.08197074  	0.06755768  
2023-06-01 10:44:49.540: [iter 52 : loss : 0.5401 = 0.0771 + 0.4556 + 0.0073, time: 21.375087]
2023-06-01 10:44:49.808: epoch 52:	0.03018259  	0.08189017  	0.06748130  
2023-06-01 10:45:11.276: [iter 53 : loss : 0.5391 = 0.0763 + 0.4553 + 0.0075, time: 21.463084]
2023-06-01 10:45:11.540: epoch 53:	0.03007894  	0.08153749  	0.06732447  
2023-06-01 10:45:33.133: [iter 54 : loss : 0.5368 = 0.0741 + 0.4551 + 0.0076, time: 21.590372]
2023-06-01 10:45:33.399: epoch 54:	0.02997530  	0.08151425  	0.06719010  
2023-06-01 10:45:54.899: [iter 55 : loss : 0.5347 = 0.0721 + 0.4549 + 0.0077, time: 21.497420]
2023-06-01 10:45:55.173: epoch 55:	0.02987906  	0.08075545  	0.06696561  
2023-06-01 10:46:16.521: [iter 56 : loss : 0.5328 = 0.0702 + 0.4547 + 0.0078, time: 21.336297]
2023-06-01 10:46:16.785: epoch 56:	0.02978281  	0.08047540  	0.06679083  
2023-06-01 10:46:38.260: [iter 57 : loss : 0.5311 = 0.0686 + 0.4546 + 0.0080, time: 21.471153]
2023-06-01 10:46:38.524: epoch 57:	0.02967917  	0.07994185  	0.06648109  
2023-06-01 10:46:38.524: Early stopping is trigger at epoch: 57
2023-06-01 10:46:38.524: best_result@epoch 32:

2023-06-01 10:46:38.524: 		0.0304      	0.0832      	0.0676      
2023-06-01 10:53:13.900: my pid: 1248
2023-06-01 10:53:13.900: model: model.general_recommender.SGL
2023-06-01 10:53:13.900: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 10:53:13.900: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 10:53:18.090: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 10:53:39.102: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.012176]
2023-06-01 10:53:39.372: epoch 1:	0.00143619  	0.00279789  	0.00238986  
2023-06-01 10:53:39.372: Find a better model.
2023-06-01 10:54:00.308: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.933158]
2023-06-01 10:54:00.603: epoch 2:	0.00163607  	0.00299639  	0.00261196  
2023-06-01 10:54:00.603: Find a better model.
2023-06-01 10:54:21.515: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 20.907612]
2023-06-01 10:54:21.806: epoch 3:	0.00233195  	0.00473967  	0.00384713  
2023-06-01 10:54:21.806: Find a better model.
2023-06-01 10:54:42.937: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 21.127416]
2023-06-01 10:54:43.245: epoch 4:	0.00266509  	0.00578939  	0.00482899  
2023-06-01 10:54:43.245: Find a better model.
2023-06-01 10:55:04.305: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.056095]
2023-06-01 10:55:04.600: epoch 5:	0.00312408  	0.00735851  	0.00571394  
2023-06-01 10:55:04.601: Find a better model.
2023-06-01 10:55:25.502: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.898096]
2023-06-01 10:55:25.790: epoch 6:	0.00362748  	0.00846281  	0.00667769  
2023-06-01 10:55:25.791: Find a better model.
2023-06-01 10:55:46.893: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 21.098659]
2023-06-01 10:55:47.197: epoch 7:	0.00430856  	0.01082550  	0.00835015  
2023-06-01 10:55:47.197: Find a better model.
2023-06-01 10:56:08.326: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 21.126029]
2023-06-01 10:56:08.616: epoch 8:	0.00510067  	0.01332103  	0.01061002  
2023-06-01 10:56:08.616: Find a better model.
2023-06-01 10:56:29.653: [iter 9 : loss : 1.1348 = 0.6914 + 0.4433 + 0.0000, time: 21.033691]
2023-06-01 10:56:29.939: epoch 9:	0.00615929  	0.01527517  	0.01266936  
2023-06-01 10:56:29.939: Find a better model.
2023-06-01 10:56:51.030: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 21.087046]
2023-06-01 10:56:51.320: epoch 10:	0.00656645  	0.01766069  	0.01459780  
2023-06-01 10:56:51.320: Find a better model.
2023-06-01 10:57:12.269: [iter 11 : loss : 1.1337 = 0.6893 + 0.4444 + 0.0000, time: 20.945022]
2023-06-01 10:57:12.554: epoch 11:	0.00774352  	0.02087095  	0.01722411  
2023-06-01 10:57:12.554: Find a better model.
2023-06-01 10:57:33.626: [iter 12 : loss : 1.1323 = 0.6872 + 0.4450 + 0.0000, time: 21.067268]
2023-06-01 10:57:33.906: epoch 12:	0.00943880  	0.02573785  	0.02139386  
2023-06-01 10:57:33.906: Find a better model.
2023-06-01 10:57:54.640: [iter 13 : loss : 1.1301 = 0.6842 + 0.4458 + 0.0001, time: 20.729022]
2023-06-01 10:57:54.922: epoch 13:	0.01220015  	0.03302851  	0.02763989  
2023-06-01 10:57:54.922: Find a better model.
2023-06-01 10:58:15.660: [iter 14 : loss : 1.1259 = 0.6793 + 0.4465 + 0.0001, time: 20.734146]
2023-06-01 10:58:15.938: epoch 14:	0.01559081  	0.04168467  	0.03565203  
2023-06-01 10:58:15.938: Find a better model.
2023-06-01 10:58:36.630: [iter 15 : loss : 1.1183 = 0.6708 + 0.4474 + 0.0001, time: 20.687026]
2023-06-01 10:58:36.906: epoch 15:	0.01887783  	0.05029878  	0.04336440  
2023-06-01 10:58:36.906: Find a better model.
2023-06-01 10:58:57.625: [iter 16 : loss : 1.1049 = 0.6562 + 0.4486 + 0.0002, time: 20.716030]
2023-06-01 10:58:57.900: epoch 16:	0.02200941  	0.05907951  	0.05096452  
2023-06-01 10:58:57.900: Find a better model.
2023-06-01 10:59:18.634: [iter 17 : loss : 1.0829 = 0.6328 + 0.4498 + 0.0003, time: 20.731051]
2023-06-01 10:59:18.910: epoch 17:	0.02483744  	0.06556110  	0.05661055  
2023-06-01 10:59:18.910: Find a better model.
2023-06-01 10:59:39.630: [iter 18 : loss : 1.0501 = 0.5979 + 0.4517 + 0.0004, time: 20.716089]
2023-06-01 10:59:39.907: epoch 18:	0.02652539  	0.07017844  	0.05999601  
2023-06-01 10:59:39.907: Find a better model.
2023-06-01 11:00:00.637: [iter 19 : loss : 1.0080 = 0.5531 + 0.4542 + 0.0006, time: 20.727395]
2023-06-01 11:00:00.914: epoch 19:	0.02795423  	0.07373930  	0.06266571  
2023-06-01 11:00:00.914: Find a better model.
2023-06-01 11:00:21.634: [iter 20 : loss : 0.9594 = 0.5014 + 0.4571 + 0.0009, time: 20.716935]
2023-06-01 11:00:21.933: epoch 20:	0.02884262  	0.07670556  	0.06417983  
2023-06-01 11:00:21.933: Find a better model.
2023-06-01 11:00:43.141: [iter 21 : loss : 0.9097 = 0.4482 + 0.4603 + 0.0012, time: 21.204148]
2023-06-01 11:00:43.422: epoch 21:	0.02949409  	0.07893288  	0.06550247  
2023-06-01 11:00:43.422: Find a better model.
2023-06-01 11:01:04.738: [iter 22 : loss : 0.8622 = 0.3975 + 0.4632 + 0.0015, time: 21.312550]
2023-06-01 11:01:05.012: epoch 22:	0.02961995  	0.07978735  	0.06569128  
2023-06-01 11:01:05.012: Find a better model.
2023-06-01 11:01:26.203: [iter 23 : loss : 0.8208 = 0.3537 + 0.4653 + 0.0018, time: 21.186149]
2023-06-01 11:01:26.476: epoch 23:	0.02961254  	0.08011366  	0.06582356  
2023-06-01 11:01:26.476: Find a better model.
2023-06-01 11:01:47.519: [iter 24 : loss : 0.7852 = 0.3164 + 0.4667 + 0.0021, time: 21.038595]
2023-06-01 11:01:47.787: epoch 24:	0.02966438  	0.08054704  	0.06625088  
2023-06-01 11:01:47.787: Find a better model.
2023-06-01 11:02:08.976: [iter 25 : loss : 0.7546 = 0.2849 + 0.4674 + 0.0024, time: 21.185186]
2023-06-01 11:02:09.249: epoch 25:	0.02996050  	0.08114365  	0.06647275  
2023-06-01 11:02:09.250: Find a better model.
2023-06-01 11:02:30.506: [iter 26 : loss : 0.7294 = 0.2592 + 0.4676 + 0.0026, time: 21.252082]
2023-06-01 11:02:30.771: epoch 26:	0.03010857  	0.08165973  	0.06672224  
2023-06-01 11:02:30.771: Find a better model.
2023-06-01 11:02:51.917: [iter 27 : loss : 0.7072 = 0.2370 + 0.4673 + 0.0029, time: 21.141348]
2023-06-01 11:02:52.193: epoch 27:	0.03005675  	0.08137149  	0.06652604  
2023-06-01 11:03:13.374: [iter 28 : loss : 0.6887 = 0.2186 + 0.4670 + 0.0032, time: 21.176071]
2023-06-01 11:03:13.655: epoch 28:	0.03016041  	0.08175927  	0.06693463  
2023-06-01 11:03:13.655: Find a better model.
2023-06-01 11:03:34.894: [iter 29 : loss : 0.6733 = 0.2034 + 0.4665 + 0.0034, time: 21.235296]
2023-06-01 11:03:35.177: epoch 29:	0.03021962  	0.08179346  	0.06729969  
2023-06-01 11:03:35.177: Find a better model.
2023-06-01 11:03:56.476: [iter 30 : loss : 0.6588 = 0.1893 + 0.4659 + 0.0036, time: 21.295027]
2023-06-01 11:03:56.739: epoch 30:	0.03015299  	0.08151840  	0.06729042  
2023-06-01 11:04:18.085: [iter 31 : loss : 0.6461 = 0.1770 + 0.4652 + 0.0039, time: 21.341453]
2023-06-01 11:04:18.351: epoch 31:	0.03024184  	0.08176429  	0.06742965  
2023-06-01 11:04:39.694: [iter 32 : loss : 0.6352 = 0.1666 + 0.4645 + 0.0041, time: 21.339036]
2023-06-01 11:04:39.958: epoch 32:	0.03038988  	0.08177396  	0.06742412  
2023-06-01 11:05:01.275: [iter 33 : loss : 0.6263 = 0.1580 + 0.4640 + 0.0043, time: 21.314381]
2023-06-01 11:05:01.537: epoch 33:	0.03045651  	0.08183274  	0.06757881  
2023-06-01 11:05:01.537: Find a better model.
2023-06-01 11:05:22.877: [iter 34 : loss : 0.6172 = 0.1494 + 0.4633 + 0.0045, time: 21.335324]
2023-06-01 11:05:23.161: epoch 34:	0.03047132  	0.08196393  	0.06764216  
2023-06-01 11:05:23.161: Find a better model.
2023-06-01 11:05:44.666: [iter 35 : loss : 0.6088 = 0.1416 + 0.4625 + 0.0047, time: 21.501021]
2023-06-01 11:05:44.928: epoch 35:	0.03044170  	0.08204155  	0.06777514  
2023-06-01 11:05:44.928: Find a better model.
2023-06-01 11:06:06.217: [iter 36 : loss : 0.6020 = 0.1351 + 0.4621 + 0.0049, time: 21.285120]
2023-06-01 11:06:06.480: epoch 36:	0.03040469  	0.08163295  	0.06751259  
2023-06-01 11:06:27.824: [iter 37 : loss : 0.5957 = 0.1292 + 0.4614 + 0.0050, time: 21.341005]
2023-06-01 11:06:28.087: epoch 37:	0.03042689  	0.08156241  	0.06756753  
2023-06-01 11:06:49.609: [iter 38 : loss : 0.5897 = 0.1235 + 0.4610 + 0.0052, time: 21.518028]
2023-06-01 11:06:49.876: epoch 38:	0.03037507  	0.08129255  	0.06752005  
2023-06-01 11:07:11.220: [iter 39 : loss : 0.5845 = 0.1187 + 0.4604 + 0.0054, time: 21.340454]
2023-06-01 11:07:11.485: epoch 39:	0.03046392  	0.08155215  	0.06759910  
2023-06-01 11:07:32.847: [iter 40 : loss : 0.5792 = 0.1137 + 0.4600 + 0.0056, time: 21.358144]
2023-06-01 11:07:33.150: epoch 40:	0.03039730  	0.08126472  	0.06754505  
2023-06-01 11:07:54.605: [iter 41 : loss : 0.5743 = 0.1090 + 0.4595 + 0.0057, time: 21.452018]
2023-06-01 11:07:54.871: epoch 41:	0.03050834  	0.08169191  	0.06768793  
2023-06-01 11:08:16.002: [iter 42 : loss : 0.5701 = 0.1050 + 0.4592 + 0.0059, time: 21.126849]
2023-06-01 11:08:16.274: epoch 42:	0.03050833  	0.08168526  	0.06766411  
2023-06-01 11:08:37.625: [iter 43 : loss : 0.5661 = 0.1013 + 0.4588 + 0.0061, time: 21.345992]
2023-06-01 11:08:37.886: epoch 43:	0.03038989  	0.08137492  	0.06747526  
2023-06-01 11:08:59.251: [iter 44 : loss : 0.5632 = 0.0987 + 0.4583 + 0.0062, time: 21.361105]
2023-06-01 11:08:59.518: epoch 44:	0.03033806  	0.08126882  	0.06733952  
2023-06-01 11:09:21.226: [iter 45 : loss : 0.5593 = 0.0950 + 0.4580 + 0.0064, time: 21.702710]
2023-06-01 11:09:21.510: epoch 45:	0.03027144  	0.08099743  	0.06717405  
2023-06-01 11:09:36.924: my pid: 7392
2023-06-01 11:09:36.924: model: model.general_recommender.SGL
2023-06-01 11:09:36.924: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 11:09:36.924: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 11:09:41.007: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 11:10:02.171: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.163451]
2023-06-01 11:10:02.435: epoch 1:	0.00152502  	0.00288674  	0.00249656  
2023-06-01 11:10:02.435: Find a better model.
2023-06-01 11:10:23.384: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.946066]
2023-06-01 11:10:23.675: epoch 2:	0.00173971  	0.00366615  	0.00283590  
2023-06-01 11:10:23.676: Find a better model.
2023-06-01 11:10:44.754: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.075420]
2023-06-01 11:10:45.043: epoch 3:	0.00206544  	0.00432867  	0.00328964  
2023-06-01 11:10:45.043: Find a better model.
2023-06-01 11:11:06.011: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.965236]
2023-06-01 11:11:06.309: epoch 4:	0.00250962  	0.00550617  	0.00454090  
2023-06-01 11:11:06.309: Find a better model.
2023-06-01 11:11:27.376: [iter 5 : loss : 1.1346 = 0.6927 + 0.4419 + 0.0000, time: 21.064392]
2023-06-01 11:11:27.666: epoch 5:	0.00295380  	0.00700134  	0.00564569  
2023-06-01 11:11:27.666: Find a better model.
2023-06-01 11:11:48.730: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.059058]
2023-06-01 11:11:49.018: epoch 6:	0.00356085  	0.00800331  	0.00659939  
2023-06-01 11:11:49.018: Find a better model.
2023-06-01 11:12:09.986: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 20.965655]
2023-06-01 11:12:10.281: epoch 7:	0.00433076  	0.01014128  	0.00838940  
2023-06-01 11:12:10.281: Find a better model.
2023-06-01 11:12:31.374: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 21.088313]
2023-06-01 11:12:31.666: epoch 8:	0.00517470  	0.01309659  	0.01049579  
2023-06-01 11:12:31.666: Find a better model.
2023-06-01 11:12:52.531: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 20.862020]
2023-06-01 11:12:52.814: epoch 9:	0.00575953  	0.01504536  	0.01227965  
2023-06-01 11:12:52.814: Find a better model.
2023-06-01 11:13:13.704: [iter 10 : loss : 1.1344 = 0.6905 + 0.4438 + 0.0000, time: 20.885180]
2023-06-01 11:13:13.989: epoch 10:	0.00660347  	0.01779297  	0.01441823  
2023-06-01 11:13:13.989: Find a better model.
2023-06-01 11:13:34.922: [iter 11 : loss : 1.1337 = 0.6892 + 0.4445 + 0.0000, time: 20.929085]
2023-06-01 11:13:35.220: epoch 11:	0.00815068  	0.02171610  	0.01764693  
2023-06-01 11:13:35.220: Find a better model.
2023-06-01 11:13:56.127: [iter 12 : loss : 1.1323 = 0.6872 + 0.4451 + 0.0000, time: 20.903056]
2023-06-01 11:13:56.430: epoch 12:	0.00983857  	0.02589430  	0.02203116  
2023-06-01 11:13:56.430: Find a better model.
2023-06-01 11:14:17.116: [iter 13 : loss : 1.1300 = 0.6842 + 0.4458 + 0.0001, time: 20.682102]
2023-06-01 11:14:17.395: epoch 13:	0.01213352  	0.03304674  	0.02805194  
2023-06-01 11:14:17.395: Find a better model.
2023-06-01 11:14:38.080: [iter 14 : loss : 1.1258 = 0.6792 + 0.4465 + 0.0001, time: 20.680772]
2023-06-01 11:14:38.353: epoch 14:	0.01516883  	0.04088388  	0.03522731  
2023-06-01 11:14:38.353: Find a better model.
2023-06-01 11:14:58.910: [iter 15 : loss : 1.1182 = 0.6707 + 0.4474 + 0.0001, time: 20.553045]
2023-06-01 11:14:59.198: epoch 15:	0.01852988  	0.04983681  	0.04351152  
2023-06-01 11:14:59.198: Find a better model.
2023-06-01 11:15:19.878: [iter 16 : loss : 1.1047 = 0.6560 + 0.4486 + 0.0002, time: 20.676593]
2023-06-01 11:15:20.170: epoch 16:	0.02154299  	0.05694779  	0.04969244  
2023-06-01 11:15:20.170: Find a better model.
2023-06-01 11:15:40.852: [iter 17 : loss : 1.0827 = 0.6325 + 0.4499 + 0.0003, time: 20.678240]
2023-06-01 11:15:41.141: epoch 17:	0.02454874  	0.06538093  	0.05549090  
2023-06-01 11:15:41.142: Find a better model.
2023-06-01 11:16:01.629: [iter 18 : loss : 1.0497 = 0.5973 + 0.4519 + 0.0004, time: 20.484479]
2023-06-01 11:16:01.901: epoch 18:	0.02632552  	0.07129145  	0.05957239  
2023-06-01 11:16:01.901: Find a better model.
2023-06-01 11:16:22.331: [iter 19 : loss : 1.0073 = 0.5522 + 0.4544 + 0.0006, time: 20.425370]
2023-06-01 11:16:22.625: epoch 19:	0.02790243  	0.07571430  	0.06271221  
2023-06-01 11:16:22.625: Find a better model.
2023-06-01 11:16:43.256: [iter 20 : loss : 0.9587 = 0.5002 + 0.4576 + 0.0009, time: 20.627149]
2023-06-01 11:16:43.532: epoch 20:	0.02878342  	0.07903774  	0.06488155  
2023-06-01 11:16:43.532: Find a better model.
2023-06-01 11:17:04.810: [iter 21 : loss : 0.9087 = 0.4468 + 0.4607 + 0.0012, time: 21.275363]
2023-06-01 11:17:05.080: epoch 21:	0.02934606  	0.08017134  	0.06563047  
2023-06-01 11:17:05.080: Find a better model.
2023-06-01 11:17:26.227: [iter 22 : loss : 0.8614 = 0.3963 + 0.4637 + 0.0015, time: 21.143035]
2023-06-01 11:17:26.497: epoch 22:	0.02951634  	0.08078633  	0.06576868  
2023-06-01 11:17:26.497: Find a better model.
2023-06-01 11:17:47.451: [iter 23 : loss : 0.8199 = 0.3524 + 0.4657 + 0.0018, time: 20.949477]
2023-06-01 11:17:47.717: epoch 23:	0.02959777  	0.08104455  	0.06610312  
2023-06-01 11:17:47.717: Find a better model.
2023-06-01 11:18:08.804: [iter 24 : loss : 0.7848 = 0.3156 + 0.4671 + 0.0021, time: 21.083230]
2023-06-01 11:18:09.073: epoch 24:	0.02967180  	0.08168037  	0.06625447  
2023-06-01 11:18:09.073: Find a better model.
2023-06-01 11:18:30.197: [iter 25 : loss : 0.7544 = 0.2843 + 0.4677 + 0.0024, time: 21.120597]
2023-06-01 11:18:30.461: epoch 25:	0.02982727  	0.08218610  	0.06668568  
2023-06-01 11:18:30.461: Find a better model.
2023-06-01 11:18:51.615: [iter 26 : loss : 0.7292 = 0.2587 + 0.4679 + 0.0026, time: 21.149162]
2023-06-01 11:18:51.881: epoch 26:	0.02984947  	0.08219076  	0.06687428  
2023-06-01 11:18:51.881: Find a better model.
2023-06-01 11:19:12.978: [iter 27 : loss : 0.7074 = 0.2369 + 0.4676 + 0.0029, time: 21.094029]
2023-06-01 11:19:13.259: epoch 27:	0.02988649  	0.08262221  	0.06698944  
2023-06-01 11:19:13.259: Find a better model.
2023-06-01 11:19:34.077: [iter 28 : loss : 0.6893 = 0.2189 + 0.4672 + 0.0032, time: 20.814326]
2023-06-01 11:19:34.342: epoch 28:	0.02999014  	0.08313858  	0.06730602  
2023-06-01 11:19:34.342: Find a better model.
2023-06-01 11:19:55.252: [iter 29 : loss : 0.6730 = 0.2030 + 0.4666 + 0.0034, time: 20.907008]
2023-06-01 11:19:55.515: epoch 29:	0.03004197  	0.08321486  	0.06754329  
2023-06-01 11:19:55.515: Find a better model.
2023-06-01 11:20:16.446: [iter 30 : loss : 0.6588 = 0.1892 + 0.4660 + 0.0036, time: 20.928103]
2023-06-01 11:20:16.710: epoch 30:	0.03017523  	0.08380741  	0.06789055  
2023-06-01 11:20:16.710: Find a better model.
2023-06-01 11:20:37.655: [iter 31 : loss : 0.6462 = 0.1770 + 0.4654 + 0.0039, time: 20.941388]
2023-06-01 11:20:37.921: epoch 31:	0.03030848  	0.08419842  	0.06808823  
2023-06-01 11:20:37.921: Find a better model.
2023-06-01 11:20:59.029: [iter 32 : loss : 0.6355 = 0.1668 + 0.4646 + 0.0041, time: 21.105153]
2023-06-01 11:20:59.296: epoch 32:	0.03033069  	0.08389256  	0.06827971  
2023-06-01 11:21:20.030: [iter 33 : loss : 0.6263 = 0.1581 + 0.4640 + 0.0043, time: 20.730828]
2023-06-01 11:21:20.297: epoch 33:	0.03038993  	0.08411607  	0.06833344  
2023-06-01 11:21:41.063: [iter 34 : loss : 0.6175 = 0.1498 + 0.4633 + 0.0045, time: 20.762752]
2023-06-01 11:21:41.327: epoch 34:	0.03033070  	0.08353063  	0.06834158  
2023-06-01 11:22:02.207: [iter 35 : loss : 0.6089 = 0.1417 + 0.4626 + 0.0047, time: 20.876051]
2023-06-01 11:22:02.472: epoch 35:	0.03029367  	0.08370768  	0.06834076  
2023-06-01 11:22:23.412: [iter 36 : loss : 0.6020 = 0.1351 + 0.4620 + 0.0049, time: 20.936776]
2023-06-01 11:22:23.676: epoch 36:	0.03034549  	0.08365929  	0.06839348  
2023-06-01 11:22:44.777: [iter 37 : loss : 0.5960 = 0.1294 + 0.4615 + 0.0050, time: 21.097343]
2023-06-01 11:22:45.039: epoch 37:	0.03029367  	0.08338828  	0.06841083  
2023-06-01 11:23:05.785: [iter 38 : loss : 0.5899 = 0.1237 + 0.4610 + 0.0052, time: 20.741273]
2023-06-01 11:23:06.046: epoch 38:	0.03047875  	0.08358917  	0.06847975  
2023-06-01 11:23:26.777: [iter 39 : loss : 0.5848 = 0.1190 + 0.4604 + 0.0054, time: 20.727187]
2023-06-01 11:23:27.041: epoch 39:	0.03047874  	0.08351555  	0.06846221  
2023-06-01 11:23:47.783: [iter 40 : loss : 0.5795 = 0.1140 + 0.4600 + 0.0056, time: 20.738419]
2023-06-01 11:23:48.045: epoch 40:	0.03044914  	0.08348487  	0.06838524  
2023-06-01 11:24:08.793: [iter 41 : loss : 0.5743 = 0.1091 + 0.4595 + 0.0057, time: 20.744417]
2023-06-01 11:24:09.054: epoch 41:	0.03048615  	0.08369564  	0.06848805  
2023-06-01 11:24:29.760: [iter 42 : loss : 0.5701 = 0.1052 + 0.4590 + 0.0059, time: 20.702030]
2023-06-01 11:24:30.025: epoch 42:	0.03061941  	0.08395844  	0.06866993  
2023-06-01 11:24:50.941: [iter 43 : loss : 0.5662 = 0.1014 + 0.4587 + 0.0061, time: 20.912617]
2023-06-01 11:24:51.213: epoch 43:	0.03050836  	0.08313001  	0.06848606  
2023-06-01 11:25:11.958: [iter 44 : loss : 0.5634 = 0.0990 + 0.4582 + 0.0062, time: 20.742046]
2023-06-01 11:25:12.228: epoch 44:	0.03047875  	0.08307286  	0.06857625  
2023-06-01 11:25:32.735: [iter 45 : loss : 0.5596 = 0.0953 + 0.4579 + 0.0064, time: 20.503527]
2023-06-01 11:25:33.000: epoch 45:	0.03044914  	0.08306970  	0.06839348  
2023-06-01 11:25:53.742: [iter 46 : loss : 0.5564 = 0.0923 + 0.4576 + 0.0065, time: 20.739041]
2023-06-01 11:25:54.004: epoch 46:	0.03041953  	0.08280112  	0.06842470  
2023-06-01 11:26:14.729: [iter 47 : loss : 0.5532 = 0.0892 + 0.4573 + 0.0066, time: 20.722496]
2023-06-01 11:26:14.994: epoch 47:	0.03039732  	0.08256036  	0.06831340  
2023-06-01 11:26:35.707: [iter 48 : loss : 0.5506 = 0.0868 + 0.4570 + 0.0068, time: 20.709957]
2023-06-01 11:26:35.969: epoch 48:	0.03019743  	0.08184406  	0.06791071  
2023-06-01 11:26:56.704: [iter 49 : loss : 0.5478 = 0.0843 + 0.4565 + 0.0069, time: 20.730698]
2023-06-01 11:26:56.967: epoch 49:	0.03019002  	0.08181169  	0.06790801  
2023-06-01 11:27:17.693: [iter 50 : loss : 0.5458 = 0.0825 + 0.4563 + 0.0071, time: 20.723136]
2023-06-01 11:27:17.957: epoch 50:	0.03004937  	0.08142582  	0.06760307  
2023-06-01 11:27:38.700: [iter 51 : loss : 0.5433 = 0.0800 + 0.4561 + 0.0072, time: 20.740052]
2023-06-01 11:27:38.963: epoch 51:	0.02998274  	0.08137383  	0.06750949  
2023-06-01 11:27:59.675: [iter 52 : loss : 0.5404 = 0.0773 + 0.4558 + 0.0073, time: 20.708266]
2023-06-01 11:27:59.938: epoch 52:	0.02992351  	0.08119364  	0.06745146  
2023-06-01 11:28:20.678: [iter 53 : loss : 0.5395 = 0.0763 + 0.4557 + 0.0075, time: 20.737054]
2023-06-01 11:28:20.940: epoch 53:	0.02985687  	0.08069866  	0.06714022  
2023-06-01 11:28:41.662: [iter 54 : loss : 0.5373 = 0.0744 + 0.4553 + 0.0076, time: 20.719158]
2023-06-01 11:28:41.925: epoch 54:	0.02976804  	0.08026124  	0.06683828  
2023-06-01 11:29:02.632: [iter 55 : loss : 0.5351 = 0.0723 + 0.4551 + 0.0077, time: 20.702615]
2023-06-01 11:29:02.897: epoch 55:	0.02964217  	0.07999680  	0.06661045  
2023-06-01 11:29:23.613: [iter 56 : loss : 0.5331 = 0.0704 + 0.4549 + 0.0078, time: 20.711497]
2023-06-01 11:29:23.876: epoch 56:	0.02967920  	0.07987089  	0.06653326  
2023-06-01 11:29:23.876: Early stopping is trigger at epoch: 56
2023-06-01 11:29:23.876: best_result@epoch 31:

2023-06-01 11:29:23.876: 		0.0303      	0.0842      	0.0681      
2023-06-01 14:31:20.141: my pid: 15068
2023-06-01 14:31:20.141: model: model.general_recommender.SGL
2023-06-01 14:31:20.141: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 14:31:20.141: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 14:31:24.173: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 14:31:44.918: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 20.744593]
2023-06-01 14:31:45.191: epoch 1:	0.00133254  	0.00242429  	0.00229936  
2023-06-01 14:31:45.191: Find a better model.
2023-06-01 14:32:06.332: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.137647]
2023-06-01 14:32:06.629: epoch 2:	0.00159165  	0.00301562  	0.00245519  
2023-06-01 14:32:06.629: Find a better model.
2023-06-01 14:32:27.893: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 21.260498]
2023-06-01 14:32:28.190: epoch 3:	0.00212467  	0.00433169  	0.00345513  
2023-06-01 14:32:28.190: Find a better model.
2023-06-01 14:32:49.446: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.251578]
2023-06-01 14:32:49.741: epoch 4:	0.00226533  	0.00527602  	0.00419133  
2023-06-01 14:32:49.741: Find a better model.
2023-06-01 14:33:10.689: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.940956]
2023-06-01 14:33:10.987: epoch 5:	0.00313888  	0.00706341  	0.00561087  
2023-06-01 14:33:10.987: Find a better model.
2023-06-01 14:33:32.269: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.278243]
2023-06-01 14:33:32.562: epoch 6:	0.00373853  	0.00858318  	0.00683319  
2023-06-01 14:33:32.562: Find a better model.
2023-06-01 14:33:53.691: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 21.125493]
2023-06-01 14:33:53.978: epoch 7:	0.00430115  	0.00994332  	0.00789069  
2023-06-01 14:33:53.978: Find a better model.
2023-06-01 14:34:15.080: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 21.098091]
2023-06-01 14:34:15.375: epoch 8:	0.00493781  	0.01233227  	0.00956739  
2023-06-01 14:34:15.375: Find a better model.
2023-06-01 14:34:36.276: [iter 9 : loss : 1.1347 = 0.6915 + 0.4433 + 0.0000, time: 20.898321]
2023-06-01 14:34:36.572: epoch 9:	0.00572992  	0.01495090  	0.01179039  
2023-06-01 14:34:36.572: Find a better model.
2023-06-01 14:34:57.455: [iter 10 : loss : 1.1345 = 0.6907 + 0.4438 + 0.0000, time: 20.879024]
2023-06-01 14:34:57.757: epoch 10:	0.00665529  	0.01726708  	0.01411907  
2023-06-01 14:34:57.757: Find a better model.
2023-06-01 14:35:18.678: [iter 11 : loss : 1.1338 = 0.6894 + 0.4443 + 0.0000, time: 20.911054]
2023-06-01 14:35:18.972: epoch 11:	0.00761767  	0.02003633  	0.01690375  
2023-06-01 14:35:18.973: Find a better model.
2023-06-01 14:35:39.845: [iter 12 : loss : 1.1323 = 0.6874 + 0.4449 + 0.0000, time: 20.869104]
2023-06-01 14:35:40.134: epoch 12:	0.00943880  	0.02516349  	0.02128600  
2023-06-01 14:35:40.134: Find a better model.
2023-06-01 14:36:01.075: [iter 13 : loss : 1.1303 = 0.6845 + 0.4457 + 0.0001, time: 20.937732]
2023-06-01 14:36:01.364: epoch 13:	0.01185222  	0.03139443  	0.02716189  
2023-06-01 14:36:01.364: Find a better model.
2023-06-01 14:36:22.417: [iter 14 : loss : 1.1262 = 0.6797 + 0.4464 + 0.0001, time: 21.048216]
2023-06-01 14:36:22.709: epoch 14:	0.01448035  	0.03835594  	0.03320266  
2023-06-01 14:36:22.709: Find a better model.
2023-06-01 14:36:43.614: [iter 15 : loss : 1.1189 = 0.6715 + 0.4473 + 0.0001, time: 20.902106]
2023-06-01 14:36:43.908: epoch 15:	0.01773032  	0.04676228  	0.04058402  
2023-06-01 14:36:43.909: Find a better model.
2023-06-01 14:37:05.003: [iter 16 : loss : 1.1059 = 0.6573 + 0.4484 + 0.0002, time: 21.091105]
2023-06-01 14:37:05.287: epoch 16:	0.02120244  	0.05635132  	0.04833532  
2023-06-01 14:37:05.287: Find a better model.
2023-06-01 14:37:26.394: [iter 17 : loss : 1.0847 = 0.6345 + 0.4499 + 0.0003, time: 21.102168]
2023-06-01 14:37:26.678: epoch 17:	0.02410453  	0.06378846  	0.05477124  
2023-06-01 14:37:26.678: Find a better model.
2023-06-01 14:37:47.387: [iter 18 : loss : 1.0524 = 0.6004 + 0.4516 + 0.0004, time: 20.703699]
2023-06-01 14:37:47.672: epoch 18:	0.02614043  	0.06910333  	0.05930743  
2023-06-01 14:37:47.672: Find a better model.
2023-06-01 14:38:08.413: [iter 19 : loss : 1.0106 = 0.5560 + 0.4540 + 0.0006, time: 20.737020]
2023-06-01 14:38:08.695: epoch 19:	0.02750262  	0.07370792  	0.06249024  
2023-06-01 14:38:08.695: Find a better model.
2023-06-01 14:38:29.582: [iter 20 : loss : 0.9624 = 0.5045 + 0.4570 + 0.0009, time: 20.883599]
2023-06-01 14:38:29.867: epoch 20:	0.02857608  	0.07623288  	0.06450883  
2023-06-01 14:38:29.867: Find a better model.
2023-06-01 14:38:51.383: [iter 21 : loss : 0.9126 = 0.4513 + 0.4601 + 0.0012, time: 21.510430]
2023-06-01 14:38:51.665: epoch 21:	0.02907951  	0.07739244  	0.06537013  
2023-06-01 14:38:51.665: Find a better model.
2023-06-01 14:39:13.336: [iter 22 : loss : 0.8647 = 0.4002 + 0.4630 + 0.0015, time: 21.666876]
2023-06-01 14:39:13.622: epoch 22:	0.02924978  	0.07802945  	0.06566393  
2023-06-01 14:39:13.623: Find a better model.
2023-06-01 14:39:34.936: [iter 23 : loss : 0.8229 = 0.3558 + 0.4653 + 0.0018, time: 21.307728]
2023-06-01 14:39:35.215: epoch 23:	0.02943487  	0.07835478  	0.06575790  
2023-06-01 14:39:35.215: Find a better model.
2023-06-01 14:39:56.521: [iter 24 : loss : 0.7872 = 0.3185 + 0.4666 + 0.0021, time: 21.301035]
2023-06-01 14:39:56.808: epoch 24:	0.02942006  	0.07854223  	0.06578443  
2023-06-01 14:39:56.808: Find a better model.
2023-06-01 14:40:18.404: [iter 25 : loss : 0.7562 = 0.2864 + 0.4674 + 0.0023, time: 21.591344]
2023-06-01 14:40:18.676: epoch 25:	0.02950891  	0.07855143  	0.06614339  
2023-06-01 14:40:18.676: Find a better model.
2023-06-01 14:40:40.128: [iter 26 : loss : 0.7309 = 0.2607 + 0.4676 + 0.0026, time: 21.447515]
2023-06-01 14:40:40.400: epoch 26:	0.02964217  	0.07931277  	0.06651109  
2023-06-01 14:40:40.400: Find a better model.
2023-06-01 14:41:01.900: [iter 27 : loss : 0.7087 = 0.2384 + 0.4674 + 0.0029, time: 21.495452]
2023-06-01 14:41:02.177: epoch 27:	0.02965696  	0.07935216  	0.06666596  
2023-06-01 14:41:02.178: Find a better model.
2023-06-01 14:41:23.535: [iter 28 : loss : 0.6901 = 0.2199 + 0.4671 + 0.0031, time: 21.354501]
2023-06-01 14:41:23.821: epoch 28:	0.02981244  	0.07975737  	0.06684387  
2023-06-01 14:41:23.821: Find a better model.
2023-06-01 14:41:45.466: [iter 29 : loss : 0.6743 = 0.2045 + 0.4664 + 0.0034, time: 21.641858]
2023-06-01 14:41:45.736: epoch 29:	0.02989386  	0.07990439  	0.06698862  
2023-06-01 14:41:45.737: Find a better model.
2023-06-01 14:42:07.474: [iter 30 : loss : 0.6594 = 0.1900 + 0.4658 + 0.0036, time: 21.733069]
2023-06-01 14:42:07.751: epoch 30:	0.02984204  	0.08014446  	0.06707301  
2023-06-01 14:42:07.752: Find a better model.
2023-06-01 14:42:29.466: [iter 31 : loss : 0.6470 = 0.1779 + 0.4653 + 0.0038, time: 21.705394]
2023-06-01 14:42:29.738: epoch 31:	0.02996050  	0.08082885  	0.06731502  
2023-06-01 14:42:29.738: Find a better model.
2023-06-01 14:42:51.460: [iter 32 : loss : 0.6359 = 0.1673 + 0.4646 + 0.0041, time: 21.718042]
2023-06-01 14:42:51.735: epoch 32:	0.02983465  	0.08067343  	0.06727192  
2023-06-01 14:43:13.439: [iter 33 : loss : 0.6272 = 0.1590 + 0.4639 + 0.0043, time: 21.698728]
2023-06-01 14:43:13.707: epoch 33:	0.03006415  	0.08055379  	0.06755646  
2023-06-01 14:43:35.504: [iter 34 : loss : 0.6178 = 0.1501 + 0.4633 + 0.0045, time: 21.793528]
2023-06-01 14:43:35.790: epoch 34:	0.03015298  	0.08074318  	0.06767042  
2023-06-01 14:43:57.432: [iter 35 : loss : 0.6090 = 0.1416 + 0.4627 + 0.0047, time: 21.636056]
2023-06-01 14:43:57.704: epoch 35:	0.03013816  	0.08055906  	0.06757320  
2023-06-01 14:44:19.427: [iter 36 : loss : 0.6025 = 0.1356 + 0.4620 + 0.0049, time: 21.720044]
2023-06-01 14:44:19.699: epoch 36:	0.03009373  	0.08012099  	0.06747641  
2023-06-01 14:44:41.435: [iter 37 : loss : 0.5961 = 0.1295 + 0.4615 + 0.0050, time: 21.733058]
2023-06-01 14:44:41.708: epoch 37:	0.03006413  	0.08050986  	0.06759729  
2023-06-01 14:45:03.374: [iter 38 : loss : 0.5899 = 0.1238 + 0.4609 + 0.0052, time: 21.661494]
2023-06-01 14:45:03.644: epoch 38:	0.02999009  	0.08055083  	0.06756542  
2023-06-01 14:45:25.410: [iter 39 : loss : 0.5848 = 0.1190 + 0.4605 + 0.0054, time: 21.761101]
2023-06-01 14:45:25.681: epoch 39:	0.02991606  	0.08037608  	0.06754059  
2023-06-01 14:45:47.385: [iter 40 : loss : 0.5794 = 0.1139 + 0.4600 + 0.0056, time: 21.698456]
2023-06-01 14:45:47.656: epoch 40:	0.02997528  	0.08042921  	0.06750865  
2023-06-01 14:46:09.190: [iter 41 : loss : 0.5745 = 0.1093 + 0.4595 + 0.0057, time: 21.530663]
2023-06-01 14:46:09.459: epoch 41:	0.02996048  	0.08027841  	0.06748081  
2023-06-01 14:46:31.149: [iter 42 : loss : 0.5704 = 0.1055 + 0.4591 + 0.0059, time: 21.686321]
2023-06-01 14:46:31.420: epoch 42:	0.02996047  	0.08039250  	0.06745335  
2023-06-01 14:46:53.161: [iter 43 : loss : 0.5665 = 0.1017 + 0.4588 + 0.0060, time: 21.736199]
2023-06-01 14:46:53.431: epoch 43:	0.02995307  	0.08013957  	0.06734198  
2023-06-01 14:47:14.762: [iter 44 : loss : 0.5639 = 0.0993 + 0.4583 + 0.0062, time: 21.327417]
2023-06-01 14:47:15.029: epoch 44:	0.02994566  	0.07983977  	0.06723662  
2023-06-01 14:47:36.531: [iter 45 : loss : 0.5596 = 0.0954 + 0.4579 + 0.0064, time: 21.496303]
2023-06-01 14:47:36.814: epoch 45:	0.02977540  	0.07924931  	0.06693096  
2023-06-01 14:47:58.380: [iter 46 : loss : 0.5567 = 0.0926 + 0.4576 + 0.0065, time: 21.562220]
2023-06-01 14:47:58.652: epoch 46:	0.02981981  	0.07915606  	0.06704172  
2023-06-01 14:48:20.159: [iter 47 : loss : 0.5534 = 0.0896 + 0.4572 + 0.0066, time: 21.503048]
2023-06-01 14:48:20.433: epoch 47:	0.02974577  	0.07917307  	0.06695813  
2023-06-01 14:48:41.996: [iter 48 : loss : 0.5508 = 0.0871 + 0.4570 + 0.0068, time: 21.559388]
2023-06-01 14:48:42.285: epoch 48:	0.02970875  	0.07883994  	0.06674451  
2023-06-01 14:49:09.817: my pid: 9144
2023-06-01 14:49:09.817: model: model.general_recommender.SGL
2023-06-01 14:49:09.817: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 14:49:09.817: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 14:49:13.869: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 14:49:35.799: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.929372]
2023-06-01 14:49:36.068: epoch 1:	0.00145099  	0.00305217  	0.00248638  
2023-06-01 14:49:36.068: Find a better model.
2023-06-01 14:49:56.929: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.858121]
2023-06-01 14:49:57.224: epoch 2:	0.00154723  	0.00343966  	0.00259538  
2023-06-01 14:49:57.224: Find a better model.
2023-06-01 14:50:18.163: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.933710]
2023-06-01 14:50:18.442: epoch 3:	0.00211727  	0.00448373  	0.00361433  
2023-06-01 14:50:18.442: Find a better model.
2023-06-01 14:50:39.354: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.907522]
2023-06-01 14:50:39.649: epoch 4:	0.00253183  	0.00514022  	0.00415657  
2023-06-01 14:50:39.649: Find a better model.
2023-06-01 14:51:00.512: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.857422]
2023-06-01 14:51:00.810: epoch 5:	0.00280575  	0.00613118  	0.00493031  
2023-06-01 14:51:00.810: Find a better model.
2023-06-01 14:51:21.523: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.710366]
2023-06-01 14:51:21.829: epoch 6:	0.00354605  	0.00821576  	0.00671523  
2023-06-01 14:51:21.829: Find a better model.
2023-06-01 14:51:42.486: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.652397]
2023-06-01 14:51:42.795: epoch 7:	0.00422712  	0.00953222  	0.00811874  
2023-06-01 14:51:42.795: Find a better model.
2023-06-01 14:52:03.300: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 20.500485]
2023-06-01 14:52:03.589: epoch 8:	0.00473793  	0.01126515  	0.00990216  
2023-06-01 14:52:03.589: Find a better model.
2023-06-01 14:52:24.140: [iter 9 : loss : 1.1346 = 0.6914 + 0.4432 + 0.0000, time: 20.547288]
2023-06-01 14:52:24.428: epoch 9:	0.00593721  	0.01560480  	0.01263500  
2023-06-01 14:52:24.428: Find a better model.
2023-06-01 14:52:44.885: [iter 10 : loss : 1.1344 = 0.6906 + 0.4437 + 0.0000, time: 20.453562]
2023-06-01 14:52:45.174: epoch 10:	0.00660347  	0.01653010  	0.01381708  
2023-06-01 14:52:45.174: Find a better model.
2023-06-01 14:53:05.650: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 20.472857]
2023-06-01 14:53:05.943: epoch 11:	0.00753624  	0.01996950  	0.01647684  
2023-06-01 14:53:05.943: Find a better model.
2023-06-01 14:53:26.481: [iter 12 : loss : 1.1323 = 0.6873 + 0.4449 + 0.0000, time: 20.535543]
2023-06-01 14:53:26.771: epoch 12:	0.00935737  	0.02529363  	0.02137206  
2023-06-01 14:53:26.771: Find a better model.
2023-06-01 14:53:47.423: [iter 13 : loss : 1.1301 = 0.6844 + 0.4457 + 0.0001, time: 20.642064]
2023-06-01 14:53:47.707: epoch 13:	0.01212613  	0.03281917  	0.02815359  
2023-06-01 14:53:47.707: Find a better model.
2023-06-01 14:54:08.232: [iter 14 : loss : 1.1259 = 0.6794 + 0.4464 + 0.0001, time: 20.520175]
2023-06-01 14:54:08.516: epoch 14:	0.01493933  	0.04088169  	0.03488972  
2023-06-01 14:54:08.516: Find a better model.
2023-06-01 14:54:28.852: [iter 15 : loss : 1.1184 = 0.6710 + 0.4473 + 0.0001, time: 20.332042]
2023-06-01 14:54:29.130: epoch 15:	0.01827075  	0.04938402  	0.04231596  
2023-06-01 14:54:29.130: Find a better model.
2023-06-01 14:54:49.456: [iter 16 : loss : 1.1051 = 0.6565 + 0.4484 + 0.0002, time: 20.321210]
2023-06-01 14:54:49.735: epoch 16:	0.02124688  	0.05688355  	0.04853857  
2023-06-01 14:54:49.735: Find a better model.
2023-06-01 14:55:10.196: [iter 17 : loss : 1.0834 = 0.6333 + 0.4498 + 0.0003, time: 20.457116]
2023-06-01 14:55:10.476: epoch 17:	0.02391205  	0.06381342  	0.05411435  
2023-06-01 14:55:10.476: Find a better model.
2023-06-01 14:55:30.844: [iter 18 : loss : 1.0506 = 0.5986 + 0.4516 + 0.0004, time: 20.364262]
2023-06-01 14:55:31.122: epoch 18:	0.02604417  	0.06992249  	0.05847720  
2023-06-01 14:55:31.123: Find a better model.
2023-06-01 14:55:51.602: [iter 19 : loss : 1.0085 = 0.5538 + 0.4541 + 0.0006, time: 20.476291]
2023-06-01 14:55:51.893: epoch 19:	0.02765068  	0.07442574  	0.06180701  
2023-06-01 14:55:51.893: Find a better model.
2023-06-01 14:56:12.386: [iter 20 : loss : 0.9602 = 0.5023 + 0.4570 + 0.0009, time: 20.488638]
2023-06-01 14:56:12.670: epoch 20:	0.02859830  	0.07755476  	0.06394417  
2023-06-01 14:56:12.671: Find a better model.
2023-06-01 14:56:33.577: [iter 21 : loss : 0.9103 = 0.4490 + 0.4601 + 0.0012, time: 20.903184]
2023-06-01 14:56:33.865: epoch 21:	0.02935343  	0.07909953  	0.06519498  
2023-06-01 14:56:33.865: Find a better model.
2023-06-01 14:56:54.916: [iter 22 : loss : 0.8626 = 0.3982 + 0.4629 + 0.0015, time: 21.047266]
2023-06-01 14:56:55.195: epoch 22:	0.02963475  	0.08001868  	0.06570598  
2023-06-01 14:56:55.195: Find a better model.
2023-06-01 14:57:16.152: [iter 23 : loss : 0.8211 = 0.3542 + 0.4651 + 0.0018, time: 20.954392]
2023-06-01 14:57:16.432: epoch 23:	0.02963476  	0.08042514  	0.06609892  
2023-06-01 14:57:16.432: Find a better model.
2023-06-01 14:57:37.571: [iter 24 : loss : 0.7854 = 0.3168 + 0.4666 + 0.0021, time: 21.136038]
2023-06-01 14:57:37.859: epoch 24:	0.02976061  	0.08143364  	0.06669407  
2023-06-01 14:57:37.859: Find a better model.
2023-06-01 14:57:58.748: [iter 25 : loss : 0.7547 = 0.2852 + 0.4672 + 0.0024, time: 20.884636]
2023-06-01 14:57:59.023: epoch 25:	0.03001973  	0.08226719  	0.06720217  
2023-06-01 14:57:59.023: Find a better model.
2023-06-01 14:58:19.919: [iter 26 : loss : 0.7294 = 0.2593 + 0.4675 + 0.0026, time: 20.892984]
2023-06-01 14:58:20.191: epoch 26:	0.03013079  	0.08262118  	0.06754620  
2023-06-01 14:58:20.191: Find a better model.
2023-06-01 14:58:41.146: [iter 27 : loss : 0.7076 = 0.2372 + 0.4675 + 0.0029, time: 20.952783]
2023-06-01 14:58:41.418: epoch 27:	0.03004937  	0.08251421  	0.06750729  
2023-06-01 14:59:02.502: [iter 28 : loss : 0.6891 = 0.2190 + 0.4669 + 0.0032, time: 21.079352]
2023-06-01 14:59:02.773: epoch 28:	0.03001234  	0.08249742  	0.06746495  
2023-06-01 14:59:23.674: [iter 29 : loss : 0.6733 = 0.2035 + 0.4664 + 0.0034, time: 20.895062]
2023-06-01 14:59:23.944: epoch 29:	0.03013819  	0.08279800  	0.06778298  
2023-06-01 14:59:23.944: Find a better model.
2023-06-01 14:59:44.734: [iter 30 : loss : 0.6586 = 0.1891 + 0.4658 + 0.0036, time: 20.786315]
2023-06-01 14:59:45.007: epoch 30:	0.03013078  	0.08270316  	0.06790381  
2023-06-01 15:00:05.889: [iter 31 : loss : 0.6464 = 0.1773 + 0.4652 + 0.0039, time: 20.878061]
2023-06-01 15:00:06.157: epoch 31:	0.03017520  	0.08260038  	0.06804670  
2023-06-01 15:00:27.071: [iter 32 : loss : 0.6354 = 0.1668 + 0.4645 + 0.0041, time: 20.910392]
2023-06-01 15:00:27.340: epoch 32:	0.03008637  	0.08231954  	0.06798500  
2023-06-01 15:00:48.119: [iter 33 : loss : 0.6260 = 0.1578 + 0.4639 + 0.0043, time: 20.776045]
2023-06-01 15:00:48.389: epoch 33:	0.03015301  	0.08260166  	0.06817396  
2023-06-01 15:01:09.461: [iter 34 : loss : 0.6174 = 0.1497 + 0.4632 + 0.0045, time: 21.068155]
2023-06-01 15:01:09.730: epoch 34:	0.03019742  	0.08272116  	0.06829024  
2023-06-01 15:01:30.871: [iter 35 : loss : 0.6086 = 0.1414 + 0.4626 + 0.0047, time: 21.138315]
2023-06-01 15:01:31.145: epoch 35:	0.03018261  	0.08290227  	0.06834831  
2023-06-01 15:01:31.145: Find a better model.
2023-06-01 15:01:52.132: [iter 36 : loss : 0.6021 = 0.1353 + 0.4620 + 0.0049, time: 20.984241]
2023-06-01 15:01:52.400: epoch 36:	0.03016779  	0.08263297  	0.06835175  
2023-06-01 15:02:13.661: [iter 37 : loss : 0.5959 = 0.1294 + 0.4615 + 0.0050, time: 21.257016]
2023-06-01 15:02:13.932: epoch 37:	0.03036028  	0.08304028  	0.06854554  
2023-06-01 15:02:13.932: Find a better model.
2023-06-01 15:02:35.467: [iter 38 : loss : 0.5895 = 0.1234 + 0.4609 + 0.0052, time: 21.530626]
2023-06-01 15:02:35.740: epoch 38:	0.03030105  	0.08238113  	0.06850153  
2023-06-01 15:02:56.680: [iter 39 : loss : 0.5844 = 0.1186 + 0.4604 + 0.0054, time: 20.936266]
2023-06-01 15:02:56.954: epoch 39:	0.03036028  	0.08241165  	0.06855948  
2023-06-01 15:03:18.043: [iter 40 : loss : 0.5794 = 0.1139 + 0.4599 + 0.0056, time: 21.085848]
2023-06-01 15:03:18.310: epoch 40:	0.03031587  	0.08229359  	0.06850140  
2023-06-01 15:03:39.601: [iter 41 : loss : 0.5745 = 0.1093 + 0.4595 + 0.0057, time: 21.287651]
2023-06-01 15:03:39.875: epoch 41:	0.03028625  	0.08199509  	0.06843020  
2023-06-01 15:04:00.649: [iter 42 : loss : 0.5702 = 0.1053 + 0.4590 + 0.0059, time: 20.769578]
2023-06-01 15:04:00.923: epoch 42:	0.03021222  	0.08180387  	0.06833947  
2023-06-01 15:04:21.816: [iter 43 : loss : 0.5658 = 0.1011 + 0.4587 + 0.0061, time: 20.890108]
2023-06-01 15:04:22.083: epoch 43:	0.03010117  	0.08149272  	0.06812401  
2023-06-01 15:04:43.161: [iter 44 : loss : 0.5633 = 0.0988 + 0.4583 + 0.0062, time: 21.074078]
2023-06-01 15:04:43.429: epoch 44:	0.03016780  	0.08187060  	0.06822910  
2023-06-01 15:05:04.588: [iter 45 : loss : 0.5594 = 0.0951 + 0.4579 + 0.0064, time: 21.155258]
2023-06-01 15:05:04.866: epoch 45:	0.03010856  	0.08200694  	0.06825200  
2023-06-01 15:05:25.967: [iter 46 : loss : 0.5567 = 0.0926 + 0.4576 + 0.0065, time: 21.096061]
2023-06-01 15:05:26.235: epoch 46:	0.03013077  	0.08215755  	0.06846974  
2023-06-01 15:05:47.357: [iter 47 : loss : 0.5531 = 0.0892 + 0.4573 + 0.0066, time: 21.117051]
2023-06-01 15:05:47.623: epoch 47:	0.02996049  	0.08162108  	0.06804786  
2023-06-01 15:06:08.781: [iter 48 : loss : 0.5505 = 0.0868 + 0.4569 + 0.0068, time: 21.153478]
2023-06-01 15:06:09.048: epoch 48:	0.02996789  	0.08143522  	0.06781306  
2023-06-01 15:06:30.134: [iter 49 : loss : 0.5480 = 0.0845 + 0.4566 + 0.0069, time: 21.082539]
2023-06-01 15:06:30.402: epoch 49:	0.02993828  	0.08101840  	0.06752842  
2023-06-01 15:06:51.527: [iter 50 : loss : 0.5458 = 0.0824 + 0.4564 + 0.0071, time: 21.120409]
2023-06-01 15:06:51.807: epoch 50:	0.02992347  	0.08084184  	0.06752883  
2023-06-01 15:07:12.738: [iter 51 : loss : 0.5429 = 0.0796 + 0.4561 + 0.0072, time: 20.928894]
2023-06-01 15:07:13.011: epoch 51:	0.02979762  	0.08036570  	0.06725895  
2023-06-01 15:07:34.117: [iter 52 : loss : 0.5406 = 0.0775 + 0.4558 + 0.0073, time: 21.101082]
2023-06-01 15:07:34.384: epoch 52:	0.02964215  	0.07988710  	0.06714784  
2023-06-01 15:07:55.506: [iter 53 : loss : 0.5399 = 0.0769 + 0.4556 + 0.0075, time: 21.118526]
2023-06-01 15:07:55.773: epoch 53:	0.02959032  	0.07986751  	0.06698325  
2023-06-01 15:08:16.726: [iter 54 : loss : 0.5372 = 0.0743 + 0.4553 + 0.0076, time: 20.943852]
2023-06-01 15:08:16.996: epoch 54:	0.02951628  	0.07941532  	0.06684376  
2023-06-01 15:08:37.889: [iter 55 : loss : 0.5352 = 0.0724 + 0.4551 + 0.0077, time: 20.889015]
2023-06-01 15:08:38.155: epoch 55:	0.02949408  	0.07951207  	0.06675024  
2023-06-01 15:08:59.117: [iter 56 : loss : 0.5332 = 0.0705 + 0.4549 + 0.0078, time: 20.959586]
2023-06-01 15:08:59.381: epoch 56:	0.02951628  	0.07932105  	0.06674293  
2023-06-01 15:09:20.121: [iter 57 : loss : 0.5313 = 0.0687 + 0.4547 + 0.0079, time: 20.736085]
2023-06-01 15:09:20.388: epoch 57:	0.02947186  	0.07903109  	0.06657957  
2023-06-01 15:09:41.266: [iter 58 : loss : 0.5300 = 0.0674 + 0.4546 + 0.0081, time: 20.873499]
2023-06-01 15:09:41.531: epoch 58:	0.02954590  	0.07894902  	0.06657261  
2023-06-01 15:10:02.480: [iter 59 : loss : 0.5286 = 0.0661 + 0.4544 + 0.0082, time: 20.946040]
2023-06-01 15:10:02.747: epoch 59:	0.02941264  	0.07861054  	0.06636944  
2023-06-01 15:10:23.475: [iter 60 : loss : 0.5273 = 0.0649 + 0.4541 + 0.0083, time: 20.725038]
2023-06-01 15:10:23.741: epoch 60:	0.02949408  	0.07875878  	0.06635246  
2023-06-01 15:10:44.479: [iter 61 : loss : 0.5255 = 0.0630 + 0.4541 + 0.0084, time: 20.734469]
2023-06-01 15:10:44.744: epoch 61:	0.02939783  	0.07846583  	0.06619798  
2023-06-01 15:11:05.667: [iter 62 : loss : 0.5250 = 0.0625 + 0.4540 + 0.0085, time: 20.918480]
2023-06-01 15:11:05.934: epoch 62:	0.02932380  	0.07820756  	0.06607673  
2023-06-01 15:11:05.934: Early stopping is trigger at epoch: 62
2023-06-01 15:11:05.934: best_result@epoch 37:

2023-06-01 15:11:05.934: 		0.0304      	0.0830      	0.0685      
2023-06-01 15:19:52.319: my pid: 15268
2023-06-01 15:19:52.319: model: model.general_recommender.SGL
2023-06-01 15:19:52.319: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 15:19:52.319: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 15:19:56.413: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 15:20:17.301: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.886771]
2023-06-01 15:20:17.575: epoch 1:	0.00118448  	0.00217694  	0.00181670  
2023-06-01 15:20:17.575: Find a better model.
2023-06-01 15:20:38.718: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.138415]
2023-06-01 15:20:39.010: epoch 2:	0.00186556  	0.00329996  	0.00273242  
2023-06-01 15:20:39.010: Find a better model.
2023-06-01 15:21:00.120: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.107072]
2023-06-01 15:21:00.403: epoch 3:	0.00208025  	0.00438991  	0.00338974  
2023-06-01 15:21:00.403: Find a better model.
2023-06-01 15:21:21.489: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.082068]
2023-06-01 15:21:21.782: epoch 4:	0.00233936  	0.00486474  	0.00395372  
2023-06-01 15:21:21.782: Find a better model.
2023-06-01 15:21:43.196: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.409802]
2023-06-01 15:21:43.479: epoch 5:	0.00284276  	0.00635210  	0.00501918  
2023-06-01 15:21:43.480: Find a better model.
2023-06-01 15:22:04.626: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 21.141425]
2023-06-01 15:22:04.922: epoch 6:	0.00338318  	0.00739066  	0.00625299  
2023-06-01 15:22:04.922: Find a better model.
2023-06-01 15:22:25.889: [iter 7 : loss : 1.1346 = 0.6923 + 0.4424 + 0.0000, time: 20.963532]
2023-06-01 15:22:26.193: epoch 7:	0.00397542  	0.00937399  	0.00799503  
2023-06-01 15:22:26.193: Find a better model.
2023-06-01 15:22:47.081: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.884187]
2023-06-01 15:22:47.375: epoch 8:	0.00498963  	0.01217237  	0.01022033  
2023-06-01 15:22:47.375: Find a better model.
2023-06-01 15:23:08.378: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.998765]
2023-06-01 15:23:08.679: epoch 9:	0.00593720  	0.01496360  	0.01228638  
2023-06-01 15:23:08.679: Find a better model.
2023-06-01 15:23:29.833: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 21.151348]
2023-06-01 15:23:30.122: epoch 10:	0.00755104  	0.01971685  	0.01591508  
2023-06-01 15:23:30.122: Find a better model.
2023-06-01 15:23:51.102: [iter 11 : loss : 1.1339 = 0.6897 + 0.4441 + 0.0000, time: 20.976098]
2023-06-01 15:23:51.392: epoch 11:	0.00867630  	0.02176307  	0.01842499  
2023-06-01 15:23:51.392: Find a better model.
2023-06-01 15:24:12.486: [iter 12 : loss : 1.1326 = 0.6878 + 0.4448 + 0.0000, time: 21.090025]
2023-06-01 15:24:12.772: epoch 12:	0.00996442  	0.02666098  	0.02252003  
2023-06-01 15:24:12.772: Find a better model.
2023-06-01 15:24:33.628: [iter 13 : loss : 1.1304 = 0.6849 + 0.4455 + 0.0001, time: 20.852441]
2023-06-01 15:24:33.907: epoch 13:	0.01216314  	0.03324915  	0.02774913  
2023-06-01 15:24:33.907: Find a better model.
2023-06-01 15:24:54.462: [iter 14 : loss : 1.1265 = 0.6801 + 0.4463 + 0.0001, time: 20.550066]
2023-06-01 15:24:54.743: epoch 14:	0.01528728  	0.04161604  	0.03503234  
2023-06-01 15:24:54.743: Find a better model.
2023-06-01 15:25:15.201: [iter 15 : loss : 1.1193 = 0.6720 + 0.4472 + 0.0001, time: 20.453597]
2023-06-01 15:25:15.477: epoch 15:	0.01870014  	0.05020384  	0.04262607  
2023-06-01 15:25:15.477: Find a better model.
2023-06-01 15:25:36.231: [iter 16 : loss : 1.1066 = 0.6582 + 0.4483 + 0.0002, time: 20.750165]
2023-06-01 15:25:36.509: epoch 16:	0.02152078  	0.05678837  	0.04904113  
2023-06-01 15:25:36.509: Find a better model.
2023-06-01 15:25:57.368: [iter 17 : loss : 1.0856 = 0.6357 + 0.4496 + 0.0003, time: 20.854990]
2023-06-01 15:25:57.638: epoch 17:	0.02405271  	0.06304332  	0.05485364  
2023-06-01 15:25:57.638: Find a better model.
2023-06-01 15:26:18.202: [iter 18 : loss : 1.0537 = 0.6019 + 0.4514 + 0.0004, time: 20.559343]
2023-06-01 15:26:18.484: epoch 18:	0.02629589  	0.06837346  	0.05890488  
2023-06-01 15:26:18.484: Find a better model.
2023-06-01 15:26:39.184: [iter 19 : loss : 1.0121 = 0.5576 + 0.4538 + 0.0006, time: 20.695899]
2023-06-01 15:26:39.462: epoch 19:	0.02779873  	0.07250500  	0.06160881  
2023-06-01 15:26:39.462: Find a better model.
2023-06-01 15:27:00.205: [iter 20 : loss : 0.9637 = 0.5061 + 0.4568 + 0.0009, time: 20.739397]
2023-06-01 15:27:00.473: epoch 20:	0.02847244  	0.07546667  	0.06336340  
2023-06-01 15:27:00.474: Find a better model.
2023-06-01 15:27:21.539: [iter 21 : loss : 0.9139 = 0.4528 + 0.4600 + 0.0011, time: 21.061984]
2023-06-01 15:27:21.814: epoch 21:	0.02896845  	0.07706407  	0.06443743  
2023-06-01 15:27:21.814: Find a better model.
2023-06-01 15:27:42.951: [iter 22 : loss : 0.8658 = 0.4015 + 0.4629 + 0.0014, time: 21.134198]
2023-06-01 15:27:43.232: epoch 22:	0.02930160  	0.07823338  	0.06491387  
2023-06-01 15:27:43.232: Find a better model.
2023-06-01 15:28:04.494: [iter 23 : loss : 0.8237 = 0.3568 + 0.4652 + 0.0018, time: 21.256984]
2023-06-01 15:28:04.765: epoch 23:	0.02933863  	0.07899001  	0.06517368  
2023-06-01 15:28:04.765: Find a better model.
2023-06-01 15:28:26.115: [iter 24 : loss : 0.7876 = 0.3190 + 0.4666 + 0.0020, time: 21.345177]
2023-06-01 15:28:26.386: epoch 24:	0.02953112  	0.07930974  	0.06547347  
2023-06-01 15:28:26.386: Find a better model.
2023-06-01 15:28:47.693: [iter 25 : loss : 0.7565 = 0.2870 + 0.4672 + 0.0023, time: 21.303121]
2023-06-01 15:28:47.964: epoch 25:	0.02953113  	0.08028959  	0.06580652  
2023-06-01 15:28:47.964: Find a better model.
2023-06-01 15:29:09.275: [iter 26 : loss : 0.7313 = 0.2611 + 0.4675 + 0.0026, time: 21.308119]
2023-06-01 15:29:09.544: epoch 26:	0.02959036  	0.08070104  	0.06595025  
2023-06-01 15:29:09.544: Find a better model.
2023-06-01 15:29:30.848: [iter 27 : loss : 0.7090 = 0.2388 + 0.4673 + 0.0029, time: 21.299402]
2023-06-01 15:29:31.115: epoch 27:	0.02969401  	0.08081859  	0.06607780  
2023-06-01 15:29:31.115: Find a better model.
2023-06-01 15:29:52.463: [iter 28 : loss : 0.6904 = 0.2203 + 0.4669 + 0.0031, time: 21.345530]
2023-06-01 15:29:52.729: epoch 28:	0.02969401  	0.08092555  	0.06602129  
2023-06-01 15:29:52.729: Find a better model.
2023-06-01 15:30:14.041: [iter 29 : loss : 0.6743 = 0.2045 + 0.4664 + 0.0034, time: 21.308107]
2023-06-01 15:30:14.308: epoch 29:	0.02953854  	0.08064944  	0.06599255  
2023-06-01 15:30:35.661: [iter 30 : loss : 0.6596 = 0.1902 + 0.4657 + 0.0036, time: 21.348140]
2023-06-01 15:30:35.927: epoch 30:	0.02964218  	0.08073393  	0.06622207  
2023-06-01 15:30:57.215: [iter 31 : loss : 0.6470 = 0.1780 + 0.4651 + 0.0038, time: 21.283113]
2023-06-01 15:30:57.479: epoch 31:	0.02975324  	0.08143911  	0.06663318  
2023-06-01 15:30:57.479: Find a better model.
2023-06-01 15:31:18.825: [iter 32 : loss : 0.6360 = 0.1674 + 0.4645 + 0.0041, time: 21.342655]
2023-06-01 15:31:19.091: epoch 32:	0.02981247  	0.08125018  	0.06658850  
2023-06-01 15:31:40.392: [iter 33 : loss : 0.6267 = 0.1588 + 0.4637 + 0.0043, time: 21.298021]
2023-06-01 15:31:40.656: epoch 33:	0.02984948  	0.08142594  	0.06671944  
2023-06-01 15:32:01.898: [iter 34 : loss : 0.6182 = 0.1506 + 0.4632 + 0.0045, time: 21.239057]
2023-06-01 15:32:02.179: epoch 34:	0.02992351  	0.08158198  	0.06668953  
2023-06-01 15:32:02.179: Find a better model.
2023-06-01 15:32:23.421: [iter 35 : loss : 0.6093 = 0.1422 + 0.4625 + 0.0047, time: 21.237256]
2023-06-01 15:32:23.686: epoch 35:	0.02991612  	0.08123269  	0.06665570  
2023-06-01 15:32:45.025: [iter 36 : loss : 0.6026 = 0.1359 + 0.4618 + 0.0048, time: 21.335093]
2023-06-01 15:32:45.297: epoch 36:	0.03005677  	0.08186825  	0.06697714  
2023-06-01 15:32:45.297: Find a better model.
2023-06-01 15:33:06.563: [iter 37 : loss : 0.5963 = 0.1299 + 0.4614 + 0.0050, time: 21.261744]
2023-06-01 15:33:06.828: epoch 37:	0.02999014  	0.08176608  	0.06699060  
2023-06-01 15:33:28.007: [iter 38 : loss : 0.5901 = 0.1240 + 0.4608 + 0.0052, time: 21.175287]
2023-06-01 15:33:28.277: epoch 38:	0.03013821  	0.08209780  	0.06710454  
2023-06-01 15:33:28.277: Find a better model.
2023-06-01 15:33:49.752: [iter 39 : loss : 0.5849 = 0.1192 + 0.4603 + 0.0054, time: 21.471019]
2023-06-01 15:33:50.023: epoch 39:	0.03012340  	0.08204886  	0.06723598  
2023-06-01 15:34:11.207: [iter 40 : loss : 0.5794 = 0.1140 + 0.4598 + 0.0056, time: 21.179789]
2023-06-01 15:34:11.471: epoch 40:	0.03017522  	0.08229978  	0.06726106  
2023-06-01 15:34:11.471: Find a better model.
2023-06-01 15:34:32.776: [iter 41 : loss : 0.5745 = 0.1094 + 0.4594 + 0.0057, time: 21.302447]
2023-06-01 15:34:33.045: epoch 41:	0.03022705  	0.08218569  	0.06740854  
2023-06-01 15:34:54.233: [iter 42 : loss : 0.5704 = 0.1056 + 0.4589 + 0.0059, time: 21.182539]
2023-06-01 15:34:54.500: epoch 42:	0.03022706  	0.08201225  	0.06746276  
2023-06-01 15:35:15.757: [iter 43 : loss : 0.5662 = 0.1017 + 0.4585 + 0.0060, time: 21.254225]
2023-06-01 15:35:16.022: epoch 43:	0.03018264  	0.08186047  	0.06737579  
2023-06-01 15:35:37.210: [iter 44 : loss : 0.5635 = 0.0991 + 0.4582 + 0.0062, time: 21.184479]
2023-06-01 15:35:37.476: epoch 44:	0.03019003  	0.08206536  	0.06741772  
2023-06-01 15:35:58.764: [iter 45 : loss : 0.5596 = 0.0955 + 0.4577 + 0.0063, time: 21.283280]
2023-06-01 15:35:59.031: epoch 45:	0.03007898  	0.08123949  	0.06715613  
2023-06-01 15:36:19.978: [iter 46 : loss : 0.5565 = 0.0925 + 0.4575 + 0.0065, time: 20.943007]
2023-06-01 15:36:20.249: epoch 46:	0.03000494  	0.08089741  	0.06702136  
2023-06-01 15:36:41.340: [iter 47 : loss : 0.5532 = 0.0894 + 0.4571 + 0.0066, time: 21.087053]
2023-06-01 15:36:41.610: epoch 47:	0.03007157  	0.08127365  	0.06717292  
2023-06-01 15:37:02.965: [iter 48 : loss : 0.5508 = 0.0872 + 0.4568 + 0.0068, time: 21.351152]
2023-06-01 15:37:03.240: epoch 48:	0.03001234  	0.08097923  	0.06693903  
2023-06-01 15:37:24.471: [iter 49 : loss : 0.5481 = 0.0848 + 0.4564 + 0.0069, time: 21.227372]
2023-06-01 15:37:24.737: epoch 49:	0.03002714  	0.08112627  	0.06695805  
2023-06-01 15:37:46.125: [iter 50 : loss : 0.5458 = 0.0826 + 0.4562 + 0.0071, time: 21.383188]
2023-06-01 15:37:46.397: epoch 50:	0.02982726  	0.08077567  	0.06661467  
2023-06-01 15:38:07.886: [iter 51 : loss : 0.5428 = 0.0797 + 0.4559 + 0.0072, time: 21.485524]
2023-06-01 15:38:08.159: epoch 51:	0.02987907  	0.08097853  	0.06668282  
2023-06-01 15:38:29.298: [iter 52 : loss : 0.5408 = 0.0777 + 0.4558 + 0.0073, time: 21.131018]
2023-06-01 15:38:29.566: epoch 52:	0.02986427  	0.08070746  	0.06652867  
2023-06-01 15:38:50.679: [iter 53 : loss : 0.5398 = 0.0769 + 0.4555 + 0.0074, time: 21.108451]
2023-06-01 15:38:50.947: epoch 53:	0.02979024  	0.08047137  	0.06648352  
2023-06-01 15:39:12.125: [iter 54 : loss : 0.5373 = 0.0745 + 0.4552 + 0.0076, time: 21.174089]
2023-06-01 15:39:12.399: epoch 54:	0.02977543  	0.07986031  	0.06633613  
2023-06-01 15:39:33.469: [iter 55 : loss : 0.5356 = 0.0729 + 0.4550 + 0.0077, time: 21.067458]
2023-06-01 15:39:33.740: epoch 55:	0.02973102  	0.07926675  	0.06602804  
2023-06-01 15:39:55.725: [iter 56 : loss : 0.5333 = 0.0708 + 0.4547 + 0.0078, time: 21.981593]
2023-06-01 15:39:56.000: epoch 56:	0.02967179  	0.07894672  	0.06602908  
2023-06-01 15:40:17.741: my pid: 9040
2023-06-01 15:40:17.741: model: model.general_recommender.SGL
2023-06-01 15:40:17.741: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 15:40:17.741: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 15:40:22.071: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 15:40:44.241: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 22.169254]
2023-06-01 15:40:44.518: epoch 1:	0.00097720  	0.00185775  	0.00155657  
2023-06-01 15:40:44.518: Find a better model.
2023-06-01 15:41:05.943: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.421371]
2023-06-01 15:41:06.250: epoch 2:	0.00165087  	0.00339836  	0.00261015  
2023-06-01 15:41:06.250: Find a better model.
2023-06-01 15:41:27.540: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.287282]
2023-06-01 15:41:27.837: epoch 3:	0.00202103  	0.00400758  	0.00315942  
2023-06-01 15:41:27.837: Find a better model.
2023-06-01 15:41:49.125: [iter 4 : loss : 1.1342 = 0.6928 + 0.4413 + 0.0000, time: 21.284477]
2023-06-01 15:41:49.410: epoch 4:	0.00236157  	0.00475203  	0.00385363  
2023-06-01 15:41:49.410: Find a better model.
2023-06-01 15:42:10.740: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.326627]
2023-06-01 15:42:11.037: epoch 5:	0.00305745  	0.00627481  	0.00500220  
2023-06-01 15:42:11.037: Find a better model.
2023-06-01 15:42:32.313: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 21.273218]
2023-06-01 15:42:32.612: epoch 6:	0.00335357  	0.00691598  	0.00555971  
2023-06-01 15:42:32.612: Find a better model.
2023-06-01 15:42:53.886: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.270487]
2023-06-01 15:42:54.197: epoch 7:	0.00397542  	0.00925659  	0.00751291  
2023-06-01 15:42:54.197: Find a better model.
2023-06-01 15:43:15.691: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.489711]
2023-06-01 15:43:15.990: epoch 8:	0.00496001  	0.01290894  	0.00962634  
2023-06-01 15:43:15.990: Find a better model.
2023-06-01 15:43:37.141: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.147540]
2023-06-01 15:43:37.426: epoch 9:	0.00653684  	0.01754567  	0.01313285  
2023-06-01 15:43:37.426: Find a better model.
2023-06-01 15:43:58.682: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.251445]
2023-06-01 15:43:58.977: epoch 10:	0.00746961  	0.02105456  	0.01685553  
2023-06-01 15:43:58.977: Find a better model.
2023-06-01 15:44:20.284: [iter 11 : loss : 1.1340 = 0.6900 + 0.4439 + 0.0000, time: 21.304709]
2023-06-01 15:44:20.576: epoch 11:	0.00868369  	0.02373989  	0.01981260  
2023-06-01 15:44:20.576: Find a better model.
2023-06-01 15:44:41.870: [iter 12 : loss : 1.1329 = 0.6884 + 0.4445 + 0.0000, time: 21.289452]
2023-06-01 15:44:42.164: epoch 12:	0.01011989  	0.02761707  	0.02332894  
2023-06-01 15:44:42.164: Find a better model.
2023-06-01 15:45:03.273: [iter 13 : loss : 1.1309 = 0.6856 + 0.4452 + 0.0000, time: 21.101213]
2023-06-01 15:45:03.562: epoch 13:	0.01201508  	0.03313431  	0.02765623  
2023-06-01 15:45:03.562: Find a better model.
2023-06-01 15:45:24.808: [iter 14 : loss : 1.1273 = 0.6812 + 0.4460 + 0.0001, time: 21.243343]
2023-06-01 15:45:25.097: epoch 14:	0.01441371  	0.03944743  	0.03404998  
2023-06-01 15:45:25.097: Find a better model.
2023-06-01 15:45:46.051: [iter 15 : loss : 1.1209 = 0.6739 + 0.4469 + 0.0001, time: 20.948366]
2023-06-01 15:45:46.339: epoch 15:	0.01795982  	0.04838078  	0.04146821  
2023-06-01 15:45:46.339: Find a better model.
2023-06-01 15:46:07.411: [iter 16 : loss : 1.1094 = 0.6613 + 0.4479 + 0.0002, time: 21.068041]
2023-06-01 15:46:07.698: epoch 16:	0.02066939  	0.05459198  	0.04790330  
2023-06-01 15:46:07.698: Find a better model.
2023-06-01 15:46:28.833: [iter 17 : loss : 1.0900 = 0.6406 + 0.4492 + 0.0003, time: 21.131560]
2023-06-01 15:46:29.123: epoch 17:	0.02414892  	0.06342009  	0.05451946  
2023-06-01 15:46:29.123: Find a better model.
2023-06-01 15:46:50.197: [iter 18 : loss : 1.0599 = 0.6086 + 0.4508 + 0.0004, time: 21.070025]
2023-06-01 15:46:50.480: epoch 18:	0.02629586  	0.06907188  	0.05898817  
2023-06-01 15:46:50.480: Find a better model.
2023-06-01 15:47:11.627: [iter 19 : loss : 1.0198 = 0.5660 + 0.4532 + 0.0006, time: 21.142514]
2023-06-01 15:47:11.908: epoch 19:	0.02778393  	0.07396304  	0.06258291  
2023-06-01 15:47:11.908: Find a better model.
2023-06-01 15:47:33.034: [iter 20 : loss : 0.9723 = 0.5153 + 0.4561 + 0.0008, time: 21.122233]
2023-06-01 15:47:33.311: epoch 20:	0.02865752  	0.07648481  	0.06423020  
2023-06-01 15:47:33.312: Find a better model.
2023-06-01 15:47:54.990: [iter 21 : loss : 0.9219 = 0.4616 + 0.4592 + 0.0011, time: 21.673336]
2023-06-01 15:47:55.275: epoch 21:	0.02923498  	0.07860918  	0.06532487  
2023-06-01 15:47:55.275: Find a better model.
2023-06-01 15:48:16.971: [iter 22 : loss : 0.8730 = 0.4093 + 0.4623 + 0.0014, time: 21.691401]
2023-06-01 15:48:17.259: epoch 22:	0.02944228  	0.07962381  	0.06543592  
2023-06-01 15:48:17.260: Find a better model.
2023-06-01 15:48:38.778: [iter 23 : loss : 0.8298 = 0.3636 + 0.4646 + 0.0017, time: 21.512876]
2023-06-01 15:48:39.053: epoch 23:	0.02956813  	0.08043573  	0.06561081  
2023-06-01 15:48:39.054: Find a better model.
2023-06-01 15:49:00.753: [iter 24 : loss : 0.7926 = 0.3244 + 0.4662 + 0.0020, time: 21.696110]
2023-06-01 15:49:01.032: epoch 24:	0.02975323  	0.08106466  	0.06611341  
2023-06-01 15:49:01.032: Find a better model.
2023-06-01 15:49:22.599: [iter 25 : loss : 0.7609 = 0.2916 + 0.4670 + 0.0023, time: 21.564189]
2023-06-01 15:49:22.882: epoch 25:	0.02991611  	0.08173235  	0.06638156  
2023-06-01 15:49:22.882: Find a better model.
2023-06-01 15:49:44.588: [iter 26 : loss : 0.7345 = 0.2647 + 0.4672 + 0.0026, time: 21.702188]
2023-06-01 15:49:44.865: epoch 26:	0.03005677  	0.08216628  	0.06666664  
2023-06-01 15:49:44.865: Find a better model.
2023-06-01 15:50:06.572: [iter 27 : loss : 0.7117 = 0.2417 + 0.4672 + 0.0028, time: 21.702735]
2023-06-01 15:50:06.840: epoch 27:	0.03004197  	0.08207989  	0.06653012  
2023-06-01 15:50:28.581: [iter 28 : loss : 0.6927 = 0.2228 + 0.4668 + 0.0031, time: 21.737404]
2023-06-01 15:50:28.852: epoch 28:	0.03011600  	0.08221247  	0.06675284  
2023-06-01 15:50:28.852: Find a better model.
2023-06-01 15:50:50.719: [iter 29 : loss : 0.6758 = 0.2063 + 0.4661 + 0.0033, time: 21.863085]
2023-06-01 15:50:50.991: epoch 29:	0.03018262  	0.08258355  	0.06711812  
2023-06-01 15:50:50.991: Find a better model.
2023-06-01 15:51:12.912: [iter 30 : loss : 0.6612 = 0.1920 + 0.4657 + 0.0036, time: 21.916856]
2023-06-01 15:51:13.199: epoch 30:	0.03015301  	0.08275840  	0.06704324  
2023-06-01 15:51:13.199: Find a better model.
2023-06-01 15:51:34.922: [iter 31 : loss : 0.6482 = 0.1795 + 0.4649 + 0.0038, time: 21.719063]
2023-06-01 15:51:35.211: epoch 31:	0.03003455  	0.08254509  	0.06707644  
2023-06-01 15:51:56.938: [iter 32 : loss : 0.6373 = 0.1690 + 0.4643 + 0.0040, time: 21.723079]
2023-06-01 15:51:57.222: epoch 32:	0.02997532  	0.08237870  	0.06696897  
2023-06-01 15:52:18.897: [iter 33 : loss : 0.6278 = 0.1600 + 0.4636 + 0.0042, time: 21.671021]
2023-06-01 15:52:19.167: epoch 33:	0.03007897  	0.08248302  	0.06732164  
2023-06-01 15:52:40.925: [iter 34 : loss : 0.6185 = 0.1512 + 0.4628 + 0.0044, time: 21.748232]
2023-06-01 15:52:41.211: epoch 34:	0.03017521  	0.08284788  	0.06733854  
2023-06-01 15:52:41.211: Find a better model.
2023-06-01 15:53:02.888: [iter 35 : loss : 0.6102 = 0.1433 + 0.4623 + 0.0046, time: 21.674081]
2023-06-01 15:53:03.156: epoch 35:	0.03022704  	0.08265271  	0.06745629  
2023-06-01 15:53:24.871: [iter 36 : loss : 0.6032 = 0.1367 + 0.4617 + 0.0048, time: 21.710786]
2023-06-01 15:53:25.141: epoch 36:	0.03028627  	0.08278351  	0.06762469  
2023-06-01 15:53:46.868: [iter 37 : loss : 0.5967 = 0.1304 + 0.4612 + 0.0050, time: 21.724393]
2023-06-01 15:53:47.139: epoch 37:	0.03032328  	0.08297546  	0.06781188  
2023-06-01 15:53:47.139: Find a better model.
2023-06-01 15:54:09.037: [iter 38 : loss : 0.5904 = 0.1247 + 0.4605 + 0.0052, time: 21.894833]
2023-06-01 15:54:09.305: epoch 38:	0.03040471  	0.08308218  	0.06793264  
2023-06-01 15:54:09.305: Find a better model.
2023-06-01 15:54:31.044: [iter 39 : loss : 0.5854 = 0.1199 + 0.4601 + 0.0054, time: 21.735959]
2023-06-01 15:54:31.316: epoch 39:	0.03030106  	0.08292782  	0.06786636  
2023-06-01 15:54:53.037: [iter 40 : loss : 0.5798 = 0.1147 + 0.4596 + 0.0055, time: 21.717375]
2023-06-01 15:54:53.312: epoch 40:	0.03032328  	0.08291396  	0.06792919  
2023-06-01 15:55:14.803: [iter 41 : loss : 0.5745 = 0.1098 + 0.4590 + 0.0057, time: 21.486422]
2023-06-01 15:55:15.071: epoch 41:	0.03019742  	0.08273396  	0.06777245  
2023-06-01 15:55:36.817: [iter 42 : loss : 0.5706 = 0.1060 + 0.4587 + 0.0059, time: 21.742556]
2023-06-01 15:55:37.087: epoch 42:	0.03013820  	0.08232344  	0.06767233  
2023-06-01 15:55:58.828: [iter 43 : loss : 0.5663 = 0.1019 + 0.4584 + 0.0060, time: 21.737061]
2023-06-01 15:55:59.096: epoch 43:	0.03017522  	0.08232753  	0.06763018  
2023-06-01 15:56:20.619: [iter 44 : loss : 0.5636 = 0.0995 + 0.4579 + 0.0062, time: 21.520059]
2023-06-01 15:56:20.894: epoch 44:	0.03003456  	0.08167315  	0.06731784  
2023-06-01 15:56:42.590: [iter 45 : loss : 0.5595 = 0.0957 + 0.4576 + 0.0063, time: 21.692058]
2023-06-01 15:56:42.861: epoch 45:	0.02998272  	0.08121818  	0.06707052  
2023-06-01 15:57:04.793: [iter 46 : loss : 0.5565 = 0.0928 + 0.4572 + 0.0065, time: 21.928759]
2023-06-01 15:57:05.065: epoch 46:	0.02991610  	0.08079006  	0.06690697  
2023-06-01 15:57:28.468: [iter 47 : loss : 0.5533 = 0.0898 + 0.4568 + 0.0066, time: 23.398757]
2023-06-01 15:57:28.761: epoch 47:	0.02987909  	0.08102400  	0.06688362  
2023-06-01 15:57:35.545: my pid: 2284
2023-06-01 15:57:35.545: model: model.general_recommender.SGL
2023-06-01 15:57:35.545: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 15:57:35.545: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 15:57:39.931: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 15:58:02.458: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.525955]
2023-06-01 15:58:02.731: epoch 1:	0.00102902  	0.00251924  	0.00187911  
2023-06-01 15:58:02.731: Find a better model.
2023-06-01 15:58:24.620: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.883959]
2023-06-01 15:58:24.912: epoch 2:	0.00137696  	0.00275435  	0.00218775  
2023-06-01 15:58:24.912: Find a better model.
2023-06-01 15:58:46.628: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 21.711415]
2023-06-01 15:58:46.922: epoch 3:	0.00173971  	0.00364173  	0.00293827  
2023-06-01 15:58:46.922: Find a better model.
2023-06-01 15:59:08.827: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 21.902530]
2023-06-01 15:59:09.122: epoch 4:	0.00219870  	0.00503063  	0.00393749  
2023-06-01 15:59:09.123: Find a better model.
2023-06-01 15:59:30.998: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.872271]
2023-06-01 15:59:31.299: epoch 5:	0.00257625  	0.00577140  	0.00476941  
2023-06-01 15:59:31.299: Find a better model.
2023-06-01 15:59:53.193: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 21.889197]
2023-06-01 15:59:53.484: epoch 6:	0.00337578  	0.00729913  	0.00575386  
2023-06-01 15:59:53.484: Find a better model.
2023-06-01 16:00:15.396: [iter 7 : loss : 1.1345 = 0.6924 + 0.4421 + 0.0000, time: 21.908491]
2023-06-01 16:00:15.686: epoch 7:	0.00405686  	0.00971104  	0.00766543  
2023-06-01 16:00:15.686: Find a better model.
2023-06-01 16:00:37.390: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 21.699433]
2023-06-01 16:00:37.681: epoch 8:	0.00506366  	0.01311956  	0.01001245  
2023-06-01 16:00:37.681: Find a better model.
2023-06-01 16:00:59.183: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 21.497046]
2023-06-01 16:00:59.469: epoch 9:	0.00590019  	0.01544960  	0.01200945  
2023-06-01 16:00:59.469: Find a better model.
2023-06-01 16:01:21.209: [iter 10 : loss : 1.1343 = 0.6911 + 0.4432 + 0.0000, time: 21.737465]
2023-06-01 16:01:21.493: epoch 10:	0.00735117  	0.02075948  	0.01610471  
2023-06-01 16:01:21.494: Find a better model.
2023-06-01 16:01:42.990: [iter 11 : loss : 1.1340 = 0.6902 + 0.4437 + 0.0000, time: 21.492924]
2023-06-01 16:01:43.281: epoch 11:	0.00860966  	0.02403962  	0.01911750  
2023-06-01 16:01:43.281: Find a better model.
2023-06-01 16:02:05.144: [iter 12 : loss : 1.1331 = 0.6888 + 0.4443 + 0.0000, time: 21.858752]
2023-06-01 16:02:05.423: epoch 12:	0.00990520  	0.02740956  	0.02247269  
2023-06-01 16:02:05.423: Find a better model.
2023-06-01 16:02:26.983: [iter 13 : loss : 1.1313 = 0.6863 + 0.4450 + 0.0000, time: 21.555140]
2023-06-01 16:02:27.268: epoch 13:	0.01202988  	0.03391069  	0.02836835  
2023-06-01 16:02:27.268: Find a better model.
2023-06-01 16:02:48.925: [iter 14 : loss : 1.1280 = 0.6822 + 0.4457 + 0.0001, time: 21.651987]
2023-06-01 16:02:49.215: epoch 14:	0.01464320  	0.04028567  	0.03432988  
2023-06-01 16:02:49.215: Find a better model.
2023-06-01 16:03:10.744: [iter 15 : loss : 1.1222 = 0.6755 + 0.4466 + 0.0001, time: 21.524956]
2023-06-01 16:03:11.022: epoch 15:	0.01763409  	0.04827076  	0.04161003  
2023-06-01 16:03:11.022: Find a better model.
2023-06-01 16:03:32.560: [iter 16 : loss : 1.1114 = 0.6639 + 0.4474 + 0.0001, time: 21.534105]
2023-06-01 16:03:32.841: epoch 16:	0.02029185  	0.05535422  	0.04783174  
2023-06-01 16:03:32.841: Find a better model.
2023-06-01 16:03:54.526: [iter 17 : loss : 1.0937 = 0.6448 + 0.4487 + 0.0002, time: 21.680339]
2023-06-01 16:03:54.805: epoch 17:	0.02317174  	0.06220585  	0.05347749  
2023-06-01 16:03:54.805: Find a better model.
2023-06-01 16:04:16.314: [iter 18 : loss : 1.0654 = 0.6147 + 0.4503 + 0.0004, time: 21.503225]
2023-06-01 16:04:16.590: epoch 18:	0.02554818  	0.06815985  	0.05821747  
2023-06-01 16:04:16.590: Find a better model.
2023-06-01 16:04:38.098: [iter 19 : loss : 1.0269 = 0.5739 + 0.4525 + 0.0005, time: 21.503258]
2023-06-01 16:04:38.379: epoch 19:	0.02699922  	0.07203227  	0.06174405  
2023-06-01 16:04:38.379: Find a better model.
2023-06-01 16:04:59.723: [iter 20 : loss : 0.9803 = 0.5243 + 0.4553 + 0.0008, time: 21.338710]
2023-06-01 16:04:59.996: epoch 20:	0.02824297  	0.07563609  	0.06399365  
2023-06-01 16:04:59.996: Find a better model.
2023-06-01 16:05:22.085: [iter 21 : loss : 0.9302 = 0.4709 + 0.4582 + 0.0011, time: 22.085451]
2023-06-01 16:05:22.359: epoch 21:	0.02903510  	0.07779717  	0.06551558  
2023-06-01 16:05:22.359: Find a better model.
2023-06-01 16:05:44.659: [iter 22 : loss : 0.8806 = 0.4179 + 0.4614 + 0.0013, time: 22.296055]
2023-06-01 16:05:44.930: epoch 22:	0.02924980  	0.07909122  	0.06599421  
2023-06-01 16:05:44.930: Find a better model.
2023-06-01 16:06:07.064: [iter 23 : loss : 0.8366 = 0.3711 + 0.4639 + 0.0017, time: 22.128918]
2023-06-01 16:06:07.335: epoch 23:	0.02939048  	0.07976408  	0.06623150  
2023-06-01 16:06:07.335: Find a better model.
2023-06-01 16:06:29.638: [iter 24 : loss : 0.7984 = 0.3308 + 0.4656 + 0.0020, time: 22.298816]
2023-06-01 16:06:29.910: epoch 24:	0.02966440  	0.08033317  	0.06670227  
2023-06-01 16:06:29.910: Find a better model.
2023-06-01 16:06:52.025: [iter 25 : loss : 0.7655 = 0.2968 + 0.4664 + 0.0022, time: 22.110490]
2023-06-01 16:06:52.299: epoch 25:	0.02967921  	0.08022080  	0.06656197  
2023-06-01 16:07:14.432: [iter 26 : loss : 0.7382 = 0.2690 + 0.4667 + 0.0025, time: 22.130096]
2023-06-01 16:07:14.702: epoch 26:	0.02966440  	0.08024447  	0.06660464  
2023-06-01 16:07:36.988: [iter 27 : loss : 0.7151 = 0.2456 + 0.4667 + 0.0028, time: 22.282401]
2023-06-01 16:07:37.265: epoch 27:	0.02970142  	0.08024323  	0.06679902  
2023-06-01 16:07:59.612: [iter 28 : loss : 0.6954 = 0.2259 + 0.4664 + 0.0031, time: 22.343174]
2023-06-01 16:07:59.880: epoch 28:	0.02966441  	0.08053903  	0.06691872  
2023-06-01 16:07:59.880: Find a better model.
2023-06-01 16:08:22.189: [iter 29 : loss : 0.6785 = 0.2093 + 0.4659 + 0.0033, time: 22.305044]
2023-06-01 16:08:22.455: epoch 29:	0.02969402  	0.08089704  	0.06707662  
2023-06-01 16:08:22.455: Find a better model.
2023-06-01 16:08:44.835: [iter 30 : loss : 0.6630 = 0.1943 + 0.4652 + 0.0035, time: 22.375106]
2023-06-01 16:08:45.104: epoch 30:	0.02979766  	0.08129116  	0.06728645  
2023-06-01 16:08:45.104: Find a better model.
2023-06-01 16:09:07.191: [iter 31 : loss : 0.6502 = 0.1819 + 0.4646 + 0.0038, time: 22.082145]
2023-06-01 16:09:07.455: epoch 31:	0.02970882  	0.08111245  	0.06727455  
2023-06-01 16:09:29.770: [iter 32 : loss : 0.6384 = 0.1706 + 0.4639 + 0.0040, time: 22.310570]
2023-06-01 16:09:30.040: epoch 32:	0.02965699  	0.08078853  	0.06732060  
2023-06-01 16:09:52.182: [iter 33 : loss : 0.6293 = 0.1618 + 0.4632 + 0.0042, time: 22.137471]
2023-06-01 16:09:52.447: epoch 33:	0.02975323  	0.08085474  	0.06747754  
2023-06-01 16:10:14.719: [iter 34 : loss : 0.6197 = 0.1527 + 0.4626 + 0.0044, time: 22.260268]
2023-06-01 16:10:14.985: epoch 34:	0.02976063  	0.08062643  	0.06754600  
2023-06-01 16:10:37.311: [iter 35 : loss : 0.6109 = 0.1444 + 0.4619 + 0.0046, time: 22.322072]
2023-06-01 16:10:37.578: epoch 35:	0.02974582  	0.08076885  	0.06773318  
2023-06-01 16:11:00.110: [iter 36 : loss : 0.6038 = 0.1378 + 0.4613 + 0.0048, time: 22.528295]
2023-06-01 16:11:00.370: epoch 36:	0.02970880  	0.08079192  	0.06782868  
2023-06-01 16:11:22.917: [iter 37 : loss : 0.5971 = 0.1314 + 0.4608 + 0.0050, time: 22.544228]
2023-06-01 16:11:23.189: epoch 37:	0.02984206  	0.08145011  	0.06801075  
2023-06-01 16:11:23.189: Find a better model.
2023-06-01 16:11:45.728: [iter 38 : loss : 0.5909 = 0.1255 + 0.4602 + 0.0052, time: 22.529697]
2023-06-01 16:11:45.994: epoch 38:	0.02975322  	0.08113765  	0.06790827  
2023-06-01 16:12:08.695: [iter 39 : loss : 0.5855 = 0.1206 + 0.4596 + 0.0053, time: 22.696290]
2023-06-01 16:12:08.966: epoch 39:	0.02964958  	0.08032271  	0.06779967  
2023-06-01 16:12:31.286: [iter 40 : loss : 0.5804 = 0.1155 + 0.4593 + 0.0055, time: 22.314569]
2023-06-01 16:12:31.554: epoch 40:	0.02959776  	0.08039862  	0.06779712  
2023-06-01 16:12:53.903: [iter 41 : loss : 0.5753 = 0.1109 + 0.4587 + 0.0057, time: 22.345223]
2023-06-01 16:12:54.175: epoch 41:	0.02970140  	0.08038810  	0.06788297  
2023-06-01 16:13:16.904: [iter 42 : loss : 0.5709 = 0.1067 + 0.4583 + 0.0058, time: 22.725144]
2023-06-01 16:13:17.174: epoch 42:	0.02965699  	0.08042964  	0.06792635  
2023-06-01 16:13:39.667: [iter 43 : loss : 0.5667 = 0.1028 + 0.4579 + 0.0060, time: 22.488617]
2023-06-01 16:13:39.933: epoch 43:	0.02956814  	0.08013158  	0.06779769  
2023-06-01 16:14:02.276: [iter 44 : loss : 0.5640 = 0.1003 + 0.4576 + 0.0062, time: 22.338460]
2023-06-01 16:14:02.541: epoch 44:	0.02963477  	0.08043911  	0.06785548  
2023-06-01 16:14:25.104: [iter 45 : loss : 0.5603 = 0.0968 + 0.4572 + 0.0063, time: 22.558593]
2023-06-01 16:14:25.362: epoch 45:	0.02973101  	0.08070080  	0.06801243  
2023-06-01 16:14:47.875: [iter 46 : loss : 0.5569 = 0.0936 + 0.4569 + 0.0065, time: 22.510047]
2023-06-01 16:14:48.143: epoch 46:	0.02971621  	0.08040949  	0.06789369  
2023-06-01 16:15:10.672: [iter 47 : loss : 0.5534 = 0.0903 + 0.4566 + 0.0066, time: 22.523594]
2023-06-01 16:15:10.945: epoch 47:	0.02956815  	0.07982099  	0.06777520  
2023-06-01 16:15:33.272: [iter 48 : loss : 0.5507 = 0.0877 + 0.4562 + 0.0067, time: 22.323180]
2023-06-01 16:15:33.542: epoch 48:	0.02967178  	0.07984696  	0.06777202  
2023-06-01 16:15:56.823: [iter 49 : loss : 0.5479 = 0.0851 + 0.4559 + 0.0069, time: 23.275924]
2023-06-01 16:15:57.132: epoch 49:	0.02954594  	0.07959019  	0.06747638  
2023-06-01 16:16:06.308: my pid: 2676
2023-06-01 16:16:06.308: model: model.general_recommender.SGL
2023-06-01 16:16:06.308: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 16:16:06.308: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 16:16:10.717: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 16:16:32.146: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.427510]
2023-06-01 16:16:32.420: epoch 1:	0.00118448  	0.00235553  	0.00202550  
2023-06-01 16:16:32.420: Find a better model.
2023-06-01 16:16:53.151: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.727700]
2023-06-01 16:16:53.431: epoch 2:	0.00175452  	0.00295176  	0.00257103  
2023-06-01 16:16:53.431: Find a better model.
2023-06-01 16:17:14.000: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.563600]
2023-06-01 16:17:14.300: epoch 3:	0.00190258  	0.00367540  	0.00296255  
2023-06-01 16:17:14.300: Find a better model.
2023-06-01 16:17:34.972: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.668523]
2023-06-01 16:17:35.275: epoch 4:	0.00230234  	0.00403839  	0.00364342  
2023-06-01 16:17:35.275: Find a better model.
2023-06-01 16:17:55.740: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.461196]
2023-06-01 16:17:56.031: epoch 5:	0.00267989  	0.00633098  	0.00468608  
2023-06-01 16:17:56.032: Find a better model.
2023-06-01 16:18:16.530: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.494715]
2023-06-01 16:18:16.817: epoch 6:	0.00348682  	0.00871261  	0.00649859  
2023-06-01 16:18:16.817: Find a better model.
2023-06-01 16:18:37.318: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 20.497785]
2023-06-01 16:18:37.609: epoch 7:	0.00369411  	0.00897768  	0.00703101  
2023-06-01 16:18:37.609: Find a better model.
2023-06-01 16:18:58.112: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.498427]
2023-06-01 16:18:58.404: epoch 8:	0.00444181  	0.01066499  	0.00830655  
2023-06-01 16:18:58.404: Find a better model.
2023-06-01 16:19:18.783: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.374487]
2023-06-01 16:19:19.069: epoch 9:	0.00570031  	0.01516679  	0.01134446  
2023-06-01 16:19:19.069: Find a better model.
2023-06-01 16:19:39.526: [iter 10 : loss : 1.1345 = 0.6908 + 0.4437 + 0.0000, time: 20.451997]
2023-06-01 16:19:39.811: epoch 10:	0.00611487  	0.01646584  	0.01252020  
2023-06-01 16:19:39.811: Find a better model.
2023-06-01 16:20:00.154: [iter 11 : loss : 1.1338 = 0.6895 + 0.4442 + 0.0000, time: 20.339097]
2023-06-01 16:20:00.428: epoch 11:	0.00719570  	0.01917313  	0.01519826  
2023-06-01 16:20:00.428: Find a better model.
2023-06-01 16:20:20.881: [iter 12 : loss : 1.1325 = 0.6876 + 0.4449 + 0.0000, time: 20.448394]
2023-06-01 16:20:21.158: epoch 12:	0.00860967  	0.02291186  	0.01890688  
2023-06-01 16:20:21.158: Find a better model.
2023-06-01 16:20:41.438: [iter 13 : loss : 1.1304 = 0.6847 + 0.4456 + 0.0001, time: 20.275176]
2023-06-01 16:20:41.714: epoch 13:	0.01097125  	0.02894369  	0.02444356  
2023-06-01 16:20:41.714: Find a better model.
2023-06-01 16:21:01.857: [iter 14 : loss : 1.1265 = 0.6800 + 0.4464 + 0.0001, time: 20.139468]
2023-06-01 16:21:02.136: epoch 14:	0.01422862  	0.03828558  	0.03157021  
2023-06-01 16:21:02.137: Find a better model.
2023-06-01 16:21:22.311: [iter 15 : loss : 1.1195 = 0.6721 + 0.4473 + 0.0001, time: 20.171052]
2023-06-01 16:21:22.585: epoch 15:	0.01732315  	0.04617191  	0.03931201  
2023-06-01 16:21:22.585: Find a better model.
2023-06-01 16:21:42.870: [iter 16 : loss : 1.1069 = 0.6584 + 0.4483 + 0.0002, time: 20.279852]
2023-06-01 16:21:43.146: epoch 16:	0.02046212  	0.05478840  	0.04688987  
2023-06-01 16:21:43.146: Find a better model.
2023-06-01 16:22:03.256: [iter 17 : loss : 1.0863 = 0.6363 + 0.4497 + 0.0003, time: 20.104600]
2023-06-01 16:22:03.532: epoch 17:	0.02361591  	0.06298942  	0.05294415  
2023-06-01 16:22:03.532: Find a better model.
2023-06-01 16:22:23.825: [iter 18 : loss : 1.0550 = 0.6031 + 0.4515 + 0.0004, time: 20.288216]
2023-06-01 16:22:24.099: epoch 18:	0.02576286  	0.06892490  	0.05733195  
2023-06-01 16:22:24.099: Find a better model.
2023-06-01 16:22:44.208: [iter 19 : loss : 1.0138 = 0.5594 + 0.4538 + 0.0006, time: 20.105396]
2023-06-01 16:22:44.476: epoch 19:	0.02747301  	0.07310347  	0.06054735  
2023-06-01 16:22:44.476: Find a better model.
2023-06-01 16:23:04.641: [iter 20 : loss : 0.9660 = 0.5086 + 0.4566 + 0.0009, time: 20.160357]
2023-06-01 16:23:04.915: epoch 20:	0.02837621  	0.07640944  	0.06249671  
2023-06-01 16:23:04.915: Find a better model.
2023-06-01 16:23:25.832: [iter 21 : loss : 0.9166 = 0.4557 + 0.4597 + 0.0011, time: 20.913955]
2023-06-01 16:23:26.104: epoch 21:	0.02900549  	0.07831642  	0.06375928  
2023-06-01 16:23:26.104: Find a better model.
2023-06-01 16:23:47.004: [iter 22 : loss : 0.8687 = 0.4045 + 0.4627 + 0.0014, time: 20.896309]
2023-06-01 16:23:47.281: epoch 22:	0.02903511  	0.07913318  	0.06419244  
2023-06-01 16:23:47.281: Find a better model.
2023-06-01 16:24:08.000: [iter 23 : loss : 0.8266 = 0.3600 + 0.4649 + 0.0017, time: 20.713619]
2023-06-01 16:24:08.280: epoch 23:	0.02931643  	0.07976582  	0.06440664  
2023-06-01 16:24:08.280: Find a better model.
2023-06-01 16:24:29.199: [iter 24 : loss : 0.7902 = 0.3218 + 0.4665 + 0.0020, time: 20.913641]
2023-06-01 16:24:29.466: epoch 24:	0.02948671  	0.08110499  	0.06477775  
2023-06-01 16:24:29.467: Find a better model.
2023-06-01 16:24:50.340: [iter 25 : loss : 0.7592 = 0.2898 + 0.4671 + 0.0023, time: 20.870361]
2023-06-01 16:24:50.609: epoch 25:	0.02958294  	0.08126599  	0.06516851  
2023-06-01 16:24:50.609: Find a better model.
2023-06-01 16:25:11.342: [iter 26 : loss : 0.7336 = 0.2635 + 0.4675 + 0.0026, time: 20.729038]
2023-06-01 16:25:11.609: epoch 26:	0.02966438  	0.08111589  	0.06551430  
2023-06-01 16:25:32.594: [iter 27 : loss : 0.7108 = 0.2409 + 0.4671 + 0.0029, time: 20.982115]
2023-06-01 16:25:32.863: epoch 27:	0.02973841  	0.08133461  	0.06577359  
2023-06-01 16:25:32.863: Find a better model.
2023-06-01 16:25:53.772: [iter 28 : loss : 0.6921 = 0.2220 + 0.4669 + 0.0031, time: 20.906031]
2023-06-01 16:25:54.040: epoch 28:	0.02977543  	0.08226745  	0.06599339  
2023-06-01 16:25:54.040: Find a better model.
2023-06-01 16:26:14.920: [iter 29 : loss : 0.6756 = 0.2059 + 0.4663 + 0.0034, time: 20.874216]
2023-06-01 16:26:15.190: epoch 29:	0.02986429  	0.08229315  	0.06620008  
2023-06-01 16:26:15.190: Find a better model.
2023-06-01 16:26:36.141: [iter 30 : loss : 0.6608 = 0.1914 + 0.4658 + 0.0036, time: 20.947615]
2023-06-01 16:26:36.412: epoch 30:	0.02976063  	0.08219595  	0.06598644  
2023-06-01 16:26:57.502: [iter 31 : loss : 0.6481 = 0.1791 + 0.4652 + 0.0038, time: 21.085690]
2023-06-01 16:26:57.769: epoch 31:	0.02969401  	0.08168146  	0.06604175  
2023-06-01 16:27:18.919: [iter 32 : loss : 0.6371 = 0.1684 + 0.4646 + 0.0040, time: 21.145183]
2023-06-01 16:27:19.186: epoch 32:	0.02981985  	0.08201016  	0.06622848  
2023-06-01 16:27:40.328: [iter 33 : loss : 0.6279 = 0.1598 + 0.4638 + 0.0042, time: 21.138273]
2023-06-01 16:27:40.594: epoch 33:	0.02984946  	0.08233563  	0.06650207  
2023-06-01 16:27:40.594: Find a better model.
2023-06-01 16:28:01.520: [iter 34 : loss : 0.6186 = 0.1509 + 0.4632 + 0.0045, time: 20.921503]
2023-06-01 16:28:01.790: epoch 34:	0.02979764  	0.08197868  	0.06649397  
2023-06-01 16:28:22.874: [iter 35 : loss : 0.6102 = 0.1429 + 0.4626 + 0.0047, time: 21.080080]
2023-06-01 16:28:23.140: epoch 35:	0.02975322  	0.08230171  	0.06640768  
2023-06-01 16:28:44.311: [iter 36 : loss : 0.6034 = 0.1366 + 0.4620 + 0.0048, time: 21.167139]
2023-06-01 16:28:44.580: epoch 36:	0.02979023  	0.08211610  	0.06654429  
2023-06-01 16:29:05.669: [iter 37 : loss : 0.5971 = 0.1305 + 0.4615 + 0.0050, time: 21.085487]
2023-06-01 16:29:05.936: epoch 37:	0.02979024  	0.08181648  	0.06662448  
2023-06-01 16:29:27.063: [iter 38 : loss : 0.5905 = 0.1243 + 0.4610 + 0.0052, time: 21.124666]
2023-06-01 16:29:27.331: epoch 38:	0.02984946  	0.08214574  	0.06664521  
2023-06-01 16:29:48.490: [iter 39 : loss : 0.5855 = 0.1197 + 0.4604 + 0.0054, time: 21.156146]
2023-06-01 16:29:48.758: epoch 39:	0.02963477  	0.08152524  	0.06644881  
2023-06-01 16:30:09.880: [iter 40 : loss : 0.5801 = 0.1146 + 0.4600 + 0.0056, time: 21.119014]
2023-06-01 16:30:10.147: epoch 40:	0.02961996  	0.08132543  	0.06655993  
2023-06-01 16:30:31.034: [iter 41 : loss : 0.5752 = 0.1100 + 0.4595 + 0.0057, time: 20.883541]
2023-06-01 16:30:31.311: epoch 41:	0.02956073  	0.08106646  	0.06641047  
2023-06-01 16:30:52.511: [iter 42 : loss : 0.5708 = 0.1058 + 0.4591 + 0.0059, time: 21.197077]
2023-06-01 16:30:52.778: epoch 42:	0.02949410  	0.08057749  	0.06611875  
2023-06-01 16:31:13.834: [iter 43 : loss : 0.5669 = 0.1021 + 0.4587 + 0.0060, time: 21.052033]
2023-06-01 16:31:14.104: epoch 43:	0.02950891  	0.08074468  	0.06608788  
2023-06-01 16:31:35.018: [iter 44 : loss : 0.5640 = 0.0994 + 0.4584 + 0.0062, time: 20.909130]
2023-06-01 16:31:35.294: epoch 44:	0.02952372  	0.08097316  	0.06623886  
2023-06-01 16:31:56.445: [iter 45 : loss : 0.5600 = 0.0957 + 0.4580 + 0.0063, time: 21.146599]
2023-06-01 16:31:56.711: epoch 45:	0.02944227  	0.08055814  	0.06609099  
2023-06-01 16:32:17.835: [iter 46 : loss : 0.5568 = 0.0927 + 0.4577 + 0.0065, time: 21.119110]
2023-06-01 16:32:18.101: epoch 46:	0.02946448  	0.08031039  	0.06611393  
2023-06-01 16:32:39.021: [iter 47 : loss : 0.5537 = 0.0897 + 0.4573 + 0.0066, time: 20.916118]
2023-06-01 16:32:39.293: epoch 47:	0.02944227  	0.08056499  	0.06617036  
2023-06-01 16:33:00.208: [iter 48 : loss : 0.5511 = 0.0872 + 0.4571 + 0.0068, time: 20.911314]
2023-06-01 16:33:00.463: epoch 48:	0.02933862  	0.07962342  	0.06586546  
2023-06-01 16:33:21.341: [iter 49 : loss : 0.5482 = 0.0847 + 0.4566 + 0.0069, time: 20.874109]
2023-06-01 16:33:21.605: epoch 49:	0.02925719  	0.07949228  	0.06584075  
2023-06-01 16:33:42.359: [iter 50 : loss : 0.5460 = 0.0824 + 0.4565 + 0.0071, time: 20.750285]
2023-06-01 16:33:42.626: epoch 50:	0.02922018  	0.07950012  	0.06572650  
2023-06-01 16:34:03.597: [iter 51 : loss : 0.5437 = 0.0803 + 0.4562 + 0.0072, time: 20.968361]
2023-06-01 16:34:03.866: epoch 51:	0.02925719  	0.07947848  	0.06564467  
2023-06-01 16:34:24.999: [iter 52 : loss : 0.5409 = 0.0777 + 0.4559 + 0.0073, time: 21.128455]
2023-06-01 16:34:25.275: epoch 52:	0.02923497  	0.07923339  	0.06559812  
2023-06-01 16:34:46.143: [iter 53 : loss : 0.5401 = 0.0769 + 0.4557 + 0.0075, time: 20.865088]
2023-06-01 16:34:46.412: epoch 53:	0.02911653  	0.07871282  	0.06538951  
2023-06-01 16:35:07.359: [iter 54 : loss : 0.5377 = 0.0746 + 0.4555 + 0.0076, time: 20.944085]
2023-06-01 16:35:07.626: epoch 54:	0.02914614  	0.07895703  	0.06524783  
2023-06-01 16:35:28.533: [iter 55 : loss : 0.5357 = 0.0728 + 0.4552 + 0.0077, time: 20.903949]
2023-06-01 16:35:28.798: epoch 55:	0.02915355  	0.07893132  	0.06533320  
2023-06-01 16:35:49.747: [iter 56 : loss : 0.5335 = 0.0707 + 0.4550 + 0.0078, time: 20.945100]
2023-06-01 16:35:50.011: epoch 56:	0.02910172  	0.07871316  	0.06512287  
2023-06-01 16:36:10.960: [iter 57 : loss : 0.5314 = 0.0686 + 0.4549 + 0.0079, time: 20.944215]
2023-06-01 16:36:11.240: epoch 57:	0.02898327  	0.07790472  	0.06484111  
2023-06-01 16:36:32.329: [iter 58 : loss : 0.5303 = 0.0677 + 0.4546 + 0.0081, time: 21.084598]
2023-06-01 16:36:32.596: epoch 58:	0.02894625  	0.07822797  	0.06488645  
2023-06-01 16:36:32.596: Early stopping is trigger at epoch: 58
2023-06-01 16:36:32.596: best_result@epoch 33:

2023-06-01 16:36:32.596: 		0.0298      	0.0823      	0.0665      
2023-06-01 16:59:13.569: my pid: 14440
2023-06-01 16:59:13.569: model: model.general_recommender.SGL
2023-06-01 16:59:13.569: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 16:59:13.569: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 16:59:17.591: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 16:59:38.286: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 20.695308]
2023-06-01 16:59:38.563: epoch 1:	0.00115487  	0.00228493  	0.00190254  
2023-06-01 16:59:38.563: Find a better model.
2023-06-01 16:59:59.640: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.072714]
2023-06-01 16:59:59.931: epoch 2:	0.00166568  	0.00371545  	0.00289654  
2023-06-01 16:59:59.931: Find a better model.
2023-06-01 17:00:20.913: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.979084]
2023-06-01 17:00:21.230: epoch 3:	0.00216168  	0.00515186  	0.00374820  
2023-06-01 17:00:21.230: Find a better model.
2023-06-01 17:00:42.211: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.977726]
2023-06-01 17:00:42.502: epoch 4:	0.00239858  	0.00474883  	0.00393039  
2023-06-01 17:01:03.402: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.895708]
2023-06-01 17:01:03.697: epoch 5:	0.00287978  	0.00620720  	0.00517905  
2023-06-01 17:01:03.697: Find a better model.
2023-06-01 17:01:24.597: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.896053]
2023-06-01 17:01:24.887: epoch 6:	0.00331655  	0.00705786  	0.00590326  
2023-06-01 17:01:24.887: Find a better model.
2023-06-01 17:01:45.818: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.927068]
2023-06-01 17:01:46.106: epoch 7:	0.00387178  	0.00933125  	0.00724821  
2023-06-01 17:01:46.106: Find a better model.
2023-06-01 17:02:07.006: [iter 8 : loss : 1.1347 = 0.6920 + 0.4428 + 0.0000, time: 20.896066]
2023-06-01 17:02:07.305: epoch 8:	0.00461948  	0.01189526  	0.00976882  
2023-06-01 17:02:07.305: Find a better model.
2023-06-01 17:02:28.171: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.862280]
2023-06-01 17:02:28.458: epoch 9:	0.00548562  	0.01473232  	0.01190058  
2023-06-01 17:02:28.458: Find a better model.
2023-06-01 17:02:49.212: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.750383]
2023-06-01 17:02:49.499: epoch 10:	0.00594461  	0.01562324  	0.01311378  
2023-06-01 17:02:49.499: Find a better model.
2023-06-01 17:03:10.237: [iter 11 : loss : 1.1338 = 0.6895 + 0.4443 + 0.0000, time: 20.733052]
2023-06-01 17:03:10.522: epoch 11:	0.00713648  	0.01898755  	0.01574626  
2023-06-01 17:03:10.523: Find a better model.
2023-06-01 17:03:31.178: [iter 12 : loss : 1.1324 = 0.6875 + 0.4449 + 0.0000, time: 20.652293]
2023-06-01 17:03:31.465: epoch 12:	0.00854304  	0.02387229  	0.01960167  
2023-06-01 17:03:31.465: Find a better model.
2023-06-01 17:03:52.136: [iter 13 : loss : 1.1305 = 0.6848 + 0.4456 + 0.0001, time: 20.666509]
2023-06-01 17:03:52.417: epoch 13:	0.01115633  	0.03030804  	0.02594843  
2023-06-01 17:03:52.418: Find a better model.
2023-06-01 17:04:13.134: [iter 14 : loss : 1.1267 = 0.6802 + 0.4464 + 0.0001, time: 20.710535]
2023-06-01 17:04:13.419: epoch 14:	0.01363637  	0.03695428  	0.03217587  
2023-06-01 17:04:13.419: Find a better model.
2023-06-01 17:04:33.967: [iter 15 : loss : 1.1198 = 0.6724 + 0.4473 + 0.0001, time: 20.542720]
2023-06-01 17:04:34.253: epoch 15:	0.01707144  	0.04473484  	0.03922755  
2023-06-01 17:04:34.254: Find a better model.
2023-06-01 17:04:54.949: [iter 16 : loss : 1.1075 = 0.6590 + 0.4483 + 0.0002, time: 20.691548]
2023-06-01 17:04:55.240: epoch 16:	0.02037327  	0.05345269  	0.04684890  
2023-06-01 17:04:55.240: Find a better model.
2023-06-01 17:05:15.913: [iter 17 : loss : 1.0871 = 0.6372 + 0.4496 + 0.0003, time: 20.666858]
2023-06-01 17:05:16.206: epoch 17:	0.02349744  	0.06206624  	0.05343492  
2023-06-01 17:05:16.206: Find a better model.
2023-06-01 17:05:36.952: [iter 18 : loss : 1.0560 = 0.6041 + 0.4515 + 0.0004, time: 20.741575]
2023-06-01 17:05:37.247: epoch 18:	0.02572583  	0.06829381  	0.05789376  
2023-06-01 17:05:37.247: Find a better model.
2023-06-01 17:05:58.143: [iter 19 : loss : 1.0151 = 0.5607 + 0.4538 + 0.0006, time: 20.891099]
2023-06-01 17:05:58.426: epoch 19:	0.02731012  	0.07249662  	0.06114089  
2023-06-01 17:05:58.426: Find a better model.
2023-06-01 17:06:19.161: [iter 20 : loss : 0.9672 = 0.5095 + 0.4568 + 0.0009, time: 20.730412]
2023-06-01 17:06:19.433: epoch 20:	0.02858351  	0.07587359  	0.06338705  
2023-06-01 17:06:19.433: Find a better model.
2023-06-01 17:06:40.922: [iter 21 : loss : 0.9174 = 0.4562 + 0.4601 + 0.0011, time: 21.482775]
2023-06-01 17:06:41.217: epoch 21:	0.02889443  	0.07714632  	0.06405688  
2023-06-01 17:06:41.218: Find a better model.
2023-06-01 17:07:02.701: [iter 22 : loss : 0.8693 = 0.4048 + 0.4631 + 0.0014, time: 21.478556]
2023-06-01 17:07:02.976: epoch 22:	0.02918318  	0.07875986  	0.06450769  
2023-06-01 17:07:02.976: Find a better model.
2023-06-01 17:07:24.079: [iter 23 : loss : 0.8271 = 0.3600 + 0.4654 + 0.0017, time: 21.099418]
2023-06-01 17:07:24.358: epoch 23:	0.02936085  	0.07877105  	0.06465046  
2023-06-01 17:07:24.358: Find a better model.
2023-06-01 17:07:45.679: [iter 24 : loss : 0.7905 = 0.3215 + 0.4670 + 0.0020, time: 21.317050]
2023-06-01 17:07:45.955: epoch 24:	0.02936826  	0.07910784  	0.06459375  
2023-06-01 17:07:45.955: Find a better model.
2023-06-01 17:08:07.219: [iter 25 : loss : 0.7592 = 0.2894 + 0.4675 + 0.0023, time: 21.259319]
2023-06-01 17:08:07.489: epoch 25:	0.02944969  	0.07968759  	0.06480864  
2023-06-01 17:08:07.489: Find a better model.
2023-06-01 17:08:28.836: [iter 26 : loss : 0.7334 = 0.2629 + 0.4679 + 0.0026, time: 21.342003]
2023-06-01 17:08:29.105: epoch 26:	0.02969400  	0.08060039  	0.06537156  
2023-06-01 17:08:29.105: Find a better model.
2023-06-01 17:08:50.661: [iter 27 : loss : 0.7110 = 0.2405 + 0.4677 + 0.0029, time: 21.551081]
2023-06-01 17:08:50.937: epoch 27:	0.02964959  	0.08041396  	0.06528846  
2023-06-01 17:09:12.443: [iter 28 : loss : 0.6922 = 0.2219 + 0.4672 + 0.0031, time: 21.502125]
2023-06-01 17:09:12.715: epoch 28:	0.02964217  	0.08068470  	0.06556480  
2023-06-01 17:09:12.715: Find a better model.
2023-06-01 17:09:34.189: [iter 29 : loss : 0.6758 = 0.2056 + 0.4668 + 0.0034, time: 21.470060]
2023-06-01 17:09:34.458: epoch 29:	0.02966440  	0.08077461  	0.06569193  
2023-06-01 17:09:34.458: Find a better model.
2023-06-01 17:09:56.007: [iter 30 : loss : 0.6611 = 0.1914 + 0.4661 + 0.0036, time: 21.545073]
2023-06-01 17:09:56.283: epoch 30:	0.02968660  	0.08070340  	0.06566595  
2023-06-01 17:10:17.783: [iter 31 : loss : 0.6484 = 0.1791 + 0.4655 + 0.0038, time: 21.496589]
2023-06-01 17:10:18.051: epoch 31:	0.02975322  	0.08078741  	0.06573842  
2023-06-01 17:10:18.051: Find a better model.
2023-06-01 17:10:39.439: [iter 32 : loss : 0.6374 = 0.1686 + 0.4648 + 0.0040, time: 21.383953]
2023-06-01 17:10:39.708: epoch 32:	0.02992351  	0.08107195  	0.06598564  
2023-06-01 17:10:39.708: Find a better model.
2023-06-01 17:11:01.185: [iter 33 : loss : 0.6284 = 0.1600 + 0.4642 + 0.0042, time: 21.472295]
2023-06-01 17:11:01.458: epoch 33:	0.03001973  	0.08081897  	0.06613283  
2023-06-01 17:11:22.999: [iter 34 : loss : 0.6192 = 0.1512 + 0.4636 + 0.0044, time: 21.537774]
2023-06-01 17:11:23.273: epoch 34:	0.03001973  	0.08117370  	0.06637254  
2023-06-01 17:11:23.273: Find a better model.
2023-06-01 17:11:44.635: [iter 35 : loss : 0.6104 = 0.1429 + 0.4628 + 0.0046, time: 21.356544]
2023-06-01 17:11:44.902: epoch 35:	0.03002713  	0.08104663  	0.06627193  
2023-06-01 17:12:06.404: [iter 36 : loss : 0.6037 = 0.1367 + 0.4622 + 0.0048, time: 21.498152]
2023-06-01 17:12:06.672: epoch 36:	0.03007895  	0.08173137  	0.06633383  
2023-06-01 17:12:06.672: Find a better model.
2023-06-01 17:12:28.171: [iter 37 : loss : 0.5973 = 0.1305 + 0.4617 + 0.0050, time: 21.495053]
2023-06-01 17:12:28.448: epoch 37:	0.03001973  	0.08124296  	0.06634743  
2023-06-01 17:12:50.155: [iter 38 : loss : 0.5911 = 0.1247 + 0.4611 + 0.0052, time: 21.704056]
2023-06-01 17:12:50.424: epoch 38:	0.03007895  	0.08112068  	0.06630717  
2023-06-01 17:13:11.734: [iter 39 : loss : 0.5860 = 0.1200 + 0.4606 + 0.0054, time: 21.306143]
2023-06-01 17:13:12.006: epoch 39:	0.03014557  	0.08138343  	0.06653611  
2023-06-01 17:13:33.369: [iter 40 : loss : 0.5803 = 0.1146 + 0.4601 + 0.0055, time: 21.358470]
2023-06-01 17:13:33.643: epoch 40:	0.03009376  	0.08135795  	0.06645709  
2023-06-01 17:13:54.948: [iter 41 : loss : 0.5752 = 0.1099 + 0.4596 + 0.0057, time: 21.300760]
2023-06-01 17:13:55.226: epoch 41:	0.03003454  	0.08115692  	0.06629892  
2023-06-01 17:14:16.353: [iter 42 : loss : 0.5710 = 0.1059 + 0.4592 + 0.0059, time: 21.123025]
2023-06-01 17:14:16.628: epoch 42:	0.02984205  	0.08058027  	0.06617401  
2023-06-01 17:14:37.983: [iter 43 : loss : 0.5670 = 0.1021 + 0.4589 + 0.0060, time: 21.350827]
2023-06-01 17:14:38.261: epoch 43:	0.03001233  	0.08104118  	0.06640164  
2023-06-01 17:14:59.526: [iter 44 : loss : 0.5644 = 0.0998 + 0.4584 + 0.0062, time: 21.260433]
2023-06-01 17:14:59.794: epoch 44:	0.02997532  	0.08062212  	0.06623521  
2023-06-01 17:15:20.938: [iter 45 : loss : 0.5600 = 0.0957 + 0.4580 + 0.0063, time: 21.138932]
2023-06-01 17:15:21.210: epoch 45:	0.02987909  	0.08070092  	0.06602359  
2023-06-01 17:15:42.292: [iter 46 : loss : 0.5571 = 0.0929 + 0.4577 + 0.0065, time: 21.079629]
2023-06-01 17:15:42.563: epoch 46:	0.02973101  	0.07994034  	0.06566974  
2023-06-01 17:16:03.691: [iter 47 : loss : 0.5540 = 0.0899 + 0.4574 + 0.0066, time: 21.123020]
2023-06-01 17:16:03.959: epoch 47:	0.02964958  	0.08009019  	0.06567264  
2023-06-01 17:16:25.106: [iter 48 : loss : 0.5516 = 0.0877 + 0.4571 + 0.0068, time: 21.143000]
2023-06-01 17:16:25.379: epoch 48:	0.02960516  	0.07942058  	0.06543822  
2023-06-01 17:16:47.610: [iter 49 : loss : 0.5487 = 0.0851 + 0.4567 + 0.0069, time: 22.226338]
2023-06-01 17:16:47.892: epoch 49:	0.02948671  	0.07897136  	0.06529066  
2023-06-01 17:17:00.870: my pid: 14104
2023-06-01 17:17:00.870: model: model.general_recommender.SGL
2023-06-01 17:17:00.870: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 17:17:00.870: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 17:17:05.398: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 17:17:26.734: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.335505]
2023-06-01 17:17:27.001: epoch 1:	0.00125111  	0.00247722  	0.00205225  
2023-06-01 17:17:27.001: Find a better model.
2023-06-01 17:17:47.718: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.714052]
2023-06-01 17:17:48.005: epoch 2:	0.00168789  	0.00378389  	0.00315659  
2023-06-01 17:17:48.005: Find a better model.
2023-06-01 17:18:08.882: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.873090]
2023-06-01 17:18:09.187: epoch 3:	0.00186556  	0.00432073  	0.00327114  
2023-06-01 17:18:09.187: Find a better model.
2023-06-01 17:18:29.915: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.722309]
2023-06-01 17:18:30.222: epoch 4:	0.00234676  	0.00479000  	0.00384169  
2023-06-01 17:18:30.222: Find a better model.
2023-06-01 17:18:50.930: [iter 5 : loss : 1.1344 = 0.6927 + 0.4418 + 0.0000, time: 20.703403]
2023-06-01 17:18:51.236: epoch 5:	0.00284276  	0.00605042  	0.00474088  
2023-06-01 17:18:51.237: Find a better model.
2023-06-01 17:19:11.881: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.640304]
2023-06-01 17:19:12.172: epoch 6:	0.00338318  	0.00700927  	0.00595354  
2023-06-01 17:19:12.172: Find a better model.
2023-06-01 17:19:32.663: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.483166]
2023-06-01 17:19:32.954: epoch 7:	0.00407166  	0.00968357  	0.00757574  
2023-06-01 17:19:32.954: Find a better model.
2023-06-01 17:19:53.646: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.687230]
2023-06-01 17:19:53.933: epoch 8:	0.00466390  	0.01199285  	0.00957031  
2023-06-01 17:19:53.933: Find a better model.
2023-06-01 17:20:14.641: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.704057]
2023-06-01 17:20:14.929: epoch 9:	0.00541159  	0.01427385  	0.01157928  
2023-06-01 17:20:14.929: Find a better model.
2023-06-01 17:20:35.486: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.552073]
2023-06-01 17:20:35.772: epoch 10:	0.00589278  	0.01632822  	0.01247539  
2023-06-01 17:20:35.773: Find a better model.
2023-06-01 17:20:56.052: [iter 11 : loss : 1.1337 = 0.6894 + 0.4443 + 0.0000, time: 20.276711]
2023-06-01 17:20:56.342: epoch 11:	0.00681815  	0.01969219  	0.01528492  
2023-06-01 17:20:56.342: Find a better model.
2023-06-01 17:21:16.841: [iter 12 : loss : 1.1324 = 0.6875 + 0.4449 + 0.0000, time: 20.494097]
2023-06-01 17:21:17.112: epoch 12:	0.00860227  	0.02477727  	0.01988442  
2023-06-01 17:21:17.112: Find a better model.
2023-06-01 17:21:37.403: [iter 13 : loss : 1.1304 = 0.6847 + 0.4456 + 0.0001, time: 20.288532]
2023-06-01 17:21:37.682: epoch 13:	0.01068994  	0.03157759  	0.02555854  
2023-06-01 17:21:37.682: Find a better model.
2023-06-01 17:21:58.031: [iter 14 : loss : 1.1266 = 0.6801 + 0.4463 + 0.0001, time: 20.344046]
2023-06-01 17:21:58.315: epoch 14:	0.01385847  	0.03958497  	0.03271140  
2023-06-01 17:21:58.315: Find a better model.
2023-06-01 17:22:18.817: [iter 15 : loss : 1.1197 = 0.6723 + 0.4473 + 0.0001, time: 20.496510]
2023-06-01 17:22:19.095: epoch 15:	0.01692341  	0.04697929  	0.03993493  
2023-06-01 17:22:19.095: Find a better model.
2023-06-01 17:22:39.409: [iter 16 : loss : 1.1073 = 0.6588 + 0.4483 + 0.0002, time: 20.310051]
2023-06-01 17:22:39.690: epoch 16:	0.02038813  	0.05599087  	0.04736609  
2023-06-01 17:22:39.690: Find a better model.
2023-06-01 17:22:59.991: [iter 17 : loss : 1.0869 = 0.6370 + 0.4497 + 0.0003, time: 20.297486]
2023-06-01 17:23:00.276: epoch 17:	0.02319394  	0.06357791  	0.05376749  
2023-06-01 17:23:00.276: Find a better model.
2023-06-01 17:23:20.594: [iter 18 : loss : 1.0559 = 0.6039 + 0.4515 + 0.0004, time: 20.312637]
2023-06-01 17:23:20.874: epoch 18:	0.02563701  	0.06937968  	0.05823005  
2023-06-01 17:23:20.874: Find a better model.
2023-06-01 17:23:41.369: [iter 19 : loss : 1.0151 = 0.5606 + 0.4538 + 0.0006, time: 20.490267]
2023-06-01 17:23:41.647: epoch 19:	0.02729535  	0.07466733  	0.06192084  
2023-06-01 17:23:41.647: Find a better model.
2023-06-01 17:24:01.828: [iter 20 : loss : 0.9674 = 0.5098 + 0.4568 + 0.0009, time: 20.175239]
2023-06-01 17:24:02.102: epoch 20:	0.02841324  	0.07764374  	0.06407131  
2023-06-01 17:24:02.102: Find a better model.
2023-06-01 17:24:22.778: [iter 21 : loss : 0.9177 = 0.4567 + 0.4599 + 0.0011, time: 20.672046]
2023-06-01 17:24:23.048: epoch 21:	0.02890186  	0.07901308  	0.06487484  
2023-06-01 17:24:23.048: Find a better model.
2023-06-01 17:24:43.784: [iter 22 : loss : 0.8699 = 0.4056 + 0.4629 + 0.0014, time: 20.731132]
2023-06-01 17:24:44.057: epoch 22:	0.02927202  	0.08026483  	0.06560582  
2023-06-01 17:24:44.057: Find a better model.
2023-06-01 17:25:04.957: [iter 23 : loss : 0.8275 = 0.3607 + 0.4650 + 0.0017, time: 20.896138]
2023-06-01 17:25:05.239: epoch 23:	0.02952373  	0.08119681  	0.06601151  
2023-06-01 17:25:05.239: Find a better model.
2023-06-01 17:25:26.313: [iter 24 : loss : 0.7912 = 0.3225 + 0.4667 + 0.0020, time: 21.070022]
2023-06-01 17:25:26.584: epoch 24:	0.02958296  	0.08168090  	0.06639508  
2023-06-01 17:25:26.584: Find a better model.
2023-06-01 17:25:47.688: [iter 25 : loss : 0.7598 = 0.2901 + 0.4674 + 0.0023, time: 21.100122]
2023-06-01 17:25:47.957: epoch 25:	0.02982726  	0.08268133  	0.06676627  
2023-06-01 17:25:47.957: Find a better model.
2023-06-01 17:26:09.325: [iter 26 : loss : 0.7339 = 0.2637 + 0.4677 + 0.0026, time: 21.363131]
2023-06-01 17:26:09.595: epoch 26:	0.02995312  	0.08318144  	0.06697260  
2023-06-01 17:26:09.595: Find a better model.
2023-06-01 17:26:30.885: [iter 27 : loss : 0.7115 = 0.2411 + 0.4675 + 0.0029, time: 21.286941]
2023-06-01 17:26:31.157: epoch 27:	0.03010118  	0.08328875  	0.06719217  
2023-06-01 17:26:31.157: Find a better model.
2023-06-01 17:26:52.288: [iter 28 : loss : 0.6925 = 0.2222 + 0.4672 + 0.0031, time: 21.127037]
2023-06-01 17:26:52.559: epoch 28:	0.03013080  	0.08343557  	0.06730732  
2023-06-01 17:26:52.559: Find a better model.
2023-06-01 17:27:13.481: [iter 29 : loss : 0.6762 = 0.2063 + 0.4665 + 0.0034, time: 20.918257]
2023-06-01 17:27:13.750: epoch 29:	0.03021963  	0.08415405  	0.06749184  
2023-06-01 17:27:13.750: Find a better model.
2023-06-01 17:27:34.913: [iter 30 : loss : 0.6613 = 0.1916 + 0.4661 + 0.0036, time: 21.157604]
2023-06-01 17:27:35.201: epoch 30:	0.03026404  	0.08446548  	0.06749842  
2023-06-01 17:27:35.201: Find a better model.
2023-06-01 17:27:56.258: [iter 31 : loss : 0.6485 = 0.1793 + 0.4653 + 0.0038, time: 21.052887]
2023-06-01 17:27:56.531: epoch 31:	0.03038249  	0.08444960  	0.06757713  
2023-06-01 17:28:17.508: [iter 32 : loss : 0.6376 = 0.1689 + 0.4647 + 0.0040, time: 20.974149]
2023-06-01 17:28:17.777: epoch 32:	0.03031586  	0.08414214  	0.06770174  
2023-06-01 17:28:39.071: [iter 33 : loss : 0.6280 = 0.1598 + 0.4640 + 0.0042, time: 21.290127]
2023-06-01 17:28:39.351: epoch 33:	0.03053795  	0.08476765  	0.06808769  
2023-06-01 17:28:39.351: Find a better model.
2023-06-01 17:29:00.684: [iter 34 : loss : 0.6192 = 0.1514 + 0.4634 + 0.0044, time: 21.329211]
2023-06-01 17:29:00.965: epoch 34:	0.03036028  	0.08419358  	0.06784072  
2023-06-01 17:29:22.436: [iter 35 : loss : 0.6103 = 0.1430 + 0.4627 + 0.0046, time: 21.468060]
2023-06-01 17:29:22.711: epoch 35:	0.03034548  	0.08425453  	0.06787662  
2023-06-01 17:29:44.248: [iter 36 : loss : 0.6035 = 0.1365 + 0.4622 + 0.0048, time: 21.532016]
2023-06-01 17:29:44.520: epoch 36:	0.03038249  	0.08465128  	0.06798454  
2023-06-01 17:30:05.830: [iter 37 : loss : 0.5975 = 0.1308 + 0.4617 + 0.0050, time: 21.307072]
2023-06-01 17:30:06.100: epoch 37:	0.03047134  	0.08471935  	0.06802434  
2023-06-01 17:30:27.236: [iter 38 : loss : 0.5907 = 0.1245 + 0.4610 + 0.0052, time: 21.133055]
2023-06-01 17:30:27.506: epoch 38:	0.03047874  	0.08473359  	0.06816414  
2023-06-01 17:30:48.594: [iter 39 : loss : 0.5856 = 0.1198 + 0.4605 + 0.0054, time: 21.083956]
2023-06-01 17:30:48.864: epoch 39:	0.03050094  	0.08479471  	0.06820953  
2023-06-01 17:30:48.864: Find a better model.
2023-06-01 17:31:10.036: [iter 40 : loss : 0.5801 = 0.1146 + 0.4600 + 0.0055, time: 21.167632]
2023-06-01 17:31:10.310: epoch 40:	0.03053055  	0.08478172  	0.06808075  
2023-06-01 17:31:31.598: [iter 41 : loss : 0.5755 = 0.1102 + 0.4596 + 0.0057, time: 21.285313]
2023-06-01 17:31:31.868: epoch 41:	0.03049353  	0.08466575  	0.06799150  
2023-06-01 17:31:52.997: [iter 42 : loss : 0.5713 = 0.1062 + 0.4592 + 0.0059, time: 21.124729]
2023-06-01 17:31:53.273: epoch 42:	0.03037508  	0.08349521  	0.06768384  
2023-06-01 17:32:14.367: [iter 43 : loss : 0.5672 = 0.1024 + 0.4588 + 0.0060, time: 21.091437]
2023-06-01 17:32:14.635: epoch 43:	0.03035287  	0.08333746  	0.06754198  
2023-06-01 17:32:36.158: [iter 44 : loss : 0.5642 = 0.0996 + 0.4584 + 0.0062, time: 21.518197]
2023-06-01 17:32:36.426: epoch 44:	0.03024182  	0.08289350  	0.06721468  
2023-06-01 17:33:03.364: my pid: 15696
2023-06-01 17:33:03.364: model: model.general_recommender.SGL
2023-06-01 17:33:03.364: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 17:33:03.364: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 17:33:07.846: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 17:33:28.964: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.118032]
2023-06-01 17:33:29.243: epoch 1:	0.00146580  	0.00305013  	0.00238767  
2023-06-01 17:33:29.243: Find a better model.
2023-06-01 17:33:50.629: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.381470]
2023-06-01 17:33:50.921: epoch 2:	0.00165087  	0.00300768  	0.00260583  
2023-06-01 17:34:11.577: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.651680]
2023-06-01 17:34:11.872: epoch 3:	0.00204324  	0.00359939  	0.00307318  
2023-06-01 17:34:11.872: Find a better model.
2023-06-01 17:34:32.752: [iter 4 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 20.877700]
2023-06-01 17:34:33.044: epoch 4:	0.00232455  	0.00508259  	0.00407583  
2023-06-01 17:34:33.044: Find a better model.
2023-06-01 17:34:53.939: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.892033]
2023-06-01 17:34:54.244: epoch 5:	0.00288718  	0.00652283  	0.00503758  
2023-06-01 17:34:54.244: Find a better model.
2023-06-01 17:35:14.977: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.728726]
2023-06-01 17:35:15.272: epoch 6:	0.00360527  	0.00839662  	0.00653703  
2023-06-01 17:35:15.273: Find a better model.
2023-06-01 17:35:36.067: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.790964]
2023-06-01 17:35:36.372: epoch 7:	0.00425674  	0.01087602  	0.00874199  
2023-06-01 17:35:36.372: Find a better model.
2023-06-01 17:35:57.302: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.926013]
2023-06-01 17:35:57.590: epoch 8:	0.00514509  	0.01359256  	0.01089599  
2023-06-01 17:35:57.590: Find a better model.
2023-06-01 17:36:18.083: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.489611]
2023-06-01 17:36:18.370: epoch 9:	0.00587058  	0.01670792  	0.01328709  
2023-06-01 17:36:18.370: Find a better model.
2023-06-01 17:36:38.764: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.390177]
2023-06-01 17:36:39.057: epoch 10:	0.00609267  	0.01687275  	0.01390026  
2023-06-01 17:36:39.057: Find a better model.
2023-06-01 17:36:59.302: [iter 11 : loss : 1.1338 = 0.6896 + 0.4442 + 0.0000, time: 20.241176]
2023-06-01 17:36:59.580: epoch 11:	0.00723272  	0.02023965  	0.01624602  
2023-06-01 17:36:59.580: Find a better model.
2023-06-01 17:37:19.882: [iter 12 : loss : 1.1326 = 0.6876 + 0.4449 + 0.0000, time: 20.298335]
2023-06-01 17:37:20.163: epoch 12:	0.00897982  	0.02470885  	0.02075714  
2023-06-01 17:37:20.163: Find a better model.
2023-06-01 17:37:40.243: [iter 13 : loss : 1.1305 = 0.6849 + 0.4456 + 0.0001, time: 20.074059]
2023-06-01 17:37:40.520: epoch 13:	0.01113411  	0.03091511  	0.02606416  
2023-06-01 17:37:40.520: Find a better model.
2023-06-01 17:38:00.886: [iter 14 : loss : 1.1268 = 0.6804 + 0.4463 + 0.0001, time: 20.361764]
2023-06-01 17:38:01.164: epoch 14:	0.01437669  	0.03857692  	0.03341741  
2023-06-01 17:38:01.164: Find a better model.
2023-06-01 17:38:21.284: [iter 15 : loss : 1.1200 = 0.6727 + 0.4472 + 0.0001, time: 20.111413]
2023-06-01 17:38:21.562: epoch 15:	0.01796723  	0.04826491  	0.04113805  
2023-06-01 17:38:21.562: Find a better model.
2023-06-01 17:38:41.856: [iter 16 : loss : 1.1078 = 0.6594 + 0.4482 + 0.0002, time: 20.291045]
2023-06-01 17:38:42.134: epoch 16:	0.02104699  	0.05632775  	0.04784351  
2023-06-01 17:38:42.134: Find a better model.
2023-06-01 17:39:02.450: [iter 17 : loss : 1.0876 = 0.6379 + 0.4495 + 0.0003, time: 20.313368]
2023-06-01 17:39:02.727: epoch 17:	0.02381582  	0.06412312  	0.05411135  
2023-06-01 17:39:02.728: Find a better model.
2023-06-01 17:39:23.063: [iter 18 : loss : 1.0568 = 0.6050 + 0.4515 + 0.0004, time: 20.330698]
2023-06-01 17:39:23.351: epoch 18:	0.02611822  	0.07038893  	0.05894517  
2023-06-01 17:39:23.351: Find a better model.
2023-06-01 17:39:43.644: [iter 19 : loss : 1.0161 = 0.5618 + 0.4538 + 0.0006, time: 20.289107]
2023-06-01 17:39:43.921: epoch 19:	0.02766550  	0.07448548  	0.06254051  
2023-06-01 17:39:43.921: Find a better model.
2023-06-01 17:40:04.212: [iter 20 : loss : 0.9685 = 0.5108 + 0.4568 + 0.0008, time: 20.286527]
2023-06-01 17:40:04.484: epoch 20:	0.02830219  	0.07626431  	0.06401037  
2023-06-01 17:40:04.484: Find a better model.
2023-06-01 17:40:25.231: [iter 21 : loss : 0.9185 = 0.4575 + 0.4600 + 0.0011, time: 20.743063]
2023-06-01 17:40:25.504: epoch 21:	0.02891665  	0.07823206  	0.06529485  
2023-06-01 17:40:25.504: Find a better model.
2023-06-01 17:40:46.257: [iter 22 : loss : 0.8705 = 0.4060 + 0.4631 + 0.0014, time: 20.750012]
2023-06-01 17:40:46.527: epoch 22:	0.02922021  	0.07890629  	0.06570824  
2023-06-01 17:40:46.527: Find a better model.
2023-06-01 17:41:07.243: [iter 23 : loss : 0.8283 = 0.3612 + 0.4654 + 0.0017, time: 20.713034]
2023-06-01 17:41:07.510: epoch 23:	0.02940528  	0.07972905  	0.06612196  
2023-06-01 17:41:07.510: Find a better model.
2023-06-01 17:41:28.567: [iter 24 : loss : 0.7919 = 0.3230 + 0.4669 + 0.0020, time: 21.052069]
2023-06-01 17:41:28.835: epoch 24:	0.02946451  	0.07991506  	0.06598025  
2023-06-01 17:41:28.835: Find a better model.
2023-06-01 17:41:49.938: [iter 25 : loss : 0.7604 = 0.2905 + 0.4676 + 0.0023, time: 21.100015]
2023-06-01 17:41:50.221: epoch 25:	0.02950893  	0.08059888  	0.06635749  
2023-06-01 17:41:50.221: Find a better model.
2023-06-01 17:42:11.011: [iter 26 : loss : 0.7344 = 0.2641 + 0.4678 + 0.0026, time: 20.785062]
2023-06-01 17:42:11.283: epoch 26:	0.02955335  	0.08056563  	0.06650373  
2023-06-01 17:42:32.221: [iter 27 : loss : 0.7119 = 0.2414 + 0.4677 + 0.0029, time: 20.934046]
2023-06-01 17:42:32.488: epoch 27:	0.02976804  	0.08102427  	0.06660008  
2023-06-01 17:42:32.488: Find a better model.
2023-06-01 17:42:53.562: [iter 28 : loss : 0.6930 = 0.2227 + 0.4672 + 0.0031, time: 21.069578]
2023-06-01 17:42:53.831: epoch 28:	0.02978284  	0.08111516  	0.06667190  
2023-06-01 17:42:53.832: Find a better model.
2023-06-01 17:43:14.932: [iter 29 : loss : 0.6766 = 0.2066 + 0.4667 + 0.0033, time: 21.096203]
2023-06-01 17:43:15.212: epoch 29:	0.02983466  	0.08187872  	0.06714285  
2023-06-01 17:43:15.212: Find a better model.
2023-06-01 17:43:36.150: [iter 30 : loss : 0.6619 = 0.1923 + 0.4660 + 0.0036, time: 20.935373]
2023-06-01 17:43:36.422: epoch 30:	0.02975323  	0.08169411  	0.06723108  
2023-06-01 17:43:57.375: [iter 31 : loss : 0.6492 = 0.1800 + 0.4654 + 0.0038, time: 20.949486]
2023-06-01 17:43:57.642: epoch 31:	0.02987168  	0.08193410  	0.06709216  
2023-06-01 17:43:57.642: Find a better model.
2023-06-01 17:44:18.535: [iter 32 : loss : 0.6381 = 0.1693 + 0.4648 + 0.0040, time: 20.890244]
2023-06-01 17:44:18.803: epoch 32:	0.02985687  	0.08210506  	0.06740581  
2023-06-01 17:44:18.803: Find a better model.
2023-06-01 17:44:39.739: [iter 33 : loss : 0.6287 = 0.1604 + 0.4641 + 0.0042, time: 20.932905]
2023-06-01 17:44:40.005: epoch 33:	0.02984207  	0.08221763  	0.06733008  
2023-06-01 17:44:40.005: Find a better model.
2023-06-01 17:45:00.967: [iter 34 : loss : 0.6199 = 0.1520 + 0.4635 + 0.0044, time: 20.959065]
2023-06-01 17:45:01.247: epoch 34:	0.02976063  	0.08180729  	0.06721466  
2023-06-01 17:45:22.323: [iter 35 : loss : 0.6109 = 0.1436 + 0.4628 + 0.0046, time: 21.071524]
2023-06-01 17:45:22.588: epoch 35:	0.02991610  	0.08241671  	0.06739889  
2023-06-01 17:45:22.588: Find a better model.
2023-06-01 17:45:43.869: [iter 36 : loss : 0.6038 = 0.1368 + 0.4622 + 0.0048, time: 21.278007]
2023-06-01 17:45:44.203: epoch 36:	0.02999754  	0.08267991  	0.06752974  
2023-06-01 17:45:44.203: Find a better model.
2023-06-01 17:46:05.760: [iter 37 : loss : 0.5979 = 0.1312 + 0.4617 + 0.0050, time: 21.550956]
2023-06-01 17:46:06.032: epoch 37:	0.03003456  	0.08259906  	0.06761700  
2023-06-01 17:46:27.163: [iter 38 : loss : 0.5915 = 0.1252 + 0.4611 + 0.0052, time: 21.126562]
2023-06-01 17:46:27.429: epoch 38:	0.03010859  	0.08260972  	0.06773165  
2023-06-01 17:46:48.317: [iter 39 : loss : 0.5863 = 0.1204 + 0.4605 + 0.0054, time: 20.884010]
2023-06-01 17:46:48.585: epoch 39:	0.03014560  	0.08241341  	0.06765229  
2023-06-01 17:47:09.479: [iter 40 : loss : 0.5807 = 0.1152 + 0.4601 + 0.0055, time: 20.889571]
2023-06-01 17:47:09.746: epoch 40:	0.03018261  	0.08233617  	0.06773131  
2023-06-01 17:47:30.889: [iter 41 : loss : 0.5758 = 0.1106 + 0.4596 + 0.0057, time: 21.140426]
2023-06-01 17:47:31.159: epoch 41:	0.03017521  	0.08247190  	0.06772985  
2023-06-01 17:47:52.088: [iter 42 : loss : 0.5713 = 0.1063 + 0.4591 + 0.0059, time: 20.925091]
2023-06-01 17:47:52.352: epoch 42:	0.03018262  	0.08263198  	0.06775026  
2023-06-01 17:48:13.079: [iter 43 : loss : 0.5674 = 0.1027 + 0.4588 + 0.0060, time: 20.723005]
2023-06-01 17:48:13.350: epoch 43:	0.03010119  	0.08250017  	0.06766719  
2023-06-01 17:48:34.410: [iter 44 : loss : 0.5646 = 0.1000 + 0.4584 + 0.0062, time: 21.056048]
2023-06-01 17:48:34.681: epoch 44:	0.03004937  	0.08212364  	0.06753098  
2023-06-01 17:48:55.798: [iter 45 : loss : 0.5605 = 0.0962 + 0.4580 + 0.0063, time: 21.113919]
2023-06-01 17:48:56.064: epoch 45:	0.03010119  	0.08225159  	0.06745009  
2023-06-01 17:49:17.045: [iter 46 : loss : 0.5575 = 0.0933 + 0.4577 + 0.0065, time: 20.977018]
2023-06-01 17:49:17.314: epoch 46:	0.03016782  	0.08214656  	0.06735612  
2023-06-01 17:49:38.225: [iter 47 : loss : 0.5541 = 0.0902 + 0.4573 + 0.0066, time: 20.907356]
2023-06-01 17:49:38.498: epoch 47:	0.02996794  	0.08179148  	0.06731920  
2023-06-01 17:49:59.604: [iter 48 : loss : 0.5515 = 0.0878 + 0.4570 + 0.0068, time: 21.103086]
2023-06-01 17:49:59.867: epoch 48:	0.02997534  	0.08173058  	0.06731047  
2023-06-01 17:50:20.830: [iter 49 : loss : 0.5487 = 0.0851 + 0.4567 + 0.0069, time: 20.959039]
2023-06-01 17:50:21.092: epoch 49:	0.02989390  	0.08177838  	0.06721639  
2023-06-01 17:50:42.039: [iter 50 : loss : 0.5465 = 0.0831 + 0.4564 + 0.0070, time: 20.942336]
2023-06-01 17:50:42.306: epoch 50:	0.02978285  	0.08090825  	0.06687211  
2023-06-01 17:51:03.231: [iter 51 : loss : 0.5437 = 0.0805 + 0.4560 + 0.0072, time: 20.920302]
2023-06-01 17:51:03.497: epoch 51:	0.02970142  	0.08038986  	0.06647661  
2023-06-01 17:51:24.583: [iter 52 : loss : 0.5413 = 0.0781 + 0.4559 + 0.0073, time: 21.083447]
2023-06-01 17:51:24.854: epoch 52:	0.02968661  	0.08037806  	0.06655688  
2023-06-01 17:51:45.971: [iter 53 : loss : 0.5405 = 0.0773 + 0.4558 + 0.0074, time: 21.113039]
2023-06-01 17:51:46.247: epoch 53:	0.02967921  	0.08005142  	0.06632148  
2023-06-01 17:52:07.199: [iter 54 : loss : 0.5381 = 0.0751 + 0.4554 + 0.0076, time: 20.947932]
2023-06-01 17:52:07.463: epoch 54:	0.02958296  	0.07928568  	0.06609407  
2023-06-01 17:52:28.593: [iter 55 : loss : 0.5359 = 0.0730 + 0.4552 + 0.0077, time: 21.125350]
2023-06-01 17:52:28.858: epoch 55:	0.02947931  	0.07903330  	0.06600986  
2023-06-01 17:52:50.004: [iter 56 : loss : 0.5338 = 0.0711 + 0.4549 + 0.0078, time: 21.142015]
2023-06-01 17:52:50.276: epoch 56:	0.02950151  	0.07915571  	0.06592675  
2023-06-01 17:53:11.338: [iter 57 : loss : 0.5319 = 0.0692 + 0.4548 + 0.0079, time: 21.056031]
2023-06-01 17:53:11.601: epoch 57:	0.02950151  	0.07891676  	0.06601405  
2023-06-01 17:53:32.565: [iter 58 : loss : 0.5309 = 0.0681 + 0.4547 + 0.0080, time: 20.960010]
2023-06-01 17:53:32.828: epoch 58:	0.02946449  	0.07837711  	0.06573161  
2023-06-01 17:53:53.969: [iter 59 : loss : 0.5292 = 0.0666 + 0.4545 + 0.0082, time: 21.137061]
2023-06-01 17:53:54.245: epoch 59:	0.02938305  	0.07826741  	0.06556866  
2023-06-01 17:54:15.157: [iter 60 : loss : 0.5277 = 0.0651 + 0.4543 + 0.0083, time: 20.908318]
2023-06-01 17:54:15.423: epoch 60:	0.02930162  	0.07782675  	0.06536564  
2023-06-01 17:54:36.371: [iter 61 : loss : 0.5263 = 0.0638 + 0.4541 + 0.0084, time: 20.944106]
2023-06-01 17:54:36.637: epoch 61:	0.02927200  	0.07747618  	0.06523660  
2023-06-01 17:54:36.637: Early stopping is trigger at epoch: 61
2023-06-01 17:54:36.637: best_result@epoch 36:

2023-06-01 17:54:36.637: 		0.0300      	0.0827      	0.0675      
2023-06-01 18:15:08.053: my pid: 2804
2023-06-01 18:15:08.053: model: model.general_recommender.SGL
2023-06-01 18:15:08.053: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 18:15:08.053: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 18:15:12.055: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 18:15:33.152: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.096368]
2023-06-01 18:15:33.429: epoch 1:	0.00133254  	0.00246236  	0.00203011  
2023-06-01 18:15:33.429: Find a better model.
2023-06-01 18:15:54.553: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.121574]
2023-06-01 18:15:54.849: epoch 2:	0.00149541  	0.00344904  	0.00255212  
2023-06-01 18:15:54.849: Find a better model.
2023-06-01 18:16:15.907: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.052516]
2023-06-01 18:16:16.204: epoch 3:	0.00207285  	0.00460897  	0.00341093  
2023-06-01 18:16:16.204: Find a better model.
2023-06-01 18:16:37.307: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.099051]
2023-06-01 18:16:37.608: epoch 4:	0.00243559  	0.00550925  	0.00407362  
2023-06-01 18:16:37.608: Find a better model.
2023-06-01 18:16:58.912: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.301361]
2023-06-01 18:16:59.209: epoch 5:	0.00286497  	0.00627007  	0.00480228  
2023-06-01 18:16:59.209: Find a better model.
2023-06-01 18:17:20.316: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 21.103060]
2023-06-01 18:17:20.618: epoch 6:	0.00336838  	0.00795605  	0.00598359  
2023-06-01 18:17:20.618: Find a better model.
2023-06-01 18:17:41.721: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.098057]
2023-06-01 18:17:42.014: epoch 7:	0.00432336  	0.01079184  	0.00843039  
2023-06-01 18:17:42.014: Find a better model.
2023-06-01 18:18:03.107: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 21.088088]
2023-06-01 18:18:03.400: epoch 8:	0.00509327  	0.01276874  	0.01023484  
2023-06-01 18:18:03.400: Find a better model.
2023-06-01 18:18:24.307: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.902437]
2023-06-01 18:18:24.602: epoch 9:	0.00585577  	0.01458428  	0.01233231  
2023-06-01 18:18:24.603: Find a better model.
2023-06-01 18:18:45.687: [iter 10 : loss : 1.1345 = 0.6909 + 0.4436 + 0.0000, time: 21.080467]
2023-06-01 18:18:45.975: epoch 10:	0.00688478  	0.01861026  	0.01524156  
2023-06-01 18:18:45.975: Find a better model.
2023-06-01 18:19:06.905: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 20.925552]
2023-06-01 18:19:07.200: epoch 11:	0.00842459  	0.02308074  	0.01907433  
2023-06-01 18:19:07.200: Find a better model.
2023-06-01 18:19:28.278: [iter 12 : loss : 1.1329 = 0.6881 + 0.4448 + 0.0000, time: 21.075109]
2023-06-01 18:19:28.575: epoch 12:	0.00936477  	0.02646174  	0.02212179  
2023-06-01 18:19:28.575: Find a better model.
2023-06-01 18:19:49.656: [iter 13 : loss : 1.1308 = 0.6853 + 0.4454 + 0.0001, time: 21.076272]
2023-06-01 18:19:49.943: epoch 13:	0.01163012  	0.03285191  	0.02784305  
2023-06-01 18:19:49.943: Find a better model.
2023-06-01 18:20:10.857: [iter 14 : loss : 1.1271 = 0.6809 + 0.4461 + 0.0001, time: 20.909383]
2023-06-01 18:20:11.142: epoch 14:	0.01452476  	0.04013906  	0.03459727  
2023-06-01 18:20:11.142: Find a better model.
2023-06-01 18:20:32.004: [iter 15 : loss : 1.1207 = 0.6735 + 0.4470 + 0.0001, time: 20.857311]
2023-06-01 18:20:32.285: epoch 15:	0.01764890  	0.04826979  	0.04150040  
2023-06-01 18:20:32.285: Find a better model.
2023-06-01 18:20:53.226: [iter 16 : loss : 1.1090 = 0.6608 + 0.4480 + 0.0002, time: 20.937780]
2023-06-01 18:20:53.523: epoch 16:	0.02080268  	0.05579776  	0.04894965  
2023-06-01 18:20:53.523: Find a better model.
2023-06-01 18:21:14.261: [iter 17 : loss : 1.0898 = 0.6402 + 0.4493 + 0.0003, time: 20.734016]
2023-06-01 18:21:14.554: epoch 17:	0.02366035  	0.06341435  	0.05522062  
2023-06-01 18:21:14.554: Find a better model.
2023-06-01 18:21:35.402: [iter 18 : loss : 1.0598 = 0.6083 + 0.4511 + 0.0004, time: 20.843334]
2023-06-01 18:21:35.686: epoch 18:	0.02615523  	0.07070477  	0.06003851  
2023-06-01 18:21:35.686: Find a better model.
2023-06-01 18:21:56.607: [iter 19 : loss : 1.0198 = 0.5659 + 0.4533 + 0.0006, time: 20.915448]
2023-06-01 18:21:56.890: epoch 19:	0.02773213  	0.07570420  	0.06354221  
2023-06-01 18:21:56.890: Find a better model.
2023-06-01 18:22:17.590: [iter 20 : loss : 0.9726 = 0.5156 + 0.4562 + 0.0008, time: 20.696238]
2023-06-01 18:22:17.871: epoch 20:	0.02902030  	0.07870697  	0.06559352  
2023-06-01 18:22:17.871: Find a better model.
2023-06-01 18:22:39.163: [iter 21 : loss : 0.9228 = 0.4623 + 0.4593 + 0.0011, time: 21.288033]
2023-06-01 18:22:39.442: epoch 21:	0.02961994  	0.07995816  	0.06648035  
2023-06-01 18:22:39.442: Find a better model.
2023-06-01 18:23:00.751: [iter 22 : loss : 0.8741 = 0.4103 + 0.4624 + 0.0014, time: 21.305459]
2023-06-01 18:23:01.028: epoch 22:	0.02979765  	0.08139043  	0.06702781  
2023-06-01 18:23:01.028: Find a better model.
2023-06-01 18:23:22.397: [iter 23 : loss : 0.8310 = 0.3646 + 0.4647 + 0.0017, time: 21.364918]
2023-06-01 18:23:22.681: epoch 23:	0.02993831  	0.08209183  	0.06678098  
2023-06-01 18:23:22.681: Find a better model.
2023-06-01 18:23:44.193: [iter 24 : loss : 0.7939 = 0.3256 + 0.4663 + 0.0020, time: 21.509016]
2023-06-01 18:23:44.478: epoch 24:	0.03004935  	0.08221445  	0.06699906  
2023-06-01 18:23:44.479: Find a better model.
2023-06-01 18:24:05.973: [iter 25 : loss : 0.7622 = 0.2928 + 0.4671 + 0.0023, time: 21.484168]
2023-06-01 18:24:06.254: epoch 25:	0.03033066  	0.08320318  	0.06744091  
2023-06-01 18:24:06.254: Find a better model.
2023-06-01 18:24:27.542: [iter 26 : loss : 0.7356 = 0.2657 + 0.4674 + 0.0026, time: 21.284020]
2023-06-01 18:24:27.815: epoch 26:	0.03030106  	0.08336729  	0.06779727  
2023-06-01 18:24:27.815: Find a better model.
2023-06-01 18:24:49.334: [iter 27 : loss : 0.7128 = 0.2427 + 0.4673 + 0.0028, time: 21.516542]
2023-06-01 18:24:49.612: epoch 27:	0.03031585  	0.08324327  	0.06788945  
2023-06-01 18:25:11.136: [iter 28 : loss : 0.6936 = 0.2235 + 0.4669 + 0.0031, time: 21.518501]
2023-06-01 18:25:11.408: epoch 28:	0.03043432  	0.08355349  	0.06826855  
2023-06-01 18:25:11.409: Find a better model.
2023-06-01 18:25:32.732: [iter 29 : loss : 0.6770 = 0.2073 + 0.4664 + 0.0033, time: 21.319227]
2023-06-01 18:25:33.006: epoch 29:	0.03050835  	0.08403785  	0.06871334  
2023-06-01 18:25:33.006: Find a better model.
2023-06-01 18:25:54.566: [iter 30 : loss : 0.6620 = 0.1926 + 0.4658 + 0.0036, time: 21.556230]
2023-06-01 18:25:54.839: epoch 30:	0.03056756  	0.08425140  	0.06874949  
2023-06-01 18:25:54.839: Find a better model.
2023-06-01 18:26:16.114: [iter 31 : loss : 0.6490 = 0.1801 + 0.4652 + 0.0038, time: 21.270070]
2023-06-01 18:26:16.388: epoch 31:	0.03050094  	0.08402866  	0.06870395  
2023-06-01 18:26:37.700: [iter 32 : loss : 0.6380 = 0.1696 + 0.4644 + 0.0040, time: 21.306493]
2023-06-01 18:26:37.978: epoch 32:	0.03056756  	0.08417195  	0.06891934  
2023-06-01 18:26:59.683: [iter 33 : loss : 0.6285 = 0.1606 + 0.4637 + 0.0042, time: 21.701213]
2023-06-01 18:26:59.959: epoch 33:	0.03063419  	0.08445618  	0.06910174  
2023-06-01 18:26:59.959: Find a better model.
2023-06-01 18:27:21.477: [iter 34 : loss : 0.6194 = 0.1517 + 0.4633 + 0.0044, time: 21.513709]
2023-06-01 18:27:21.749: epoch 34:	0.03065639  	0.08432045  	0.06921171  
2023-06-01 18:27:43.080: [iter 35 : loss : 0.6108 = 0.1435 + 0.4626 + 0.0046, time: 21.327597]
2023-06-01 18:27:43.353: epoch 35:	0.03064899  	0.08470611  	0.06923090  
2023-06-01 18:27:43.353: Find a better model.
2023-06-01 18:28:05.047: [iter 36 : loss : 0.6036 = 0.1369 + 0.4619 + 0.0048, time: 21.690307]
2023-06-01 18:28:05.325: epoch 36:	0.03071563  	0.08467110  	0.06930056  
2023-06-01 18:28:26.851: [iter 37 : loss : 0.5973 = 0.1309 + 0.4614 + 0.0050, time: 21.521188]
2023-06-01 18:28:27.124: epoch 37:	0.03063418  	0.08419835  	0.06909074  
2023-06-01 18:28:48.632: [iter 38 : loss : 0.5909 = 0.1249 + 0.4608 + 0.0052, time: 21.503997]
2023-06-01 18:28:48.909: epoch 38:	0.03059716  	0.08426998  	0.06918491  
2023-06-01 18:29:10.426: [iter 39 : loss : 0.5857 = 0.1201 + 0.4603 + 0.0054, time: 21.512058]
2023-06-01 18:29:10.700: epoch 39:	0.03071561  	0.08447415  	0.06940276  
2023-06-01 18:29:32.212: [iter 40 : loss : 0.5805 = 0.1152 + 0.4597 + 0.0055, time: 21.507452]
2023-06-01 18:29:32.497: epoch 40:	0.03069341  	0.08455659  	0.06952306  
2023-06-01 18:29:54.008: [iter 41 : loss : 0.5754 = 0.1103 + 0.4593 + 0.0057, time: 21.506027]
2023-06-01 18:29:54.284: epoch 41:	0.03073042  	0.08436454  	0.06947520  
2023-06-01 18:30:16.015: [iter 42 : loss : 0.5710 = 0.1062 + 0.4589 + 0.0059, time: 21.728090]
2023-06-01 18:30:16.288: epoch 42:	0.03072302  	0.08451676  	0.06948000  
2023-06-01 18:30:37.830: [iter 43 : loss : 0.5669 = 0.1023 + 0.4586 + 0.0060, time: 21.539137]
2023-06-01 18:30:38.104: epoch 43:	0.03070822  	0.08441014  	0.06961593  
2023-06-01 18:30:59.829: [iter 44 : loss : 0.5640 = 0.0997 + 0.4582 + 0.0062, time: 21.720044]
2023-06-01 18:31:00.109: epoch 44:	0.03067861  	0.08430572  	0.06945546  
2023-06-01 18:31:21.818: [iter 45 : loss : 0.5602 = 0.0961 + 0.4578 + 0.0063, time: 21.705029]
2023-06-01 18:31:22.093: epoch 45:	0.03058977  	0.08397645  	0.06936102  
2023-06-01 18:31:43.412: [iter 46 : loss : 0.5574 = 0.0934 + 0.4575 + 0.0065, time: 21.313181]
2023-06-01 18:31:43.688: epoch 46:	0.03053794  	0.08367471  	0.06913235  
2023-06-01 18:32:05.180: [iter 47 : loss : 0.5536 = 0.0900 + 0.4570 + 0.0066, time: 21.488302]
2023-06-01 18:32:05.454: epoch 47:	0.03043429  	0.08348437  	0.06905203  
2023-06-01 18:32:27.163: [iter 48 : loss : 0.5510 = 0.0874 + 0.4568 + 0.0068, time: 21.704626]
2023-06-01 18:32:27.438: epoch 48:	0.03039728  	0.08330850  	0.06887567  
2023-06-01 18:32:48.968: [iter 49 : loss : 0.5485 = 0.0852 + 0.4564 + 0.0069, time: 21.527058]
2023-06-01 18:32:49.240: epoch 49:	0.03033806  	0.08275386  	0.06866704  
2023-06-01 18:33:10.958: [iter 50 : loss : 0.5464 = 0.0831 + 0.4563 + 0.0070, time: 21.714150]
2023-06-01 18:33:11.232: epoch 50:	0.03028624  	0.08249398  	0.06849811  
2023-06-01 18:33:32.949: [iter 51 : loss : 0.5435 = 0.0803 + 0.4560 + 0.0072, time: 21.712113]
2023-06-01 18:33:33.224: epoch 51:	0.03012337  	0.08190189  	0.06837016  
2023-06-01 18:33:54.947: [iter 52 : loss : 0.5407 = 0.0777 + 0.4557 + 0.0073, time: 21.720226]
2023-06-01 18:33:55.219: epoch 52:	0.02993828  	0.08092137  	0.06806859  
2023-06-01 18:34:16.769: [iter 53 : loss : 0.5399 = 0.0770 + 0.4554 + 0.0074, time: 21.545511]
2023-06-01 18:34:17.043: epoch 53:	0.03002711  	0.08123440  	0.06808425  
2023-06-01 18:34:38.714: [iter 54 : loss : 0.5377 = 0.0749 + 0.4552 + 0.0076, time: 21.667752]
2023-06-01 18:34:38.988: epoch 54:	0.02992347  	0.08075304  	0.06777020  
2023-06-01 18:35:00.358: [iter 55 : loss : 0.5358 = 0.0730 + 0.4551 + 0.0077, time: 21.367653]
2023-06-01 18:35:00.639: epoch 55:	0.02985684  	0.08077119  	0.06760888  
2023-06-01 18:35:22.113: [iter 56 : loss : 0.5336 = 0.0710 + 0.4549 + 0.0078, time: 21.469371]
2023-06-01 18:35:22.386: epoch 56:	0.02988646  	0.08073253  	0.06749303  
2023-06-01 18:35:43.957: [iter 57 : loss : 0.5318 = 0.0692 + 0.4546 + 0.0079, time: 21.567152]
2023-06-01 18:35:44.228: epoch 57:	0.02973839  	0.08033784  	0.06724296  
2023-06-01 18:36:05.515: [iter 58 : loss : 0.5304 = 0.0680 + 0.4544 + 0.0080, time: 21.283578]
2023-06-01 18:36:05.786: epoch 58:	0.02973099  	0.08017968  	0.06713536  
2023-06-01 18:36:27.269: [iter 59 : loss : 0.5285 = 0.0661 + 0.4542 + 0.0082, time: 21.478037]
2023-06-01 18:36:27.548: epoch 59:	0.02970138  	0.07961632  	0.06690915  
2023-06-01 18:36:49.071: [iter 60 : loss : 0.5275 = 0.0651 + 0.4541 + 0.0083, time: 21.518240]
2023-06-01 18:36:49.343: epoch 60:	0.02966436  	0.07926820  	0.06674497  
2023-06-01 18:36:49.343: Early stopping is trigger at epoch: 60
2023-06-01 18:36:49.343: best_result@epoch 35:

2023-06-01 18:36:49.343: 		0.0306      	0.0847      	0.0692      
2023-06-01 18:52:10.260: my pid: 4100
2023-06-01 18:52:10.260: model: model.general_recommender.SGL
2023-06-01 18:52:10.260: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 18:52:10.260: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 18:52:14.257: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 18:52:35.580: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.322732]
2023-06-01 18:52:35.856: epoch 1:	0.00131774  	0.00271001  	0.00221953  
2023-06-01 18:52:35.856: Find a better model.
2023-06-01 18:52:57.572: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.711854]
2023-06-01 18:52:57.870: epoch 2:	0.00142138  	0.00310625  	0.00259956  
2023-06-01 18:52:57.870: Find a better model.
2023-06-01 18:53:19.419: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.546286]
2023-06-01 18:53:19.711: epoch 3:	0.00203583  	0.00375826  	0.00331528  
2023-06-01 18:53:19.711: Find a better model.
2023-06-01 18:53:41.146: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.431462]
2023-06-01 18:53:41.438: epoch 4:	0.00208025  	0.00460798  	0.00345129  
2023-06-01 18:53:41.438: Find a better model.
2023-06-01 18:54:02.977: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.534753]
2023-06-01 18:54:03.269: epoch 5:	0.00282795  	0.00685647  	0.00512996  
2023-06-01 18:54:03.269: Find a better model.
2023-06-01 18:54:24.774: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.501747]
2023-06-01 18:54:25.076: epoch 6:	0.00331655  	0.00722213  	0.00584395  
2023-06-01 18:54:25.076: Find a better model.
2023-06-01 18:54:46.596: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.515330]
2023-06-01 18:54:46.892: epoch 7:	0.00396062  	0.00925946  	0.00724987  
2023-06-01 18:54:46.892: Find a better model.
2023-06-01 18:55:08.389: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.493457]
2023-06-01 18:55:08.679: epoch 8:	0.00473793  	0.01148590  	0.00928387  
2023-06-01 18:55:08.679: Find a better model.
2023-06-01 18:55:30.131: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.448011]
2023-06-01 18:55:30.421: epoch 9:	0.00569291  	0.01434855  	0.01180723  
2023-06-01 18:55:30.421: Find a better model.
2023-06-01 18:55:51.771: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.347036]
2023-06-01 18:55:52.058: epoch 10:	0.00641839  	0.01677264  	0.01389488  
2023-06-01 18:55:52.058: Find a better model.
2023-06-01 18:56:13.549: [iter 11 : loss : 1.1340 = 0.6901 + 0.4439 + 0.0000, time: 21.486607]
2023-06-01 18:56:13.848: epoch 11:	0.00765469  	0.02075414  	0.01751116  
2023-06-01 18:56:13.848: Find a better model.
2023-06-01 18:56:34.926: [iter 12 : loss : 1.1329 = 0.6884 + 0.4445 + 0.0000, time: 21.074751]
2023-06-01 18:56:35.211: epoch 12:	0.00909826  	0.02473247  	0.02026121  
2023-06-01 18:56:35.211: Find a better model.
2023-06-01 18:56:56.534: [iter 13 : loss : 1.1310 = 0.6858 + 0.4452 + 0.0000, time: 21.319077]
2023-06-01 18:56:56.827: epoch 13:	0.01124516  	0.03043299  	0.02551535  
2023-06-01 18:56:56.827: Find a better model.
2023-06-01 18:57:17.890: [iter 14 : loss : 1.1277 = 0.6817 + 0.4459 + 0.0001, time: 21.058295]
2023-06-01 18:57:18.172: epoch 14:	0.01436929  	0.03888014  	0.03314532  
2023-06-01 18:57:18.172: Find a better model.
2023-06-01 18:57:39.483: [iter 15 : loss : 1.1218 = 0.6749 + 0.4468 + 0.0001, time: 21.307285]
2023-06-01 18:57:39.779: epoch 15:	0.01744161  	0.04741444  	0.04080715  
2023-06-01 18:57:39.779: Find a better model.
2023-06-01 18:58:00.896: [iter 16 : loss : 1.1109 = 0.6629 + 0.4478 + 0.0002, time: 21.112410]
2023-06-01 18:58:01.175: epoch 16:	0.02053614  	0.05509153  	0.04713889  
2023-06-01 18:58:01.175: Find a better model.
2023-06-01 18:58:22.498: [iter 17 : loss : 1.0927 = 0.6435 + 0.4490 + 0.0002, time: 21.318045]
2023-06-01 18:58:22.790: epoch 17:	0.02344562  	0.06179215  	0.05309007  
2023-06-01 18:58:22.790: Find a better model.
2023-06-01 18:58:43.873: [iter 18 : loss : 1.0641 = 0.6131 + 0.4507 + 0.0004, time: 21.079246]
2023-06-01 18:58:44.154: epoch 18:	0.02548152  	0.06764530  	0.05776225  
2023-06-01 18:58:44.154: Find a better model.
2023-06-01 18:59:05.478: [iter 19 : loss : 1.0253 = 0.5719 + 0.4529 + 0.0006, time: 21.320707]
2023-06-01 18:59:05.771: epoch 19:	0.02708804  	0.07238315  	0.06131849  
2023-06-01 18:59:05.771: Find a better model.
2023-06-01 18:59:26.911: [iter 20 : loss : 0.9786 = 0.5222 + 0.4556 + 0.0008, time: 21.136364]
2023-06-01 18:59:27.189: epoch 20:	0.02810229  	0.07522395  	0.06352963  
2023-06-01 18:59:27.189: Find a better model.
2023-06-01 18:59:48.850: [iter 21 : loss : 0.9288 = 0.4690 + 0.4588 + 0.0011, time: 21.656852]
2023-06-01 18:59:49.129: epoch 21:	0.02865013  	0.07698558  	0.06482679  
2023-06-01 18:59:49.129: Find a better model.
2023-06-01 19:00:10.819: [iter 22 : loss : 0.8798 = 0.4167 + 0.4618 + 0.0014, time: 21.687345]
2023-06-01 19:00:11.099: epoch 22:	0.02884262  	0.07779020  	0.06517909  
2023-06-01 19:00:11.099: Find a better model.
2023-06-01 19:00:32.837: [iter 23 : loss : 0.8365 = 0.3704 + 0.4644 + 0.0017, time: 21.734504]
2023-06-01 19:00:33.111: epoch 23:	0.02919057  	0.07887207  	0.06558557  
2023-06-01 19:00:33.111: Find a better model.
2023-06-01 19:00:54.624: [iter 24 : loss : 0.7982 = 0.3304 + 0.4659 + 0.0020, time: 21.510013]
2023-06-01 19:00:54.899: epoch 24:	0.02920539  	0.07925009  	0.06585173  
2023-06-01 19:00:54.899: Find a better model.
2023-06-01 19:01:16.596: [iter 25 : loss : 0.7657 = 0.2967 + 0.4668 + 0.0023, time: 21.693508]
2023-06-01 19:01:16.875: epoch 25:	0.02953854  	0.08044855  	0.06648735  
2023-06-01 19:01:16.875: Find a better model.
2023-06-01 19:01:38.618: [iter 26 : loss : 0.7388 = 0.2690 + 0.4672 + 0.0025, time: 21.738110]
2023-06-01 19:01:38.889: epoch 26:	0.02947191  	0.08024942  	0.06653767  
2023-06-01 19:02:00.587: [iter 27 : loss : 0.7157 = 0.2458 + 0.4671 + 0.0028, time: 21.694105]
2023-06-01 19:02:00.854: epoch 27:	0.02958296  	0.08066335  	0.06652161  
2023-06-01 19:02:00.854: Find a better model.
2023-06-01 19:02:22.788: [iter 28 : loss : 0.6961 = 0.2262 + 0.4668 + 0.0031, time: 21.929046]
2023-06-01 19:02:23.059: epoch 28:	0.02957556  	0.08074965  	0.06660006  
2023-06-01 19:02:23.059: Find a better model.
2023-06-01 19:02:45.168: [iter 29 : loss : 0.6791 = 0.2096 + 0.4662 + 0.0033, time: 22.103924]
2023-06-01 19:02:45.442: epoch 29:	0.02956815  	0.08079279  	0.06668308  
2023-06-01 19:02:45.442: Find a better model.
2023-06-01 19:03:07.564: [iter 30 : loss : 0.6637 = 0.1946 + 0.4656 + 0.0035, time: 22.118052]
2023-06-01 19:03:07.842: epoch 30:	0.02950892  	0.08055422  	0.06663828  
2023-06-01 19:03:30.158: [iter 31 : loss : 0.6508 = 0.1820 + 0.4650 + 0.0038, time: 22.312050]
2023-06-01 19:03:30.430: epoch 31:	0.02967178  	0.08090884  	0.06687523  
2023-06-01 19:03:30.430: Find a better model.
2023-06-01 19:03:52.560: [iter 32 : loss : 0.6395 = 0.1712 + 0.4644 + 0.0040, time: 22.126154]
2023-06-01 19:03:52.839: epoch 32:	0.02967918  	0.08095460  	0.06692192  
2023-06-01 19:03:52.839: Find a better model.
2023-06-01 19:04:15.153: [iter 33 : loss : 0.6300 = 0.1621 + 0.4636 + 0.0042, time: 22.309109]
2023-06-01 19:04:15.425: epoch 33:	0.02980503  	0.08119342  	0.06694498  
2023-06-01 19:04:15.425: Find a better model.
2023-06-01 19:04:37.725: [iter 34 : loss : 0.6204 = 0.1531 + 0.4629 + 0.0044, time: 22.296045]
2023-06-01 19:04:37.999: epoch 34:	0.02982724  	0.08128314  	0.06709556  
2023-06-01 19:04:37.999: Find a better model.
2023-06-01 19:04:59.767: [iter 35 : loss : 0.6117 = 0.1448 + 0.4623 + 0.0046, time: 21.764051]
2023-06-01 19:05:00.046: epoch 35:	0.02979023  	0.08094896  	0.06704309  
2023-06-01 19:05:22.104: [iter 36 : loss : 0.6047 = 0.1381 + 0.4618 + 0.0048, time: 22.054168]
2023-06-01 19:05:22.372: epoch 36:	0.02987167  	0.08137735  	0.06714179  
2023-06-01 19:05:22.372: Find a better model.
2023-06-01 19:05:44.530: [iter 37 : loss : 0.5981 = 0.1319 + 0.4612 + 0.0050, time: 22.153451]
2023-06-01 19:05:44.810: epoch 37:	0.02970879  	0.08051565  	0.06698059  
2023-06-01 19:06:06.718: [iter 38 : loss : 0.5916 = 0.1258 + 0.4606 + 0.0052, time: 21.904328]
2023-06-01 19:06:06.993: epoch 38:	0.02973100  	0.08053938  	0.06702724  
2023-06-01 19:06:28.895: [iter 39 : loss : 0.5863 = 0.1209 + 0.4601 + 0.0053, time: 21.898627]
2023-06-01 19:06:29.163: epoch 39:	0.02981243  	0.08110774  	0.06704825  
2023-06-01 19:06:51.481: [iter 40 : loss : 0.5808 = 0.1157 + 0.4596 + 0.0055, time: 22.315402]
2023-06-01 19:06:51.766: epoch 40:	0.02973100  	0.08103146  	0.06703250  
2023-06-01 19:07:13.522: [iter 41 : loss : 0.5760 = 0.1112 + 0.4591 + 0.0057, time: 21.749233]
2023-06-01 19:07:13.804: epoch 41:	0.02980503  	0.08089361  	0.06701225  
2023-06-01 19:07:35.654: [iter 42 : loss : 0.5715 = 0.1070 + 0.4587 + 0.0058, time: 21.846199]
2023-06-01 19:07:35.920: epoch 42:	0.02980503  	0.08042788  	0.06687763  
2023-06-01 19:07:57.691: [iter 43 : loss : 0.5675 = 0.1031 + 0.4583 + 0.0060, time: 21.767373]
2023-06-01 19:07:57.962: epoch 43:	0.02970878  	0.07985498  	0.06671566  
2023-06-01 19:08:19.665: [iter 44 : loss : 0.5648 = 0.1006 + 0.4580 + 0.0062, time: 21.699414]
2023-06-01 19:08:19.938: epoch 44:	0.02967177  	0.07993776  	0.06677179  
2023-06-01 19:08:41.854: [iter 45 : loss : 0.5608 = 0.0969 + 0.4576 + 0.0063, time: 21.912980]
2023-06-01 19:08:42.123: epoch 45:	0.02961255  	0.07967519  	0.06664545  
2023-06-01 19:09:03.852: [iter 46 : loss : 0.5573 = 0.0937 + 0.4572 + 0.0065, time: 21.724621]
2023-06-01 19:09:04.121: epoch 46:	0.02953111  	0.07934797  	0.06644321  
2023-06-01 19:09:25.825: [iter 47 : loss : 0.5539 = 0.0904 + 0.4569 + 0.0066, time: 21.701210]
2023-06-01 19:09:26.093: epoch 47:	0.02950889  	0.07922801  	0.06637530  
2023-06-01 19:09:47.854: [iter 48 : loss : 0.5515 = 0.0882 + 0.4566 + 0.0067, time: 21.756362]
2023-06-01 19:09:48.126: epoch 48:	0.02947929  	0.07894719  	0.06614217  
2023-06-01 19:10:09.857: [iter 49 : loss : 0.5488 = 0.0857 + 0.4562 + 0.0069, time: 21.726051]
2023-06-01 19:10:10.124: epoch 49:	0.02951630  	0.07874245  	0.06606264  
2023-06-01 19:10:31.812: [iter 50 : loss : 0.5466 = 0.0836 + 0.4560 + 0.0070, time: 21.684402]
2023-06-01 19:10:32.082: epoch 50:	0.02943486  	0.07819439  	0.06590957  
2023-06-01 19:10:53.849: [iter 51 : loss : 0.5435 = 0.0807 + 0.4557 + 0.0072, time: 21.763233]
2023-06-01 19:10:54.118: epoch 51:	0.02933122  	0.07787463  	0.06574129  
2023-06-01 19:11:15.986: [iter 52 : loss : 0.5412 = 0.0784 + 0.4555 + 0.0073, time: 21.863075]
2023-06-01 19:11:16.256: epoch 52:	0.02934602  	0.07780000  	0.06575745  
2023-06-01 19:11:38.200: [iter 53 : loss : 0.5402 = 0.0776 + 0.4552 + 0.0074, time: 21.940056]
2023-06-01 19:11:38.471: epoch 53:	0.02922016  	0.07781479  	0.06558398  
2023-06-01 19:12:00.576: [iter 54 : loss : 0.5381 = 0.0755 + 0.4550 + 0.0075, time: 22.099566]
2023-06-01 19:12:00.853: epoch 54:	0.02922016  	0.07730761  	0.06536324  
2023-06-01 19:12:22.794: [iter 55 : loss : 0.5359 = 0.0734 + 0.4548 + 0.0077, time: 21.937009]
2023-06-01 19:12:23.063: epoch 55:	0.02913132  	0.07716959  	0.06513995  
2023-06-01 19:12:44.752: [iter 56 : loss : 0.5336 = 0.0712 + 0.4546 + 0.0078, time: 21.684022]
2023-06-01 19:12:45.018: epoch 56:	0.02914613  	0.07705104  	0.06511378  
2023-06-01 19:13:06.965: [iter 57 : loss : 0.5319 = 0.0696 + 0.4544 + 0.0079, time: 21.942436]
2023-06-01 19:13:07.233: epoch 57:	0.02902027  	0.07674636  	0.06486506  
2023-06-01 19:13:28.963: [iter 58 : loss : 0.5306 = 0.0683 + 0.4543 + 0.0080, time: 21.726182]
2023-06-01 19:13:29.236: epoch 58:	0.02887961  	0.07615815  	0.06453311  
2023-06-01 19:13:51.154: [iter 59 : loss : 0.5288 = 0.0666 + 0.4541 + 0.0081, time: 21.914299]
2023-06-01 19:13:51.427: epoch 59:	0.02899066  	0.07608452  	0.06461845  
2023-06-01 19:14:13.165: [iter 60 : loss : 0.5279 = 0.0658 + 0.4539 + 0.0082, time: 21.734059]
2023-06-01 19:14:13.432: epoch 60:	0.02898325  	0.07583878  	0.06449279  
2023-06-01 19:14:35.353: [iter 61 : loss : 0.5259 = 0.0639 + 0.4537 + 0.0084, time: 21.917034]
2023-06-01 19:14:35.621: epoch 61:	0.02892403  	0.07576045  	0.06431372  
2023-06-01 19:14:35.621: Early stopping is trigger at epoch: 61
2023-06-01 19:14:35.621: best_result@epoch 36:

2023-06-01 19:14:35.621: 		0.0299      	0.0814      	0.0671      
2023-06-01 19:17:44.943: my pid: 11336
2023-06-01 19:17:44.943: model: model.general_recommender.SGL
2023-06-01 19:17:44.943: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 19:17:44.943: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 19:17:48.941: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 19:18:09.649: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.707958]
2023-06-01 19:18:09.921: epoch 1:	0.00134735  	0.00257531  	0.00218738  
2023-06-01 19:18:09.921: Find a better model.
2023-06-01 19:18:30.826: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.902707]
2023-06-01 19:18:31.119: epoch 2:	0.00165087  	0.00333653  	0.00281177  
2023-06-01 19:18:31.119: Find a better model.
2023-06-01 19:18:52.050: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.928675]
2023-06-01 19:18:52.343: epoch 3:	0.00208025  	0.00488818  	0.00377989  
2023-06-01 19:18:52.343: Find a better model.
2023-06-01 19:19:13.181: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.833798]
2023-06-01 19:19:13.477: epoch 4:	0.00228754  	0.00482927  	0.00426536  
2023-06-01 19:19:34.219: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.738385]
2023-06-01 19:19:34.521: epoch 5:	0.00313148  	0.00693002  	0.00532840  
2023-06-01 19:19:34.521: Find a better model.
2023-06-01 19:19:55.027: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.502049]
2023-06-01 19:19:55.319: epoch 6:	0.00345721  	0.00852081  	0.00692000  
2023-06-01 19:19:55.319: Find a better model.
2023-06-01 19:20:16.014: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.691202]
2023-06-01 19:20:16.306: epoch 7:	0.00393841  	0.00974718  	0.00777298  
2023-06-01 19:20:16.306: Find a better model.
2023-06-01 19:20:36.968: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.659051]
2023-06-01 19:20:37.259: epoch 8:	0.00464909  	0.01282058  	0.00977537  
2023-06-01 19:20:37.260: Find a better model.
2023-06-01 19:20:57.996: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.733712]
2023-06-01 19:20:58.284: epoch 9:	0.00537458  	0.01509071  	0.01143250  
2023-06-01 19:20:58.284: Find a better model.
2023-06-01 19:21:18.964: [iter 10 : loss : 1.1344 = 0.6908 + 0.4435 + 0.0000, time: 20.677153]
2023-06-01 19:21:19.250: epoch 10:	0.00586318  	0.01641391  	0.01295427  
2023-06-01 19:21:19.250: Find a better model.
2023-06-01 19:21:39.961: [iter 11 : loss : 1.1339 = 0.6896 + 0.4442 + 0.0000, time: 20.707412]
2023-06-01 19:21:40.253: epoch 11:	0.00681815  	0.02017409  	0.01583621  
2023-06-01 19:21:40.254: Find a better model.
2023-06-01 19:22:00.985: [iter 12 : loss : 1.1325 = 0.6877 + 0.4447 + 0.0000, time: 20.727560]
2023-06-01 19:22:01.270: epoch 12:	0.00847642  	0.02460138  	0.01999370  
2023-06-01 19:22:01.270: Find a better model.
2023-06-01 19:22:21.959: [iter 13 : loss : 1.1306 = 0.6850 + 0.4455 + 0.0001, time: 20.685370]
2023-06-01 19:22:22.243: epoch 13:	0.01048264  	0.03017343  	0.02494429  
2023-06-01 19:22:22.243: Find a better model.
2023-06-01 19:22:42.723: [iter 14 : loss : 1.1269 = 0.6806 + 0.4462 + 0.0001, time: 20.476578]
2023-06-01 19:22:43.010: epoch 14:	0.01351792  	0.03931411  	0.03269330  
2023-06-01 19:22:43.010: Find a better model.
2023-06-01 19:23:03.535: [iter 15 : loss : 1.1203 = 0.6731 + 0.4471 + 0.0001, time: 20.520144]
2023-06-01 19:23:03.815: epoch 15:	0.01681233  	0.04759725  	0.04057015  
2023-06-01 19:23:03.815: Find a better model.
2023-06-01 19:23:24.146: [iter 16 : loss : 1.1084 = 0.6600 + 0.4482 + 0.0002, time: 20.327260]
2023-06-01 19:23:24.434: epoch 16:	0.01999572  	0.05538581  	0.04771633  
2023-06-01 19:23:24.435: Find a better model.
2023-06-01 19:23:44.490: [iter 17 : loss : 1.0886 = 0.6388 + 0.4496 + 0.0003, time: 20.052019]
2023-06-01 19:23:44.769: epoch 17:	0.02304586  	0.06329232  	0.05407514  
2023-06-01 19:23:44.769: Find a better model.
2023-06-01 19:24:05.120: [iter 18 : loss : 1.0581 = 0.6064 + 0.4513 + 0.0004, time: 20.347457]
2023-06-01 19:24:05.413: epoch 18:	0.02539270  	0.06891911  	0.05883188  
2023-06-01 19:24:05.414: Find a better model.
2023-06-01 19:24:25.871: [iter 19 : loss : 1.0177 = 0.5634 + 0.4537 + 0.0006, time: 20.453511]
2023-06-01 19:24:26.149: epoch 19:	0.02692516  	0.07310502  	0.06203641  
2023-06-01 19:24:26.149: Find a better model.
2023-06-01 19:24:46.458: [iter 20 : loss : 0.9704 = 0.5129 + 0.4567 + 0.0008, time: 20.305386]
2023-06-01 19:24:46.739: epoch 20:	0.02806528  	0.07634729  	0.06441020  
2023-06-01 19:24:46.739: Find a better model.
2023-06-01 19:25:07.469: [iter 21 : loss : 0.9208 = 0.4598 + 0.4598 + 0.0011, time: 20.725512]
2023-06-01 19:25:07.748: epoch 21:	0.02847987  	0.07713468  	0.06523210  
2023-06-01 19:25:07.748: Find a better model.
2023-06-01 19:25:28.623: [iter 22 : loss : 0.8723 = 0.4082 + 0.4627 + 0.0014, time: 20.870169]
2023-06-01 19:25:28.906: epoch 22:	0.02898330  	0.07887740  	0.06590079  
2023-06-01 19:25:28.906: Find a better model.
2023-06-01 19:25:49.690: [iter 23 : loss : 0.8300 = 0.3632 + 0.4651 + 0.0017, time: 20.780534]
2023-06-01 19:25:49.967: epoch 23:	0.02927942  	0.07972817  	0.06625711  
2023-06-01 19:25:49.967: Find a better model.
2023-06-01 19:26:11.009: [iter 24 : loss : 0.7936 = 0.3248 + 0.4668 + 0.0020, time: 21.038683]
2023-06-01 19:26:11.286: epoch 24:	0.02936827  	0.08031665  	0.06667180  
2023-06-01 19:26:11.286: Find a better model.
2023-06-01 19:26:32.261: [iter 25 : loss : 0.7619 = 0.2922 + 0.4674 + 0.0023, time: 20.970802]
2023-06-01 19:26:32.541: epoch 25:	0.02949412  	0.08122411  	0.06700771  
2023-06-01 19:26:32.541: Find a better model.
2023-06-01 19:26:53.428: [iter 26 : loss : 0.7356 = 0.2653 + 0.4677 + 0.0026, time: 20.883808]
2023-06-01 19:26:53.702: epoch 26:	0.02956075  	0.08173667  	0.06701130  
2023-06-01 19:26:53.702: Find a better model.
2023-06-01 19:27:14.599: [iter 27 : loss : 0.7132 = 0.2426 + 0.4678 + 0.0028, time: 20.892795]
2023-06-01 19:27:14.870: epoch 27:	0.02961259  	0.08200427  	0.06721205  
2023-06-01 19:27:14.870: Find a better model.
2023-06-01 19:27:35.832: [iter 28 : loss : 0.6941 = 0.2237 + 0.4673 + 0.0031, time: 20.958285]
2023-06-01 19:27:36.101: epoch 28:	0.02961999  	0.08207283  	0.06716850  
2023-06-01 19:27:36.101: Find a better model.
2023-06-01 19:27:57.200: [iter 29 : loss : 0.6778 = 0.2076 + 0.4668 + 0.0033, time: 21.095009]
2023-06-01 19:27:57.475: epoch 29:	0.02975325  	0.08314206  	0.06773286  
2023-06-01 19:27:57.475: Find a better model.
2023-06-01 19:28:18.602: [iter 30 : loss : 0.6626 = 0.1930 + 0.4660 + 0.0036, time: 21.123404]
2023-06-01 19:28:18.870: epoch 30:	0.02977545  	0.08318076  	0.06774266  
2023-06-01 19:28:18.870: Find a better model.
2023-06-01 19:28:40.014: [iter 31 : loss : 0.6497 = 0.1804 + 0.4655 + 0.0038, time: 21.139172]
2023-06-01 19:28:40.274: epoch 31:	0.02975324  	0.08294056  	0.06760742  
2023-06-01 19:29:01.217: [iter 32 : loss : 0.6387 = 0.1699 + 0.4648 + 0.0040, time: 20.939607]
2023-06-01 19:29:01.488: epoch 32:	0.02981987  	0.08311085  	0.06792374  
2023-06-01 19:29:22.573: [iter 33 : loss : 0.6292 = 0.1609 + 0.4641 + 0.0042, time: 21.081568]
2023-06-01 19:29:22.841: epoch 33:	0.02994572  	0.08356769  	0.06815321  
2023-06-01 19:29:22.841: Find a better model.
2023-06-01 19:29:43.990: [iter 34 : loss : 0.6202 = 0.1522 + 0.4636 + 0.0044, time: 21.146006]
2023-06-01 19:29:44.260: epoch 34:	0.02996052  	0.08325059  	0.06830892  
2023-06-01 19:30:05.413: [iter 35 : loss : 0.6113 = 0.1439 + 0.4627 + 0.0046, time: 21.148590]
2023-06-01 19:30:05.683: epoch 35:	0.02993092  	0.08326499  	0.06833953  
2023-06-01 19:30:26.763: [iter 36 : loss : 0.6045 = 0.1374 + 0.4622 + 0.0048, time: 21.076150]
2023-06-01 19:30:27.033: epoch 36:	0.02972363  	0.08271303  	0.06806934  
2023-06-01 19:30:48.196: [iter 37 : loss : 0.5982 = 0.1314 + 0.4618 + 0.0050, time: 21.159224]
2023-06-01 19:30:48.469: epoch 37:	0.02975324  	0.08269302  	0.06805281  
2023-06-01 19:31:09.567: [iter 38 : loss : 0.5914 = 0.1252 + 0.4611 + 0.0052, time: 21.095405]
2023-06-01 19:31:09.831: epoch 38:	0.02968661  	0.08230256  	0.06793859  
2023-06-01 19:31:31.128: [iter 39 : loss : 0.5865 = 0.1206 + 0.4606 + 0.0054, time: 21.293566]
2023-06-01 19:31:31.410: epoch 39:	0.02983467  	0.08273529  	0.06815465  
2023-06-01 19:31:52.549: [iter 40 : loss : 0.5810 = 0.1153 + 0.4602 + 0.0055, time: 21.134934]
2023-06-01 19:31:52.815: epoch 40:	0.02967180  	0.08242203  	0.06802548  
2023-06-01 19:32:13.957: [iter 41 : loss : 0.5759 = 0.1106 + 0.4596 + 0.0057, time: 21.138093]
2023-06-01 19:32:14.225: epoch 41:	0.02959038  	0.08222007  	0.06804072  
2023-06-01 19:32:35.504: [iter 42 : loss : 0.5716 = 0.1065 + 0.4592 + 0.0059, time: 21.274175]
2023-06-01 19:32:35.769: epoch 42:	0.02965700  	0.08232137  	0.06817152  
2023-06-01 19:32:56.729: [iter 43 : loss : 0.5677 = 0.1028 + 0.4589 + 0.0060, time: 20.955551]
2023-06-01 19:32:56.993: epoch 43:	0.02956815  	0.08201638  	0.06809899  
2023-06-01 19:33:18.428: [iter 44 : loss : 0.5651 = 0.1004 + 0.4585 + 0.0062, time: 21.431585]
2023-06-01 19:33:18.699: epoch 44:	0.02964218  	0.08210380  	0.06815282  
2023-06-01 19:33:43.586: my pid: 5192
2023-06-01 19:33:43.586: model: model.general_recommender.SGL
2023-06-01 19:33:43.586: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 19:33:43.586: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 19:33:48.012: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 19:34:09.871: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.858621]
2023-06-01 19:34:10.165: epoch 1:	0.00116968  	0.00239628  	0.00200123  
2023-06-01 19:34:10.165: Find a better model.
2023-06-01 19:34:31.076: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.907702]
2023-06-01 19:34:31.375: epoch 2:	0.00203583  	0.00378073  	0.00307750  
2023-06-01 19:34:31.376: Find a better model.
2023-06-01 19:34:51.934: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.549341]
2023-06-01 19:34:52.229: epoch 3:	0.00197661  	0.00415934  	0.00384676  
2023-06-01 19:34:52.229: Find a better model.
2023-06-01 19:35:13.074: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.839854]
2023-06-01 19:35:13.369: epoch 4:	0.00228754  	0.00432986  	0.00357868  
2023-06-01 19:35:13.369: Find a better model.
2023-06-01 19:35:34.059: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.682156]
2023-06-01 19:35:34.353: epoch 5:	0.00282795  	0.00562049  	0.00482064  
2023-06-01 19:35:34.353: Find a better model.
2023-06-01 19:35:55.059: [iter 6 : loss : 1.1346 = 0.6925 + 0.4420 + 0.0000, time: 20.703079]
2023-06-01 19:35:55.352: epoch 6:	0.00316849  	0.00747804  	0.00596360  
2023-06-01 19:35:55.352: Find a better model.
2023-06-01 19:36:15.767: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.410839]
2023-06-01 19:36:16.061: epoch 7:	0.00359787  	0.00831012  	0.00693833  
2023-06-01 19:36:16.061: Find a better model.
2023-06-01 19:36:36.855: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.790035]
2023-06-01 19:36:37.147: epoch 8:	0.00433817  	0.01074012  	0.00873781  
2023-06-01 19:36:37.147: Find a better model.
2023-06-01 19:36:57.909: [iter 9 : loss : 1.1346 = 0.6915 + 0.4430 + 0.0000, time: 20.758702]
2023-06-01 19:36:58.200: epoch 9:	0.00490820  	0.01295017  	0.01028293  
2023-06-01 19:36:58.200: Find a better model.
2023-06-01 19:37:19.022: [iter 10 : loss : 1.1345 = 0.6908 + 0.4436 + 0.0000, time: 20.818226]
2023-06-01 19:37:19.311: epoch 10:	0.00527094  	0.01445080  	0.01142647  
2023-06-01 19:37:19.311: Find a better model.
2023-06-01 19:37:39.886: [iter 11 : loss : 1.1338 = 0.6897 + 0.4442 + 0.0000, time: 20.572471]
2023-06-01 19:37:40.177: epoch 11:	0.00621111  	0.01692450  	0.01366240  
2023-06-01 19:37:40.177: Find a better model.
2023-06-01 19:38:00.830: [iter 12 : loss : 1.1326 = 0.6878 + 0.4447 + 0.0000, time: 20.649074]
2023-06-01 19:38:01.117: epoch 12:	0.00803964  	0.02208019  	0.01793426  
2023-06-01 19:38:01.117: Find a better model.
2023-06-01 19:38:21.640: [iter 13 : loss : 1.1306 = 0.6851 + 0.4455 + 0.0001, time: 20.518464]
2023-06-01 19:38:21.918: epoch 13:	0.01031978  	0.02902015  	0.02433409  
2023-06-01 19:38:21.918: Find a better model.
2023-06-01 19:38:42.403: [iter 14 : loss : 1.1270 = 0.6807 + 0.4462 + 0.0001, time: 20.481114]
2023-06-01 19:38:42.681: epoch 14:	0.01311074  	0.03625536  	0.03086201  
2023-06-01 19:38:42.681: Find a better model.
2023-06-01 19:39:03.194: [iter 15 : loss : 1.1206 = 0.6733 + 0.4472 + 0.0001, time: 20.509130]
2023-06-01 19:39:03.480: epoch 15:	0.01655323  	0.04466304  	0.03842710  
2023-06-01 19:39:03.480: Find a better model.
2023-06-01 19:39:23.987: [iter 16 : loss : 1.1087 = 0.6604 + 0.4481 + 0.0002, time: 20.503054]
2023-06-01 19:39:24.269: epoch 16:	0.01990687  	0.05302035  	0.04543738  
2023-06-01 19:39:24.269: Find a better model.
2023-06-01 19:39:44.761: [iter 17 : loss : 1.0892 = 0.6395 + 0.4495 + 0.0003, time: 20.488213]
2023-06-01 19:39:45.042: epoch 17:	0.02274232  	0.05985986  	0.05144486  
2023-06-01 19:39:45.042: Find a better model.
2023-06-01 19:40:05.408: [iter 18 : loss : 1.0589 = 0.6071 + 0.4513 + 0.0004, time: 20.361072]
2023-06-01 19:40:05.684: epoch 18:	0.02507437  	0.06564572  	0.05622213  
2023-06-01 19:40:05.684: Find a better model.
2023-06-01 19:40:26.199: [iter 19 : loss : 1.0186 = 0.5643 + 0.4537 + 0.0006, time: 20.509628]
2023-06-01 19:40:26.483: epoch 19:	0.02684373  	0.07073727  	0.05978994  
2023-06-01 19:40:26.483: Find a better model.
2023-06-01 19:40:46.964: [iter 20 : loss : 0.9709 = 0.5134 + 0.4567 + 0.0008, time: 20.476057]
2023-06-01 19:40:47.243: epoch 20:	0.02788019  	0.07418031  	0.06199951  
2023-06-01 19:40:47.243: Find a better model.
2023-06-01 19:41:08.321: [iter 21 : loss : 0.9212 = 0.4600 + 0.4600 + 0.0011, time: 21.074146]
2023-06-01 19:41:08.600: epoch 21:	0.02854647  	0.07597405  	0.06304108  
2023-06-01 19:41:08.600: Find a better model.
2023-06-01 19:41:29.686: [iter 22 : loss : 0.8729 = 0.4084 + 0.4631 + 0.0014, time: 21.082097]
2023-06-01 19:41:29.961: epoch 22:	0.02867234  	0.07633143  	0.06349993  
2023-06-01 19:41:29.961: Find a better model.
2023-06-01 19:41:50.927: [iter 23 : loss : 0.8301 = 0.3631 + 0.4654 + 0.0017, time: 20.961472]
2023-06-01 19:41:51.205: epoch 23:	0.02888704  	0.07733849  	0.06402681  
2023-06-01 19:41:51.205: Find a better model.
2023-06-01 19:42:12.361: [iter 24 : loss : 0.7935 = 0.3244 + 0.4671 + 0.0020, time: 21.152584]
2023-06-01 19:42:12.635: epoch 24:	0.02904990  	0.07804971  	0.06428164  
2023-06-01 19:42:12.635: Find a better model.
2023-06-01 19:42:33.727: [iter 25 : loss : 0.7620 = 0.2920 + 0.4677 + 0.0023, time: 21.089149]
2023-06-01 19:42:34.001: epoch 25:	0.02926461  	0.07893308  	0.06474809  
2023-06-01 19:42:34.001: Find a better model.
2023-06-01 19:42:54.936: [iter 26 : loss : 0.7360 = 0.2655 + 0.4679 + 0.0026, time: 20.931574]
2023-06-01 19:42:55.206: epoch 26:	0.02941267  	0.07918899  	0.06497935  
2023-06-01 19:42:55.206: Find a better model.
2023-06-01 19:43:16.337: [iter 27 : loss : 0.7131 = 0.2424 + 0.4678 + 0.0028, time: 21.127679]
2023-06-01 19:43:16.609: epoch 27:	0.02939045  	0.07915831  	0.06508155  
2023-06-01 19:43:37.487: [iter 28 : loss : 0.6940 = 0.2236 + 0.4673 + 0.0031, time: 20.874529]
2023-06-01 19:43:37.756: epoch 28:	0.02954593  	0.07974433  	0.06547707  
2023-06-01 19:43:37.756: Find a better model.
2023-06-01 19:43:58.701: [iter 29 : loss : 0.6776 = 0.2074 + 0.4669 + 0.0033, time: 20.942071]
2023-06-01 19:43:58.972: epoch 29:	0.02966437  	0.08031525  	0.06579670  
2023-06-01 19:43:58.972: Find a better model.
2023-06-01 19:44:20.080: [iter 30 : loss : 0.6628 = 0.1931 + 0.4662 + 0.0036, time: 21.104701]
2023-06-01 19:44:20.349: epoch 30:	0.02974580  	0.08018484  	0.06588189  
2023-06-01 19:44:41.270: [iter 31 : loss : 0.6498 = 0.1806 + 0.4655 + 0.0038, time: 20.917111]
2023-06-01 19:44:41.545: epoch 31:	0.02984204  	0.08084680  	0.06616513  
2023-06-01 19:44:41.545: Find a better model.
2023-06-01 19:45:02.431: [iter 32 : loss : 0.6386 = 0.1698 + 0.4648 + 0.0040, time: 20.882075]
2023-06-01 19:45:02.701: epoch 32:	0.02991607  	0.08113867  	0.06629002  
2023-06-01 19:45:02.702: Find a better model.
2023-06-01 19:45:23.642: [iter 33 : loss : 0.6293 = 0.1609 + 0.4642 + 0.0042, time: 20.937044]
2023-06-01 19:45:23.911: epoch 33:	0.02986424  	0.08062430  	0.06614710  
2023-06-01 19:45:45.021: [iter 34 : loss : 0.6199 = 0.1521 + 0.4634 + 0.0044, time: 21.106088]
2023-06-01 19:45:45.289: epoch 34:	0.02992347  	0.08099993  	0.06632572  
2023-06-01 19:46:06.286: [iter 35 : loss : 0.6114 = 0.1439 + 0.4628 + 0.0046, time: 20.992434]
2023-06-01 19:46:06.560: epoch 35:	0.02985684  	0.08072634  	0.06629119  
2023-06-01 19:46:27.827: [iter 36 : loss : 0.6047 = 0.1377 + 0.4622 + 0.0048, time: 21.264349]
2023-06-01 19:46:28.099: epoch 36:	0.02991607  	0.08086584  	0.06641833  
2023-06-01 19:46:49.422: [iter 37 : loss : 0.5982 = 0.1315 + 0.4617 + 0.0050, time: 21.318559]
2023-06-01 19:46:49.689: epoch 37:	0.02990868  	0.08069285  	0.06632822  
2023-06-01 19:47:10.801: [iter 38 : loss : 0.5917 = 0.1254 + 0.4612 + 0.0052, time: 21.108741]
2023-06-01 19:47:11.069: epoch 38:	0.02994569  	0.08086982  	0.06642866  
2023-06-01 19:47:32.371: [iter 39 : loss : 0.5864 = 0.1205 + 0.4606 + 0.0054, time: 21.298203]
2023-06-01 19:47:32.637: epoch 39:	0.02985686  	0.08034144  	0.06637821  
2023-06-01 19:47:53.806: [iter 40 : loss : 0.5809 = 0.1153 + 0.4601 + 0.0055, time: 21.165033]
2023-06-01 19:47:54.076: epoch 40:	0.02981243  	0.07993168  	0.06631118  
2023-06-01 19:48:15.365: [iter 41 : loss : 0.5761 = 0.1108 + 0.4596 + 0.0057, time: 21.286610]
2023-06-01 19:48:15.637: epoch 41:	0.02975320  	0.07969567  	0.06639621  
2023-06-01 19:48:36.815: [iter 42 : loss : 0.5716 = 0.1065 + 0.4593 + 0.0059, time: 21.174174]
2023-06-01 19:48:37.082: epoch 42:	0.02975319  	0.07936738  	0.06629434  
2023-06-01 19:48:58.441: [iter 43 : loss : 0.5677 = 0.1029 + 0.4588 + 0.0060, time: 21.354351]
2023-06-01 19:48:58.708: epoch 43:	0.02966436  	0.07940339  	0.06623625  
2023-06-01 19:49:19.783: [iter 44 : loss : 0.5646 = 0.0999 + 0.4585 + 0.0062, time: 21.071948]
2023-06-01 19:49:20.048: epoch 44:	0.02970137  	0.07895128  	0.06614906  
2023-06-01 19:49:41.359: [iter 45 : loss : 0.5607 = 0.0964 + 0.4580 + 0.0063, time: 21.308051]
2023-06-01 19:49:41.631: epoch 45:	0.02967916  	0.07909667  	0.06615468  
2023-06-01 19:50:02.945: [iter 46 : loss : 0.5579 = 0.0937 + 0.4577 + 0.0065, time: 21.311211]
2023-06-01 19:50:03.215: epoch 46:	0.02961252  	0.07887280  	0.06589989  
2023-06-01 19:50:24.541: [iter 47 : loss : 0.5542 = 0.0902 + 0.4574 + 0.0066, time: 21.322070]
2023-06-01 19:50:24.806: epoch 47:	0.02957551  	0.07866564  	0.06582323  
2023-06-01 19:50:46.583: [iter 48 : loss : 0.5517 = 0.0880 + 0.4570 + 0.0067, time: 21.773066]
2023-06-01 19:50:47.105: epoch 48:	0.02953849  	0.07879864  	0.06585978  
2023-06-01 19:51:06.786: my pid: 2464
2023-06-01 19:51:06.786: model: model.general_recommender.SGL
2023-06-01 19:51:06.786: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 19:51:06.786: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 19:51:11.062: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 19:51:33.162: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.100497]
2023-06-01 19:51:33.491: epoch 1:	0.00148060  	0.00328196  	0.00256696  
2023-06-01 19:51:33.491: Find a better model.
2023-06-01 19:51:54.528: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.032459]
2023-06-01 19:51:54.820: epoch 2:	0.00157684  	0.00337158  	0.00271587  
2023-06-01 19:51:54.820: Find a better model.
2023-06-01 19:52:15.539: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.716392]
2023-06-01 19:52:15.839: epoch 3:	0.00220610  	0.00506938  	0.00370112  
2023-06-01 19:52:15.839: Find a better model.
2023-06-01 19:52:36.723: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.880297]
2023-06-01 19:52:37.016: epoch 4:	0.00247261  	0.00541289  	0.00402794  
2023-06-01 19:52:37.016: Find a better model.
2023-06-01 19:52:57.720: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.699796]
2023-06-01 19:52:58.013: epoch 5:	0.00267989  	0.00612855  	0.00466047  
2023-06-01 19:52:58.013: Find a better model.
2023-06-01 19:53:18.744: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.728277]
2023-06-01 19:53:19.037: epoch 6:	0.00310927  	0.00760012  	0.00584339  
2023-06-01 19:53:19.037: Find a better model.
2023-06-01 19:53:39.568: [iter 7 : loss : 1.1347 = 0.6923 + 0.4423 + 0.0000, time: 20.527627]
2023-06-01 19:53:39.860: epoch 7:	0.00393101  	0.00991382  	0.00839197  
2023-06-01 19:53:39.860: Find a better model.
2023-06-01 19:54:00.489: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.626050]
2023-06-01 19:54:00.759: epoch 8:	0.00449363  	0.01201707  	0.00928528  
2023-06-01 19:54:00.759: Find a better model.
2023-06-01 19:54:21.346: [iter 9 : loss : 1.1346 = 0.6915 + 0.4431 + 0.0000, time: 20.583258]
2023-06-01 19:54:21.638: epoch 9:	0.00492300  	0.01320091  	0.01045080  
2023-06-01 19:54:21.638: Find a better model.
2023-06-01 19:54:42.281: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.639489]
2023-06-01 19:54:42.567: epoch 10:	0.00572992  	0.01514250  	0.01219411  
2023-06-01 19:54:42.567: Find a better model.
2023-06-01 19:55:03.247: [iter 11 : loss : 1.1337 = 0.6896 + 0.4441 + 0.0000, time: 20.677392]
2023-06-01 19:55:03.533: epoch 11:	0.00644060  	0.01732255  	0.01383494  
2023-06-01 19:55:03.533: Find a better model.
2023-06-01 19:55:24.060: [iter 12 : loss : 1.1326 = 0.6877 + 0.4448 + 0.0000, time: 20.524064]
2023-06-01 19:55:24.347: epoch 12:	0.00758806  	0.02114508  	0.01719967  
2023-06-01 19:55:24.348: Find a better model.
2023-06-01 19:55:44.866: [iter 13 : loss : 1.1306 = 0.6851 + 0.4455 + 0.0001, time: 20.515089]
2023-06-01 19:55:45.158: epoch 13:	0.01019393  	0.02835114  	0.02323133  
2023-06-01 19:55:45.158: Find a better model.
2023-06-01 19:56:05.699: [iter 14 : loss : 1.1270 = 0.6807 + 0.4462 + 0.0001, time: 20.537998]
2023-06-01 19:56:05.976: epoch 14:	0.01282943  	0.03607630  	0.02987723  
2023-06-01 19:56:05.976: Find a better model.
2023-06-01 19:56:26.467: [iter 15 : loss : 1.1204 = 0.6732 + 0.4471 + 0.0001, time: 20.485285]
2023-06-01 19:56:26.747: epoch 15:	0.01639035  	0.04481193  	0.03811975  
2023-06-01 19:56:26.747: Find a better model.
2023-06-01 19:56:47.209: [iter 16 : loss : 1.1086 = 0.6603 + 0.4482 + 0.0002, time: 20.458648]
2023-06-01 19:56:47.498: epoch 16:	0.01974401  	0.05341693  	0.04530903  
2023-06-01 19:56:47.498: Find a better model.
2023-06-01 19:57:08.049: [iter 17 : loss : 1.0889 = 0.6392 + 0.4495 + 0.0003, time: 20.547100]
2023-06-01 19:57:08.329: epoch 17:	0.02308288  	0.06203480  	0.05221568  
2023-06-01 19:57:08.329: Find a better model.
2023-06-01 19:57:28.841: [iter 18 : loss : 1.0585 = 0.6069 + 0.4513 + 0.0004, time: 20.508276]
2023-06-01 19:57:29.121: epoch 18:	0.02540751  	0.06774759  	0.05667423  
2023-06-01 19:57:29.121: Find a better model.
2023-06-01 19:57:49.638: [iter 19 : loss : 1.0184 = 0.5641 + 0.4537 + 0.0006, time: 20.512285]
2023-06-01 19:57:49.916: epoch 19:	0.02710286  	0.07226000  	0.05993298  
2023-06-01 19:57:49.916: Find a better model.
2023-06-01 19:58:10.232: [iter 20 : loss : 0.9708 = 0.5133 + 0.4567 + 0.0008, time: 20.312065]
2023-06-01 19:58:10.517: epoch 20:	0.02823555  	0.07537162  	0.06217850  
2023-06-01 19:58:10.517: Find a better model.
2023-06-01 19:58:31.824: [iter 21 : loss : 0.9210 = 0.4598 + 0.4600 + 0.0011, time: 21.303258]
2023-06-01 19:58:32.105: epoch 21:	0.02897587  	0.07796406  	0.06401135  
2023-06-01 19:58:32.105: Find a better model.
2023-06-01 19:58:53.235: [iter 22 : loss : 0.8728 = 0.4083 + 0.4631 + 0.0014, time: 21.124427]
2023-06-01 19:58:53.520: epoch 22:	0.02925722  	0.07930885  	0.06462627  
2023-06-01 19:58:53.520: Find a better model.
2023-06-01 19:59:14.599: [iter 23 : loss : 0.8301 = 0.3630 + 0.4654 + 0.0017, time: 21.076360]
2023-06-01 19:59:14.871: epoch 23:	0.02934604  	0.08028406  	0.06505685  
2023-06-01 19:59:14.871: Find a better model.
2023-06-01 19:59:35.771: [iter 24 : loss : 0.7937 = 0.3246 + 0.4671 + 0.0020, time: 20.896227]
2023-06-01 19:59:36.041: epoch 24:	0.02941268  	0.08070376  	0.06532776  
2023-06-01 19:59:36.041: Find a better model.
2023-06-01 19:59:57.201: [iter 25 : loss : 0.7620 = 0.2920 + 0.4677 + 0.0023, time: 21.155349]
2023-06-01 19:59:57.484: epoch 25:	0.02958294  	0.08176306  	0.06572708  
2023-06-01 19:59:57.484: Find a better model.
2023-06-01 20:00:18.584: [iter 26 : loss : 0.7357 = 0.2652 + 0.4679 + 0.0026, time: 21.096267]
2023-06-01 20:00:18.853: epoch 26:	0.02979765  	0.08230170  	0.06610270  
2023-06-01 20:00:18.853: Find a better model.
2023-06-01 20:00:39.800: [iter 27 : loss : 0.7132 = 0.2425 + 0.4679 + 0.0028, time: 20.943374]
2023-06-01 20:00:40.070: epoch 27:	0.02993091  	0.08263037  	0.06638474  
2023-06-01 20:00:40.070: Find a better model.
2023-06-01 20:01:01.171: [iter 28 : loss : 0.6940 = 0.2234 + 0.4675 + 0.0031, time: 21.097163]
2023-06-01 20:01:01.452: epoch 28:	0.03005675  	0.08319973  	0.06675996  
2023-06-01 20:01:01.452: Find a better model.
2023-06-01 20:01:22.331: [iter 29 : loss : 0.6779 = 0.2076 + 0.4669 + 0.0033, time: 20.875154]
2023-06-01 20:01:22.602: epoch 29:	0.03033068  	0.08425533  	0.06722136  
2023-06-01 20:01:22.602: Find a better model.
2023-06-01 20:01:43.705: [iter 30 : loss : 0.6626 = 0.1927 + 0.4663 + 0.0036, time: 21.100537]
2023-06-01 20:01:43.978: epoch 30:	0.03029366  	0.08381579  	0.06722472  
2023-06-01 20:02:05.306: [iter 31 : loss : 0.6499 = 0.1805 + 0.4656 + 0.0038, time: 21.322276]
2023-06-01 20:02:05.576: epoch 31:	0.03035290  	0.08388101  	0.06736956  
2023-06-01 20:02:26.773: [iter 32 : loss : 0.6390 = 0.1701 + 0.4649 + 0.0040, time: 21.194514]
2023-06-01 20:02:27.045: epoch 32:	0.03038991  	0.08393006  	0.06749434  
2023-06-01 20:02:48.332: [iter 33 : loss : 0.6291 = 0.1606 + 0.4642 + 0.0042, time: 21.283287]
2023-06-01 20:02:48.604: epoch 33:	0.03042692  	0.08337945  	0.06737040  
2023-06-01 20:03:09.870: [iter 34 : loss : 0.6203 = 0.1522 + 0.4636 + 0.0044, time: 21.262590]
2023-06-01 20:03:10.138: epoch 34:	0.03033066  	0.08319315  	0.06739348  
2023-06-01 20:03:31.340: [iter 35 : loss : 0.6115 = 0.1441 + 0.4628 + 0.0046, time: 21.199096]
2023-06-01 20:03:31.609: epoch 35:	0.03044911  	0.08355844  	0.06769749  
2023-06-01 20:03:52.896: [iter 36 : loss : 0.6046 = 0.1374 + 0.4623 + 0.0048, time: 21.282066]
2023-06-01 20:03:53.163: epoch 36:	0.03029364  	0.08315699  	0.06763792  
2023-06-01 20:04:14.464: [iter 37 : loss : 0.5980 = 0.1313 + 0.4617 + 0.0050, time: 21.295649]
2023-06-01 20:04:14.729: epoch 37:	0.03036027  	0.08339389  	0.06780846  
2023-06-01 20:04:35.833: [iter 38 : loss : 0.5919 = 0.1255 + 0.4612 + 0.0052, time: 21.099311]
2023-06-01 20:04:36.102: epoch 38:	0.03043430  	0.08387485  	0.06773601  
2023-06-01 20:04:57.437: [iter 39 : loss : 0.5866 = 0.1206 + 0.4606 + 0.0054, time: 21.329482]
2023-06-01 20:04:57.705: epoch 39:	0.03067861  	0.08426234  	0.06801289  
2023-06-01 20:04:57.705: Find a better model.
2023-06-01 20:05:19.077: [iter 40 : loss : 0.5810 = 0.1154 + 0.4601 + 0.0055, time: 21.368583]
2023-06-01 20:05:19.348: epoch 40:	0.03061938  	0.08382635  	0.06776959  
2023-06-01 20:05:40.658: [iter 41 : loss : 0.5760 = 0.1108 + 0.4596 + 0.0057, time: 21.306144]
2023-06-01 20:05:40.925: epoch 41:	0.03056016  	0.08366798  	0.06776966  
2023-06-01 20:06:02.058: [iter 42 : loss : 0.5718 = 0.1067 + 0.4593 + 0.0059, time: 21.128791]
2023-06-01 20:06:02.328: epoch 42:	0.03053055  	0.08359867  	0.06774958  
2023-06-01 20:06:23.622: [iter 43 : loss : 0.5675 = 0.1026 + 0.4589 + 0.0060, time: 21.289351]
2023-06-01 20:06:23.890: epoch 43:	0.03047873  	0.08342519  	0.06758035  
2023-06-01 20:06:45.054: [iter 44 : loss : 0.5648 = 0.1002 + 0.4585 + 0.0062, time: 21.158594]
2023-06-01 20:06:45.318: epoch 44:	0.03043431  	0.08313766  	0.06751012  
2023-06-01 20:07:06.622: [iter 45 : loss : 0.5605 = 0.0961 + 0.4581 + 0.0063, time: 21.299011]
2023-06-01 20:07:06.890: epoch 45:	0.03044172  	0.08317306  	0.06750910  
2023-06-01 20:07:28.065: [iter 46 : loss : 0.5575 = 0.0933 + 0.4577 + 0.0065, time: 21.171075]
2023-06-01 20:07:28.334: epoch 46:	0.03042691  	0.08310439  	0.06741398  
2023-06-01 20:07:49.413: [iter 47 : loss : 0.5544 = 0.0904 + 0.4574 + 0.0066, time: 21.074076]
2023-06-01 20:07:49.683: epoch 47:	0.03045652  	0.08293183  	0.06725708  
2023-06-01 20:08:10.787: [iter 48 : loss : 0.5515 = 0.0878 + 0.4569 + 0.0068, time: 21.101238]
2023-06-01 20:08:11.055: epoch 48:	0.03034547  	0.08267767  	0.06709760  
2023-06-01 20:08:32.228: [iter 49 : loss : 0.5488 = 0.0852 + 0.4567 + 0.0069, time: 21.169019]
2023-06-01 20:08:32.508: epoch 49:	0.03025663  	0.08256185  	0.06692149  
2023-06-01 20:08:53.596: [iter 50 : loss : 0.5466 = 0.0832 + 0.4564 + 0.0070, time: 21.084369]
2023-06-01 20:08:53.867: epoch 50:	0.03024182  	0.08219738  	0.06672595  
2023-06-01 20:09:14.999: [iter 51 : loss : 0.5439 = 0.0805 + 0.4562 + 0.0072, time: 21.127020]
2023-06-01 20:09:15.266: epoch 51:	0.03025663  	0.08177477  	0.06666762  
2023-06-01 20:09:36.385: [iter 52 : loss : 0.5413 = 0.0782 + 0.4559 + 0.0073, time: 21.116187]
2023-06-01 20:09:36.655: epoch 52:	0.03022700  	0.08166128  	0.06654217  
2023-06-01 20:09:57.599: [iter 53 : loss : 0.5404 = 0.0772 + 0.4558 + 0.0074, time: 20.939055]
2023-06-01 20:09:57.867: epoch 53:	0.03024182  	0.08179304  	0.06681313  
2023-06-01 20:10:18.975: [iter 54 : loss : 0.5380 = 0.0750 + 0.4555 + 0.0076, time: 21.104803]
2023-06-01 20:10:19.242: epoch 54:	0.03007895  	0.08134425  	0.06650148  
2023-06-01 20:10:40.170: [iter 55 : loss : 0.5360 = 0.0731 + 0.4552 + 0.0077, time: 20.923427]
2023-06-01 20:10:40.449: epoch 55:	0.03006415  	0.08107731  	0.06644778  
2023-06-01 20:11:01.327: [iter 56 : loss : 0.5338 = 0.0711 + 0.4550 + 0.0078, time: 20.873391]
2023-06-01 20:11:01.602: epoch 56:	0.03003453  	0.08087631  	0.06625362  
2023-06-01 20:11:22.726: [iter 57 : loss : 0.5320 = 0.0693 + 0.4548 + 0.0079, time: 21.120240]
2023-06-01 20:11:22.993: epoch 57:	0.02995310  	0.08073720  	0.06611963  
2023-06-01 20:11:44.159: [iter 58 : loss : 0.5308 = 0.0681 + 0.4546 + 0.0080, time: 21.162481]
2023-06-01 20:11:44.442: epoch 58:	0.02977542  	0.08022979  	0.06590024  
2023-06-01 20:12:05.366: [iter 59 : loss : 0.5289 = 0.0663 + 0.4544 + 0.0082, time: 20.919483]
2023-06-01 20:12:05.635: epoch 59:	0.02982725  	0.08038104  	0.06585246  
2023-06-01 20:12:26.699: [iter 60 : loss : 0.5280 = 0.0655 + 0.4543 + 0.0083, time: 21.060118]
2023-06-01 20:12:26.959: epoch 60:	0.02970138  	0.08015095  	0.06574170  
2023-06-01 20:12:48.287: [iter 61 : loss : 0.5267 = 0.0642 + 0.4541 + 0.0084, time: 21.324128]
2023-06-01 20:12:48.555: epoch 61:	0.02962735  	0.07990945  	0.06565611  
2023-06-01 20:13:09.504: [iter 62 : loss : 0.5253 = 0.0628 + 0.4540 + 0.0085, time: 20.945175]
2023-06-01 20:13:09.780: epoch 62:	0.02956073  	0.07979025  	0.06537651  
2023-06-01 20:13:30.904: [iter 63 : loss : 0.5243 = 0.0618 + 0.4538 + 0.0086, time: 21.118069]
2023-06-01 20:13:31.170: epoch 63:	0.02953111  	0.07955848  	0.06528354  
2023-06-01 20:13:52.312: [iter 64 : loss : 0.5228 = 0.0604 + 0.4537 + 0.0087, time: 21.139316]
2023-06-01 20:13:52.587: epoch 64:	0.02937564  	0.07906350  	0.06508008  
2023-06-01 20:13:52.587: Early stopping is trigger at epoch: 64
2023-06-01 20:13:52.587: best_result@epoch 39:

2023-06-01 20:13:52.587: 		0.0307      	0.0843      	0.0680      
2023-06-01 20:45:46.001: my pid: 2392
2023-06-01 20:45:46.002: model: model.general_recommender.SGL
2023-06-01 20:45:46.002: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 20:45:46.002: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 20:45:50.309: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 20:46:11.045: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.735261]
2023-06-01 20:46:11.331: epoch 1:	0.00123630  	0.00223099  	0.00183067  
2023-06-01 20:46:11.331: Find a better model.
2023-06-01 20:46:32.214: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.880078]
2023-06-01 20:46:32.510: epoch 2:	0.00176192  	0.00308313  	0.00265128  
2023-06-01 20:46:32.510: Find a better model.
2023-06-01 20:46:53.392: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.879018]
2023-06-01 20:46:53.691: epoch 3:	0.00208765  	0.00401357  	0.00308850  
2023-06-01 20:46:53.691: Find a better model.
2023-06-01 20:47:14.587: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.890857]
2023-06-01 20:47:14.887: epoch 4:	0.00216909  	0.00490258  	0.00379989  
2023-06-01 20:47:14.887: Find a better model.
2023-06-01 20:47:35.813: [iter 5 : loss : 1.1344 = 0.6927 + 0.4416 + 0.0000, time: 20.920738]
2023-06-01 20:47:36.111: epoch 5:	0.00256885  	0.00558320  	0.00451249  
2023-06-01 20:47:36.111: Find a better model.
2023-06-01 20:47:56.969: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.854085]
2023-06-01 20:47:57.267: epoch 6:	0.00309446  	0.00711338  	0.00547966  
2023-06-01 20:47:57.267: Find a better model.
2023-06-01 20:48:18.381: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.110676]
2023-06-01 20:48:18.674: epoch 7:	0.00362748  	0.00833921  	0.00699639  
2023-06-01 20:48:18.674: Find a better model.
2023-06-01 20:48:39.828: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 21.149337]
2023-06-01 20:48:40.126: epoch 8:	0.00439739  	0.01126788  	0.00874842  
2023-06-01 20:48:40.126: Find a better model.
2023-06-01 20:49:01.004: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 20.873493]
2023-06-01 20:49:01.294: epoch 9:	0.00518210  	0.01356109  	0.01072247  
2023-06-01 20:49:01.294: Find a better model.
2023-06-01 20:49:22.395: [iter 10 : loss : 1.1345 = 0.6910 + 0.4435 + 0.0000, time: 21.097000]
2023-06-01 20:49:22.689: epoch 10:	0.00633696  	0.01757422  	0.01383409  
2023-06-01 20:49:22.689: Find a better model.
2023-06-01 20:49:43.578: [iter 11 : loss : 1.1340 = 0.6900 + 0.4440 + 0.0000, time: 20.885188]
2023-06-01 20:49:43.871: epoch 11:	0.00745481  	0.02076773  	0.01687998  
2023-06-01 20:49:43.871: Find a better model.
2023-06-01 20:50:04.750: [iter 12 : loss : 1.1328 = 0.6882 + 0.4446 + 0.0000, time: 20.875097]
2023-06-01 20:50:05.044: epoch 12:	0.00845421  	0.02367060  	0.01926976  
2023-06-01 20:50:05.045: Find a better model.
2023-06-01 20:50:25.725: [iter 13 : loss : 1.1309 = 0.6855 + 0.4454 + 0.0000, time: 20.676752]
2023-06-01 20:50:26.009: epoch 13:	0.01000884  	0.02787895  	0.02407049  
2023-06-01 20:50:26.009: Find a better model.
2023-06-01 20:50:46.729: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 20.717666]
2023-06-01 20:50:47.015: epoch 14:	0.01352533  	0.03688197  	0.03223373  
2023-06-01 20:50:47.015: Find a better model.
2023-06-01 20:51:07.548: [iter 15 : loss : 1.1213 = 0.6741 + 0.4470 + 0.0001, time: 20.529127]
2023-06-01 20:51:07.836: epoch 15:	0.01645697  	0.04482881  	0.03904617  
2023-06-01 20:51:07.836: Find a better model.
2023-06-01 20:51:28.340: [iter 16 : loss : 1.1100 = 0.6617 + 0.4481 + 0.0002, time: 20.498564]
2023-06-01 20:51:28.623: epoch 16:	0.01974399  	0.05310908  	0.04633253  
2023-06-01 20:51:28.624: Find a better model.
2023-06-01 20:51:49.126: [iter 17 : loss : 1.0911 = 0.6415 + 0.4493 + 0.0002, time: 20.498735]
2023-06-01 20:51:49.410: epoch 17:	0.02315688  	0.06253017  	0.05363957  
2023-06-01 20:51:49.410: Find a better model.
2023-06-01 20:52:09.985: [iter 18 : loss : 1.0617 = 0.6102 + 0.4510 + 0.0004, time: 20.571548]
2023-06-01 20:52:10.285: epoch 18:	0.02537047  	0.06818829  	0.05836101  
2023-06-01 20:52:10.285: Find a better model.
2023-06-01 20:52:30.903: [iter 19 : loss : 1.0220 = 0.5680 + 0.4534 + 0.0006, time: 20.612710]
2023-06-01 20:52:31.201: epoch 19:	0.02703620  	0.07315104  	0.06175194  
2023-06-01 20:52:31.201: Find a better model.
2023-06-01 20:52:51.911: [iter 20 : loss : 0.9751 = 0.5179 + 0.4564 + 0.0008, time: 20.705106]
2023-06-01 20:52:52.205: epoch 20:	0.02816149  	0.07633189  	0.06377687  
2023-06-01 20:52:52.205: Find a better model.
2023-06-01 20:53:13.259: [iter 21 : loss : 0.9253 = 0.4645 + 0.4597 + 0.0011, time: 21.050093]
2023-06-01 20:53:13.542: epoch 21:	0.02862052  	0.07744853  	0.06480996  
2023-06-01 20:53:13.542: Find a better model.
2023-06-01 20:53:34.696: [iter 22 : loss : 0.8765 = 0.4124 + 0.4627 + 0.0014, time: 21.150048]
2023-06-01 20:53:34.977: epoch 22:	0.02890184  	0.07893595  	0.06534135  
2023-06-01 20:53:34.977: Find a better model.
2023-06-01 20:53:56.069: [iter 23 : loss : 0.8336 = 0.3668 + 0.4651 + 0.0017, time: 21.088013]
2023-06-01 20:53:56.352: epoch 23:	0.02903509  	0.07978220  	0.06593487  
2023-06-01 20:53:56.352: Find a better model.
2023-06-01 20:54:17.713: [iter 24 : loss : 0.7963 = 0.3276 + 0.4667 + 0.0020, time: 21.358116]
2023-06-01 20:54:17.995: epoch 24:	0.02927198  	0.08067251  	0.06641811  
2023-06-01 20:54:17.996: Find a better model.
2023-06-01 20:54:39.272: [iter 25 : loss : 0.7644 = 0.2947 + 0.4674 + 0.0023, time: 21.272329]
2023-06-01 20:54:39.556: epoch 25:	0.02944969  	0.08121373  	0.06657709  
2023-06-01 20:54:39.556: Find a better model.
2023-06-01 20:55:00.667: [iter 26 : loss : 0.7379 = 0.2676 + 0.4677 + 0.0026, time: 21.108419]
2023-06-01 20:55:00.944: epoch 26:	0.02951632  	0.08136182  	0.06673982  
2023-06-01 20:55:00.944: Find a better model.
2023-06-01 20:55:22.215: [iter 27 : loss : 0.7150 = 0.2446 + 0.4676 + 0.0028, time: 21.266765]
2023-06-01 20:55:22.493: epoch 27:	0.02969399  	0.08192112  	0.06707578  
2023-06-01 20:55:22.493: Find a better model.
2023-06-01 20:55:43.809: [iter 28 : loss : 0.6955 = 0.2251 + 0.4673 + 0.0031, time: 21.312207]
2023-06-01 20:55:44.096: epoch 28:	0.02976803  	0.08212411  	0.06731160  
2023-06-01 20:55:44.096: Find a better model.
2023-06-01 20:56:05.603: [iter 29 : loss : 0.6789 = 0.2090 + 0.4666 + 0.0033, time: 21.503206]
2023-06-01 20:56:05.881: epoch 29:	0.02970138  	0.08183967  	0.06725472  
2023-06-01 20:56:27.376: [iter 30 : loss : 0.6639 = 0.1943 + 0.4661 + 0.0036, time: 21.490547]
2023-06-01 20:56:27.653: epoch 30:	0.02974579  	0.08222217  	0.06748003  
2023-06-01 20:56:27.653: Find a better model.
2023-06-01 20:56:49.197: [iter 31 : loss : 0.6510 = 0.1818 + 0.4654 + 0.0038, time: 21.539775]
2023-06-01 20:56:49.473: epoch 31:	0.02984945  	0.08239505  	0.06759997  
2023-06-01 20:56:49.473: Find a better model.
2023-06-01 20:57:10.580: [iter 32 : loss : 0.6397 = 0.1711 + 0.4647 + 0.0040, time: 21.103017]
2023-06-01 20:57:10.859: epoch 32:	0.02991608  	0.08247390  	0.06773970  
2023-06-01 20:57:10.859: Find a better model.
2023-06-01 20:57:32.563: [iter 33 : loss : 0.6302 = 0.1620 + 0.4640 + 0.0042, time: 21.700369]
2023-06-01 20:57:32.837: epoch 33:	0.02999751  	0.08216041  	0.06779837  
2023-06-01 20:57:54.546: [iter 34 : loss : 0.6207 = 0.1529 + 0.4634 + 0.0044, time: 21.705059]
2023-06-01 20:57:54.826: epoch 34:	0.02999011  	0.08247099  	0.06783644  
2023-06-01 20:58:16.334: [iter 35 : loss : 0.6122 = 0.1449 + 0.4627 + 0.0046, time: 21.503629]
2023-06-01 20:58:16.611: epoch 35:	0.03004193  	0.08292433  	0.06805271  
2023-06-01 20:58:16.611: Find a better model.
2023-06-01 20:58:38.332: [iter 36 : loss : 0.6046 = 0.1377 + 0.4621 + 0.0048, time: 21.717435]
2023-06-01 20:58:38.593: epoch 36:	0.03001233  	0.08276235  	0.06791101  
2023-06-01 20:59:00.194: [iter 37 : loss : 0.5988 = 0.1322 + 0.4616 + 0.0050, time: 21.597159]
2023-06-01 20:59:00.459: epoch 37:	0.02992349  	0.08254936  	0.06777596  
2023-06-01 20:59:21.943: [iter 38 : loss : 0.5922 = 0.1260 + 0.4610 + 0.0052, time: 21.481091]
2023-06-01 20:59:22.230: epoch 38:	0.03004935  	0.08282162  	0.06810444  
2023-06-01 20:59:43.758: [iter 39 : loss : 0.5870 = 0.1212 + 0.4604 + 0.0053, time: 21.523633]
2023-06-01 20:59:44.036: epoch 39:	0.03010857  	0.08292643  	0.06814462  
2023-06-01 20:59:44.036: Find a better model.
2023-06-01 21:00:05.944: [iter 40 : loss : 0.5814 = 0.1159 + 0.4600 + 0.0055, time: 21.903266]
2023-06-01 21:00:06.227: epoch 40:	0.03009377  	0.08290060  	0.06814490  
2023-06-01 21:00:28.094: [iter 41 : loss : 0.5765 = 0.1113 + 0.4596 + 0.0057, time: 21.863110]
2023-06-01 21:00:28.376: epoch 41:	0.02999012  	0.08230019  	0.06805822  
2023-06-01 21:00:50.097: [iter 42 : loss : 0.5720 = 0.1071 + 0.4591 + 0.0058, time: 21.716059]
2023-06-01 21:00:50.381: epoch 42:	0.03001973  	0.08220011  	0.06798492  
2023-06-01 21:01:12.089: [iter 43 : loss : 0.5678 = 0.1030 + 0.4588 + 0.0060, time: 21.703992]
2023-06-01 21:01:12.372: epoch 43:	0.03000493  	0.08197182  	0.06782585  
2023-06-01 21:01:34.058: [iter 44 : loss : 0.5649 = 0.1004 + 0.4583 + 0.0062, time: 21.682541]
2023-06-01 21:01:34.350: epoch 44:	0.03004935  	0.08201032  	0.06775160  
2023-06-01 21:01:56.065: [iter 45 : loss : 0.5609 = 0.0965 + 0.4581 + 0.0063, time: 21.711349]
2023-06-01 21:01:56.344: epoch 45:	0.03001973  	0.08193617  	0.06765413  
2023-06-01 21:02:18.246: [iter 46 : loss : 0.5577 = 0.0936 + 0.4576 + 0.0065, time: 21.898047]
2023-06-01 21:02:18.524: epoch 46:	0.03003455  	0.08185800  	0.06753138  
2023-06-01 21:02:40.068: [iter 47 : loss : 0.5545 = 0.0905 + 0.4573 + 0.0066, time: 21.539844]
2023-06-01 21:02:40.350: epoch 47:	0.02991610  	0.08143792  	0.06732234  
2023-06-01 21:03:02.032: [iter 48 : loss : 0.5518 = 0.0881 + 0.4570 + 0.0067, time: 21.678439]
2023-06-01 21:03:02.314: epoch 48:	0.02980506  	0.08113524  	0.06724405  
2023-06-01 21:03:24.230: [iter 49 : loss : 0.5491 = 0.0856 + 0.4567 + 0.0069, time: 21.913117]
2023-06-01 21:03:24.510: epoch 49:	0.02969401  	0.08060592  	0.06711878  
2023-06-01 21:03:46.021: [iter 50 : loss : 0.5466 = 0.0832 + 0.4564 + 0.0070, time: 21.508110]
2023-06-01 21:03:46.307: epoch 50:	0.02963479  	0.08030328  	0.06689079  
2023-06-01 21:04:07.829: [iter 51 : loss : 0.5440 = 0.0806 + 0.4561 + 0.0072, time: 21.517750]
2023-06-01 21:04:08.103: epoch 51:	0.02958296  	0.08002207  	0.06674382  
2023-06-01 21:04:29.841: [iter 52 : loss : 0.5416 = 0.0785 + 0.4558 + 0.0073, time: 21.735420]
2023-06-01 21:04:30.121: epoch 52:	0.02959776  	0.07977286  	0.06662564  
2023-06-01 21:04:51.831: [iter 53 : loss : 0.5406 = 0.0776 + 0.4556 + 0.0074, time: 21.707213]
2023-06-01 21:04:52.109: epoch 53:	0.02951632  	0.07946651  	0.06639379  
2023-06-01 21:05:13.828: [iter 54 : loss : 0.5377 = 0.0748 + 0.4554 + 0.0076, time: 21.714113]
2023-06-01 21:05:14.111: epoch 54:	0.02933864  	0.07898880  	0.06599448  
2023-06-01 21:05:35.859: [iter 55 : loss : 0.5360 = 0.0732 + 0.4552 + 0.0077, time: 21.743060]
2023-06-01 21:05:36.155: epoch 55:	0.02933124  	0.07874445  	0.06584719  
2023-06-01 21:05:57.786: [iter 56 : loss : 0.5341 = 0.0714 + 0.4549 + 0.0078, time: 21.623165]
2023-06-01 21:05:58.062: epoch 56:	0.02913136  	0.07805457  	0.06559321  
2023-06-01 21:06:19.783: [iter 57 : loss : 0.5321 = 0.0695 + 0.4548 + 0.0079, time: 21.717516]
2023-06-01 21:06:20.064: epoch 57:	0.02903512  	0.07793456  	0.06548991  
2023-06-01 21:06:41.762: [iter 58 : loss : 0.5307 = 0.0681 + 0.4546 + 0.0080, time: 21.693414]
2023-06-01 21:06:42.041: epoch 58:	0.02897588  	0.07749099  	0.06517560  
2023-06-01 21:07:03.755: [iter 59 : loss : 0.5292 = 0.0667 + 0.4544 + 0.0081, time: 21.710036]
2023-06-01 21:07:04.033: epoch 59:	0.02894627  	0.07750056  	0.06516959  
2023-06-01 21:07:25.762: [iter 60 : loss : 0.5280 = 0.0654 + 0.4543 + 0.0083, time: 21.725346]
2023-06-01 21:07:26.039: epoch 60:	0.02899069  	0.07742563  	0.06516187  
2023-06-01 21:07:47.763: [iter 61 : loss : 0.5267 = 0.0642 + 0.4541 + 0.0084, time: 21.721195]
2023-06-01 21:07:48.042: epoch 61:	0.02889445  	0.07688913  	0.06488676  
2023-06-01 21:08:09.771: [iter 62 : loss : 0.5253 = 0.0628 + 0.4540 + 0.0085, time: 21.725577]
2023-06-01 21:08:10.050: epoch 62:	0.02877600  	0.07661220  	0.06468855  
2023-06-01 21:08:31.773: [iter 63 : loss : 0.5240 = 0.0616 + 0.4539 + 0.0086, time: 21.720098]
2023-06-01 21:08:32.050: epoch 63:	0.02866494  	0.07622909  	0.06446235  
2023-06-01 21:08:53.737: [iter 64 : loss : 0.5230 = 0.0607 + 0.4537 + 0.0087, time: 21.683511]
2023-06-01 21:08:54.015: epoch 64:	0.02857610  	0.07604606  	0.06437126  
2023-06-01 21:08:54.015: Early stopping is trigger at epoch: 64
2023-06-01 21:08:54.015: best_result@epoch 39:

2023-06-01 21:08:54.015: 		0.0301      	0.0829      	0.0681      
2023-06-01 21:11:48.881: my pid: 13996
2023-06-01 21:11:48.881: model: model.general_recommender.SGL
2023-06-01 21:11:48.881: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 21:11:48.881: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 21:11:52.974: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 21:12:14.357: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.382895]
2023-06-01 21:12:14.625: epoch 1:	0.00149541  	0.00331865  	0.00249180  
2023-06-01 21:12:14.625: Find a better model.
2023-06-01 21:12:35.886: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.257875]
2023-06-01 21:12:36.177: epoch 2:	0.00184335  	0.00355939  	0.00286668  
2023-06-01 21:12:36.177: Find a better model.
2023-06-01 21:12:57.343: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.163250]
2023-06-01 21:12:57.646: epoch 3:	0.00181374  	0.00381438  	0.00313240  
2023-06-01 21:12:57.646: Find a better model.
2023-06-01 21:13:18.694: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.045752]
2023-06-01 21:13:18.989: epoch 4:	0.00225792  	0.00498765  	0.00388928  
2023-06-01 21:13:18.989: Find a better model.
2023-06-01 21:13:40.123: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.130617]
2023-06-01 21:13:40.417: epoch 5:	0.00248001  	0.00510824  	0.00423793  
2023-06-01 21:13:40.417: Find a better model.
2023-06-01 21:14:01.500: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.079484]
2023-06-01 21:14:01.794: epoch 6:	0.00310927  	0.00688189  	0.00551158  
2023-06-01 21:14:01.794: Find a better model.
2023-06-01 21:14:23.062: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.262246]
2023-06-01 21:14:23.354: epoch 7:	0.00374593  	0.00913534  	0.00725597  
2023-06-01 21:14:23.354: Find a better model.
2023-06-01 21:14:44.471: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.112999]
2023-06-01 21:14:44.762: epoch 8:	0.00465649  	0.01205722  	0.00915721  
2023-06-01 21:14:44.762: Find a better model.
2023-06-01 21:15:05.875: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.108532]
2023-06-01 21:15:06.163: epoch 9:	0.00545601  	0.01437823  	0.01117027  
2023-06-01 21:15:06.163: Find a better model.
2023-06-01 21:15:27.249: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.081527]
2023-06-01 21:15:27.547: epoch 10:	0.00647022  	0.01813536  	0.01450604  
2023-06-01 21:15:27.547: Find a better model.
2023-06-01 21:15:48.660: [iter 11 : loss : 1.1340 = 0.6901 + 0.4439 + 0.0000, time: 21.109413]
2023-06-01 21:15:48.944: epoch 11:	0.00797301  	0.02256414  	0.01814627  
2023-06-01 21:15:48.944: Find a better model.
2023-06-01 21:16:10.010: [iter 12 : loss : 1.1330 = 0.6885 + 0.4444 + 0.0000, time: 21.062554]
2023-06-01 21:16:10.296: epoch 12:	0.00920190  	0.02556888  	0.02140248  
2023-06-01 21:16:10.296: Find a better model.
2023-06-01 21:16:31.414: [iter 13 : loss : 1.1312 = 0.6859 + 0.4452 + 0.0000, time: 21.114252]
2023-06-01 21:16:31.698: epoch 13:	0.01116372  	0.03077773  	0.02599533  
2023-06-01 21:16:31.698: Find a better model.
2023-06-01 21:16:52.621: [iter 14 : loss : 1.1277 = 0.6816 + 0.4460 + 0.0001, time: 20.918566]
2023-06-01 21:16:52.905: epoch 14:	0.01393991  	0.03707684  	0.03249415  
2023-06-01 21:16:52.905: Find a better model.
2023-06-01 21:17:13.854: [iter 15 : loss : 1.1216 = 0.6747 + 0.4468 + 0.0001, time: 20.944166]
2023-06-01 21:17:14.132: epoch 15:	0.01693819  	0.04479190  	0.03957368  
2023-06-01 21:17:14.132: Find a better model.
2023-06-01 21:17:35.198: [iter 16 : loss : 1.1106 = 0.6625 + 0.4479 + 0.0002, time: 21.063478]
2023-06-01 21:17:35.476: epoch 16:	0.02049175  	0.05444825  	0.04714772  
2023-06-01 21:17:35.476: Find a better model.
2023-06-01 21:17:56.393: [iter 17 : loss : 1.0920 = 0.6426 + 0.4492 + 0.0002, time: 20.909424]
2023-06-01 21:17:56.671: epoch 17:	0.02346786  	0.06237670  	0.05327051  
2023-06-01 21:17:56.671: Find a better model.
2023-06-01 21:18:17.576: [iter 18 : loss : 1.0628 = 0.6115 + 0.4509 + 0.0004, time: 20.900429]
2023-06-01 21:18:17.856: epoch 18:	0.02571844  	0.06807452  	0.05791295  
2023-06-01 21:18:17.856: Find a better model.
2023-06-01 21:18:38.978: [iter 19 : loss : 1.0234 = 0.5696 + 0.4533 + 0.0006, time: 21.118077]
2023-06-01 21:18:39.257: epoch 19:	0.02733235  	0.07219000  	0.06146214  
2023-06-01 21:18:39.257: Find a better model.
2023-06-01 21:19:00.215: [iter 20 : loss : 0.9764 = 0.5195 + 0.4561 + 0.0008, time: 20.954028]
2023-06-01 21:19:00.492: epoch 20:	0.02815410  	0.07398514  	0.06315628  
2023-06-01 21:19:00.492: Find a better model.
2023-06-01 21:19:22.187: [iter 21 : loss : 0.9266 = 0.4662 + 0.4593 + 0.0011, time: 21.686429]
2023-06-01 21:19:22.466: epoch 21:	0.02876857  	0.07592094  	0.06382646  
2023-06-01 21:19:22.466: Find a better model.
2023-06-01 21:19:44.173: [iter 22 : loss : 0.8780 = 0.4142 + 0.4624 + 0.0014, time: 21.702377]
2023-06-01 21:19:44.449: epoch 22:	0.02904248  	0.07640587  	0.06411693  
2023-06-01 21:19:44.450: Find a better model.
2023-06-01 21:20:06.162: [iter 23 : loss : 0.8348 = 0.3683 + 0.4648 + 0.0017, time: 21.708382]
2023-06-01 21:20:06.436: epoch 23:	0.02922756  	0.07755498  	0.06461901  
2023-06-01 21:20:06.436: Find a better model.
2023-06-01 21:20:28.121: [iter 24 : loss : 0.7974 = 0.3290 + 0.4664 + 0.0020, time: 21.681093]
2023-06-01 21:20:28.396: epoch 24:	0.02931640  	0.07798713  	0.06498785  
2023-06-01 21:20:28.396: Find a better model.
2023-06-01 21:20:50.172: [iter 25 : loss : 0.7651 = 0.2958 + 0.4671 + 0.0023, time: 21.771238]
2023-06-01 21:20:50.447: epoch 25:	0.02947188  	0.07851384  	0.06520982  
2023-06-01 21:20:50.447: Find a better model.
2023-06-01 21:21:12.132: [iter 26 : loss : 0.7384 = 0.2684 + 0.4675 + 0.0025, time: 21.680113]
2023-06-01 21:21:12.403: epoch 26:	0.02934602  	0.07838009  	0.06529599  
2023-06-01 21:21:33.923: [iter 27 : loss : 0.7151 = 0.2450 + 0.4673 + 0.0028, time: 21.515108]
2023-06-01 21:21:34.201: epoch 27:	0.02949410  	0.07921800  	0.06568448  
2023-06-01 21:21:34.201: Find a better model.
2023-06-01 21:21:55.915: [iter 28 : loss : 0.6956 = 0.2256 + 0.4669 + 0.0031, time: 21.709976]
2023-06-01 21:21:56.185: epoch 28:	0.02965698  	0.07975005  	0.06602971  
2023-06-01 21:21:56.185: Find a better model.
2023-06-01 21:22:17.736: [iter 29 : loss : 0.6790 = 0.2094 + 0.4662 + 0.0033, time: 21.547025]
2023-06-01 21:22:18.005: epoch 29:	0.02977542  	0.08023302  	0.06621812  
2023-06-01 21:22:18.005: Find a better model.
2023-06-01 21:22:39.497: [iter 30 : loss : 0.6639 = 0.1947 + 0.4657 + 0.0036, time: 21.488086]
2023-06-01 21:22:39.770: epoch 30:	0.02966438  	0.07987797  	0.06603358  
2023-06-01 21:23:01.315: [iter 31 : loss : 0.6508 = 0.1819 + 0.4652 + 0.0038, time: 21.542245]
2023-06-01 21:23:01.587: epoch 31:	0.02970878  	0.07993864  	0.06616634  
2023-06-01 21:23:23.071: [iter 32 : loss : 0.6399 = 0.1713 + 0.4646 + 0.0040, time: 21.480005]
2023-06-01 21:23:23.346: epoch 32:	0.02970879  	0.08006803  	0.06625969  
2023-06-01 21:23:44.901: [iter 33 : loss : 0.6299 = 0.1619 + 0.4638 + 0.0042, time: 21.550522]
2023-06-01 21:23:45.176: epoch 33:	0.02974581  	0.08021279  	0.06642836  
2023-06-01 21:24:07.267: [iter 34 : loss : 0.6207 = 0.1532 + 0.4631 + 0.0044, time: 22.088059]
2023-06-01 21:24:07.545: epoch 34:	0.02977541  	0.08018159  	0.06660239  
2023-06-01 21:24:29.307: [iter 35 : loss : 0.6119 = 0.1449 + 0.4624 + 0.0046, time: 21.757869]
2023-06-01 21:24:29.580: epoch 35:	0.02981243  	0.07997612  	0.06666823  
2023-06-01 21:24:51.093: [iter 36 : loss : 0.6048 = 0.1381 + 0.4619 + 0.0048, time: 21.508956]
2023-06-01 21:24:51.361: epoch 36:	0.02997530  	0.08052675  	0.06700320  
2023-06-01 21:24:51.361: Find a better model.
2023-06-01 21:25:14.288: [iter 37 : loss : 0.5985 = 0.1322 + 0.4613 + 0.0050, time: 22.923562]
2023-06-01 21:25:14.623: epoch 37:	0.02989388  	0.07998950  	0.06676137  
2023-06-01 21:25:37.836: [iter 38 : loss : 0.5919 = 0.1259 + 0.4608 + 0.0052, time: 23.209588]
2023-06-01 21:25:38.115: epoch 38:	0.02997531  	0.07980725  	0.06692788  
2023-06-01 21:26:00.227: [iter 39 : loss : 0.5868 = 0.1213 + 0.4602 + 0.0053, time: 22.106715]
2023-06-01 21:26:00.483: epoch 39:	0.03006415  	0.07959922  	0.06690791  
2023-06-01 21:26:22.592: [iter 40 : loss : 0.5813 = 0.1161 + 0.4598 + 0.0055, time: 22.099258]
2023-06-01 21:26:22.860: epoch 40:	0.03001233  	0.07967933  	0.06688136  
2023-06-01 21:26:44.800: [iter 41 : loss : 0.5760 = 0.1112 + 0.4592 + 0.0057, time: 21.937136]
2023-06-01 21:26:45.078: epoch 41:	0.02994569  	0.07948335  	0.06690416  
2023-06-01 21:27:07.234: [iter 42 : loss : 0.5719 = 0.1072 + 0.4589 + 0.0058, time: 22.152611]
2023-06-01 21:27:07.993: epoch 42:	0.02999752  	0.07947557  	0.06688798  
2023-06-01 21:27:30.606: [iter 43 : loss : 0.5678 = 0.1033 + 0.4585 + 0.0060, time: 22.597224]
2023-06-01 21:27:30.872: epoch 43:	0.03005674  	0.07917406  	0.06678995  
2023-06-01 21:27:53.127: [iter 44 : loss : 0.5648 = 0.1006 + 0.4581 + 0.0062, time: 22.251148]
2023-06-01 21:27:53.387: epoch 44:	0.02998271  	0.07876888  	0.06651986  
2023-06-01 21:28:16.569: [iter 45 : loss : 0.5607 = 0.0966 + 0.4578 + 0.0063, time: 23.177560]
2023-06-01 21:28:16.838: epoch 45:	0.02993090  	0.07877976  	0.06623330  
2023-06-01 21:28:40.221: [iter 46 : loss : 0.5575 = 0.0936 + 0.4574 + 0.0065, time: 23.378937]
2023-06-01 21:28:40.539: epoch 46:	0.02976803  	0.07840224  	0.06613658  
2023-06-01 21:29:03.800: [iter 47 : loss : 0.5541 = 0.0904 + 0.4571 + 0.0066, time: 23.255757]
2023-06-01 21:29:04.076: epoch 47:	0.02977544  	0.07857587  	0.06617265  
2023-06-01 21:29:26.714: [iter 48 : loss : 0.5516 = 0.0882 + 0.4567 + 0.0067, time: 22.633162]
2023-06-01 21:29:26.994: epoch 48:	0.02973842  	0.07874582  	0.06603494  
2023-06-01 21:29:49.692: [iter 49 : loss : 0.5488 = 0.0855 + 0.4564 + 0.0069, time: 22.694279]
2023-06-01 21:29:49.968: epoch 49:	0.02968660  	0.07821393  	0.06581511  
2023-06-01 21:30:12.664: [iter 50 : loss : 0.5465 = 0.0834 + 0.4561 + 0.0070, time: 22.691053]
2023-06-01 21:30:12.942: epoch 50:	0.02959776  	0.07802090  	0.06571846  
2023-06-01 21:30:35.725: [iter 51 : loss : 0.5438 = 0.0808 + 0.4559 + 0.0072, time: 22.778478]
2023-06-01 21:30:36.008: epoch 51:	0.02961996  	0.07788407  	0.06565222  
2023-06-01 21:30:58.627: [iter 52 : loss : 0.5412 = 0.0783 + 0.4556 + 0.0073, time: 22.613096]
2023-06-01 21:30:58.906: epoch 52:	0.02951632  	0.07757626  	0.06550360  
2023-06-01 21:31:21.477: [iter 53 : loss : 0.5405 = 0.0775 + 0.4555 + 0.0074, time: 22.566633]
2023-06-01 21:31:21.751: epoch 53:	0.02946449  	0.07762453  	0.06551670  
2023-06-01 21:31:43.362: [iter 54 : loss : 0.5379 = 0.0751 + 0.4552 + 0.0076, time: 21.608108]
2023-06-01 21:31:43.632: epoch 54:	0.02936085  	0.07715282  	0.06534369  
2023-06-01 21:32:05.307: [iter 55 : loss : 0.5357 = 0.0730 + 0.4549 + 0.0077, time: 21.671131]
2023-06-01 21:32:05.581: epoch 55:	0.02915356  	0.07652537  	0.06506564  
2023-06-01 21:32:27.298: [iter 56 : loss : 0.5340 = 0.0714 + 0.4547 + 0.0078, time: 21.713228]
2023-06-01 21:32:27.580: epoch 56:	0.02910915  	0.07623389  	0.06477126  
2023-06-01 21:32:49.374: [iter 57 : loss : 0.5321 = 0.0697 + 0.4546 + 0.0079, time: 21.788929]
2023-06-01 21:32:49.644: epoch 57:	0.02900550  	0.07607114  	0.06462633  
2023-06-01 21:33:12.491: [iter 58 : loss : 0.5305 = 0.0680 + 0.4544 + 0.0080, time: 22.842657]
2023-06-01 21:33:12.762: epoch 58:	0.02904992  	0.07637811  	0.06463471  
2023-06-01 21:33:34.324: [iter 59 : loss : 0.5290 = 0.0667 + 0.4542 + 0.0081, time: 21.558614]
2023-06-01 21:33:34.634: epoch 59:	0.02893888  	0.07616290  	0.06444647  
2023-06-01 21:33:56.594: [iter 60 : loss : 0.5277 = 0.0653 + 0.4542 + 0.0083, time: 21.956638]
2023-06-01 21:33:56.849: epoch 60:	0.02887224  	0.07562473  	0.06432316  
2023-06-01 21:34:18.370: [iter 61 : loss : 0.5262 = 0.0639 + 0.4539 + 0.0084, time: 21.515631]
2023-06-01 21:34:18.642: epoch 61:	0.02883522  	0.07537207  	0.06423613  
2023-06-01 21:34:18.643: Early stopping is trigger at epoch: 61
2023-06-01 21:34:18.643: best_result@epoch 36:

2023-06-01 21:34:18.643: 		0.0300      	0.0805      	0.0670      
2023-06-01 21:37:38.939: my pid: 7396
2023-06-01 21:37:38.939: model: model.general_recommender.SGL
2023-06-01 21:37:38.939: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-01 21:37:38.939: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-01 21:37:43.300: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-01 21:38:04.836: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.536366]
2023-06-01 21:38:05.119: epoch 1:	0.00145099  	0.00325606  	0.00242279  
2023-06-01 21:38:05.119: Find a better model.
2023-06-01 21:38:26.617: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.495252]
2023-06-01 21:38:26.911: epoch 2:	0.00176192  	0.00298717  	0.00264615  
2023-06-01 21:38:48.387: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.472513]
2023-06-01 21:38:48.682: epoch 3:	0.00163607  	0.00352593  	0.00263063  
2023-06-01 21:38:48.682: Find a better model.
2023-06-01 21:39:10.175: [iter 4 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 21.490233]
2023-06-01 21:39:10.469: epoch 4:	0.00227273  	0.00480561  	0.00388199  
2023-06-01 21:39:10.469: Find a better model.
2023-06-01 21:39:31.768: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.295830]
2023-06-01 21:39:32.069: epoch 5:	0.00250963  	0.00550808  	0.00462481  
2023-06-01 21:39:32.069: Find a better model.
2023-06-01 21:39:53.619: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.546653]
2023-06-01 21:39:53.914: epoch 6:	0.00298342  	0.00725341  	0.00532330  
2023-06-01 21:39:53.914: Find a better model.
2023-06-01 21:40:15.476: [iter 7 : loss : 1.1344 = 0.6924 + 0.4421 + 0.0000, time: 21.559377]
2023-06-01 21:40:15.768: epoch 7:	0.00393841  	0.01045730  	0.00782869  
2023-06-01 21:40:15.768: Find a better model.
2023-06-01 21:40:37.929: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 22.157255]
2023-06-01 21:40:38.241: epoch 8:	0.00446402  	0.01182510  	0.00905696  
2023-06-01 21:40:38.241: Find a better model.
2023-06-01 21:41:00.586: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 22.340459]
2023-06-01 21:41:00.871: epoch 9:	0.00567810  	0.01582120  	0.01256912  
2023-06-01 21:41:00.871: Find a better model.
2023-06-01 21:41:22.858: [iter 10 : loss : 1.1343 = 0.6911 + 0.4432 + 0.0000, time: 21.982945]
2023-06-01 21:41:23.176: epoch 10:	0.00624813  	0.01764989  	0.01405995  
2023-06-01 21:41:23.176: Find a better model.
2023-06-01 21:41:45.577: [iter 11 : loss : 1.1340 = 0.6903 + 0.4437 + 0.0000, time: 22.397687]
2023-06-01 21:41:45.873: epoch 11:	0.00751403  	0.02120644  	0.01697244  
2023-06-01 21:41:45.873: Find a better model.
2023-06-01 21:42:07.113: [iter 12 : loss : 1.1332 = 0.6889 + 0.4443 + 0.0000, time: 21.235149]
2023-06-01 21:42:07.399: epoch 12:	0.00909086  	0.02564302  	0.02088613  
2023-06-01 21:42:07.399: Find a better model.
2023-06-01 21:42:28.705: [iter 13 : loss : 1.1315 = 0.6865 + 0.4450 + 0.0000, time: 21.302590]
2023-06-01 21:42:29.018: epoch 13:	0.01078617  	0.03033598  	0.02562525  
2023-06-01 21:42:29.018: Find a better model.
2023-06-01 21:42:50.086: [iter 14 : loss : 1.1283 = 0.6825 + 0.4457 + 0.0001, time: 21.063998]
2023-06-01 21:42:50.370: epoch 14:	0.01342168  	0.03627261  	0.03184107  
2023-06-01 21:42:50.370: Find a better model.
2023-06-01 21:43:11.516: [iter 15 : loss : 1.1227 = 0.6761 + 0.4465 + 0.0001, time: 21.143138]
2023-06-01 21:43:11.800: epoch 15:	0.01636814  	0.04393560  	0.03796174  
2023-06-01 21:43:11.800: Find a better model.
2023-06-01 21:43:32.950: [iter 16 : loss : 1.1126 = 0.6649 + 0.4476 + 0.0001, time: 21.147584]
2023-06-01 21:43:33.230: epoch 16:	0.01933682  	0.05143049  	0.04460439  
2023-06-01 21:43:33.230: Find a better model.
2023-06-01 21:43:54.502: [iter 17 : loss : 1.0955 = 0.6465 + 0.4487 + 0.0002, time: 21.268012]
2023-06-01 21:43:54.782: epoch 17:	0.02296442  	0.06096032  	0.05198072  
2023-06-01 21:43:54.782: Find a better model.
2023-06-01 21:44:15.878: [iter 18 : loss : 1.0680 = 0.6174 + 0.4503 + 0.0004, time: 21.092303]
2023-06-01 21:44:16.162: epoch 18:	0.02542971  	0.06756451  	0.05749170  
2023-06-01 21:44:16.162: Find a better model.
2023-06-01 21:44:37.434: [iter 19 : loss : 1.0304 = 0.5773 + 0.4525 + 0.0005, time: 21.268281]
2023-06-01 21:44:37.710: epoch 19:	0.02688815  	0.07128804  	0.06096408  
2023-06-01 21:44:37.710: Find a better model.
2023-06-01 21:44:58.825: [iter 20 : loss : 0.9840 = 0.5281 + 0.4551 + 0.0008, time: 21.111156]
2023-06-01 21:44:59.114: epoch 20:	0.02831697  	0.07525802  	0.06361044  
2023-06-01 21:44:59.114: Find a better model.
2023-06-01 21:45:20.794: [iter 21 : loss : 0.9346 = 0.4750 + 0.4585 + 0.0010, time: 21.675486]
2023-06-01 21:45:21.076: epoch 21:	0.02919794  	0.07805520  	0.06512576  
2023-06-01 21:45:21.076: Find a better model.
2023-06-01 21:45:42.819: [iter 22 : loss : 0.8848 = 0.4220 + 0.4616 + 0.0013, time: 21.739018]
2023-06-01 21:45:43.098: epoch 22:	0.02932379  	0.07929502  	0.06551925  
2023-06-01 21:45:43.098: Find a better model.
2023-06-01 21:46:04.828: [iter 23 : loss : 0.8405 = 0.3748 + 0.4641 + 0.0016, time: 21.727156]
2023-06-01 21:46:05.111: epoch 23:	0.02959032  	0.08058795  	0.06584907  
2023-06-01 21:46:05.111: Find a better model.
2023-06-01 21:46:26.805: [iter 24 : loss : 0.8019 = 0.3340 + 0.4660 + 0.0019, time: 21.690226]
2023-06-01 21:46:27.084: epoch 24:	0.02968657  	0.08103227  	0.06602791  
2023-06-01 21:46:27.084: Find a better model.
2023-06-01 21:46:48.773: [iter 25 : loss : 0.7689 = 0.2998 + 0.4668 + 0.0022, time: 21.685077]
2023-06-01 21:46:49.057: epoch 25:	0.02973840  	0.08141364  	0.06625823  
2023-06-01 21:46:49.057: Find a better model.
2023-06-01 21:47:10.760: [iter 26 : loss : 0.7414 = 0.2717 + 0.4671 + 0.0025, time: 21.700042]
2023-06-01 21:47:11.036: epoch 26:	0.02995308  	0.08145290  	0.06633203  
2023-06-01 21:47:11.036: Find a better model.
2023-06-01 21:47:32.749: [iter 27 : loss : 0.7177 = 0.2479 + 0.4671 + 0.0028, time: 21.709444]
2023-06-01 21:47:33.032: epoch 27:	0.03008635  	0.08229229  	0.06675510  
2023-06-01 21:47:33.032: Find a better model.
2023-06-01 21:47:54.737: [iter 28 : loss : 0.6979 = 0.2280 + 0.4669 + 0.0030, time: 21.700650]
2023-06-01 21:47:55.023: epoch 28:	0.03010116  	0.08243170  	0.06679707  
2023-06-01 21:47:55.023: Find a better model.
2023-06-01 21:48:16.562: [iter 29 : loss : 0.6808 = 0.2113 + 0.4662 + 0.0033, time: 21.535372]
2023-06-01 21:48:16.833: epoch 29:	0.03010116  	0.08262520  	0.06708549  
2023-06-01 21:48:16.833: Find a better model.
2023-06-01 21:48:38.540: [iter 30 : loss : 0.6654 = 0.1962 + 0.4657 + 0.0035, time: 21.703684]
2023-06-01 21:48:38.811: epoch 30:	0.03014557  	0.08241256  	0.06707339  
2023-06-01 21:49:00.358: [iter 31 : loss : 0.6520 = 0.1833 + 0.4650 + 0.0038, time: 21.543052]
2023-06-01 21:49:00.617: epoch 31:	0.03010115  	0.08278849  	0.06732066  
2023-06-01 21:49:00.618: Find a better model.
2023-06-01 21:49:22.338: [iter 32 : loss : 0.6408 = 0.1724 + 0.4644 + 0.0040, time: 21.717100]
2023-06-01 21:49:22.607: epoch 32:	0.03025663  	0.08320446  	0.06751158  
2023-06-01 21:49:22.607: Find a better model.
2023-06-01 21:49:44.307: [iter 33 : loss : 0.6306 = 0.1628 + 0.4636 + 0.0042, time: 21.697039]
2023-06-01 21:49:44.576: epoch 33:	0.03021221  	0.08282842  	0.06759995  
2023-06-01 21:50:06.295: [iter 34 : loss : 0.6218 = 0.1542 + 0.4632 + 0.0044, time: 21.715066]
2023-06-01 21:50:06.559: epoch 34:	0.03017520  	0.08261079  	0.06758465  
2023-06-01 21:50:28.315: [iter 35 : loss : 0.6122 = 0.1453 + 0.4623 + 0.0046, time: 21.753056]
2023-06-01 21:50:28.578: epoch 35:	0.03017519  	0.08245204  	0.06759016  
2023-06-01 21:50:50.315: [iter 36 : loss : 0.6056 = 0.1391 + 0.4617 + 0.0048, time: 21.734161]
2023-06-01 21:50:50.578: epoch 36:	0.03031585  	0.08268566  	0.06755551  
2023-06-01 21:51:12.481: [iter 37 : loss : 0.5989 = 0.1327 + 0.4612 + 0.0050, time: 21.898077]
2023-06-01 21:51:12.746: epoch 37:	0.03046391  	0.08304139  	0.06776071  
2023-06-01 21:51:34.517: [iter 38 : loss : 0.5922 = 0.1264 + 0.4606 + 0.0051, time: 21.767183]
2023-06-01 21:51:34.784: epoch 38:	0.03041949  	0.08283845  	0.06772760  
2023-06-01 21:51:56.263: [iter 39 : loss : 0.5871 = 0.1217 + 0.4601 + 0.0053, time: 21.475398]
2023-06-01 21:51:56.526: epoch 39:	0.03042688  	0.08290857  	0.06764115  
2023-06-01 21:52:18.468: [iter 40 : loss : 0.5813 = 0.1163 + 0.4595 + 0.0055, time: 21.938061]
2023-06-01 21:52:18.736: epoch 40:	0.03044909  	0.08284746  	0.06759010  
2023-06-01 21:52:40.498: [iter 41 : loss : 0.5766 = 0.1117 + 0.4592 + 0.0057, time: 21.759160]
2023-06-01 21:52:40.762: epoch 41:	0.03027882  	0.08250766  	0.06755879  
2023-06-01 21:53:02.462: [iter 42 : loss : 0.5719 = 0.1072 + 0.4588 + 0.0058, time: 21.697026]
2023-06-01 21:53:02.729: epoch 42:	0.03033064  	0.08276793  	0.06742493  
2023-06-01 21:53:24.490: [iter 43 : loss : 0.5680 = 0.1037 + 0.4583 + 0.0060, time: 21.757027]
2023-06-01 21:53:24.752: epoch 43:	0.03022699  	0.08207549  	0.06720553  
2023-06-01 21:53:46.634: [iter 44 : loss : 0.5650 = 0.1008 + 0.4580 + 0.0061, time: 21.878032]
2023-06-01 21:53:46.896: epoch 44:	0.03015296  	0.08183870  	0.06701217  
2023-06-01 21:54:08.634: [iter 45 : loss : 0.5610 = 0.0971 + 0.4576 + 0.0063, time: 21.733024]
2023-06-01 21:54:08.897: epoch 45:	0.03023440  	0.08206481  	0.06707525  
2023-06-01 21:54:30.625: [iter 46 : loss : 0.5579 = 0.0943 + 0.4572 + 0.0064, time: 21.725503]
2023-06-01 21:54:30.887: epoch 46:	0.03025661  	0.08189650  	0.06709382  
2023-06-01 21:54:52.449: [iter 47 : loss : 0.5545 = 0.0910 + 0.4569 + 0.0066, time: 21.559154]
2023-06-01 21:54:52.711: epoch 47:	0.03019738  	0.08137868  	0.06680632  
2023-06-01 21:55:14.393: [iter 48 : loss : 0.5519 = 0.0886 + 0.4566 + 0.0067, time: 21.678603]
2023-06-01 21:55:14.654: epoch 48:	0.03014556  	0.08103556  	0.06669233  
2023-06-01 21:55:36.408: [iter 49 : loss : 0.5491 = 0.0860 + 0.4562 + 0.0069, time: 21.749601]
2023-06-01 21:55:36.668: epoch 49:	0.03017517  	0.08105774  	0.06663087  
2023-06-01 21:55:58.388: [iter 50 : loss : 0.5467 = 0.0837 + 0.4560 + 0.0070, time: 21.717023]
2023-06-01 21:55:58.651: epoch 50:	0.03007152  	0.08098847  	0.06654864  
2023-06-01 21:56:20.418: [iter 51 : loss : 0.5441 = 0.0812 + 0.4557 + 0.0071, time: 21.764118]
2023-06-01 21:56:20.683: epoch 51:	0.03010852  	0.08072913  	0.06650469  
2023-06-01 21:56:42.372: [iter 52 : loss : 0.5414 = 0.0787 + 0.4554 + 0.0073, time: 21.686033]
2023-06-01 21:56:42.634: epoch 52:	0.03008632  	0.08067445  	0.06647072  
2023-06-01 21:57:04.381: [iter 53 : loss : 0.5400 = 0.0773 + 0.4552 + 0.0074, time: 21.744519]
2023-06-01 21:57:04.641: epoch 53:	0.02995307  	0.08009128  	0.06625415  
2023-06-01 21:57:26.170: [iter 54 : loss : 0.5379 = 0.0754 + 0.4549 + 0.0075, time: 21.525108]
2023-06-01 21:57:26.431: epoch 54:	0.02988644  	0.07962938  	0.06591498  
2023-06-01 21:57:47.744: [iter 55 : loss : 0.5361 = 0.0737 + 0.4547 + 0.0077, time: 21.309084]
2023-06-01 21:57:48.015: epoch 55:	0.02988644  	0.07976369  	0.06585434  
2023-06-01 21:58:09.755: [iter 56 : loss : 0.5339 = 0.0716 + 0.4546 + 0.0078, time: 21.735997]
2023-06-01 21:58:10.025: epoch 56:	0.02994566  	0.07999645  	0.06596054  
2023-06-01 21:58:31.566: [iter 57 : loss : 0.5321 = 0.0697 + 0.4544 + 0.0079, time: 21.538167]
2023-06-01 21:58:31.828: epoch 57:	0.02996787  	0.08012319  	0.06601806  
2023-06-01 21:58:31.828: Early stopping is trigger at epoch: 57
2023-06-01 21:58:31.828: best_result@epoch 32:

2023-06-01 21:58:31.828: 		0.0303      	0.0832      	0.0675      
2023-06-02 09:12:43.609: my pid: 15204
2023-06-02 09:12:43.609: model: model.general_recommender.SGL
2023-06-02 09:12:43.609: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 09:12:43.609: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 09:12:47.632: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 09:13:08.678: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.045368]
2023-06-02 09:13:08.959: epoch 1:	0.00120669  	0.00241532  	0.00189338  
2023-06-02 09:13:08.959: Find a better model.
2023-06-02 09:13:29.691: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.728205]
2023-06-02 09:13:29.983: epoch 2:	0.00179153  	0.00332253  	0.00294850  
2023-06-02 09:13:29.983: Find a better model.
2023-06-02 09:13:50.846: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 20.859119]
2023-06-02 09:13:51.151: epoch 3:	0.00205804  	0.00447957  	0.00345028  
2023-06-02 09:13:51.151: Find a better model.
2023-06-02 09:14:12.068: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.913213]
2023-06-02 09:14:12.355: epoch 4:	0.00199141  	0.00433370  	0.00366472  
2023-06-02 09:14:33.219: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.859035]
2023-06-02 09:14:33.512: epoch 5:	0.00280574  	0.00650740  	0.00489819  
2023-06-02 09:14:33.512: Find a better model.
2023-06-02 09:14:54.464: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.947476]
2023-06-02 09:14:54.758: epoch 6:	0.00294640  	0.00738273  	0.00549240  
2023-06-02 09:14:54.758: Find a better model.
2023-06-02 09:15:15.595: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.832826]
2023-06-02 09:15:15.883: epoch 7:	0.00357566  	0.00907046  	0.00679777  
2023-06-02 09:15:15.884: Find a better model.
2023-06-02 09:15:36.843: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.955331]
2023-06-02 09:15:37.149: epoch 8:	0.00437518  	0.01090658  	0.00868914  
2023-06-02 09:15:37.149: Find a better model.
2023-06-02 09:15:57.848: [iter 9 : loss : 1.1347 = 0.6916 + 0.4431 + 0.0000, time: 20.693680]
2023-06-02 09:15:58.152: epoch 9:	0.00508586  	0.01361679  	0.01032700  
2023-06-02 09:15:58.152: Find a better model.
2023-06-02 09:16:19.019: [iter 10 : loss : 1.1344 = 0.6909 + 0.4435 + 0.0000, time: 20.863029]
2023-06-02 09:16:19.306: epoch 10:	0.00578174  	0.01630088  	0.01221930  
2023-06-02 09:16:19.306: Find a better model.
2023-06-02 09:16:40.021: [iter 11 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 20.711682]
2023-06-02 09:16:40.312: epoch 11:	0.00635177  	0.01811464  	0.01392359  
2023-06-02 09:16:40.312: Find a better model.
2023-06-02 09:17:01.002: [iter 12 : loss : 1.1327 = 0.6880 + 0.4447 + 0.0000, time: 20.684452]
2023-06-02 09:17:01.283: epoch 12:	0.00770651  	0.02250686  	0.01762819  
2023-06-02 09:17:01.283: Find a better model.
2023-06-02 09:17:22.023: [iter 13 : loss : 1.1308 = 0.6854 + 0.4454 + 0.0000, time: 20.735395]
2023-06-02 09:17:22.303: epoch 13:	0.00960907  	0.02861613  	0.02261373  
2023-06-02 09:17:22.303: Find a better model.
2023-06-02 09:17:42.999: [iter 14 : loss : 1.1274 = 0.6812 + 0.4461 + 0.0001, time: 20.691355]
2023-06-02 09:17:43.279: epoch 14:	0.01251109  	0.03610916  	0.02926908  
2023-06-02 09:17:43.279: Find a better model.
2023-06-02 09:18:03.977: [iter 15 : loss : 1.1213 = 0.6741 + 0.4470 + 0.0001, time: 20.693058]
2023-06-02 09:18:04.259: epoch 15:	0.01586473  	0.04412491  	0.03671411  
2023-06-02 09:18:04.259: Find a better model.
2023-06-02 09:18:24.794: [iter 16 : loss : 1.1100 = 0.6618 + 0.4480 + 0.0002, time: 20.531145]
2023-06-02 09:18:25.075: epoch 16:	0.01964775  	0.05374765  	0.04497256  
2023-06-02 09:18:25.075: Find a better model.
2023-06-02 09:18:45.556: [iter 17 : loss : 1.0914 = 0.6417 + 0.4494 + 0.0002, time: 20.477374]
2023-06-02 09:18:45.836: epoch 17:	0.02290520  	0.06182560  	0.05161911  
2023-06-02 09:18:45.836: Find a better model.
2023-06-02 09:19:06.359: [iter 18 : loss : 1.0620 = 0.6105 + 0.4511 + 0.0004, time: 20.517281]
2023-06-02 09:19:06.645: epoch 18:	0.02515579  	0.06751455  	0.05630952  
2023-06-02 09:19:06.645: Find a better model.
2023-06-02 09:19:27.137: [iter 19 : loss : 1.0226 = 0.5686 + 0.4534 + 0.0006, time: 20.488104]
2023-06-02 09:19:27.415: epoch 19:	0.02683632  	0.07151978  	0.05978421  
2023-06-02 09:19:27.415: Find a better model.
2023-06-02 09:19:47.966: [iter 20 : loss : 0.9759 = 0.5186 + 0.4565 + 0.0008, time: 20.547357]
2023-06-02 09:19:48.246: epoch 20:	0.02777654  	0.07433338  	0.06179030  
2023-06-02 09:19:48.247: Find a better model.
2023-06-02 09:20:09.362: [iter 21 : loss : 0.9262 = 0.4655 + 0.4596 + 0.0011, time: 21.111688]
2023-06-02 09:20:09.642: epoch 21:	0.02847985  	0.07675855  	0.06302619  
2023-06-02 09:20:09.642: Find a better model.
2023-06-02 09:20:30.897: [iter 22 : loss : 0.8776 = 0.4135 + 0.4627 + 0.0014, time: 21.250312]
2023-06-02 09:20:31.183: epoch 22:	0.02869456  	0.07772527  	0.06371301  
2023-06-02 09:20:31.183: Find a better model.
2023-06-02 09:20:52.137: [iter 23 : loss : 0.8347 = 0.3679 + 0.4652 + 0.0017, time: 20.948196]
2023-06-02 09:20:52.416: epoch 23:	0.02890184  	0.07844932  	0.06399880  
2023-06-02 09:20:52.417: Find a better model.
2023-06-02 09:21:13.503: [iter 24 : loss : 0.7972 = 0.3284 + 0.4668 + 0.0020, time: 21.083047]
2023-06-02 09:21:13.778: epoch 24:	0.02899808  	0.07850293  	0.06392525  
2023-06-02 09:21:13.778: Find a better model.
2023-06-02 09:21:35.107: [iter 25 : loss : 0.7653 = 0.2955 + 0.4675 + 0.0023, time: 21.325196]
2023-06-02 09:21:35.382: epoch 25:	0.02910914  	0.07908527  	0.06445571  
2023-06-02 09:21:35.382: Find a better model.
2023-06-02 09:21:56.851: [iter 26 : loss : 0.7387 = 0.2683 + 0.4679 + 0.0025, time: 21.464263]
2023-06-02 09:21:57.137: epoch 26:	0.02935346  	0.08003941  	0.06477362  
2023-06-02 09:21:57.137: Find a better model.
2023-06-02 09:22:18.121: [iter 27 : loss : 0.7158 = 0.2453 + 0.4677 + 0.0028, time: 20.981581]
2023-06-02 09:22:18.391: epoch 27:	0.02950894  	0.08058736  	0.06513155  
2023-06-02 09:22:18.391: Find a better model.
2023-06-02 09:22:39.448: [iter 28 : loss : 0.6965 = 0.2260 + 0.4675 + 0.0031, time: 21.052833]
2023-06-02 09:22:39.722: epoch 28:	0.02952376  	0.08108557  	0.06526714  
2023-06-02 09:22:39.722: Find a better model.
2023-06-02 09:23:00.848: [iter 29 : loss : 0.6796 = 0.2094 + 0.4669 + 0.0033, time: 21.122200]
2023-06-02 09:23:01.133: epoch 29:	0.02971624  	0.08149706  	0.06563993  
2023-06-02 09:23:01.133: Find a better model.
2023-06-02 09:23:22.060: [iter 30 : loss : 0.6644 = 0.1946 + 0.4662 + 0.0035, time: 20.923102]
2023-06-02 09:23:22.331: epoch 30:	0.02964961  	0.08190711  	0.06577342  
2023-06-02 09:23:22.331: Find a better model.
2023-06-02 09:23:43.442: [iter 31 : loss : 0.6513 = 0.1819 + 0.4656 + 0.0038, time: 21.106108]
2023-06-02 09:23:43.711: epoch 31:	0.02964221  	0.08191932  	0.06595572  
2023-06-02 09:23:43.711: Find a better model.
2023-06-02 09:24:05.047: [iter 32 : loss : 0.6401 = 0.1713 + 0.4648 + 0.0040, time: 21.332647]
2023-06-02 09:24:05.322: epoch 32:	0.02981988  	0.08205825  	0.06623892  
2023-06-02 09:24:05.322: Find a better model.
2023-06-02 09:24:26.237: [iter 33 : loss : 0.6301 = 0.1618 + 0.4641 + 0.0042, time: 20.911027]
2023-06-02 09:24:26.509: epoch 33:	0.02976065  	0.08196822  	0.06634612  
2023-06-02 09:24:47.879: [iter 34 : loss : 0.6212 = 0.1533 + 0.4635 + 0.0044, time: 21.367086]
2023-06-02 09:24:48.162: epoch 34:	0.02974585  	0.08205626  	0.06641930  
2023-06-02 09:25:09.653: [iter 35 : loss : 0.6120 = 0.1446 + 0.4628 + 0.0046, time: 21.487015]
2023-06-02 09:25:09.926: epoch 35:	0.02966440  	0.08161236  	0.06613888  
2023-06-02 09:25:31.006: [iter 36 : loss : 0.6053 = 0.1383 + 0.4623 + 0.0048, time: 21.074391]
2023-06-02 09:25:31.273: epoch 36:	0.02967920  	0.08164547  	0.06620061  
2023-06-02 09:25:52.417: [iter 37 : loss : 0.5990 = 0.1323 + 0.4617 + 0.0050, time: 21.141226]
2023-06-02 09:25:52.688: epoch 37:	0.02967920  	0.08165978  	0.06619359  
2023-06-02 09:26:13.981: [iter 38 : loss : 0.5923 = 0.1260 + 0.4611 + 0.0052, time: 21.289102]
2023-06-02 09:26:14.257: epoch 38:	0.02971621  	0.08157621  	0.06621541  
2023-06-02 09:26:35.582: [iter 39 : loss : 0.5873 = 0.1213 + 0.4607 + 0.0053, time: 21.321185]
2023-06-02 09:26:35.850: epoch 39:	0.02975322  	0.08138665  	0.06624742  
2023-06-02 09:26:57.202: [iter 40 : loss : 0.5815 = 0.1159 + 0.4601 + 0.0055, time: 21.347799]
2023-06-02 09:26:57.473: epoch 40:	0.02972361  	0.08131739  	0.06619499  
2023-06-02 09:27:18.821: [iter 41 : loss : 0.5765 = 0.1111 + 0.4597 + 0.0057, time: 21.345462]
2023-06-02 09:27:19.089: epoch 41:	0.02970140  	0.08156686  	0.06611052  
2023-06-02 09:27:40.373: [iter 42 : loss : 0.5723 = 0.1071 + 0.4593 + 0.0058, time: 21.278908]
2023-06-02 09:27:40.639: epoch 42:	0.02964958  	0.08128343  	0.06601284  
2023-06-02 09:28:01.966: [iter 43 : loss : 0.5682 = 0.1033 + 0.4589 + 0.0060, time: 21.324143]
2023-06-02 09:28:02.241: epoch 43:	0.02972361  	0.08143198  	0.06606323  
2023-06-02 09:28:23.762: [iter 44 : loss : 0.5653 = 0.1007 + 0.4585 + 0.0062, time: 21.516068]
2023-06-02 09:28:24.031: epoch 44:	0.02978285  	0.08139089  	0.06612376  
2023-06-02 09:28:45.539: [iter 45 : loss : 0.5612 = 0.0968 + 0.4581 + 0.0063, time: 21.504009]
2023-06-02 09:28:45.810: epoch 45:	0.02974583  	0.08124866  	0.06594546  
2023-06-02 09:29:07.539: [iter 46 : loss : 0.5576 = 0.0935 + 0.4576 + 0.0065, time: 21.726202]
2023-06-02 09:29:07.807: epoch 46:	0.02970141  	0.08113418  	0.06586181  
2023-06-02 09:29:29.554: [iter 47 : loss : 0.5545 = 0.0905 + 0.4575 + 0.0066, time: 21.743298]
2023-06-02 09:29:29.828: epoch 47:	0.02970881  	0.08103250  	0.06572030  
2023-06-02 09:29:51.382: [iter 48 : loss : 0.5521 = 0.0882 + 0.4572 + 0.0067, time: 21.548693]
2023-06-02 09:29:51.653: epoch 48:	0.02964958  	0.08069348  	0.06557139  
2023-06-02 09:30:13.105: [iter 49 : loss : 0.5494 = 0.0857 + 0.4568 + 0.0069, time: 21.449037]
2023-06-02 09:30:13.373: epoch 49:	0.02961996  	0.08040295  	0.06531917  
2023-06-02 09:30:36.280: [iter 50 : loss : 0.5474 = 0.0839 + 0.4565 + 0.0070, time: 22.903493]
2023-06-02 09:30:36.541: epoch 50:	0.02961996  	0.08056067  	0.06531984  
2023-06-02 09:31:01.290: my pid: 468
2023-06-02 09:31:01.290: model: model.general_recommender.SGL
2023-06-02 09:31:01.290: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 09:31:01.290: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 09:31:05.726: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 09:31:27.781: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.053756]
2023-06-02 09:31:28.064: epoch 1:	0.00122890  	0.00216991  	0.00187332  
2023-06-02 09:31:28.064: Find a better model.
2023-06-02 09:31:49.640: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 21.572335]
2023-06-02 09:31:49.968: epoch 2:	0.00175452  	0.00344043  	0.00278641  
2023-06-02 09:31:49.968: Find a better model.
2023-06-02 09:32:11.495: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 21.521636]
2023-06-02 09:32:11.787: epoch 3:	0.00194700  	0.00418832  	0.00323903  
2023-06-02 09:32:11.787: Find a better model.
2023-06-02 09:32:32.976: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.185272]
2023-06-02 09:32:33.274: epoch 4:	0.00215428  	0.00438038  	0.00337343  
2023-06-02 09:32:33.274: Find a better model.
2023-06-02 09:32:54.581: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.303357]
2023-06-02 09:32:54.874: epoch 5:	0.00245040  	0.00511253  	0.00405294  
2023-06-02 09:32:54.874: Find a better model.
2023-06-02 09:33:16.004: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 21.126889]
2023-06-02 09:33:16.302: epoch 6:	0.00315369  	0.00743910  	0.00582560  
2023-06-02 09:33:16.302: Find a better model.
2023-06-02 09:33:37.360: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 21.053573]
2023-06-02 09:33:37.644: epoch 7:	0.00362008  	0.00885998  	0.00723560  
2023-06-02 09:33:37.644: Find a better model.
2023-06-02 09:33:59.019: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 21.371559]
2023-06-02 09:33:59.312: epoch 8:	0.00419011  	0.01078786  	0.00872972  
2023-06-02 09:33:59.312: Find a better model.
2023-06-02 09:34:22.094: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 22.776977]
2023-06-02 09:34:22.424: epoch 9:	0.00490079  	0.01349663  	0.01042035  
2023-06-02 09:34:22.424: Find a better model.
2023-06-02 09:34:43.852: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.423387]
2023-06-02 09:34:44.162: epoch 10:	0.00579655  	0.01641091  	0.01310133  
2023-06-02 09:34:44.162: Find a better model.
2023-06-02 09:35:05.167: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 21.001582]
2023-06-02 09:35:05.455: epoch 11:	0.00655905  	0.01886783  	0.01502436  
2023-06-02 09:35:05.455: Find a better model.
2023-06-02 09:35:26.975: [iter 12 : loss : 1.1328 = 0.6881 + 0.4446 + 0.0000, time: 21.515347]
2023-06-02 09:35:27.274: epoch 12:	0.00785457  	0.02209222  	0.01827489  
2023-06-02 09:35:27.274: Find a better model.
2023-06-02 09:35:48.361: [iter 13 : loss : 1.1309 = 0.6855 + 0.4453 + 0.0000, time: 21.082854]
2023-06-02 09:35:48.644: epoch 13:	0.00976454  	0.02746569  	0.02342567  
2023-06-02 09:35:48.645: Find a better model.
2023-06-02 09:36:10.255: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 21.607314]
2023-06-02 09:36:10.540: epoch 14:	0.01261473  	0.03516808  	0.02978373  
2023-06-02 09:36:10.540: Find a better model.
2023-06-02 09:36:33.249: [iter 15 : loss : 1.1214 = 0.6743 + 0.4470 + 0.0001, time: 22.705056]
2023-06-02 09:36:33.552: epoch 15:	0.01605720  	0.04369726  	0.03736795  
2023-06-02 09:36:33.552: Find a better model.
2023-06-02 09:36:56.346: [iter 16 : loss : 1.1102 = 0.6621 + 0.4480 + 0.0002, time: 22.789628]
2023-06-02 09:36:56.645: epoch 16:	0.01947007  	0.05259404  	0.04507432  
2023-06-02 09:36:56.645: Find a better model.
2023-06-02 09:37:18.065: [iter 17 : loss : 1.0915 = 0.6420 + 0.4493 + 0.0002, time: 21.415965]
2023-06-02 09:37:18.350: epoch 17:	0.02259424  	0.06085765  	0.05161528  
2023-06-02 09:37:18.350: Find a better model.
2023-06-02 09:37:39.116: [iter 18 : loss : 1.0623 = 0.6109 + 0.4510 + 0.0004, time: 20.761075]
2023-06-02 09:37:39.391: epoch 18:	0.02515578  	0.06669180  	0.05683055  
2023-06-02 09:37:39.392: Find a better model.
2023-06-02 09:38:00.356: [iter 19 : loss : 1.0231 = 0.5693 + 0.4532 + 0.0006, time: 20.961688]
2023-06-02 09:38:00.633: epoch 19:	0.02690296  	0.07158468  	0.06081092  
2023-06-02 09:38:00.633: Find a better model.
2023-06-02 09:38:22.056: [iter 20 : loss : 0.9765 = 0.5196 + 0.4561 + 0.0008, time: 21.418260]
2023-06-02 09:38:22.344: epoch 20:	0.02771733  	0.07435434  	0.06299260  
2023-06-02 09:38:22.344: Find a better model.
2023-06-02 09:38:45.547: [iter 21 : loss : 0.9269 = 0.4667 + 0.4591 + 0.0011, time: 23.199452]
2023-06-02 09:38:45.855: epoch 21:	0.02853169  	0.07679190  	0.06405269  
2023-06-02 09:38:45.855: Find a better model.
2023-06-02 09:39:08.918: [iter 22 : loss : 0.8784 = 0.4149 + 0.4622 + 0.0014, time: 23.057651]
2023-06-02 09:39:09.217: epoch 22:	0.02904252  	0.07842766  	0.06506921  
2023-06-02 09:39:09.217: Find a better model.
2023-06-02 09:39:30.109: [iter 23 : loss : 0.8355 = 0.3693 + 0.4646 + 0.0017, time: 20.888309]
2023-06-02 09:39:30.376: epoch 23:	0.02895368  	0.07852811  	0.06515842  
2023-06-02 09:39:30.376: Find a better model.
2023-06-02 09:39:51.346: [iter 24 : loss : 0.7981 = 0.3300 + 0.4662 + 0.0020, time: 20.966063]
2023-06-02 09:39:51.618: epoch 24:	0.02910174  	0.07977447  	0.06575970  
2023-06-02 09:39:51.618: Find a better model.
2023-06-02 09:40:12.704: [iter 25 : loss : 0.7657 = 0.2964 + 0.4670 + 0.0023, time: 21.083026]
2023-06-02 09:40:12.970: epoch 25:	0.02934604  	0.08009591  	0.06617700  
2023-06-02 09:40:12.970: Find a better model.
2023-06-02 09:40:33.866: [iter 26 : loss : 0.7392 = 0.2691 + 0.4675 + 0.0025, time: 20.893115]
2023-06-02 09:40:34.143: epoch 26:	0.02950892  	0.08068401  	0.06658234  
2023-06-02 09:40:34.143: Find a better model.
2023-06-02 09:40:55.044: [iter 27 : loss : 0.7159 = 0.2457 + 0.4674 + 0.0028, time: 20.896253]
2023-06-02 09:40:55.312: epoch 27:	0.02969399  	0.08123270  	0.06678973  
2023-06-02 09:40:55.312: Find a better model.
2023-06-02 09:41:16.747: [iter 28 : loss : 0.6964 = 0.2262 + 0.4671 + 0.0031, time: 21.432074]
2023-06-02 09:41:17.017: epoch 28:	0.02970139  	0.08055325  	0.06665387  
2023-06-02 09:41:39.246: [iter 29 : loss : 0.6798 = 0.2099 + 0.4666 + 0.0033, time: 22.224288]
2023-06-02 09:41:39.523: epoch 29:	0.02984205  	0.08121081  	0.06706791  
2023-06-02 09:42:01.445: [iter 30 : loss : 0.6646 = 0.1949 + 0.4661 + 0.0035, time: 21.918062]
2023-06-02 09:42:01.761: epoch 30:	0.02981244  	0.08081283  	0.06714738  
2023-06-02 09:42:25.641: [iter 31 : loss : 0.6514 = 0.1822 + 0.4654 + 0.0038, time: 23.876845]
2023-06-02 09:42:25.935: epoch 31:	0.02976062  	0.08082820  	0.06727858  
2023-06-02 09:42:47.299: [iter 32 : loss : 0.6404 = 0.1715 + 0.4648 + 0.0040, time: 21.359150]
2023-06-02 09:42:47.565: epoch 32:	0.02978284  	0.08107285  	0.06737225  
2023-06-02 09:43:08.631: [iter 33 : loss : 0.6304 = 0.1621 + 0.4641 + 0.0042, time: 21.063090]
2023-06-02 09:43:08.898: epoch 33:	0.02987169  	0.08133224  	0.06740630  
2023-06-02 09:43:08.898: Find a better model.
2023-06-02 09:43:30.068: [iter 34 : loss : 0.6212 = 0.1534 + 0.4634 + 0.0044, time: 21.166066]
2023-06-02 09:43:30.342: epoch 34:	0.02974583  	0.08042995  	0.06721429  
2023-06-02 09:43:51.467: [iter 35 : loss : 0.6122 = 0.1448 + 0.4628 + 0.0046, time: 21.121055]
2023-06-02 09:43:51.733: epoch 35:	0.02974583  	0.08050498  	0.06732053  
2023-06-02 09:44:12.877: [iter 36 : loss : 0.6053 = 0.1383 + 0.4622 + 0.0048, time: 21.140197]
2023-06-02 09:44:13.157: epoch 36:	0.02967919  	0.08033615  	0.06723626  
2023-06-02 09:44:34.476: [iter 37 : loss : 0.5986 = 0.1319 + 0.4617 + 0.0050, time: 21.314289]
2023-06-02 09:44:34.747: epoch 37:	0.02977544  	0.08045335  	0.06736780  
2023-06-02 09:44:56.017: [iter 38 : loss : 0.5925 = 0.1261 + 0.4612 + 0.0052, time: 21.266209]
2023-06-02 09:44:56.287: epoch 38:	0.02984948  	0.08073042  	0.06752525  
2023-06-02 09:45:17.419: [iter 39 : loss : 0.5874 = 0.1214 + 0.4607 + 0.0053, time: 21.129030]
2023-06-02 09:45:17.693: epoch 39:	0.02986428  	0.08067319  	0.06758617  
2023-06-02 09:45:39.009: [iter 40 : loss : 0.5819 = 0.1162 + 0.4601 + 0.0055, time: 21.311363]
2023-06-02 09:45:39.282: epoch 40:	0.02987169  	0.08066162  	0.06759914  
2023-06-02 09:46:00.402: [iter 41 : loss : 0.5766 = 0.1113 + 0.4596 + 0.0057, time: 21.117587]
2023-06-02 09:46:00.655: epoch 41:	0.02981986  	0.08037348  	0.06753083  
2023-06-02 09:46:21.826: [iter 42 : loss : 0.5723 = 0.1072 + 0.4593 + 0.0058, time: 21.167627]
2023-06-02 09:46:22.092: epoch 42:	0.02980505  	0.08000573  	0.06740794  
2023-06-02 09:46:43.570: [iter 43 : loss : 0.5683 = 0.1034 + 0.4589 + 0.0060, time: 21.475501]
2023-06-02 09:46:43.834: epoch 43:	0.02970142  	0.07974401  	0.06720579  
2023-06-02 09:47:05.217: [iter 44 : loss : 0.5656 = 0.1010 + 0.4585 + 0.0062, time: 21.378664]
2023-06-02 09:47:05.482: epoch 44:	0.02964959  	0.07971623  	0.06725718  
2023-06-02 09:47:26.953: [iter 45 : loss : 0.5613 = 0.0970 + 0.4580 + 0.0063, time: 21.468489]
2023-06-02 09:47:27.221: epoch 45:	0.02964219  	0.07935584  	0.06706111  
2023-06-02 09:47:48.563: [iter 46 : loss : 0.5580 = 0.0938 + 0.4578 + 0.0065, time: 21.336694]
2023-06-02 09:47:48.836: epoch 46:	0.02958295  	0.07902364  	0.06695653  
2023-06-02 09:48:10.128: [iter 47 : loss : 0.5548 = 0.0908 + 0.4574 + 0.0066, time: 21.289084]
2023-06-02 09:48:10.403: epoch 47:	0.02959036  	0.07925541  	0.06686635  
2023-06-02 09:48:31.764: [iter 48 : loss : 0.5521 = 0.0884 + 0.4570 + 0.0067, time: 21.357058]
2023-06-02 09:48:32.030: epoch 48:	0.02944229  	0.07890196  	0.06684254  
2023-06-02 09:48:53.531: [iter 49 : loss : 0.5496 = 0.0859 + 0.4567 + 0.0069, time: 21.496035]
2023-06-02 09:48:53.796: epoch 49:	0.02937566  	0.07846253  	0.06654038  
2023-06-02 09:49:15.129: [iter 50 : loss : 0.5472 = 0.0837 + 0.4565 + 0.0070, time: 21.330077]
2023-06-02 09:49:15.394: epoch 50:	0.02945709  	0.07864994  	0.06653735  
2023-06-02 09:49:36.538: [iter 51 : loss : 0.5443 = 0.0810 + 0.4562 + 0.0072, time: 21.140107]
2023-06-02 09:49:36.804: epoch 51:	0.02930162  	0.07855017  	0.06639283  
2023-06-02 09:49:58.329: [iter 52 : loss : 0.5419 = 0.0787 + 0.4559 + 0.0073, time: 21.522202]
2023-06-02 09:49:58.594: epoch 52:	0.02923499  	0.07839841  	0.06617761  
2023-06-02 09:50:19.742: [iter 53 : loss : 0.5406 = 0.0774 + 0.4558 + 0.0074, time: 21.144084]
2023-06-02 09:50:20.005: epoch 53:	0.02915356  	0.07805774  	0.06600713  
2023-06-02 09:50:41.297: [iter 54 : loss : 0.5386 = 0.0756 + 0.4555 + 0.0075, time: 21.289064]
2023-06-02 09:50:41.563: epoch 54:	0.02915355  	0.07766613  	0.06586415  
2023-06-02 09:51:02.894: [iter 55 : loss : 0.5364 = 0.0735 + 0.4553 + 0.0077, time: 21.327593]
2023-06-02 09:51:03.172: epoch 55:	0.02905731  	0.07728239  	0.06557981  
2023-06-02 09:51:24.323: [iter 56 : loss : 0.5345 = 0.0716 + 0.4552 + 0.0078, time: 21.147050]
2023-06-02 09:51:24.588: epoch 56:	0.02916095  	0.07753222  	0.06570716  
2023-06-02 09:51:45.880: [iter 57 : loss : 0.5326 = 0.0699 + 0.4548 + 0.0079, time: 21.287521]
2023-06-02 09:51:46.158: epoch 57:	0.02905731  	0.07677154  	0.06532519  
2023-06-02 09:52:07.295: [iter 58 : loss : 0.5310 = 0.0681 + 0.4548 + 0.0080, time: 21.132560]
2023-06-02 09:52:07.561: epoch 58:	0.02907212  	0.07651243  	0.06525774  
2023-06-02 09:52:07.561: Early stopping is trigger at epoch: 58
2023-06-02 09:52:07.561: best_result@epoch 33:

2023-06-02 09:52:07.561: 		0.0299      	0.0813      	0.0674      
2023-06-02 10:02:01.910: my pid: 11900
2023-06-02 10:02:01.910: model: model.general_recommender.SGL
2023-06-02 10:02:01.910: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 10:02:01.910: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 10:02:06.045: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 10:02:27.807: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.762586]
2023-06-02 10:02:28.073: epoch 1:	0.00140657  	0.00299336  	0.00225154  
2023-06-02 10:02:28.073: Find a better model.
2023-06-02 10:02:51.077: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 23.000319]
2023-06-02 10:02:51.399: epoch 2:	0.00152502  	0.00307791  	0.00236942  
2023-06-02 10:02:51.400: Find a better model.
2023-06-02 10:03:12.773: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.369313]
2023-06-02 10:03:13.070: epoch 3:	0.00199141  	0.00403465  	0.00328491  
2023-06-02 10:03:13.070: Find a better model.
2023-06-02 10:03:34.031: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.956271]
2023-06-02 10:03:34.324: epoch 4:	0.00230234  	0.00499849  	0.00364493  
2023-06-02 10:03:34.324: Find a better model.
2023-06-02 10:03:55.262: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 20.934641]
2023-06-02 10:03:55.565: epoch 5:	0.00256144  	0.00571936  	0.00456596  
2023-06-02 10:03:55.566: Find a better model.
2023-06-02 10:04:16.255: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.685181]
2023-06-02 10:04:16.564: epoch 6:	0.00323512  	0.00779840  	0.00599702  
2023-06-02 10:04:16.564: Find a better model.
2023-06-02 10:04:37.236: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.668057]
2023-06-02 10:04:37.541: epoch 7:	0.00353124  	0.00879145  	0.00688616  
2023-06-02 10:04:37.541: Find a better model.
2023-06-02 10:04:58.225: [iter 8 : loss : 1.1347 = 0.6920 + 0.4426 + 0.0000, time: 20.679404]
2023-06-02 10:04:58.558: epoch 8:	0.00428635  	0.01190176  	0.00898705  
2023-06-02 10:04:58.559: Find a better model.
2023-06-02 10:05:20.620: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 22.056938]
2023-06-02 10:05:20.941: epoch 9:	0.00533756  	0.01555538  	0.01163929  
2023-06-02 10:05:20.942: Find a better model.
2023-06-02 10:05:44.029: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 23.082140]
2023-06-02 10:05:44.341: epoch 10:	0.00627033  	0.01755355  	0.01418677  
2023-06-02 10:05:44.341: Find a better model.
2023-06-02 10:06:05.394: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 21.048117]
2023-06-02 10:06:05.677: epoch 11:	0.00698842  	0.02043721  	0.01652805  
2023-06-02 10:06:05.677: Find a better model.
2023-06-02 10:06:26.382: [iter 12 : loss : 1.1328 = 0.6882 + 0.4446 + 0.0000, time: 20.701031]
2023-06-02 10:06:26.666: epoch 12:	0.00830614  	0.02462903  	0.02018881  
2023-06-02 10:06:26.667: Find a better model.
2023-06-02 10:06:47.417: [iter 13 : loss : 1.1309 = 0.6855 + 0.4453 + 0.0000, time: 20.745807]
2023-06-02 10:06:47.703: epoch 13:	0.01045303  	0.03098409  	0.02597199  
2023-06-02 10:06:47.703: Find a better model.
2023-06-02 10:07:08.365: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 20.658049]
2023-06-02 10:07:08.643: epoch 14:	0.01348091  	0.03968832  	0.03322466  
2023-06-02 10:07:08.643: Find a better model.
2023-06-02 10:07:30.302: [iter 15 : loss : 1.1213 = 0.6742 + 0.4470 + 0.0001, time: 21.654257]
2023-06-02 10:07:30.622: epoch 15:	0.01661246  	0.04788867  	0.04041047  
2023-06-02 10:07:30.622: Find a better model.
2023-06-02 10:07:51.981: [iter 16 : loss : 1.1101 = 0.6619 + 0.4481 + 0.0002, time: 21.353091]
2023-06-02 10:07:52.286: epoch 16:	0.02025482  	0.05575792  	0.04764320  
2023-06-02 10:07:52.286: Find a better model.
2023-06-02 10:08:14.823: [iter 17 : loss : 1.0911 = 0.6415 + 0.4493 + 0.0002, time: 22.533175]
2023-06-02 10:08:15.125: epoch 17:	0.02331240  	0.06432472  	0.05427405  
2023-06-02 10:08:15.125: Find a better model.
2023-06-02 10:08:36.341: [iter 18 : loss : 1.0617 = 0.6101 + 0.4511 + 0.0004, time: 21.210974]
2023-06-02 10:08:36.629: epoch 18:	0.02566661  	0.07085041  	0.05901079  
2023-06-02 10:08:36.629: Find a better model.
2023-06-02 10:08:57.767: [iter 19 : loss : 1.0220 = 0.5681 + 0.4534 + 0.0006, time: 21.133729]
2023-06-02 10:08:58.039: epoch 19:	0.02749522  	0.07563528  	0.06271261  
2023-06-02 10:08:58.040: Find a better model.
2023-06-02 10:09:19.014: [iter 20 : loss : 0.9752 = 0.5179 + 0.4564 + 0.0008, time: 20.970671]
2023-06-02 10:09:19.307: epoch 20:	0.02850205  	0.07805903  	0.06455854  
2023-06-02 10:09:19.307: Find a better model.
2023-06-02 10:09:41.361: [iter 21 : loss : 0.9254 = 0.4646 + 0.4596 + 0.0011, time: 22.049330]
2023-06-02 10:09:41.639: epoch 21:	0.02911651  	0.08015134  	0.06580497  
2023-06-02 10:09:41.639: Find a better model.
2023-06-02 10:10:02.723: [iter 22 : loss : 0.8768 = 0.4126 + 0.4628 + 0.0014, time: 21.080278]
2023-06-02 10:10:02.996: epoch 22:	0.02937564  	0.08134415  	0.06647135  
2023-06-02 10:10:02.996: Find a better model.
2023-06-02 10:10:24.133: [iter 23 : loss : 0.8339 = 0.3670 + 0.4652 + 0.0017, time: 21.133292]
2023-06-02 10:10:24.401: epoch 23:	0.02950150  	0.08155162  	0.06670175  
2023-06-02 10:10:24.401: Find a better model.
2023-06-02 10:10:45.505: [iter 24 : loss : 0.7966 = 0.3278 + 0.4668 + 0.0020, time: 21.099847]
2023-06-02 10:10:45.774: epoch 24:	0.02974582  	0.08217920  	0.06717723  
2023-06-02 10:10:45.774: Find a better model.
2023-06-02 10:11:06.947: [iter 25 : loss : 0.7647 = 0.2949 + 0.4676 + 0.0023, time: 21.169719]
2023-06-02 10:11:07.213: epoch 25:	0.02990869  	0.08286492  	0.06757877  
2023-06-02 10:11:07.213: Find a better model.
2023-06-02 10:11:28.484: [iter 26 : loss : 0.7380 = 0.2677 + 0.4678 + 0.0025, time: 21.268625]
2023-06-02 10:11:28.756: epoch 26:	0.02993090  	0.08281407  	0.06787310  
2023-06-02 10:11:50.221: [iter 27 : loss : 0.7151 = 0.2445 + 0.4678 + 0.0028, time: 21.462039]
2023-06-02 10:11:50.491: epoch 27:	0.02994571  	0.08262491  	0.06790440  
2023-06-02 10:12:11.646: [iter 28 : loss : 0.6957 = 0.2253 + 0.4673 + 0.0031, time: 21.151252]
2023-06-02 10:12:11.912: epoch 28:	0.03009376  	0.08299335  	0.06814259  
2023-06-02 10:12:11.912: Find a better model.
2023-06-02 10:12:33.104: [iter 29 : loss : 0.6789 = 0.2088 + 0.4669 + 0.0033, time: 21.187035]
2023-06-02 10:12:33.376: epoch 29:	0.03022701  	0.08316153  	0.06838496  
2023-06-02 10:12:33.377: Find a better model.
2023-06-02 10:12:54.665: [iter 30 : loss : 0.6640 = 0.1942 + 0.4662 + 0.0036, time: 21.285313]
2023-06-02 10:12:54.933: epoch 30:	0.03021221  	0.08363301  	0.06850756  
2023-06-02 10:12:54.933: Find a better model.
2023-06-02 10:13:16.245: [iter 31 : loss : 0.6510 = 0.1816 + 0.4656 + 0.0038, time: 21.308561]
2023-06-02 10:13:16.521: epoch 31:	0.03016039  	0.08342300  	0.06860696  
2023-06-02 10:13:37.800: [iter 32 : loss : 0.6397 = 0.1707 + 0.4650 + 0.0040, time: 21.268142]
2023-06-02 10:13:38.066: epoch 32:	0.03027884  	0.08362702  	0.06856000  
2023-06-02 10:13:59.233: [iter 33 : loss : 0.6302 = 0.1617 + 0.4643 + 0.0042, time: 21.163439]
2023-06-02 10:13:59.500: epoch 33:	0.03036027  	0.08376633  	0.06865884  
2023-06-02 10:13:59.500: Find a better model.
2023-06-02 10:14:20.675: [iter 34 : loss : 0.6212 = 0.1532 + 0.4636 + 0.0044, time: 21.171633]
2023-06-02 10:14:20.941: epoch 34:	0.03039729  	0.08391269  	0.06882233  
2023-06-02 10:14:20.942: Find a better model.
2023-06-02 10:14:42.201: [iter 35 : loss : 0.6120 = 0.1446 + 0.4628 + 0.0046, time: 21.256098]
2023-06-02 10:14:42.468: epoch 35:	0.03026403  	0.08323742  	0.06844841  
2023-06-02 10:15:03.618: [iter 36 : loss : 0.6050 = 0.1379 + 0.4623 + 0.0048, time: 21.145251]
2023-06-02 10:15:03.884: epoch 36:	0.03041210  	0.08336988  	0.06858436  
2023-06-02 10:15:25.253: [iter 37 : loss : 0.5988 = 0.1320 + 0.4618 + 0.0050, time: 21.365472]
2023-06-02 10:15:25.536: epoch 37:	0.03039729  	0.08337999  	0.06866056  
2023-06-02 10:15:46.985: [iter 38 : loss : 0.5925 = 0.1260 + 0.4613 + 0.0052, time: 21.445237]
2023-06-02 10:15:47.255: epoch 38:	0.03028625  	0.08326378  	0.06857798  
2023-06-02 10:16:08.616: [iter 39 : loss : 0.5870 = 0.1210 + 0.4606 + 0.0053, time: 21.356996]
2023-06-02 10:16:08.883: epoch 39:	0.03032327  	0.08347096  	0.06866047  
2023-06-02 10:16:30.176: [iter 40 : loss : 0.5814 = 0.1157 + 0.4601 + 0.0055, time: 21.289006]
2023-06-02 10:16:30.442: epoch 40:	0.03025663  	0.08298032  	0.06864504  
2023-06-02 10:16:51.934: [iter 41 : loss : 0.5761 = 0.1108 + 0.4596 + 0.0057, time: 21.489235]
2023-06-02 10:16:52.200: epoch 41:	0.03020481  	0.08266718  	0.06853183  
2023-06-02 10:17:13.587: [iter 42 : loss : 0.5719 = 0.1067 + 0.4593 + 0.0058, time: 21.383432]
2023-06-02 10:17:13.859: epoch 42:	0.03015299  	0.08240113  	0.06852999  
2023-06-02 10:17:35.137: [iter 43 : loss : 0.5680 = 0.1030 + 0.4589 + 0.0060, time: 21.273945]
2023-06-02 10:17:35.405: epoch 43:	0.03011597  	0.08209968  	0.06854679  
2023-06-02 10:17:56.942: [iter 44 : loss : 0.5650 = 0.1004 + 0.4585 + 0.0061, time: 21.533006]
2023-06-02 10:17:57.210: epoch 44:	0.03013818  	0.08185298  	0.06858633  
2023-06-02 10:18:18.569: [iter 45 : loss : 0.5608 = 0.0964 + 0.4581 + 0.0063, time: 21.355861]
2023-06-02 10:18:18.838: epoch 45:	0.03003454  	0.08188897  	0.06845018  
2023-06-02 10:18:39.947: [iter 46 : loss : 0.5577 = 0.0936 + 0.4577 + 0.0065, time: 21.104337]
2023-06-02 10:18:40.213: epoch 46:	0.02991608  	0.08177156  	0.06815214  
2023-06-02 10:19:01.586: [iter 47 : loss : 0.5546 = 0.0905 + 0.4575 + 0.0066, time: 21.369053]
2023-06-02 10:19:01.850: epoch 47:	0.02985686  	0.08148836  	0.06823196  
2023-06-02 10:19:23.397: [iter 48 : loss : 0.5518 = 0.0880 + 0.4571 + 0.0067, time: 21.543473]
2023-06-02 10:19:23.764: epoch 48:	0.02985686  	0.08115126  	0.06812929  
2023-06-02 10:19:45.756: [iter 49 : loss : 0.5490 = 0.0853 + 0.4567 + 0.0069, time: 21.987588]
2023-06-02 10:19:46.080: epoch 49:	0.02979763  	0.08109733  	0.06811468  
2023-06-02 10:20:09.684: [iter 50 : loss : 0.5470 = 0.0835 + 0.4565 + 0.0070, time: 23.599281]
2023-06-02 10:20:09.982: epoch 50:	0.02973099  	0.08122560  	0.06805591  
2023-06-02 10:20:39.142: my pid: 15480
2023-06-02 10:20:39.142: model: model.general_recommender.SGL
2023-06-02 10:20:39.142: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 10:20:39.142: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 10:20:43.443: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 10:21:05.780: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 22.337273]
2023-06-02 10:21:06.103: epoch 1:	0.00141398  	0.00265888  	0.00216035  
2023-06-02 10:21:06.104: Find a better model.
2023-06-02 10:21:27.721: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 21.614206]
2023-06-02 10:21:28.009: epoch 2:	0.00168789  	0.00344275  	0.00292297  
2023-06-02 10:21:28.010: Find a better model.
2023-06-02 10:21:51.284: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 23.270902]
2023-06-02 10:21:51.627: epoch 3:	0.00179153  	0.00351075  	0.00301484  
2023-06-02 10:21:51.627: Find a better model.
2023-06-02 10:22:14.653: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 23.022130]
2023-06-02 10:22:14.970: epoch 4:	0.00236157  	0.00523656  	0.00398059  
2023-06-02 10:22:14.970: Find a better model.
2023-06-02 10:22:36.111: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.136567]
2023-06-02 10:22:36.403: epoch 5:	0.00297601  	0.00698261  	0.00514237  
2023-06-02 10:22:36.403: Find a better model.
2023-06-02 10:22:57.281: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 20.873351]
2023-06-02 10:22:57.585: epoch 6:	0.00333136  	0.00794641  	0.00605731  
2023-06-02 10:22:57.585: Find a better model.
2023-06-02 10:23:18.245: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 20.654833]
2023-06-02 10:23:18.544: epoch 7:	0.00398283  	0.00991560  	0.00786426  
2023-06-02 10:23:18.545: Find a better model.
2023-06-02 10:23:39.309: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 20.756383]
2023-06-02 10:23:39.608: epoch 8:	0.00440480  	0.01178193  	0.00904360  
2023-06-02 10:23:39.608: Find a better model.
2023-06-02 10:24:00.261: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.648695]
2023-06-02 10:24:00.552: epoch 9:	0.00523393  	0.01480255  	0.01100769  
2023-06-02 10:24:00.553: Find a better model.
2023-06-02 10:24:21.308: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.751999]
2023-06-02 10:24:21.606: epoch 10:	0.00587058  	0.01625881  	0.01293033  
2023-06-02 10:24:21.606: Find a better model.
2023-06-02 10:24:42.241: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 20.632326]
2023-06-02 10:24:42.525: epoch 11:	0.00671452  	0.01872197  	0.01570920  
2023-06-02 10:24:42.525: Find a better model.
2023-06-02 10:25:03.267: [iter 12 : loss : 1.1329 = 0.6882 + 0.4446 + 0.0000, time: 20.732666]
2023-06-02 10:25:03.564: epoch 12:	0.00837277  	0.02474654  	0.02029174  
2023-06-02 10:25:03.564: Find a better model.
2023-06-02 10:25:24.219: [iter 13 : loss : 1.1309 = 0.6857 + 0.4452 + 0.0000, time: 20.651126]
2023-06-02 10:25:24.499: epoch 13:	0.01066773  	0.03021967  	0.02556509  
2023-06-02 10:25:24.499: Find a better model.
2023-06-02 10:25:45.256: [iter 14 : loss : 1.1277 = 0.6816 + 0.4460 + 0.0001, time: 20.753093]
2023-06-02 10:25:45.539: epoch 14:	0.01343649  	0.03937749  	0.03275287  
2023-06-02 10:25:45.539: Find a better model.
2023-06-02 10:26:06.018: [iter 15 : loss : 1.1219 = 0.6748 + 0.4470 + 0.0001, time: 20.469034]
2023-06-02 10:26:06.307: epoch 15:	0.01660506  	0.04779766  	0.03947357  
2023-06-02 10:26:06.307: Find a better model.
2023-06-02 10:26:26.803: [iter 16 : loss : 1.1110 = 0.6629 + 0.4479 + 0.0002, time: 20.492341]
2023-06-02 10:26:27.083: epoch 16:	0.01954412  	0.05436902  	0.04606250  
2023-06-02 10:26:27.083: Find a better model.
2023-06-02 10:26:47.568: [iter 17 : loss : 1.0929 = 0.6435 + 0.4491 + 0.0002, time: 20.480421]
2023-06-02 10:26:47.842: epoch 17:	0.02294961  	0.06342914  	0.05304473  
2023-06-02 10:26:47.842: Find a better model.
2023-06-02 10:27:08.420: [iter 18 : loss : 1.0642 = 0.6131 + 0.4508 + 0.0004, time: 20.573406]
2023-06-02 10:27:08.707: epoch 18:	0.02549633  	0.06992915  	0.05824089  
2023-06-02 10:27:08.707: Find a better model.
2023-06-02 10:27:29.340: [iter 19 : loss : 1.0256 = 0.5720 + 0.4531 + 0.0006, time: 20.629218]
2023-06-02 10:27:29.627: epoch 19:	0.02731752  	0.07385626  	0.06182798  
2023-06-02 10:27:29.627: Find a better model.
2023-06-02 10:27:50.188: [iter 20 : loss : 0.9791 = 0.5225 + 0.4558 + 0.0008, time: 20.556142]
2023-06-02 10:27:50.471: epoch 20:	0.02828737  	0.07748882  	0.06377251  
2023-06-02 10:27:50.471: Find a better model.
2023-06-02 10:28:11.576: [iter 21 : loss : 0.9296 = 0.4696 + 0.4590 + 0.0011, time: 21.100793]
2023-06-02 10:28:11.852: epoch 21:	0.02887963  	0.07997102  	0.06531498  
2023-06-02 10:28:11.852: Find a better model.
2023-06-02 10:28:33.137: [iter 22 : loss : 0.8807 = 0.4172 + 0.4622 + 0.0014, time: 21.279305]
2023-06-02 10:28:33.414: epoch 22:	0.02896848  	0.08045067  	0.06579827  
2023-06-02 10:28:33.414: Find a better model.
2023-06-02 10:28:54.743: [iter 23 : loss : 0.8373 = 0.3710 + 0.4646 + 0.0017, time: 21.325466]
2023-06-02 10:28:55.016: epoch 23:	0.02920538  	0.08094564  	0.06599773  
2023-06-02 10:28:55.016: Find a better model.
2023-06-02 10:29:16.304: [iter 24 : loss : 0.7994 = 0.3312 + 0.4662 + 0.0020, time: 21.284521]
2023-06-02 10:29:16.591: epoch 24:	0.02950151  	0.08226912  	0.06639007  
2023-06-02 10:29:16.591: Find a better model.
2023-06-02 10:29:37.719: [iter 25 : loss : 0.7669 = 0.2975 + 0.4672 + 0.0022, time: 21.124579]
2023-06-02 10:29:37.997: epoch 25:	0.02969400  	0.08261254  	0.06646287  
2023-06-02 10:29:37.997: Find a better model.
2023-06-02 10:29:59.121: [iter 26 : loss : 0.7398 = 0.2697 + 0.4675 + 0.0025, time: 21.119263]
2023-06-02 10:29:59.393: epoch 26:	0.02984947  	0.08370449  	0.06692939  
2023-06-02 10:29:59.393: Find a better model.
2023-06-02 10:30:20.522: [iter 27 : loss : 0.7168 = 0.2464 + 0.4676 + 0.0028, time: 21.124133]
2023-06-02 10:30:20.793: epoch 27:	0.02987909  	0.08341775  	0.06706353  
2023-06-02 10:30:42.043: [iter 28 : loss : 0.6971 = 0.2268 + 0.4672 + 0.0031, time: 21.247052]
2023-06-02 10:30:42.315: epoch 28:	0.02987169  	0.08357936  	0.06722345  
2023-06-02 10:31:03.320: [iter 29 : loss : 0.6802 = 0.2103 + 0.4666 + 0.0033, time: 21.000204]
2023-06-02 10:31:03.601: epoch 29:	0.02987909  	0.08352292  	0.06745680  
2023-06-02 10:31:24.640: [iter 30 : loss : 0.6650 = 0.1955 + 0.4660 + 0.0035, time: 21.035106]
2023-06-02 10:31:24.907: epoch 30:	0.02977544  	0.08343820  	0.06752332  
2023-06-02 10:31:46.063: [iter 31 : loss : 0.6519 = 0.1828 + 0.4654 + 0.0038, time: 21.152154]
2023-06-02 10:31:46.331: epoch 31:	0.02972362  	0.08371943  	0.06772333  
2023-06-02 10:31:46.331: Find a better model.
2023-06-02 10:32:07.415: [iter 32 : loss : 0.6403 = 0.1716 + 0.4647 + 0.0040, time: 21.079017]
2023-06-02 10:32:07.688: epoch 32:	0.02984947  	0.08414643  	0.06787715  
2023-06-02 10:32:07.688: Find a better model.
2023-06-02 10:32:28.824: [iter 33 : loss : 0.6305 = 0.1622 + 0.4640 + 0.0042, time: 21.131099]
2023-06-02 10:32:29.094: epoch 33:	0.02998273  	0.08464321  	0.06812143  
2023-06-02 10:32:29.094: Find a better model.
2023-06-02 10:32:50.434: [iter 34 : loss : 0.6213 = 0.1535 + 0.4635 + 0.0044, time: 21.334696]
2023-06-02 10:32:50.705: epoch 34:	0.03004196  	0.08494115  	0.06821403  
2023-06-02 10:32:50.705: Find a better model.
2023-06-02 10:33:11.806: [iter 35 : loss : 0.6124 = 0.1451 + 0.4627 + 0.0046, time: 21.096503]
2023-06-02 10:33:12.076: epoch 35:	0.03019002  	0.08483735  	0.06822462  
2023-06-02 10:33:33.418: [iter 36 : loss : 0.6054 = 0.1385 + 0.4621 + 0.0048, time: 21.338016]
2023-06-02 10:33:33.689: epoch 36:	0.03019741  	0.08467534  	0.06825495  
2023-06-02 10:33:54.977: [iter 37 : loss : 0.5986 = 0.1321 + 0.4615 + 0.0050, time: 21.283234]
2023-06-02 10:33:55.245: epoch 37:	0.03021223  	0.08453421  	0.06830443  
2023-06-02 10:34:16.623: [iter 38 : loss : 0.5927 = 0.1265 + 0.4610 + 0.0052, time: 21.375077]
2023-06-02 10:34:16.892: epoch 38:	0.03009378  	0.08396193  	0.06812350  
2023-06-02 10:34:38.193: [iter 39 : loss : 0.5871 = 0.1213 + 0.4604 + 0.0053, time: 21.296027]
2023-06-02 10:34:38.462: epoch 39:	0.03003455  	0.08349021  	0.06791975  
2023-06-02 10:35:00.027: [iter 40 : loss : 0.5816 = 0.1161 + 0.4600 + 0.0055, time: 21.562735]
2023-06-02 10:35:00.289: epoch 40:	0.03001235  	0.08343720  	0.06782569  
2023-06-02 10:35:21.769: [iter 41 : loss : 0.5763 = 0.1112 + 0.4595 + 0.0057, time: 21.475844]
2023-06-02 10:35:22.040: epoch 41:	0.02989390  	0.08316919  	0.06770401  
2023-06-02 10:35:43.408: [iter 42 : loss : 0.5720 = 0.1070 + 0.4591 + 0.0058, time: 21.363051]
2023-06-02 10:35:43.676: epoch 42:	0.02996053  	0.08295391  	0.06781412  
2023-06-02 10:36:05.134: [iter 43 : loss : 0.5679 = 0.1031 + 0.4588 + 0.0060, time: 21.453727]
2023-06-02 10:36:05.402: epoch 43:	0.03001975  	0.08322639  	0.06795561  
2023-06-02 10:36:26.778: [iter 44 : loss : 0.5652 = 0.1007 + 0.4584 + 0.0061, time: 21.373317]
2023-06-02 10:36:27.048: epoch 44:	0.02990871  	0.08276766  	0.06783064  
2023-06-02 10:36:48.334: [iter 45 : loss : 0.5610 = 0.0967 + 0.4579 + 0.0063, time: 21.280482]
2023-06-02 10:36:48.612: epoch 45:	0.02986429  	0.08259953  	0.06786972  
2023-06-02 10:37:10.157: [iter 46 : loss : 0.5576 = 0.0935 + 0.4576 + 0.0064, time: 21.542546]
2023-06-02 10:37:10.428: epoch 46:	0.02968661  	0.08209639  	0.06757037  
2023-06-02 10:37:31.952: [iter 47 : loss : 0.5546 = 0.0907 + 0.4573 + 0.0066, time: 21.519727]
2023-06-02 10:37:32.218: epoch 47:	0.02954594  	0.08190743  	0.06747498  
2023-06-02 10:37:53.536: [iter 48 : loss : 0.5516 = 0.0879 + 0.4570 + 0.0067, time: 21.314590]
2023-06-02 10:37:53.806: epoch 48:	0.02959036  	0.08221424  	0.06748845  
2023-06-02 10:38:15.178: [iter 49 : loss : 0.5493 = 0.0859 + 0.4566 + 0.0069, time: 21.368866]
2023-06-02 10:38:15.444: epoch 49:	0.02945710  	0.08171987  	0.06724447  
2023-06-02 10:38:36.720: [iter 50 : loss : 0.5465 = 0.0831 + 0.4564 + 0.0070, time: 21.272099]
2023-06-02 10:38:36.990: epoch 50:	0.02936826  	0.08132978  	0.06708125  
2023-06-02 10:38:58.331: [iter 51 : loss : 0.5440 = 0.0808 + 0.4560 + 0.0072, time: 21.336203]
2023-06-02 10:38:58.605: epoch 51:	0.02941268  	0.08105575  	0.06708808  
2023-06-02 10:39:19.919: [iter 52 : loss : 0.5415 = 0.0783 + 0.4558 + 0.0073, time: 21.309963]
2023-06-02 10:39:20.186: epoch 52:	0.02939047  	0.08101590  	0.06710678  
2023-06-02 10:39:41.367: [iter 53 : loss : 0.5403 = 0.0773 + 0.4556 + 0.0074, time: 21.176051]
2023-06-02 10:39:41.637: epoch 53:	0.02931643  	0.08111004  	0.06699389  
2023-06-02 10:40:03.060: [iter 54 : loss : 0.5383 = 0.0753 + 0.4554 + 0.0075, time: 21.418024]
2023-06-02 10:40:03.328: epoch 54:	0.02932383  	0.08102310  	0.06683628  
2023-06-02 10:40:24.907: [iter 55 : loss : 0.5362 = 0.0734 + 0.4551 + 0.0077, time: 21.575013]
2023-06-02 10:40:25.177: epoch 55:	0.02913875  	0.08069542  	0.06665909  
2023-06-02 10:40:46.452: [iter 56 : loss : 0.5339 = 0.0713 + 0.4549 + 0.0078, time: 21.271304]
2023-06-02 10:40:46.725: epoch 56:	0.02906471  	0.08045965  	0.06649700  
2023-06-02 10:41:08.065: [iter 57 : loss : 0.5319 = 0.0693 + 0.4547 + 0.0079, time: 21.335120]
2023-06-02 10:41:08.329: epoch 57:	0.02902770  	0.08011283  	0.06632327  
2023-06-02 10:41:29.647: [iter 58 : loss : 0.5306 = 0.0680 + 0.4545 + 0.0080, time: 21.314024]
2023-06-02 10:41:29.912: epoch 58:	0.02899808  	0.07989854  	0.06625077  
2023-06-02 10:41:51.033: [iter 59 : loss : 0.5290 = 0.0664 + 0.4544 + 0.0081, time: 21.115307]
2023-06-02 10:41:51.299: epoch 59:	0.02893887  	0.07966084  	0.06615760  
2023-06-02 10:41:51.299: Early stopping is trigger at epoch: 59
2023-06-02 10:41:51.299: best_result@epoch 34:

2023-06-02 10:41:51.299: 		0.0300      	0.0849      	0.0682      
2023-06-02 10:49:56.736: my pid: 14316
2023-06-02 10:49:56.736: model: model.general_recommender.SGL
2023-06-02 10:49:56.736: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 10:49:56.736: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 10:50:01.201: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 10:50:22.507: [iter 1 : loss : 1.1342 = 0.6931 + 0.4412 + 0.0000, time: 21.304656]
2023-06-02 10:50:22.771: epoch 1:	0.00138437  	0.00273892  	0.00218523  
2023-06-02 10:50:22.771: Find a better model.
2023-06-02 10:50:43.954: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.180123]
2023-06-02 10:50:44.242: epoch 2:	0.00160646  	0.00285169  	0.00242541  
2023-06-02 10:50:44.242: Find a better model.
2023-06-02 10:51:05.709: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.463452]
2023-06-02 10:51:06.005: epoch 3:	0.00158425  	0.00360642  	0.00273285  
2023-06-02 10:51:06.005: Find a better model.
2023-06-02 10:51:27.119: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 21.109578]
2023-06-02 10:51:27.412: epoch 4:	0.00205804  	0.00415252  	0.00338396  
2023-06-02 10:51:27.412: Find a better model.
2023-06-02 10:51:48.508: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.091629]
2023-06-02 10:51:48.797: epoch 5:	0.00238377  	0.00520412  	0.00407882  
2023-06-02 10:51:48.797: Find a better model.
2023-06-02 10:52:09.871: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 21.071021]
2023-06-02 10:52:10.156: epoch 6:	0.00291679  	0.00714002  	0.00564088  
2023-06-02 10:52:10.156: Find a better model.
2023-06-02 10:52:31.104: [iter 7 : loss : 1.1346 = 0.6924 + 0.4422 + 0.0000, time: 20.943607]
2023-06-02 10:52:31.394: epoch 7:	0.00348682  	0.00887754  	0.00681287  
2023-06-02 10:52:31.394: Find a better model.
2023-06-02 10:52:52.431: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 21.033195]
2023-06-02 10:52:52.718: epoch 8:	0.00426414  	0.01053921  	0.00812660  
2023-06-02 10:52:52.718: Find a better model.
2023-06-02 10:53:13.912: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 21.191722]
2023-06-02 10:53:14.200: epoch 9:	0.00509327  	0.01305057  	0.01028915  
2023-06-02 10:53:14.200: Find a better model.
2023-06-02 10:53:35.304: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 21.099432]
2023-06-02 10:53:35.590: epoch 10:	0.00602604  	0.01581105  	0.01260961  
2023-06-02 10:53:35.590: Find a better model.
2023-06-02 10:53:56.482: [iter 11 : loss : 1.1340 = 0.6902 + 0.4438 + 0.0000, time: 20.888459]
2023-06-02 10:53:56.768: epoch 11:	0.00685517  	0.01837548  	0.01507492  
2023-06-02 10:53:56.768: Find a better model.
2023-06-02 10:54:17.844: [iter 12 : loss : 1.1330 = 0.6886 + 0.4444 + 0.0000, time: 21.072187]
2023-06-02 10:54:18.131: epoch 12:	0.00795821  	0.02188252  	0.01826498  
2023-06-02 10:54:18.131: Find a better model.
2023-06-02 10:54:39.052: [iter 13 : loss : 1.1312 = 0.6861 + 0.4451 + 0.0000, time: 20.918762]
2023-06-02 10:54:39.334: epoch 13:	0.01014210  	0.02923420  	0.02387409  
2023-06-02 10:54:39.334: Find a better model.
2023-06-02 10:55:00.036: [iter 14 : loss : 1.1280 = 0.6821 + 0.4458 + 0.0001, time: 20.696351]
2023-06-02 10:55:00.307: epoch 14:	0.01278500  	0.03674350  	0.03058917  
2023-06-02 10:55:00.307: Find a better model.
2023-06-02 10:55:21.044: [iter 15 : loss : 1.1224 = 0.6755 + 0.4468 + 0.0001, time: 20.732287]
2023-06-02 10:55:21.322: epoch 15:	0.01608683  	0.04433600  	0.03750426  
2023-06-02 10:55:21.322: Find a better model.
2023-06-02 10:55:42.018: [iter 16 : loss : 1.1120 = 0.6641 + 0.4477 + 0.0001, time: 20.693476]
2023-06-02 10:55:42.292: epoch 16:	0.01940345  	0.05194734  	0.04491193  
2023-06-02 10:55:42.292: Find a better model.
2023-06-02 10:56:03.014: [iter 17 : loss : 1.0945 = 0.6453 + 0.4490 + 0.0002, time: 20.717297]
2023-06-02 10:56:03.289: epoch 17:	0.02259425  	0.06011641  	0.05178800  
2023-06-02 10:56:03.289: Find a better model.
2023-06-02 10:56:24.002: [iter 18 : loss : 1.0666 = 0.6155 + 0.4507 + 0.0004, time: 20.710385]
2023-06-02 10:56:24.278: epoch 18:	0.02502254  	0.06631840  	0.05686826  
2023-06-02 10:56:24.278: Find a better model.
2023-06-02 10:56:44.953: [iter 19 : loss : 1.0285 = 0.5751 + 0.4529 + 0.0005, time: 20.670364]
2023-06-02 10:56:45.229: epoch 19:	0.02699180  	0.07211407  	0.06105714  
2023-06-02 10:56:45.229: Find a better model.
2023-06-02 10:57:06.006: [iter 20 : loss : 0.9823 = 0.5259 + 0.4556 + 0.0008, time: 20.773856]
2023-06-02 10:57:06.283: epoch 20:	0.02819113  	0.07561230  	0.06347013  
2023-06-02 10:57:06.283: Find a better model.
2023-06-02 10:57:27.768: [iter 21 : loss : 0.9325 = 0.4728 + 0.4587 + 0.0010, time: 21.480070]
2023-06-02 10:57:28.044: epoch 21:	0.02867975  	0.07696640  	0.06453581  
2023-06-02 10:57:28.044: Find a better model.
2023-06-02 10:57:49.339: [iter 22 : loss : 0.8834 = 0.4202 + 0.4618 + 0.0013, time: 21.290045]
2023-06-02 10:57:49.608: epoch 22:	0.02919055  	0.07787796  	0.06510213  
2023-06-02 10:57:49.608: Find a better model.
2023-06-02 10:58:10.899: [iter 23 : loss : 0.8394 = 0.3733 + 0.4645 + 0.0016, time: 21.286328]
2023-06-02 10:58:11.172: epoch 23:	0.02941266  	0.07893097  	0.06535628  
2023-06-02 10:58:11.172: Find a better model.
2023-06-02 10:58:32.521: [iter 24 : loss : 0.8012 = 0.3331 + 0.4662 + 0.0019, time: 21.344822]
2023-06-02 10:58:32.791: epoch 24:	0.02933862  	0.07884979  	0.06535691  
2023-06-02 10:58:54.089: [iter 25 : loss : 0.7684 = 0.2992 + 0.4670 + 0.0022, time: 21.294737]
2023-06-02 10:58:54.360: epoch 25:	0.02953112  	0.07971166  	0.06561181  
2023-06-02 10:58:54.360: Find a better model.
2023-06-02 10:59:15.701: [iter 26 : loss : 0.7410 = 0.2711 + 0.4674 + 0.0025, time: 21.336334]
2023-06-02 10:59:15.982: epoch 26:	0.02978282  	0.08021670  	0.06598578  
2023-06-02 10:59:15.982: Find a better model.
2023-06-02 10:59:37.513: [iter 27 : loss : 0.7175 = 0.2473 + 0.4675 + 0.0028, time: 21.528084]
2023-06-02 10:59:37.781: epoch 27:	0.02984945  	0.08099275  	0.06625993  
2023-06-02 10:59:37.782: Find a better model.
2023-06-02 10:59:58.929: [iter 28 : loss : 0.6979 = 0.2277 + 0.4671 + 0.0030, time: 21.143546]
2023-06-02 10:59:59.195: epoch 28:	0.03000492  	0.08152018  	0.06648470  
2023-06-02 10:59:59.195: Find a better model.
2023-06-02 11:00:20.485: [iter 29 : loss : 0.6807 = 0.2109 + 0.4665 + 0.0033, time: 21.284563]
2023-06-02 11:00:20.754: epoch 29:	0.03007156  	0.08184605  	0.06678611  
2023-06-02 11:00:20.754: Find a better model.
2023-06-02 11:00:42.302: [iter 30 : loss : 0.6653 = 0.1957 + 0.4661 + 0.0035, time: 21.543071]
2023-06-02 11:00:42.570: epoch 30:	0.03002715  	0.08196916  	0.06678068  
2023-06-02 11:00:42.570: Find a better model.
2023-06-02 11:01:04.078: [iter 31 : loss : 0.6522 = 0.1831 + 0.4654 + 0.0038, time: 21.505505]
2023-06-02 11:01:04.346: epoch 31:	0.02996792  	0.08165140  	0.06681713  
2023-06-02 11:01:25.830: [iter 32 : loss : 0.6409 = 0.1722 + 0.4647 + 0.0040, time: 21.480028]
2023-06-02 11:01:26.104: epoch 32:	0.02997533  	0.08174110  	0.06683648  
2023-06-02 11:01:47.673: [iter 33 : loss : 0.6311 = 0.1629 + 0.4641 + 0.0042, time: 21.565049]
2023-06-02 11:01:47.954: epoch 33:	0.03001975  	0.08185183  	0.06712043  
2023-06-02 11:02:09.428: [iter 34 : loss : 0.6217 = 0.1540 + 0.4633 + 0.0044, time: 21.469348]
2023-06-02 11:02:09.695: epoch 34:	0.02992350  	0.08129118  	0.06694391  
2023-06-02 11:02:31.104: [iter 35 : loss : 0.6127 = 0.1454 + 0.4627 + 0.0046, time: 21.405353]
2023-06-02 11:02:31.375: epoch 35:	0.02995313  	0.08113610  	0.06682429  
2023-06-02 11:02:53.065: [iter 36 : loss : 0.6054 = 0.1386 + 0.4621 + 0.0048, time: 21.686012]
2023-06-02 11:02:53.333: epoch 36:	0.03002716  	0.08126817  	0.06707364  
2023-06-02 11:03:14.818: [iter 37 : loss : 0.5990 = 0.1325 + 0.4615 + 0.0050, time: 21.481594]
2023-06-02 11:03:15.085: epoch 37:	0.03007158  	0.08151887  	0.06727131  
2023-06-02 11:03:36.618: [iter 38 : loss : 0.5922 = 0.1262 + 0.4609 + 0.0051, time: 21.528389]
2023-06-02 11:03:36.884: epoch 38:	0.02994573  	0.08079404  	0.06694875  
2023-06-02 11:03:58.453: [iter 39 : loss : 0.5874 = 0.1216 + 0.4604 + 0.0053, time: 21.565434]
2023-06-02 11:03:58.720: epoch 39:	0.02990132  	0.08077668  	0.06680390  
2023-06-02 11:04:19.847: [iter 40 : loss : 0.5815 = 0.1162 + 0.4599 + 0.0055, time: 21.123116]
2023-06-02 11:04:20.117: epoch 40:	0.02990131  	0.08064483  	0.06683818  
2023-06-02 11:04:41.629: [iter 41 : loss : 0.5764 = 0.1113 + 0.4595 + 0.0057, time: 21.507948]
2023-06-02 11:04:41.895: epoch 41:	0.02980507  	0.08040737  	0.06675143  
2023-06-02 11:05:03.656: [iter 42 : loss : 0.5722 = 0.1073 + 0.4590 + 0.0058, time: 21.757486]
2023-06-02 11:05:03.929: epoch 42:	0.02986429  	0.08027487  	0.06673212  
2023-06-02 11:05:25.216: [iter 43 : loss : 0.5683 = 0.1037 + 0.4586 + 0.0060, time: 21.277078]
2023-06-02 11:05:25.484: epoch 43:	0.02973104  	0.07985604  	0.06650292  
2023-06-02 11:05:46.799: [iter 44 : loss : 0.5652 = 0.1009 + 0.4582 + 0.0061, time: 21.310574]
2023-06-02 11:05:47.073: epoch 44:	0.02967921  	0.07947872  	0.06650297  
2023-06-02 11:06:08.561: [iter 45 : loss : 0.5611 = 0.0970 + 0.4578 + 0.0063, time: 21.484899]
2023-06-02 11:06:08.830: epoch 45:	0.02961259  	0.07908084  	0.06633949  
2023-06-02 11:06:30.039: [iter 46 : loss : 0.5578 = 0.0938 + 0.4575 + 0.0064, time: 21.206647]
2023-06-02 11:06:30.305: epoch 46:	0.02961259  	0.07904092  	0.06625400  
2023-06-02 11:06:51.586: [iter 47 : loss : 0.5544 = 0.0907 + 0.4571 + 0.0066, time: 21.275871]
2023-06-02 11:06:51.852: epoch 47:	0.02958297  	0.07905499  	0.06620800  
2023-06-02 11:07:13.212: [iter 48 : loss : 0.5519 = 0.0883 + 0.4569 + 0.0067, time: 21.356917]
2023-06-02 11:07:13.479: epoch 48:	0.02950893  	0.07878846  	0.06608805  
2023-06-02 11:07:34.743: [iter 49 : loss : 0.5490 = 0.0856 + 0.4565 + 0.0069, time: 21.259274]
2023-06-02 11:07:35.018: epoch 49:	0.02933125  	0.07814700  	0.06573103  
2023-06-02 11:07:56.552: [iter 50 : loss : 0.5468 = 0.0835 + 0.4563 + 0.0070, time: 21.530034]
2023-06-02 11:07:56.820: epoch 50:	0.02929423  	0.07802934  	0.06554278  
2023-06-02 11:08:18.523: [iter 51 : loss : 0.5440 = 0.0809 + 0.4560 + 0.0071, time: 21.700161]
2023-06-02 11:08:18.796: epoch 51:	0.02930164  	0.07775169  	0.06548123  
2023-06-02 11:08:40.346: [iter 52 : loss : 0.5413 = 0.0784 + 0.4557 + 0.0073, time: 21.539768]
2023-06-02 11:08:40.611: epoch 52:	0.02916099  	0.07746600  	0.06518393  
2023-06-02 11:09:02.112: [iter 53 : loss : 0.5405 = 0.0776 + 0.4555 + 0.0074, time: 21.496523]
2023-06-02 11:09:02.379: epoch 53:	0.02911656  	0.07733888  	0.06501848  
2023-06-02 11:09:23.727: [iter 54 : loss : 0.5383 = 0.0755 + 0.4552 + 0.0075, time: 21.343578]
2023-06-02 11:09:24.000: epoch 54:	0.02907954  	0.07697994  	0.06485013  
2023-06-02 11:09:45.088: [iter 55 : loss : 0.5359 = 0.0732 + 0.4551 + 0.0076, time: 21.084085]
2023-06-02 11:09:45.353: epoch 55:	0.02903512  	0.07659351  	0.06474913  
2023-06-02 11:09:45.353: Early stopping is trigger at epoch: 55
2023-06-02 11:09:45.353: best_result@epoch 30:

2023-06-02 11:09:45.353: 		0.0300      	0.0820      	0.0668      
2023-06-02 11:16:22.314: my pid: 12148
2023-06-02 11:16:22.314: model: model.general_recommender.SGL
2023-06-02 11:16:22.314: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 11:16:22.314: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 11:16:26.665: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 11:16:49.114: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.449705]
2023-06-02 11:16:49.399: epoch 1:	0.00126592  	0.00232918  	0.00208609  
2023-06-02 11:16:49.400: Find a better model.
2023-06-02 11:17:13.091: [iter 2 : loss : 1.1337 = 0.6930 + 0.4406 + 0.0000, time: 23.686199]
2023-06-02 11:17:13.413: epoch 2:	0.00133254  	0.00323557  	0.00231994  
2023-06-02 11:17:13.414: Find a better model.
2023-06-02 11:17:37.406: [iter 3 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 23.989273]
2023-06-02 11:17:37.737: epoch 3:	0.00158425  	0.00317601  	0.00272251  
2023-06-02 11:17:59.994: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 22.253164]
2023-06-02 11:18:00.292: epoch 4:	0.00205804  	0.00454658  	0.00338503  
2023-06-02 11:18:00.292: Find a better model.
2023-06-02 11:18:22.004: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.709167]
2023-06-02 11:18:22.301: epoch 5:	0.00239858  	0.00566576  	0.00438693  
2023-06-02 11:18:22.301: Find a better model.
2023-06-02 11:18:43.984: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.678606]
2023-06-02 11:18:44.287: epoch 6:	0.00310927  	0.00782910  	0.00586068  
2023-06-02 11:18:44.287: Find a better model.
2023-06-02 11:19:06.146: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 21.855238]
2023-06-02 11:19:06.440: epoch 7:	0.00359787  	0.00897087  	0.00704555  
2023-06-02 11:19:06.440: Find a better model.
2023-06-02 11:19:28.167: [iter 8 : loss : 1.1345 = 0.6921 + 0.4423 + 0.0000, time: 21.724307]
2023-06-02 11:19:28.458: epoch 8:	0.00430856  	0.01105026  	0.00862353  
2023-06-02 11:19:28.458: Find a better model.
2023-06-02 11:19:50.176: [iter 9 : loss : 1.1345 = 0.6917 + 0.4427 + 0.0000, time: 21.714621]
2023-06-02 11:19:50.464: epoch 9:	0.00524133  	0.01516220  	0.01163980  
2023-06-02 11:19:50.464: Find a better model.
2023-06-02 11:20:12.141: [iter 10 : loss : 1.1344 = 0.6912 + 0.4432 + 0.0000, time: 21.674128]
2023-06-02 11:20:12.430: epoch 10:	0.00627034  	0.01731399  	0.01389113  
2023-06-02 11:20:12.430: Find a better model.
2023-06-02 11:20:34.153: [iter 11 : loss : 1.1340 = 0.6904 + 0.4436 + 0.0000, time: 21.719208]
2023-06-02 11:20:34.431: epoch 11:	0.00773612  	0.02217599  	0.01766082  
2023-06-02 11:20:34.431: Find a better model.
2023-06-02 11:20:56.147: [iter 12 : loss : 1.1333 = 0.6891 + 0.4441 + 0.0000, time: 21.712150]
2023-06-02 11:20:56.434: epoch 12:	0.00908345  	0.02627953  	0.02091821  
2023-06-02 11:20:56.434: Find a better model.
2023-06-02 11:21:17.947: [iter 13 : loss : 1.1317 = 0.6868 + 0.4448 + 0.0000, time: 21.509111]
2023-06-02 11:21:18.243: epoch 13:	0.01088982  	0.03094137  	0.02570422  
2023-06-02 11:21:18.243: Find a better model.
2023-06-02 11:21:39.899: [iter 14 : loss : 1.1286 = 0.6830 + 0.4455 + 0.0001, time: 21.652371]
2023-06-02 11:21:40.180: epoch 14:	0.01322180  	0.03796752  	0.03160485  
2023-06-02 11:21:40.180: Find a better model.
2023-06-02 11:22:01.730: [iter 15 : loss : 1.1235 = 0.6769 + 0.4465 + 0.0001, time: 21.541028]
2023-06-02 11:22:02.014: epoch 15:	0.01610164  	0.04485224  	0.03804389  
2023-06-02 11:22:02.014: Find a better model.
2023-06-02 11:22:23.598: [iter 16 : loss : 1.1138 = 0.6663 + 0.4474 + 0.0001, time: 21.580372]
2023-06-02 11:22:23.895: epoch 16:	0.01949969  	0.05416951  	0.04584797  
2023-06-02 11:22:23.895: Find a better model.
2023-06-02 11:22:45.903: [iter 17 : loss : 1.0974 = 0.6486 + 0.4486 + 0.0002, time: 22.003156]
2023-06-02 11:22:46.219: epoch 17:	0.02269790  	0.06195995  	0.05256019  
2023-06-02 11:22:46.219: Find a better model.
2023-06-02 11:23:07.794: [iter 18 : loss : 1.0710 = 0.6204 + 0.4503 + 0.0003, time: 21.568252]
2023-06-02 11:23:08.085: epoch 18:	0.02500031  	0.06822564  	0.05799904  
2023-06-02 11:23:08.085: Find a better model.
2023-06-02 11:23:31.519: [iter 19 : loss : 1.0341 = 0.5812 + 0.4524 + 0.0005, time: 23.429566]
2023-06-02 11:23:31.815: epoch 19:	0.02694735  	0.07296960  	0.06196608  
2023-06-02 11:23:31.815: Find a better model.
2023-06-02 11:23:53.204: [iter 20 : loss : 0.9885 = 0.5326 + 0.4551 + 0.0007, time: 21.383010]
2023-06-02 11:23:53.477: epoch 20:	0.02819111  	0.07623456  	0.06418533  
2023-06-02 11:23:53.477: Find a better model.
2023-06-02 11:24:15.192: [iter 21 : loss : 0.9387 = 0.4793 + 0.4584 + 0.0010, time: 21.709350]
2023-06-02 11:24:15.465: epoch 21:	0.02887962  	0.07792432  	0.06543045  
2023-06-02 11:24:15.465: Find a better model.
2023-06-02 11:24:36.990: [iter 22 : loss : 0.8891 = 0.4263 + 0.4615 + 0.0013, time: 21.521117]
2023-06-02 11:24:37.265: epoch 22:	0.02930161  	0.07916247  	0.06610291  
2023-06-02 11:24:37.266: Find a better model.
2023-06-02 11:24:58.989: [iter 23 : loss : 0.8447 = 0.3788 + 0.4643 + 0.0016, time: 21.720257]
2023-06-02 11:24:59.266: epoch 23:	0.02942748  	0.08008128  	0.06668638  
2023-06-02 11:24:59.266: Find a better model.
2023-06-02 11:25:20.981: [iter 24 : loss : 0.8059 = 0.3379 + 0.4660 + 0.0019, time: 21.711092]
2023-06-02 11:25:21.257: epoch 24:	0.02944969  	0.08046038  	0.06665953  
2023-06-02 11:25:21.257: Find a better model.
2023-06-02 11:25:43.156: [iter 25 : loss : 0.7723 = 0.3031 + 0.4670 + 0.0022, time: 21.896137]
2023-06-02 11:25:43.423: epoch 25:	0.02959775  	0.08105601  	0.06687338  
2023-06-02 11:25:43.423: Find a better model.
2023-06-02 11:26:05.182: [iter 26 : loss : 0.7445 = 0.2746 + 0.4674 + 0.0025, time: 21.756101]
2023-06-02 11:26:05.448: epoch 26:	0.02983466  	0.08129391  	0.06718133  
2023-06-02 11:26:05.449: Find a better model.
2023-06-02 11:26:27.558: [iter 27 : loss : 0.7204 = 0.2502 + 0.4674 + 0.0027, time: 22.106001]
2023-06-02 11:26:27.824: epoch 27:	0.02994571  	0.08203100  	0.06725628  
2023-06-02 11:26:27.824: Find a better model.
2023-06-02 11:26:49.930: [iter 28 : loss : 0.7001 = 0.2301 + 0.4671 + 0.0030, time: 22.101045]
2023-06-02 11:26:50.214: epoch 28:	0.03004196  	0.08280586  	0.06749668  
2023-06-02 11:26:50.214: Find a better model.
2023-06-02 11:27:12.155: [iter 29 : loss : 0.6830 = 0.2131 + 0.4666 + 0.0033, time: 21.935993]
2023-06-02 11:27:12.424: epoch 29:	0.03009379  	0.08284827  	0.06759021  
2023-06-02 11:27:12.424: Find a better model.
2023-06-02 11:27:34.699: [iter 30 : loss : 0.6671 = 0.1976 + 0.4660 + 0.0035, time: 22.270472]
2023-06-02 11:27:34.964: epoch 30:	0.03017521  	0.08297464  	0.06772751  
2023-06-02 11:27:34.964: Find a better model.
2023-06-02 11:27:56.909: [iter 31 : loss : 0.6540 = 0.1850 + 0.4652 + 0.0037, time: 21.940074]
2023-06-02 11:27:57.175: epoch 31:	0.03013079  	0.08276846  	0.06778906  
2023-06-02 11:28:19.118: [iter 32 : loss : 0.6425 = 0.1739 + 0.4646 + 0.0039, time: 21.939295]
2023-06-02 11:28:19.387: epoch 32:	0.03011598  	0.08248953  	0.06783240  
2023-06-02 11:28:41.300: [iter 33 : loss : 0.6325 = 0.1644 + 0.4639 + 0.0042, time: 21.910038]
2023-06-02 11:28:41.564: epoch 33:	0.03012338  	0.08230359  	0.06787436  
2023-06-02 11:29:03.545: [iter 34 : loss : 0.6230 = 0.1554 + 0.4633 + 0.0044, time: 21.977041]
2023-06-02 11:29:03.810: epoch 34:	0.03021963  	0.08259261  	0.06803980  
2023-06-02 11:29:25.906: [iter 35 : loss : 0.6137 = 0.1464 + 0.4627 + 0.0046, time: 22.092037]
2023-06-02 11:29:26.171: epoch 35:	0.03024183  	0.08292295  	0.06821439  
2023-06-02 11:29:48.070: [iter 36 : loss : 0.6067 = 0.1399 + 0.4620 + 0.0047, time: 21.895045]
2023-06-02 11:29:48.342: epoch 36:	0.03019741  	0.08298753  	0.06816484  
2023-06-02 11:29:48.342: Find a better model.
2023-06-02 11:30:10.067: [iter 37 : loss : 0.5999 = 0.1336 + 0.4614 + 0.0049, time: 21.719539]
2023-06-02 11:30:10.333: epoch 37:	0.03010116  	0.08261798  	0.06800580  
2023-06-02 11:30:32.293: [iter 38 : loss : 0.5935 = 0.1275 + 0.4609 + 0.0051, time: 21.956100]
2023-06-02 11:30:32.560: epoch 38:	0.03018999  	0.08284225  	0.06817375  
2023-06-02 11:30:54.663: [iter 39 : loss : 0.5881 = 0.1226 + 0.4602 + 0.0053, time: 22.099587]
2023-06-02 11:30:54.926: epoch 39:	0.03011596  	0.08264919  	0.06817365  
2023-06-02 11:31:16.848: [iter 40 : loss : 0.5825 = 0.1172 + 0.4598 + 0.0055, time: 21.918360]
2023-06-02 11:31:17.111: epoch 40:	0.03003453  	0.08232790  	0.06803446  
2023-06-02 11:31:39.052: [iter 41 : loss : 0.5773 = 0.1124 + 0.4593 + 0.0056, time: 21.936968]
2023-06-02 11:31:39.325: epoch 41:	0.03010114  	0.08211434  	0.06785930  
2023-06-02 11:32:01.269: [iter 42 : loss : 0.5728 = 0.1081 + 0.4589 + 0.0058, time: 21.941139]
2023-06-02 11:32:01.531: epoch 42:	0.03010114  	0.08204611  	0.06776670  
2023-06-02 11:32:23.817: [iter 43 : loss : 0.5687 = 0.1042 + 0.4585 + 0.0060, time: 22.283183]
2023-06-02 11:32:24.082: epoch 43:	0.03012335  	0.08186294  	0.06767654  
2023-06-02 11:32:46.003: [iter 44 : loss : 0.5656 = 0.1013 + 0.4582 + 0.0061, time: 21.917177]
2023-06-02 11:32:46.272: epoch 44:	0.03011595  	0.08207674  	0.06767819  
2023-06-02 11:33:08.214: [iter 45 : loss : 0.5616 = 0.0977 + 0.4577 + 0.0063, time: 21.936830]
2023-06-02 11:33:08.476: epoch 45:	0.03017518  	0.08206648  	0.06764799  
2023-06-02 11:33:30.233: [iter 46 : loss : 0.5585 = 0.0948 + 0.4573 + 0.0064, time: 21.752742]
2023-06-02 11:33:30.496: epoch 46:	0.03019740  	0.08244488  	0.06775181  
2023-06-02 11:33:52.217: [iter 47 : loss : 0.5552 = 0.0916 + 0.4571 + 0.0066, time: 21.716411]
2023-06-02 11:33:52.481: epoch 47:	0.03014558  	0.08217335  	0.06763520  
2023-06-02 11:34:14.224: [iter 48 : loss : 0.5525 = 0.0890 + 0.4567 + 0.0067, time: 21.738020]
2023-06-02 11:34:14.485: epoch 48:	0.03009374  	0.08182077  	0.06745782  
2023-06-02 11:34:36.169: [iter 49 : loss : 0.5498 = 0.0866 + 0.4563 + 0.0069, time: 21.678051]
2023-06-02 11:34:36.431: epoch 49:	0.02998270  	0.08127557  	0.06743744  
2023-06-02 11:34:58.421: [iter 50 : loss : 0.5472 = 0.0841 + 0.4561 + 0.0070, time: 21.986011]
2023-06-02 11:34:58.671: epoch 50:	0.02991607  	0.08089797  	0.06715408  
2023-06-02 11:35:20.800: [iter 51 : loss : 0.5444 = 0.0814 + 0.4558 + 0.0071, time: 22.125899]
2023-06-02 11:35:21.063: epoch 51:	0.02984944  	0.08058255  	0.06700900  
2023-06-02 11:35:42.945: [iter 52 : loss : 0.5419 = 0.0792 + 0.4555 + 0.0072, time: 21.877064]
2023-06-02 11:35:43.219: epoch 52:	0.02987165  	0.08027906  	0.06691712  
2023-06-02 11:36:05.152: [iter 53 : loss : 0.5411 = 0.0784 + 0.4554 + 0.0074, time: 21.930048]
2023-06-02 11:36:05.417: epoch 53:	0.02981243  	0.08025865  	0.06686170  
2023-06-02 11:36:27.333: [iter 54 : loss : 0.5384 = 0.0759 + 0.4550 + 0.0075, time: 21.911039]
2023-06-02 11:36:27.594: epoch 54:	0.02973841  	0.07954093  	0.06649456  
2023-06-02 11:36:49.532: [iter 55 : loss : 0.5364 = 0.0738 + 0.4549 + 0.0076, time: 21.933018]
2023-06-02 11:36:49.795: epoch 55:	0.02967918  	0.07942831  	0.06644168  
2023-06-02 11:37:11.755: [iter 56 : loss : 0.5344 = 0.0719 + 0.4547 + 0.0078, time: 21.956028]
2023-06-02 11:37:12.016: epoch 56:	0.02961994  	0.07915667  	0.06624527  
2023-06-02 11:37:33.906: [iter 57 : loss : 0.5326 = 0.0702 + 0.4545 + 0.0079, time: 21.886046]
2023-06-02 11:37:34.168: epoch 57:	0.02953112  	0.07944345  	0.06611381  
2023-06-02 11:37:55.927: [iter 58 : loss : 0.5308 = 0.0686 + 0.4543 + 0.0080, time: 21.754116]
2023-06-02 11:37:56.198: epoch 58:	0.02946449  	0.07909265  	0.06593332  
2023-06-02 11:38:18.087: [iter 59 : loss : 0.5294 = 0.0672 + 0.4541 + 0.0081, time: 21.878997]
2023-06-02 11:38:18.354: epoch 59:	0.02944227  	0.07863466  	0.06579311  
2023-06-02 11:38:40.279: [iter 60 : loss : 0.5278 = 0.0657 + 0.4539 + 0.0082, time: 21.922002]
2023-06-02 11:38:40.540: epoch 60:	0.02923498  	0.07789330  	0.06554766  
2023-06-02 11:39:02.092: [iter 61 : loss : 0.5265 = 0.0644 + 0.4538 + 0.0083, time: 21.549032]
2023-06-02 11:39:02.359: epoch 61:	0.02919797  	0.07795649  	0.06548376  
2023-06-02 11:39:02.359: Early stopping is trigger at epoch: 61
2023-06-02 11:39:02.359: best_result@epoch 36:

2023-06-02 11:39:02.359: 		0.0302      	0.0830      	0.0682      
2023-06-02 14:30:30.575: my pid: 12056
2023-06-02 14:30:30.575: model: model.general_recommender.SGL
2023-06-02 14:30:30.575: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 14:30:30.575: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 14:30:34.578: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 14:30:56.578: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.999321]
2023-06-02 14:30:56.854: epoch 1:	0.00114747  	0.00250094  	0.00194138  
2023-06-02 14:30:56.854: Find a better model.
2023-06-02 14:31:18.966: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 22.107160]
2023-06-02 14:31:19.268: epoch 2:	0.00151022  	0.00341402  	0.00239962  
2023-06-02 14:31:19.268: Find a better model.
2023-06-02 14:31:41.581: [iter 3 : loss : 1.1337 = 0.6929 + 0.4408 + 0.0000, time: 22.309920]
2023-06-02 14:31:41.876: epoch 3:	0.00160646  	0.00309484  	0.00270312  
2023-06-02 14:32:04.816: [iter 4 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 22.935565]
2023-06-02 14:32:05.152: epoch 4:	0.00224312  	0.00490755  	0.00383735  
2023-06-02 14:32:05.152: Find a better model.
2023-06-02 14:32:27.309: [iter 5 : loss : 1.1340 = 0.6927 + 0.4413 + 0.0000, time: 22.153241]
2023-06-02 14:32:27.604: epoch 5:	0.00248742  	0.00646294  	0.00492540  
2023-06-02 14:32:27.604: Find a better model.
2023-06-02 14:32:49.726: [iter 6 : loss : 1.1342 = 0.6926 + 0.4416 + 0.0000, time: 22.119135]
2023-06-02 14:32:50.023: epoch 6:	0.00290199  	0.00725112  	0.00600196  
2023-06-02 14:32:50.023: Find a better model.
2023-06-02 14:33:11.920: [iter 7 : loss : 1.1343 = 0.6924 + 0.4419 + 0.0000, time: 21.894105]
2023-06-02 14:33:12.214: epoch 7:	0.00333876  	0.00893669  	0.00689316  
2023-06-02 14:33:12.214: Find a better model.
2023-06-02 14:33:34.340: [iter 8 : loss : 1.1344 = 0.6922 + 0.4422 + 0.0000, time: 22.123013]
2023-06-02 14:33:34.616: epoch 8:	0.00381996  	0.01028028  	0.00825725  
2023-06-02 14:33:34.616: Find a better model.
2023-06-02 14:33:56.751: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 22.131596]
2023-06-02 14:33:57.040: epoch 9:	0.00498963  	0.01433831  	0.01115114  
2023-06-02 14:33:57.040: Find a better model.
2023-06-02 14:34:19.707: [iter 10 : loss : 1.1343 = 0.6913 + 0.4430 + 0.0000, time: 22.663050]
2023-06-02 14:34:19.992: epoch 10:	0.00614449  	0.01689515  	0.01348079  
2023-06-02 14:34:19.992: Find a better model.
2023-06-02 14:34:43.610: [iter 11 : loss : 1.1340 = 0.6906 + 0.4434 + 0.0000, time: 23.613950]
2023-06-02 14:34:43.922: epoch 11:	0.00761027  	0.02190638  	0.01690653  
2023-06-02 14:34:43.922: Find a better model.
2023-06-02 14:35:06.338: [iter 12 : loss : 1.1333 = 0.6894 + 0.4439 + 0.0000, time: 22.411329]
2023-06-02 14:35:06.618: epoch 12:	0.00902423  	0.02566818  	0.02090666  
2023-06-02 14:35:06.618: Find a better model.
2023-06-02 14:35:28.539: [iter 13 : loss : 1.1321 = 0.6875 + 0.4445 + 0.0000, time: 21.916495]
2023-06-02 14:35:28.817: epoch 13:	0.01130439  	0.03255829  	0.02578556  
2023-06-02 14:35:28.817: Find a better model.
2023-06-02 14:35:51.320: [iter 14 : loss : 1.1294 = 0.6841 + 0.4453 + 0.0001, time: 22.498349]
2023-06-02 14:35:51.595: epoch 14:	0.01343649  	0.03803832  	0.03104364  
2023-06-02 14:35:51.595: Find a better model.
2023-06-02 14:36:14.045: [iter 15 : loss : 1.1245 = 0.6783 + 0.4462 + 0.0001, time: 22.444603]
2023-06-02 14:36:14.315: epoch 15:	0.01647919  	0.04648795  	0.03814349  
2023-06-02 14:36:14.315: Find a better model.
2023-06-02 14:36:37.448: [iter 16 : loss : 1.1157 = 0.6684 + 0.4471 + 0.0001, time: 23.129622]
2023-06-02 14:36:37.768: epoch 16:	0.01978840  	0.05409397  	0.04543220  
2023-06-02 14:36:37.768: Find a better model.
2023-06-02 14:37:02.645: [iter 17 : loss : 1.1006 = 0.6520 + 0.4484 + 0.0002, time: 24.871334]
2023-06-02 14:37:02.967: epoch 17:	0.02261646  	0.06159417  	0.05217643  
2023-06-02 14:37:02.967: Find a better model.
2023-06-02 14:37:25.241: [iter 18 : loss : 1.0754 = 0.6252 + 0.4499 + 0.0003, time: 22.268522]
2023-06-02 14:37:25.508: epoch 18:	0.02523721  	0.06726115  	0.05719620  
2023-06-02 14:37:25.508: Find a better model.
2023-06-02 14:37:48.343: [iter 19 : loss : 1.0396 = 0.5871 + 0.4520 + 0.0005, time: 22.829574]
2023-06-02 14:37:48.628: epoch 19:	0.02665864  	0.07219755  	0.06087504  
2023-06-02 14:37:48.628: Find a better model.
2023-06-02 14:38:13.707: [iter 20 : loss : 0.9948 = 0.5393 + 0.4548 + 0.0007, time: 25.074285]
2023-06-02 14:38:14.029: epoch 20:	0.02796163  	0.07533383  	0.06318378  
2023-06-02 14:38:14.029: Find a better model.
2023-06-02 14:38:38.470: [iter 21 : loss : 0.9450 = 0.4860 + 0.4580 + 0.0010, time: 24.434394]
2023-06-02 14:38:38.759: epoch 21:	0.02857610  	0.07706181  	0.06443328  
2023-06-02 14:38:38.759: Find a better model.
2023-06-02 14:39:03.131: [iter 22 : loss : 0.8949 = 0.4324 + 0.4612 + 0.0013, time: 24.366490]
2023-06-02 14:39:03.410: epoch 22:	0.02885742  	0.07831796  	0.06504811  
2023-06-02 14:39:03.410: Find a better model.
2023-06-02 14:39:27.170: [iter 23 : loss : 0.8496 = 0.3843 + 0.4638 + 0.0016, time: 23.754894]
2023-06-02 14:39:27.451: epoch 23:	0.02901290  	0.07892031  	0.06532054  
2023-06-02 14:39:27.451: Find a better model.
2023-06-02 14:39:51.534: [iter 24 : loss : 0.8100 = 0.3423 + 0.4658 + 0.0019, time: 24.079387]
2023-06-02 14:39:51.822: epoch 24:	0.02888704  	0.07851239  	0.06535020  
2023-06-02 14:40:15.988: [iter 25 : loss : 0.7759 = 0.3070 + 0.4667 + 0.0022, time: 24.161063]
2023-06-02 14:40:16.314: epoch 25:	0.02915354  	0.07904368  	0.06562365  
2023-06-02 14:40:16.314: Find a better model.
2023-06-02 14:40:40.821: [iter 26 : loss : 0.7475 = 0.2779 + 0.4671 + 0.0024, time: 24.501569]
2023-06-02 14:40:41.123: epoch 26:	0.02943487  	0.08059656  	0.06612267  
2023-06-02 14:40:41.124: Find a better model.
2023-06-02 14:41:07.169: [iter 27 : loss : 0.7234 = 0.2535 + 0.4672 + 0.0027, time: 26.042763]
2023-06-02 14:41:07.482: epoch 27:	0.02950151  	0.08086960  	0.06637436  
2023-06-02 14:41:07.482: Find a better model.
2023-06-02 14:41:31.656: [iter 28 : loss : 0.7027 = 0.2329 + 0.4668 + 0.0030, time: 24.170796]
2023-06-02 14:41:31.934: epoch 28:	0.02976802  	0.08156216  	0.06661842  
2023-06-02 14:41:31.934: Find a better model.
2023-06-02 14:41:55.592: [iter 29 : loss : 0.6851 = 0.2156 + 0.4663 + 0.0032, time: 23.652050]
2023-06-02 14:41:55.870: epoch 29:	0.02987167  	0.08185615  	0.06702794  
2023-06-02 14:41:55.870: Find a better model.
2023-06-02 14:42:18.553: [iter 30 : loss : 0.6693 = 0.2001 + 0.4657 + 0.0035, time: 22.678257]
2023-06-02 14:42:18.823: epoch 30:	0.03004194  	0.08248095  	0.06729989  
2023-06-02 14:42:18.823: Find a better model.
2023-06-02 14:42:41.540: [iter 31 : loss : 0.6553 = 0.1865 + 0.4651 + 0.0037, time: 22.713032]
2023-06-02 14:42:41.809: epoch 31:	0.03004195  	0.08254919  	0.06738764  
2023-06-02 14:42:41.809: Find a better model.
2023-06-02 14:43:04.370: [iter 32 : loss : 0.6438 = 0.1755 + 0.4645 + 0.0039, time: 22.557733]
2023-06-02 14:43:04.638: epoch 32:	0.03010857  	0.08280662  	0.06748916  
2023-06-02 14:43:04.638: Find a better model.
2023-06-02 14:43:27.509: [iter 33 : loss : 0.6336 = 0.1658 + 0.4637 + 0.0041, time: 22.866443]
2023-06-02 14:43:27.778: epoch 33:	0.03003454  	0.08224653  	0.06734424  
2023-06-02 14:43:50.901: [iter 34 : loss : 0.6238 = 0.1564 + 0.4630 + 0.0043, time: 23.118038]
2023-06-02 14:43:51.179: epoch 34:	0.03013078  	0.08240380  	0.06752664  
2023-06-02 14:44:13.735: [iter 35 : loss : 0.6145 = 0.1478 + 0.4622 + 0.0045, time: 22.552438]
2023-06-02 14:44:14.001: epoch 35:	0.03020482  	0.08250254  	0.06763560  
2023-06-02 14:44:36.540: [iter 36 : loss : 0.6072 = 0.1409 + 0.4616 + 0.0047, time: 22.533455]
2023-06-02 14:44:36.806: epoch 36:	0.03029366  	0.08226568  	0.06757920  
2023-06-02 14:44:59.685: [iter 37 : loss : 0.6008 = 0.1347 + 0.4612 + 0.0049, time: 22.875282]
2023-06-02 14:44:59.955: epoch 37:	0.03038250  	0.08259620  	0.06772353  
2023-06-02 14:45:22.669: [iter 38 : loss : 0.5942 = 0.1285 + 0.4606 + 0.0051, time: 22.711111]
2023-06-02 14:45:22.935: epoch 38:	0.03022703  	0.08214712  	0.06749570  
2023-06-02 14:45:45.641: [iter 39 : loss : 0.5886 = 0.1233 + 0.4600 + 0.0053, time: 22.701742]
2023-06-02 14:45:45.908: epoch 39:	0.03032327  	0.08227462  	0.06767136  
2023-06-02 14:46:08.843: [iter 40 : loss : 0.5829 = 0.1179 + 0.4595 + 0.0054, time: 22.932446]
2023-06-02 14:46:09.126: epoch 40:	0.03047874  	0.08226903  	0.06780174  
2023-06-02 14:46:31.859: [iter 41 : loss : 0.5778 = 0.1131 + 0.4590 + 0.0056, time: 22.728304]
2023-06-02 14:46:32.142: epoch 41:	0.03045652  	0.08172709  	0.06759092  
2023-06-02 14:46:54.835: [iter 42 : loss : 0.5732 = 0.1088 + 0.4586 + 0.0058, time: 22.690463]
2023-06-02 14:46:55.120: epoch 42:	0.03033067  	0.08118402  	0.06746102  
2023-06-02 14:47:17.831: [iter 43 : loss : 0.5690 = 0.1049 + 0.4582 + 0.0059, time: 22.706528]
2023-06-02 14:47:18.104: epoch 43:	0.03033807  	0.08085056  	0.06735042  
2023-06-02 14:47:40.805: [iter 44 : loss : 0.5661 = 0.1021 + 0.4579 + 0.0061, time: 22.691005]
2023-06-02 14:47:41.073: epoch 44:	0.03027144  	0.08108284  	0.06741606  
2023-06-02 14:48:04.531: [iter 45 : loss : 0.5623 = 0.0986 + 0.4575 + 0.0062, time: 23.453981]
2023-06-02 14:48:04.790: epoch 45:	0.03013818  	0.08058122  	0.06726731  
2023-06-02 14:48:28.280: [iter 46 : loss : 0.5587 = 0.0952 + 0.4571 + 0.0064, time: 23.487071]
2023-06-02 14:48:28.549: epoch 46:	0.03021220  	0.08035958  	0.06714897  
2023-06-02 14:48:51.878: [iter 47 : loss : 0.5554 = 0.0922 + 0.4567 + 0.0065, time: 23.324647]
2023-06-02 14:48:52.157: epoch 47:	0.03015298  	0.08003813  	0.06728344  
2023-06-02 14:49:15.225: [iter 48 : loss : 0.5522 = 0.0893 + 0.4563 + 0.0067, time: 23.063308]
2023-06-02 14:49:15.521: epoch 48:	0.03015298  	0.08002005  	0.06725547  
2023-06-02 14:49:38.476: [iter 49 : loss : 0.5498 = 0.0869 + 0.4560 + 0.0068, time: 22.952065]
2023-06-02 14:49:38.744: epoch 49:	0.03012338  	0.07988420  	0.06722210  
2023-06-02 14:50:01.808: [iter 50 : loss : 0.5473 = 0.0847 + 0.4557 + 0.0070, time: 23.061537]
2023-06-02 14:50:02.076: epoch 50:	0.03006414  	0.07991017  	0.06707224  
2023-06-02 14:50:25.637: [iter 51 : loss : 0.5446 = 0.0819 + 0.4556 + 0.0071, time: 23.556669]
2023-06-02 14:50:25.938: epoch 51:	0.03001232  	0.07957985  	0.06695834  
2023-06-02 14:50:49.357: [iter 52 : loss : 0.5416 = 0.0793 + 0.4551 + 0.0072, time: 23.415183]
2023-06-02 14:50:49.639: epoch 52:	0.03006414  	0.07992844  	0.06712316  
2023-06-02 14:51:13.059: [iter 53 : loss : 0.5408 = 0.0784 + 0.4550 + 0.0074, time: 23.414572]
2023-06-02 14:51:13.344: epoch 53:	0.02999011  	0.07950105  	0.06694117  
2023-06-02 14:51:36.640: [iter 54 : loss : 0.5386 = 0.0762 + 0.4548 + 0.0075, time: 23.293303]
2023-06-02 14:51:36.916: epoch 54:	0.02999011  	0.07953863  	0.06696406  
2023-06-02 14:51:59.960: [iter 55 : loss : 0.5364 = 0.0742 + 0.4546 + 0.0076, time: 23.040032]
2023-06-02 14:52:00.248: epoch 55:	0.02996050  	0.07926507  	0.06688388  
2023-06-02 14:52:23.866: [iter 56 : loss : 0.5343 = 0.0723 + 0.4543 + 0.0077, time: 23.613828]
2023-06-02 14:52:24.176: epoch 56:	0.02996050  	0.07940441  	0.06687832  
2023-06-02 14:52:47.907: [iter 57 : loss : 0.5324 = 0.0704 + 0.4541 + 0.0079, time: 23.727242]
2023-06-02 14:52:48.206: epoch 57:	0.02994569  	0.07938807  	0.06686880  
2023-06-02 14:52:48.206: Early stopping is trigger at epoch: 57
2023-06-02 14:52:48.206: best_result@epoch 32:

2023-06-02 14:52:48.206: 		0.0301      	0.0828      	0.0675      
2023-06-02 15:07:25.671: my pid: 13584
2023-06-02 15:07:25.671: model: model.general_recommender.SGL
2023-06-02 15:07:25.671: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 15:07:25.671: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 15:07:29.853: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 15:07:50.981: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.126728]
2023-06-02 15:07:51.242: epoch 1:	0.00147320  	0.00311625  	0.00246160  
2023-06-02 15:07:51.242: Find a better model.
2023-06-02 15:08:13.979: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 22.732873]
2023-06-02 15:08:14.299: epoch 2:	0.00186556  	0.00367127  	0.00304520  
2023-06-02 15:08:14.299: Find a better model.
2023-06-02 15:08:37.214: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 22.910610]
2023-06-02 15:08:37.546: epoch 3:	0.00208025  	0.00367878  	0.00312150  
2023-06-02 15:08:37.546: Find a better model.
2023-06-02 15:08:58.867: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.317161]
2023-06-02 15:08:59.157: epoch 4:	0.00270951  	0.00538028  	0.00431047  
2023-06-02 15:08:59.157: Find a better model.
2023-06-02 15:09:20.571: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 21.410459]
2023-06-02 15:09:20.873: epoch 5:	0.00339059  	0.00700443  	0.00559875  
2023-06-02 15:09:20.873: Find a better model.
2023-06-02 15:09:43.194: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 22.317590]
2023-06-02 15:09:43.533: epoch 6:	0.00381996  	0.00861051  	0.00682860  
2023-06-02 15:09:43.533: Find a better model.
2023-06-02 15:10:06.187: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 22.650503]
2023-06-02 15:10:06.515: epoch 7:	0.00472312  	0.01130587  	0.00911435  
2023-06-02 15:10:06.515: Find a better model.
2023-06-02 15:10:27.453: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 20.934171]
2023-06-02 15:10:27.740: epoch 8:	0.00569291  	0.01371649  	0.01111854  
2023-06-02 15:10:27.740: Find a better model.
2023-06-02 15:10:48.221: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 20.477052]
2023-06-02 15:10:48.507: epoch 9:	0.00649983  	0.01589953  	0.01310365  
2023-06-02 15:10:48.507: Find a better model.
2023-06-02 15:11:08.615: [iter 10 : loss : 1.1343 = 0.6904 + 0.4439 + 0.0000, time: 20.103043]
2023-06-02 15:11:08.899: epoch 10:	0.00720311  	0.01824263  	0.01473298  
2023-06-02 15:11:08.899: Find a better model.
2023-06-02 15:11:29.240: [iter 11 : loss : 1.1335 = 0.6889 + 0.4446 + 0.0000, time: 20.337031]
2023-06-02 15:11:29.530: epoch 11:	0.00821731  	0.02103022  	0.01777499  
2023-06-02 15:11:29.530: Find a better model.
2023-06-02 15:11:49.838: [iter 12 : loss : 1.1320 = 0.6867 + 0.4453 + 0.0000, time: 20.303947]
2023-06-02 15:11:50.116: epoch 12:	0.01017171  	0.02679923  	0.02266460  
2023-06-02 15:11:50.116: Find a better model.
2023-06-02 15:12:10.234: [iter 13 : loss : 1.1295 = 0.6835 + 0.4459 + 0.0001, time: 20.113628]
2023-06-02 15:12:10.515: epoch 13:	0.01262214  	0.03344887  	0.02884988  
2023-06-02 15:12:10.515: Find a better model.
2023-06-02 15:12:30.813: [iter 14 : loss : 1.1248 = 0.6779 + 0.4467 + 0.0001, time: 20.293217]
2023-06-02 15:12:31.094: epoch 14:	0.01533172  	0.04076338  	0.03491249  
2023-06-02 15:12:31.094: Find a better model.
2023-06-02 15:12:51.399: [iter 15 : loss : 1.1162 = 0.6684 + 0.4477 + 0.0001, time: 20.299228]
2023-06-02 15:12:51.685: epoch 15:	0.01913693  	0.05036114  	0.04303724  
2023-06-02 15:12:51.685: Find a better model.
2023-06-02 15:13:12.596: [iter 16 : loss : 1.1009 = 0.6521 + 0.4486 + 0.0002, time: 20.905886]
2023-06-02 15:13:12.876: epoch 16:	0.02229073  	0.05915227  	0.05092583  
2023-06-02 15:13:12.876: Find a better model.
2023-06-02 15:13:33.818: [iter 17 : loss : 1.0768 = 0.6264 + 0.4501 + 0.0003, time: 20.937365]
2023-06-02 15:13:34.084: epoch 17:	0.02469680  	0.06574757  	0.05663466  
2023-06-02 15:13:34.084: Find a better model.
2023-06-02 15:13:55.006: [iter 18 : loss : 1.0415 = 0.5889 + 0.4521 + 0.0005, time: 20.917633]
2023-06-02 15:13:55.284: epoch 18:	0.02655500  	0.07040922  	0.06003047  
2023-06-02 15:13:55.284: Find a better model.
2023-06-02 15:14:15.987: [iter 19 : loss : 0.9973 = 0.5417 + 0.4549 + 0.0007, time: 20.698976]
2023-06-02 15:14:16.263: epoch 19:	0.02815411  	0.07527264  	0.06342865  
2023-06-02 15:14:16.263: Find a better model.
2023-06-02 15:14:36.810: [iter 20 : loss : 0.9477 = 0.4886 + 0.4582 + 0.0010, time: 20.543154]
2023-06-02 15:14:37.084: epoch 20:	0.02896846  	0.07735690  	0.06514985  
2023-06-02 15:14:37.084: Find a better model.
2023-06-02 15:14:57.988: [iter 21 : loss : 0.8983 = 0.4356 + 0.4615 + 0.0012, time: 20.899122]
2023-06-02 15:14:58.262: epoch 21:	0.02941266  	0.07864295  	0.06571919  
2023-06-02 15:14:58.262: Find a better model.
2023-06-02 15:15:18.966: [iter 22 : loss : 0.8515 = 0.3857 + 0.4642 + 0.0015, time: 20.699944]
2023-06-02 15:15:19.235: epoch 22:	0.02969398  	0.07973298  	0.06619909  
2023-06-02 15:15:19.235: Find a better model.
2023-06-02 15:15:39.984: [iter 23 : loss : 0.8117 = 0.3435 + 0.4663 + 0.0018, time: 20.745213]
2023-06-02 15:15:40.256: epoch 23:	0.02981983  	0.08027316  	0.06622608  
2023-06-02 15:15:40.256: Find a better model.
2023-06-02 15:16:01.132: [iter 24 : loss : 0.7771 = 0.3078 + 0.4672 + 0.0021, time: 20.872086]
2023-06-02 15:16:01.410: epoch 24:	0.03000492  	0.08131725  	0.06669769  
2023-06-02 15:16:01.410: Find a better model.
2023-06-02 15:16:23.118: [iter 25 : loss : 0.7478 = 0.2777 + 0.4677 + 0.0024, time: 21.698934]
2023-06-02 15:16:23.398: epoch 25:	0.03023442  	0.08208618  	0.06703379  
2023-06-02 15:16:23.398: Find a better model.
2023-06-02 15:16:46.214: [iter 26 : loss : 0.7234 = 0.2528 + 0.4679 + 0.0027, time: 22.810545]
2023-06-02 15:16:46.517: epoch 26:	0.03023441  	0.08242983  	0.06713423  
2023-06-02 15:16:46.517: Find a better model.
2023-06-02 15:17:09.603: [iter 27 : loss : 0.7024 = 0.2320 + 0.4674 + 0.0030, time: 23.081859]
2023-06-02 15:17:09.896: epoch 27:	0.03008636  	0.08199912  	0.06687234  
2023-06-02 15:17:30.740: [iter 28 : loss : 0.6844 = 0.2142 + 0.4670 + 0.0032, time: 20.840677]
2023-06-02 15:17:31.010: epoch 28:	0.03018260  	0.08218040  	0.06710126  
2023-06-02 15:17:51.459: [iter 29 : loss : 0.6692 = 0.1994 + 0.4663 + 0.0035, time: 20.444445]
2023-06-02 15:17:51.731: epoch 29:	0.03036767  	0.08273830  	0.06750873  
2023-06-02 15:17:51.731: Find a better model.
2023-06-02 15:18:12.433: [iter 30 : loss : 0.6549 = 0.1855 + 0.4657 + 0.0037, time: 20.697591]
2023-06-02 15:18:12.703: epoch 30:	0.03036027  	0.08249433  	0.06738916  
2023-06-02 15:18:33.460: [iter 31 : loss : 0.6430 = 0.1739 + 0.4651 + 0.0039, time: 20.753613]
2023-06-02 15:18:33.733: epoch 31:	0.03030845  	0.08218918  	0.06725957  
2023-06-02 15:18:54.462: [iter 32 : loss : 0.6328 = 0.1642 + 0.4645 + 0.0041, time: 20.724020]
2023-06-02 15:18:54.732: epoch 32:	0.03037507  	0.08219373  	0.06742534  
2023-06-02 15:19:15.468: [iter 33 : loss : 0.6237 = 0.1558 + 0.4636 + 0.0043, time: 20.733399]
2023-06-02 15:19:15.736: epoch 33:	0.03039727  	0.08249956  	0.06764746  
2023-06-02 15:19:36.652: [iter 34 : loss : 0.6152 = 0.1474 + 0.4632 + 0.0045, time: 20.910557]
2023-06-02 15:19:36.919: epoch 34:	0.03031583  	0.08240664  	0.06752843  
2023-06-02 15:19:57.450: [iter 35 : loss : 0.6069 = 0.1398 + 0.4625 + 0.0047, time: 20.528080]
2023-06-02 15:19:57.719: epoch 35:	0.03038988  	0.08226455  	0.06757559  
2023-06-02 15:20:18.444: [iter 36 : loss : 0.6002 = 0.1333 + 0.4619 + 0.0049, time: 20.720123]
2023-06-02 15:20:18.716: epoch 36:	0.03045651  	0.08260866  	0.06768446  
2023-06-02 15:20:40.648: [iter 37 : loss : 0.5940 = 0.1275 + 0.4614 + 0.0051, time: 21.928227]
2023-06-02 15:20:40.915: epoch 37:	0.03041950  	0.08211785  	0.06754982  
2023-06-02 15:21:03.688: [iter 38 : loss : 0.5882 = 0.1220 + 0.4608 + 0.0053, time: 22.769041]
2023-06-02 15:21:03.984: epoch 38:	0.03044170  	0.08198868  	0.06752699  
2023-06-02 15:21:26.844: [iter 39 : loss : 0.5832 = 0.1174 + 0.4604 + 0.0054, time: 22.855849]
2023-06-02 15:21:27.139: epoch 39:	0.03039729  	0.08200802  	0.06749526  
2023-06-02 15:21:48.222: [iter 40 : loss : 0.5781 = 0.1126 + 0.4599 + 0.0056, time: 21.080001]
2023-06-02 15:21:48.498: epoch 40:	0.03034546  	0.08175883  	0.06746346  
2023-06-02 15:22:09.179: [iter 41 : loss : 0.5732 = 0.1080 + 0.4593 + 0.0058, time: 20.676550]
2023-06-02 15:22:09.458: epoch 41:	0.03031585  	0.08193485  	0.06746657  
2023-06-02 15:22:30.189: [iter 42 : loss : 0.5689 = 0.1040 + 0.4590 + 0.0059, time: 20.727493]
2023-06-02 15:22:30.468: epoch 42:	0.03028624  	0.08161197  	0.06736597  
2023-06-02 15:22:51.149: [iter 43 : loss : 0.5650 = 0.1003 + 0.4586 + 0.0061, time: 20.678245]
2023-06-02 15:22:51.432: epoch 43:	0.03018259  	0.08122045  	0.06706763  
2023-06-02 15:23:12.148: [iter 44 : loss : 0.5623 = 0.0978 + 0.4582 + 0.0062, time: 20.711438]
2023-06-02 15:23:12.431: epoch 44:	0.03009376  	0.08097170  	0.06686443  
2023-06-02 15:23:33.163: [iter 45 : loss : 0.5585 = 0.0942 + 0.4578 + 0.0064, time: 20.727606]
2023-06-02 15:23:33.449: epoch 45:	0.03007155  	0.08110261  	0.06685647  
2023-06-02 15:23:54.156: [iter 46 : loss : 0.5556 = 0.0916 + 0.4575 + 0.0065, time: 20.702436]
2023-06-02 15:23:54.438: epoch 46:	0.03010116  	0.08115845  	0.06683828  
2023-06-02 15:24:15.408: [iter 47 : loss : 0.5524 = 0.0885 + 0.4572 + 0.0067, time: 20.964927]
2023-06-02 15:24:15.699: epoch 47:	0.03008634  	0.08080210  	0.06663245  
2023-06-02 15:24:37.186: [iter 48 : loss : 0.5497 = 0.0860 + 0.4569 + 0.0068, time: 21.481822]
2023-06-02 15:24:37.466: epoch 48:	0.03010855  	0.08101810  	0.06671610  
2023-06-02 15:24:58.637: [iter 49 : loss : 0.5472 = 0.0837 + 0.4565 + 0.0070, time: 21.167099]
2023-06-02 15:24:58.935: epoch 49:	0.02999011  	0.08037664  	0.06659368  
2023-06-02 15:25:22.035: [iter 50 : loss : 0.5451 = 0.0816 + 0.4563 + 0.0071, time: 23.095633]
2023-06-02 15:25:22.307: epoch 50:	0.03010115  	0.08086947  	0.06669712  
2023-06-02 15:25:44.047: [iter 51 : loss : 0.5421 = 0.0788 + 0.4560 + 0.0072, time: 21.736320]
2023-06-02 15:25:44.308: epoch 51:	0.02993827  	0.08046102  	0.06663385  
2023-06-02 15:26:00.795: my pid: 3628
2023-06-02 15:26:00.795: model: model.general_recommender.SGL
2023-06-02 15:26:00.795: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 15:26:00.795: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 15:26:05.173: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 15:26:27.724: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 22.550018]
2023-06-02 15:26:28.008: epoch 1:	0.00136956  	0.00279948  	0.00234637  
2023-06-02 15:26:28.008: Find a better model.
2023-06-02 15:26:49.959: [iter 2 : loss : 1.1341 = 0.6930 + 0.4411 + 0.0000, time: 21.947197]
2023-06-02 15:26:50.278: epoch 2:	0.00205064  	0.00342005  	0.00296493  
2023-06-02 15:26:50.278: Find a better model.
2023-06-02 15:27:11.326: [iter 3 : loss : 1.1343 = 0.6929 + 0.4413 + 0.0000, time: 21.043492]
2023-06-02 15:27:11.619: epoch 3:	0.00248742  	0.00431755  	0.00366544  
2023-06-02 15:27:11.620: Find a better model.
2023-06-02 15:27:32.096: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 20.471927]
2023-06-02 15:27:32.387: epoch 4:	0.00286497  	0.00544243  	0.00436115  
2023-06-02 15:27:32.387: Find a better model.
2023-06-02 15:27:52.896: [iter 5 : loss : 1.1346 = 0.6926 + 0.4420 + 0.0000, time: 20.504771]
2023-06-02 15:27:53.189: epoch 5:	0.00343500  	0.00765780  	0.00567347  
2023-06-02 15:27:53.189: Find a better model.
2023-06-02 15:28:13.492: [iter 6 : loss : 1.1348 = 0.6924 + 0.4423 + 0.0000, time: 20.300082]
2023-06-02 15:28:13.784: epoch 6:	0.00409387  	0.01006373  	0.00777498  
2023-06-02 15:28:13.784: Find a better model.
2023-06-02 15:28:34.279: [iter 7 : loss : 1.1349 = 0.6922 + 0.4427 + 0.0000, time: 20.492471]
2023-06-02 15:28:34.574: epoch 7:	0.00484157  	0.01193137  	0.00984523  
2023-06-02 15:28:34.574: Find a better model.
2023-06-02 15:28:55.567: [iter 8 : loss : 1.1349 = 0.6918 + 0.4430 + 0.0000, time: 20.988653]
2023-06-02 15:28:55.861: epoch 8:	0.00564849  	0.01441304  	0.01142721  
2023-06-02 15:28:55.861: Find a better model.
2023-06-02 15:29:17.776: [iter 9 : loss : 1.1347 = 0.6912 + 0.4435 + 0.0000, time: 21.912695]
2023-06-02 15:29:18.082: epoch 9:	0.00621852  	0.01618110  	0.01271930  
2023-06-02 15:29:18.082: Find a better model.
2023-06-02 15:29:40.513: [iter 10 : loss : 1.1342 = 0.6901 + 0.4440 + 0.0000, time: 22.426631]
2023-06-02 15:29:40.822: epoch 10:	0.00678854  	0.01731339  	0.01429276  
2023-06-02 15:29:40.822: Find a better model.
2023-06-02 15:30:01.068: [iter 11 : loss : 1.1334 = 0.6887 + 0.4447 + 0.0000, time: 20.242257]
2023-06-02 15:30:01.355: epoch 11:	0.00862447  	0.02326103  	0.01873187  
2023-06-02 15:30:01.355: Find a better model.
2023-06-02 15:30:21.482: [iter 12 : loss : 1.1319 = 0.6866 + 0.4453 + 0.0000, time: 20.124468]
2023-06-02 15:30:21.766: epoch 12:	0.01039380  	0.02776038  	0.02334057  
2023-06-02 15:30:21.766: Find a better model.
2023-06-02 15:30:42.270: [iter 13 : loss : 1.1294 = 0.6833 + 0.4460 + 0.0001, time: 20.500312]
2023-06-02 15:30:42.564: epoch 13:	0.01324400  	0.03506248  	0.02949741  
2023-06-02 15:30:42.565: Find a better model.
2023-06-02 15:31:02.869: [iter 14 : loss : 1.1246 = 0.6778 + 0.4467 + 0.0001, time: 20.300038]
2023-06-02 15:31:03.147: epoch 14:	0.01598319  	0.04250266  	0.03636566  
2023-06-02 15:31:03.147: Find a better model.
2023-06-02 15:31:23.227: [iter 15 : loss : 1.1161 = 0.6683 + 0.4477 + 0.0001, time: 20.075996]
2023-06-02 15:31:23.512: epoch 15:	0.01941825  	0.05131000  	0.04419443  
2023-06-02 15:31:23.512: Find a better model.
2023-06-02 15:31:44.227: [iter 16 : loss : 1.1007 = 0.6520 + 0.4485 + 0.0002, time: 20.711282]
2023-06-02 15:31:44.512: epoch 16:	0.02257944  	0.05891206  	0.05128530  
2023-06-02 15:31:44.512: Find a better model.
2023-06-02 15:32:05.178: [iter 17 : loss : 1.0769 = 0.6265 + 0.4500 + 0.0003, time: 20.663500]
2023-06-02 15:32:05.466: epoch 17:	0.02532606  	0.06713798  	0.05719945  
2023-06-02 15:32:05.466: Find a better model.
2023-06-02 15:32:25.986: [iter 18 : loss : 1.0417 = 0.5891 + 0.4521 + 0.0005, time: 20.516985]
2023-06-02 15:32:26.265: epoch 18:	0.02747300  	0.07283312  	0.06153379  
2023-06-02 15:32:26.265: Find a better model.
2023-06-02 15:32:47.200: [iter 19 : loss : 0.9978 = 0.5423 + 0.4548 + 0.0007, time: 20.930604]
2023-06-02 15:32:47.486: epoch 19:	0.02855385  	0.07650688  	0.06373351  
2023-06-02 15:32:47.486: Find a better model.
2023-06-02 15:33:08.181: [iter 20 : loss : 0.9484 = 0.4893 + 0.4581 + 0.0010, time: 20.691423]
2023-06-02 15:33:08.472: epoch 20:	0.02931641  	0.07838212  	0.06492901  
2023-06-02 15:33:08.472: Find a better model.
2023-06-02 15:33:29.198: [iter 21 : loss : 0.8989 = 0.4362 + 0.4615 + 0.0012, time: 20.722061]
2023-06-02 15:33:29.484: epoch 21:	0.02970879  	0.07932003  	0.06558163  
2023-06-02 15:33:29.484: Find a better model.
2023-06-02 15:33:50.173: [iter 22 : loss : 0.8523 = 0.3866 + 0.4641 + 0.0015, time: 20.685447]
2023-06-02 15:33:50.465: epoch 22:	0.02994570  	0.08050520  	0.06632220  
2023-06-02 15:33:50.466: Find a better model.
2023-06-02 15:34:11.425: [iter 23 : loss : 0.8123 = 0.3442 + 0.4662 + 0.0018, time: 20.955812]
2023-06-02 15:34:11.700: epoch 23:	0.03007154  	0.08102630  	0.06654152  
2023-06-02 15:34:11.700: Find a better model.
2023-06-02 15:34:32.176: [iter 24 : loss : 0.7779 = 0.3085 + 0.4672 + 0.0021, time: 20.472033]
2023-06-02 15:34:32.460: epoch 24:	0.02999751  	0.08111824  	0.06666917  
2023-06-02 15:34:32.460: Find a better model.
2023-06-02 15:34:53.135: [iter 25 : loss : 0.7486 = 0.2784 + 0.4678 + 0.0024, time: 20.671859]
2023-06-02 15:34:53.406: epoch 25:	0.03001232  	0.08158846  	0.06681246  
2023-06-02 15:34:53.406: Find a better model.
2023-06-02 15:35:13.978: [iter 26 : loss : 0.7243 = 0.2538 + 0.4678 + 0.0027, time: 20.567638]
2023-06-02 15:35:14.247: epoch 26:	0.03024182  	0.08235078  	0.06723078  
2023-06-02 15:35:14.247: Find a better model.
2023-06-02 15:35:34.941: [iter 27 : loss : 0.7031 = 0.2326 + 0.4675 + 0.0030, time: 20.690593]
2023-06-02 15:35:35.208: epoch 27:	0.03031586  	0.08287466  	0.06746165  
2023-06-02 15:35:35.208: Find a better model.
2023-06-02 15:35:55.734: [iter 28 : loss : 0.6855 = 0.2152 + 0.4671 + 0.0032, time: 20.521517]
2023-06-02 15:35:55.999: epoch 28:	0.03044912  	0.08338397  	0.06764162  
2023-06-02 15:35:55.999: Find a better model.
2023-06-02 15:36:16.505: [iter 29 : loss : 0.6701 = 0.2003 + 0.4663 + 0.0034, time: 20.503119]
2023-06-02 15:36:16.771: epoch 29:	0.03053055  	0.08313926  	0.06769674  
2023-06-02 15:36:37.346: [iter 30 : loss : 0.6558 = 0.1863 + 0.4658 + 0.0037, time: 20.571312]
2023-06-02 15:36:37.621: epoch 30:	0.03047873  	0.08317951  	0.06797709  
2023-06-02 15:36:58.520: [iter 31 : loss : 0.6439 = 0.1748 + 0.4652 + 0.0039, time: 20.894824]
2023-06-02 15:36:58.791: epoch 31:	0.03041209  	0.08309386  	0.06785244  
2023-06-02 15:37:19.878: [iter 32 : loss : 0.6331 = 0.1645 + 0.4644 + 0.0041, time: 21.083000]
2023-06-02 15:37:20.146: epoch 32:	0.03043432  	0.08306118  	0.06793907  
2023-06-02 15:37:41.079: [iter 33 : loss : 0.6243 = 0.1563 + 0.4637 + 0.0043, time: 20.928948]
2023-06-02 15:37:41.349: epoch 33:	0.03063421  	0.08363729  	0.06818495  
2023-06-02 15:37:41.349: Find a better model.
2023-06-02 15:38:02.304: [iter 34 : loss : 0.6157 = 0.1481 + 0.4631 + 0.0045, time: 20.951252]
2023-06-02 15:38:02.570: epoch 34:	0.03045652  	0.08291233  	0.06808301  
2023-06-02 15:38:23.285: [iter 35 : loss : 0.6075 = 0.1403 + 0.4624 + 0.0047, time: 20.710417]
2023-06-02 15:38:23.554: epoch 35:	0.03060459  	0.08352932  	0.06825639  
2023-06-02 15:38:44.280: [iter 36 : loss : 0.6008 = 0.1341 + 0.4618 + 0.0049, time: 20.723174]
2023-06-02 15:38:44.548: epoch 36:	0.03065640  	0.08363207  	0.06849874  
2023-06-02 15:39:05.481: [iter 37 : loss : 0.5947 = 0.1282 + 0.4614 + 0.0051, time: 20.927655]
2023-06-02 15:39:05.742: epoch 37:	0.03056756  	0.08324695  	0.06842274  
2023-06-02 15:39:27.201: [iter 38 : loss : 0.5887 = 0.1226 + 0.4608 + 0.0053, time: 21.456335]
2023-06-02 15:39:27.490: epoch 38:	0.03056015  	0.08355175  	0.06847900  
2023-06-02 15:39:50.473: [iter 39 : loss : 0.5836 = 0.1179 + 0.4602 + 0.0054, time: 22.980361]
2023-06-02 15:39:50.764: epoch 39:	0.03057496  	0.08378270  	0.06855022  
2023-06-02 15:39:50.764: Find a better model.
2023-06-02 15:40:12.041: [iter 40 : loss : 0.5785 = 0.1131 + 0.4599 + 0.0056, time: 21.273667]
2023-06-02 15:40:12.309: epoch 40:	0.03056756  	0.08369176  	0.06864923  
2023-06-02 15:40:33.237: [iter 41 : loss : 0.5736 = 0.1084 + 0.4594 + 0.0058, time: 20.924332]
2023-06-02 15:40:33.509: epoch 41:	0.03056756  	0.08338650  	0.06861484  
2023-06-02 15:40:54.811: [iter 42 : loss : 0.5695 = 0.1046 + 0.4590 + 0.0059, time: 21.299108]
2023-06-02 15:40:55.113: epoch 42:	0.03049352  	0.08301377  	0.06856468  
2023-06-02 15:41:16.315: [iter 43 : loss : 0.5655 = 0.1008 + 0.4587 + 0.0061, time: 21.198000]
2023-06-02 15:41:16.588: epoch 43:	0.03051573  	0.08274077  	0.06843478  
2023-06-02 15:41:37.654: [iter 44 : loss : 0.5626 = 0.0981 + 0.4582 + 0.0062, time: 21.061309]
2023-06-02 15:41:37.922: epoch 44:	0.03048613  	0.08263995  	0.06850501  
2023-06-02 15:41:59.046: [iter 45 : loss : 0.5590 = 0.0947 + 0.4579 + 0.0064, time: 21.119216]
2023-06-02 15:41:59.322: epoch 45:	0.03044171  	0.08228740  	0.06825578  
2023-06-02 15:42:22.560: [iter 46 : loss : 0.5557 = 0.0916 + 0.4575 + 0.0065, time: 23.233858]
2023-06-02 15:42:22.844: epoch 46:	0.03032325  	0.08213297  	0.06812598  
2023-06-02 15:42:44.585: [iter 47 : loss : 0.5527 = 0.0888 + 0.4572 + 0.0067, time: 21.737108]
2023-06-02 15:42:44.849: epoch 47:	0.03027882  	0.08166805  	0.06807715  
2023-06-02 15:43:05.805: [iter 48 : loss : 0.5501 = 0.0864 + 0.4569 + 0.0068, time: 20.952095]
2023-06-02 15:43:06.070: epoch 48:	0.03018259  	0.08124421  	0.06792938  
2023-06-02 15:43:27.143: [iter 49 : loss : 0.5476 = 0.0840 + 0.4566 + 0.0070, time: 21.068552]
2023-06-02 15:43:27.409: epoch 49:	0.03021221  	0.08119768  	0.06804013  
2023-06-02 15:43:48.377: [iter 50 : loss : 0.5456 = 0.0822 + 0.4564 + 0.0071, time: 20.962032]
2023-06-02 15:43:48.643: epoch 50:	0.03002711  	0.08071762  	0.06775790  
2023-06-02 15:44:09.374: [iter 51 : loss : 0.5428 = 0.0796 + 0.4560 + 0.0072, time: 20.728599]
2023-06-02 15:44:09.643: epoch 51:	0.02988646  	0.08000290  	0.06737829  
2023-06-02 15:44:30.545: [iter 52 : loss : 0.5401 = 0.0769 + 0.4558 + 0.0074, time: 20.896465]
2023-06-02 15:44:30.809: epoch 52:	0.02984945  	0.08000137  	0.06726442  
2023-06-02 15:44:51.742: [iter 53 : loss : 0.5390 = 0.0760 + 0.4555 + 0.0075, time: 20.929539]
2023-06-02 15:44:52.005: epoch 53:	0.02976801  	0.07957133  	0.06708977  
2023-06-02 15:45:12.730: [iter 54 : loss : 0.5369 = 0.0740 + 0.4553 + 0.0076, time: 20.721347]
2023-06-02 15:45:12.995: epoch 54:	0.02968657  	0.07933211  	0.06711804  
2023-06-02 15:45:33.934: [iter 55 : loss : 0.5349 = 0.0720 + 0.4552 + 0.0077, time: 20.934070]
2023-06-02 15:45:34.198: epoch 55:	0.02959032  	0.07907804  	0.06687494  
2023-06-02 15:45:55.124: [iter 56 : loss : 0.5329 = 0.0701 + 0.4549 + 0.0079, time: 20.922072]
2023-06-02 15:45:55.389: epoch 56:	0.02943485  	0.07843874  	0.06660174  
2023-06-02 15:46:16.504: [iter 57 : loss : 0.5312 = 0.0685 + 0.4548 + 0.0080, time: 21.111243]
2023-06-02 15:46:16.763: epoch 57:	0.02944967  	0.07877412  	0.06661996  
2023-06-02 15:46:38.025: [iter 58 : loss : 0.5302 = 0.0676 + 0.4546 + 0.0081, time: 21.257514]
2023-06-02 15:46:38.289: epoch 58:	0.02948669  	0.07864153  	0.06656518  
2023-06-02 15:46:59.097: [iter 59 : loss : 0.5282 = 0.0656 + 0.4544 + 0.0082, time: 20.803191]
2023-06-02 15:46:59.364: epoch 59:	0.02936823  	0.07835449  	0.06649554  
2023-06-02 15:47:20.097: [iter 60 : loss : 0.5273 = 0.0646 + 0.4544 + 0.0083, time: 20.730071]
2023-06-02 15:47:20.363: epoch 60:	0.02933862  	0.07809547  	0.06634256  
2023-06-02 15:47:41.281: [iter 61 : loss : 0.5256 = 0.0631 + 0.4541 + 0.0084, time: 20.915752]
2023-06-02 15:47:41.551: epoch 61:	0.02932381  	0.07806723  	0.06618985  
2023-06-02 15:48:02.474: [iter 62 : loss : 0.5247 = 0.0622 + 0.4539 + 0.0085, time: 20.919617]
2023-06-02 15:48:02.740: epoch 62:	0.02920535  	0.07780661  	0.06598105  
2023-06-02 15:48:23.445: [iter 63 : loss : 0.5233 = 0.0609 + 0.4538 + 0.0086, time: 20.701007]
2023-06-02 15:48:23.708: epoch 63:	0.02920534  	0.07733588  	0.06591590  
2023-06-02 15:48:44.464: [iter 64 : loss : 0.5221 = 0.0597 + 0.4536 + 0.0087, time: 20.751074]
2023-06-02 15:48:44.729: epoch 64:	0.02916092  	0.07689860  	0.06585055  
2023-06-02 15:48:44.729: Early stopping is trigger at epoch: 64
2023-06-02 15:48:44.729: best_result@epoch 39:

2023-06-02 15:48:44.729: 		0.0306      	0.0838      	0.0686      
2023-06-02 15:57:16.582: my pid: 13240
2023-06-02 15:57:16.582: model: model.general_recommender.SGL
2023-06-02 15:57:16.583: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 15:57:16.583: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 15:57:20.605: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 15:57:41.085: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 20.478982]
2023-06-02 15:57:41.354: epoch 1:	0.00142878  	0.00313628  	0.00255931  
2023-06-02 15:57:41.354: Find a better model.
2023-06-02 15:58:02.011: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.652477]
2023-06-02 15:58:02.306: epoch 2:	0.00211727  	0.00401552  	0.00340936  
2023-06-02 15:58:02.306: Find a better model.
2023-06-02 15:58:22.823: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 20.513198]
2023-06-02 15:58:23.251: epoch 3:	0.00255404  	0.00485833  	0.00420555  
2023-06-02 15:58:23.251: Find a better model.
2023-06-02 15:58:45.226: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.970788]
2023-06-02 15:58:45.536: epoch 4:	0.00307225  	0.00639382  	0.00543158  
2023-06-02 15:58:45.536: Find a better model.
2023-06-02 15:59:07.172: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 21.633245]
2023-06-02 15:59:07.483: epoch 5:	0.00341279  	0.00738436  	0.00584296  
2023-06-02 15:59:07.484: Find a better model.
2023-06-02 15:59:29.018: [iter 6 : loss : 1.1347 = 0.6925 + 0.4423 + 0.0000, time: 21.530075]
2023-06-02 15:59:29.322: epoch 6:	0.00398283  	0.00975071  	0.00760551  
2023-06-02 15:59:29.322: Find a better model.
2023-06-02 15:59:51.014: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.689126]
2023-06-02 15:59:51.299: epoch 7:	0.00467130  	0.01121791  	0.00899793  
2023-06-02 15:59:51.299: Find a better model.
2023-06-02 16:00:11.717: [iter 8 : loss : 1.1348 = 0.6918 + 0.4430 + 0.0000, time: 20.412623]
2023-06-02 16:00:12.007: epoch 8:	0.00550043  	0.01284051  	0.01032016  
2023-06-02 16:00:12.007: Find a better model.
2023-06-02 16:00:32.457: [iter 9 : loss : 1.1347 = 0.6913 + 0.4435 + 0.0000, time: 20.445404]
2023-06-02 16:00:32.744: epoch 9:	0.00588538  	0.01554470  	0.01223183  
2023-06-02 16:00:32.744: Find a better model.
2023-06-02 16:00:53.042: [iter 10 : loss : 1.1342 = 0.6902 + 0.4440 + 0.0000, time: 20.295337]
2023-06-02 16:00:53.333: epoch 10:	0.00652944  	0.01614631  	0.01308230  
2023-06-02 16:00:53.333: Find a better model.
2023-06-02 16:01:13.639: [iter 11 : loss : 1.1334 = 0.6888 + 0.4446 + 0.0000, time: 20.302001]
2023-06-02 16:01:13.938: epoch 11:	0.00795081  	0.02045380  	0.01694816  
2023-06-02 16:01:13.938: Find a better model.
2023-06-02 16:01:34.231: [iter 12 : loss : 1.1320 = 0.6867 + 0.4453 + 0.0000, time: 20.289836]
2023-06-02 16:01:34.515: epoch 12:	0.01042342  	0.02658043  	0.02321867  
2023-06-02 16:01:34.515: Find a better model.
2023-06-02 16:01:54.824: [iter 13 : loss : 1.1295 = 0.6835 + 0.4459 + 0.0001, time: 20.305069]
2023-06-02 16:01:55.110: epoch 13:	0.01285904  	0.03316090  	0.02981612  
2023-06-02 16:01:55.110: Find a better model.
2023-06-02 16:02:15.601: [iter 14 : loss : 1.1248 = 0.6781 + 0.4467 + 0.0001, time: 20.488085]
2023-06-02 16:02:15.884: epoch 14:	0.01630151  	0.04332626  	0.03774878  
2023-06-02 16:02:15.884: Find a better model.
2023-06-02 16:02:36.247: [iter 15 : loss : 1.1165 = 0.6687 + 0.4476 + 0.0001, time: 20.354167]
2023-06-02 16:02:36.529: epoch 15:	0.01961073  	0.05216521  	0.04517951  
2023-06-02 16:02:36.529: Find a better model.
2023-06-02 16:02:57.579: [iter 16 : loss : 1.1015 = 0.6527 + 0.4485 + 0.0002, time: 21.047444]
2023-06-02 16:02:57.855: epoch 16:	0.02262386  	0.06085481  	0.05193892  
2023-06-02 16:02:57.855: Find a better model.
2023-06-02 16:03:18.555: [iter 17 : loss : 1.0777 = 0.6274 + 0.4500 + 0.0003, time: 20.696088]
2023-06-02 16:03:18.830: epoch 17:	0.02516319  	0.06854249  	0.05744917  
2023-06-02 16:03:18.830: Find a better model.
2023-06-02 16:03:39.746: [iter 18 : loss : 1.0429 = 0.5904 + 0.4521 + 0.0005, time: 20.911412]
2023-06-02 16:03:40.026: epoch 18:	0.02694737  	0.07316267  	0.06127017  
2023-06-02 16:03:40.026: Find a better model.
2023-06-02 16:04:00.790: [iter 19 : loss : 0.9991 = 0.5437 + 0.4547 + 0.0007, time: 20.760330]
2023-06-02 16:04:01.072: epoch 19:	0.02823553  	0.07720471  	0.06388934  
2023-06-02 16:04:01.072: Find a better model.
2023-06-02 16:04:22.805: [iter 20 : loss : 0.9500 = 0.4910 + 0.4580 + 0.0009, time: 21.729084]
2023-06-02 16:04:23.128: epoch 20:	0.02926458  	0.08031051  	0.06572872  
2023-06-02 16:04:23.128: Find a better model.
2023-06-02 16:04:45.430: [iter 21 : loss : 0.9007 = 0.4381 + 0.4613 + 0.0012, time: 22.298670]
2023-06-02 16:04:45.727: epoch 21:	0.02967176  	0.08174750  	0.06647146  
2023-06-02 16:04:45.727: Find a better model.
2023-06-02 16:05:07.795: [iter 22 : loss : 0.8537 = 0.3882 + 0.4640 + 0.0015, time: 22.062274]
2023-06-02 16:05:08.104: epoch 22:	0.02987906  	0.08347389  	0.06717384  
2023-06-02 16:05:08.104: Find a better model.
2023-06-02 16:05:29.485: [iter 23 : loss : 0.8136 = 0.3456 + 0.4661 + 0.0018, time: 21.376163]
2023-06-02 16:05:29.760: epoch 23:	0.03017519  	0.08419939  	0.06746124  
2023-06-02 16:05:29.760: Find a better model.
2023-06-02 16:05:51.359: [iter 24 : loss : 0.7789 = 0.3094 + 0.4673 + 0.0021, time: 21.593713]
2023-06-02 16:05:51.633: epoch 24:	0.03032326  	0.08424040  	0.06764939  
2023-06-02 16:05:51.633: Find a better model.
2023-06-02 16:06:13.028: [iter 25 : loss : 0.7492 = 0.2791 + 0.4677 + 0.0024, time: 21.390781]
2023-06-02 16:06:13.305: epoch 25:	0.03032325  	0.08417078  	0.06769811  
2023-06-02 16:06:35.646: [iter 26 : loss : 0.7246 = 0.2542 + 0.4677 + 0.0027, time: 22.336486]
2023-06-02 16:06:35.960: epoch 26:	0.03032327  	0.08454035  	0.06785115  
2023-06-02 16:06:35.960: Find a better model.
2023-06-02 16:06:57.979: [iter 27 : loss : 0.7036 = 0.2331 + 0.4675 + 0.0030, time: 22.015330]
2023-06-02 16:06:58.267: epoch 27:	0.03035289  	0.08486569  	0.06794921  
2023-06-02 16:06:58.267: Find a better model.
2023-06-02 16:07:20.397: [iter 28 : loss : 0.6857 = 0.2153 + 0.4672 + 0.0032, time: 22.126968]
2023-06-02 16:07:20.683: epoch 28:	0.03036768  	0.08490125  	0.06810845  
2023-06-02 16:07:20.683: Find a better model.
2023-06-02 16:07:42.839: [iter 29 : loss : 0.6703 = 0.2004 + 0.4665 + 0.0034, time: 22.148239]
2023-06-02 16:07:43.136: epoch 29:	0.03033066  	0.08485381  	0.06827858  
2023-06-02 16:08:05.342: [iter 30 : loss : 0.6559 = 0.1864 + 0.4658 + 0.0037, time: 22.200087]
2023-06-02 16:08:05.635: epoch 30:	0.03030846  	0.08458807  	0.06820390  
2023-06-02 16:08:27.237: [iter 31 : loss : 0.6439 = 0.1748 + 0.4652 + 0.0039, time: 21.598269]
2023-06-02 16:08:27.505: epoch 31:	0.03033067  	0.08455942  	0.06835819  
2023-06-02 16:08:48.452: [iter 32 : loss : 0.6335 = 0.1649 + 0.4645 + 0.0041, time: 20.942060]
2023-06-02 16:08:48.721: epoch 32:	0.03035288  	0.08472086  	0.06853689  
2023-06-02 16:09:09.625: [iter 33 : loss : 0.6243 = 0.1562 + 0.4638 + 0.0043, time: 20.900964]
2023-06-02 16:09:09.893: epoch 33:	0.03036767  	0.08404516  	0.06844337  
2023-06-02 16:09:30.841: [iter 34 : loss : 0.6158 = 0.1481 + 0.4632 + 0.0045, time: 20.944569]
2023-06-02 16:09:31.114: epoch 34:	0.03036027  	0.08415540  	0.06844430  
2023-06-02 16:09:51.858: [iter 35 : loss : 0.6071 = 0.1399 + 0.4626 + 0.0047, time: 20.740381]
2023-06-02 16:09:52.130: epoch 35:	0.03035287  	0.08400892  	0.06860264  
2023-06-02 16:10:13.032: [iter 36 : loss : 0.6006 = 0.1337 + 0.4619 + 0.0049, time: 20.897878]
2023-06-02 16:10:13.301: epoch 36:	0.03042690  	0.08450834  	0.06872676  
2023-06-02 16:10:34.382: [iter 37 : loss : 0.5947 = 0.1283 + 0.4613 + 0.0051, time: 21.077065]
2023-06-02 16:10:34.651: epoch 37:	0.03044911  	0.08428174  	0.06873353  
2023-06-02 16:10:55.572: [iter 38 : loss : 0.5884 = 0.1224 + 0.4607 + 0.0053, time: 20.918042]
2023-06-02 16:10:55.838: epoch 38:	0.03043430  	0.08443375  	0.06888190  
2023-06-02 16:11:16.622: [iter 39 : loss : 0.5833 = 0.1175 + 0.4604 + 0.0054, time: 20.779086]
2023-06-02 16:11:16.890: epoch 39:	0.03046392  	0.08456235  	0.06925338  
2023-06-02 16:11:37.777: [iter 40 : loss : 0.5784 = 0.1128 + 0.4600 + 0.0056, time: 20.881381]
2023-06-02 16:11:38.048: epoch 40:	0.03045651  	0.08450895  	0.06920108  
2023-06-02 16:11:58.770: [iter 41 : loss : 0.5734 = 0.1083 + 0.4594 + 0.0058, time: 20.718051]
2023-06-02 16:11:59.042: epoch 41:	0.03050834  	0.08481947  	0.06939795  
2023-06-02 16:12:19.786: [iter 42 : loss : 0.5693 = 0.1044 + 0.4590 + 0.0059, time: 20.738907]
2023-06-02 16:12:20.056: epoch 42:	0.03054536  	0.08492557  	0.06932144  
2023-06-02 16:12:20.056: Find a better model.
2023-06-02 16:12:41.159: [iter 43 : loss : 0.5653 = 0.1006 + 0.4586 + 0.0061, time: 21.099631]
2023-06-02 16:12:41.446: epoch 43:	0.03047132  	0.08458734  	0.06923684  
2023-06-02 16:13:03.627: [iter 44 : loss : 0.5631 = 0.0986 + 0.4583 + 0.0062, time: 22.176203]
2023-06-02 16:13:03.973: epoch 44:	0.03048613  	0.08466420  	0.06928611  
2023-06-02 16:13:26.648: [iter 45 : loss : 0.5589 = 0.0946 + 0.4580 + 0.0064, time: 22.670923]
2023-06-02 16:13:26.933: epoch 45:	0.03046390  	0.08477932  	0.06911693  
2023-06-02 16:13:49.204: [iter 46 : loss : 0.5558 = 0.0918 + 0.4575 + 0.0065, time: 22.261806]
2023-06-02 16:13:49.498: epoch 46:	0.03048613  	0.08465372  	0.06906551  
2023-06-02 16:14:11.560: [iter 47 : loss : 0.5531 = 0.0891 + 0.4573 + 0.0067, time: 22.058347]
2023-06-02 16:14:11.844: epoch 47:	0.03041950  	0.08455259  	0.06911232  
2023-06-02 16:14:34.273: [iter 48 : loss : 0.5499 = 0.0863 + 0.4568 + 0.0068, time: 22.424525]
2023-06-02 16:14:34.562: epoch 48:	0.03027884  	0.08412212  	0.06880980  
2023-06-02 16:14:56.949: [iter 49 : loss : 0.5473 = 0.0838 + 0.4565 + 0.0070, time: 22.379844]
2023-06-02 16:14:57.237: epoch 49:	0.03014558  	0.08386980  	0.06868834  
2023-06-02 16:15:19.156: [iter 50 : loss : 0.5451 = 0.0817 + 0.4564 + 0.0071, time: 21.914831]
2023-06-02 16:15:19.444: epoch 50:	0.02999012  	0.08331022  	0.06847887  
2023-06-02 16:15:40.467: [iter 51 : loss : 0.5424 = 0.0792 + 0.4560 + 0.0072, time: 21.020035]
2023-06-02 16:15:40.736: epoch 51:	0.03002714  	0.08342233  	0.06859016  
2023-06-02 16:16:01.666: [iter 52 : loss : 0.5403 = 0.0771 + 0.4558 + 0.0073, time: 20.926328]
2023-06-02 16:16:01.947: epoch 52:	0.02995310  	0.08297614  	0.06840438  
2023-06-02 16:16:22.627: [iter 53 : loss : 0.5390 = 0.0760 + 0.4556 + 0.0075, time: 20.676028]
2023-06-02 16:16:22.894: epoch 53:	0.02993089  	0.08279870  	0.06817555  
2023-06-02 16:16:43.840: [iter 54 : loss : 0.5369 = 0.0740 + 0.4553 + 0.0076, time: 20.940187]
2023-06-02 16:16:44.112: epoch 54:	0.02981244  	0.08225995  	0.06804646  
2023-06-02 16:17:04.847: [iter 55 : loss : 0.5348 = 0.0720 + 0.4551 + 0.0077, time: 20.730247]
2023-06-02 16:17:05.116: epoch 55:	0.02974580  	0.08165618  	0.06769169  
2023-06-02 16:17:25.662: [iter 56 : loss : 0.5331 = 0.0704 + 0.4548 + 0.0079, time: 20.541052]
2023-06-02 16:17:25.944: epoch 56:	0.02960514  	0.08101947  	0.06742515  
2023-06-02 16:17:46.608: [iter 57 : loss : 0.5312 = 0.0684 + 0.4548 + 0.0080, time: 20.661277]
2023-06-02 16:17:46.879: epoch 57:	0.02953111  	0.08075947  	0.06742697  
2023-06-02 16:18:07.611: [iter 58 : loss : 0.5298 = 0.0670 + 0.4546 + 0.0081, time: 20.728153]
2023-06-02 16:18:07.877: epoch 58:	0.02954592  	0.08089901  	0.06724475  
2023-06-02 16:18:28.435: [iter 59 : loss : 0.5284 = 0.0659 + 0.4544 + 0.0082, time: 20.553583]
2023-06-02 16:18:28.702: epoch 59:	0.02950891  	0.08054300  	0.06718633  
2023-06-02 16:18:49.427: [iter 60 : loss : 0.5271 = 0.0646 + 0.4542 + 0.0083, time: 20.720305]
2023-06-02 16:18:49.697: epoch 60:	0.02933864  	0.08003989  	0.06678118  
2023-06-02 16:19:10.400: [iter 61 : loss : 0.5253 = 0.0629 + 0.4540 + 0.0084, time: 20.698179]
2023-06-02 16:19:10.669: epoch 61:	0.02924981  	0.07982172  	0.06655906  
2023-06-02 16:19:31.366: [iter 62 : loss : 0.5248 = 0.0624 + 0.4539 + 0.0085, time: 20.693799]
2023-06-02 16:19:31.633: epoch 62:	0.02923500  	0.07996651  	0.06654972  
2023-06-02 16:19:52.401: [iter 63 : loss : 0.5236 = 0.0611 + 0.4538 + 0.0086, time: 20.765051]
2023-06-02 16:19:52.669: epoch 63:	0.02910174  	0.07960939  	0.06649785  
2023-06-02 16:20:13.383: [iter 64 : loss : 0.5221 = 0.0597 + 0.4536 + 0.0087, time: 20.711067]
2023-06-02 16:20:13.649: epoch 64:	0.02913876  	0.07939546  	0.06628077  
2023-06-02 16:20:34.529: [iter 65 : loss : 0.5211 = 0.0587 + 0.4536 + 0.0088, time: 20.876021]
2023-06-02 16:20:34.795: epoch 65:	0.02895367  	0.07916900  	0.06626092  
2023-06-02 16:20:55.551: [iter 66 : loss : 0.5199 = 0.0575 + 0.4534 + 0.0090, time: 20.751203]
2023-06-02 16:20:55.816: epoch 66:	0.02887224  	0.07909162  	0.06597629  
2023-06-02 16:21:16.543: [iter 67 : loss : 0.5190 = 0.0568 + 0.4532 + 0.0091, time: 20.723057]
2023-06-02 16:21:16.808: epoch 67:	0.02878340  	0.07847293  	0.06561802  
2023-06-02 16:21:16.808: Early stopping is trigger at epoch: 67
2023-06-02 16:21:16.809: best_result@epoch 42:

2023-06-02 16:21:16.809: 		0.0305      	0.0849      	0.0693      
2023-06-02 16:49:50.738: my pid: 1400
2023-06-02 16:49:50.738: model: model.general_recommender.SGL
2023-06-02 16:49:50.738: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 16:49:50.738: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 16:49:55.415: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 16:50:17.657: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 22.241190]
2023-06-02 16:50:17.941: epoch 1:	0.00122890  	0.00226294  	0.00197152  
2023-06-02 16:50:17.941: Find a better model.
2023-06-02 16:50:40.100: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 22.154922]
2023-06-02 16:50:40.445: epoch 2:	0.00175452  	0.00322440  	0.00273182  
2023-06-02 16:50:40.445: Find a better model.
2023-06-02 16:51:02.709: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 22.261724]
2023-06-02 16:51:03.028: epoch 3:	0.00218389  	0.00455071  	0.00341320  
2023-06-02 16:51:03.028: Find a better model.
2023-06-02 16:51:24.698: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.666655]
2023-06-02 16:51:24.989: epoch 4:	0.00265028  	0.00528285  	0.00432979  
2023-06-02 16:51:24.989: Find a better model.
2023-06-02 16:51:46.243: [iter 5 : loss : 1.1346 = 0.6926 + 0.4419 + 0.0000, time: 21.251228]
2023-06-02 16:51:46.547: epoch 5:	0.00306485  	0.00618714  	0.00509432  
2023-06-02 16:51:46.547: Find a better model.
2023-06-02 16:52:07.642: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.092085]
2023-06-02 16:52:07.933: epoch 6:	0.00401244  	0.00876116  	0.00706466  
2023-06-02 16:52:07.934: Find a better model.
2023-06-02 16:52:29.030: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.093091]
2023-06-02 16:52:29.323: epoch 7:	0.00496742  	0.01205179  	0.00947663  
2023-06-02 16:52:29.323: Find a better model.
2023-06-02 16:52:50.407: [iter 8 : loss : 1.1348 = 0.6919 + 0.4430 + 0.0000, time: 21.080022]
2023-06-02 16:52:50.697: epoch 8:	0.00581135  	0.01444685  	0.01162993  
2023-06-02 16:52:50.697: Find a better model.
2023-06-02 16:53:11.785: [iter 9 : loss : 1.1348 = 0.6913 + 0.4434 + 0.0000, time: 21.085245]
2023-06-02 16:53:12.070: epoch 9:	0.00647762  	0.01728873  	0.01412234  
2023-06-02 16:53:12.070: Find a better model.
2023-06-02 16:53:32.804: [iter 10 : loss : 1.1344 = 0.6904 + 0.4440 + 0.0000, time: 20.731175]
2023-06-02 16:53:33.090: epoch 10:	0.00704024  	0.01832657  	0.01527409  
2023-06-02 16:53:33.090: Find a better model.
2023-06-02 16:53:53.812: [iter 11 : loss : 1.1335 = 0.6890 + 0.4445 + 0.0000, time: 20.719041]
2023-06-02 16:53:54.101: epoch 11:	0.00879474  	0.02308093  	0.01905508  
2023-06-02 16:53:54.101: Find a better model.
2023-06-02 16:54:14.971: [iter 12 : loss : 1.1320 = 0.6869 + 0.4451 + 0.0000, time: 20.865539]
2023-06-02 16:54:15.260: epoch 12:	0.01074175  	0.02801635  	0.02335261  
2023-06-02 16:54:15.260: Find a better model.
2023-06-02 16:54:35.958: [iter 13 : loss : 1.1297 = 0.6836 + 0.4460 + 0.0001, time: 20.694550]
2023-06-02 16:54:36.239: epoch 13:	0.01338466  	0.03574116  	0.03019944  
2023-06-02 16:54:36.239: Find a better model.
2023-06-02 16:54:56.951: [iter 14 : loss : 1.1249 = 0.6782 + 0.4467 + 0.0001, time: 20.707170]
2023-06-02 16:54:57.235: epoch 14:	0.01658283  	0.04371455  	0.03777208  
2023-06-02 16:54:57.235: Find a better model.
2023-06-02 16:55:17.958: [iter 15 : loss : 1.1165 = 0.6688 + 0.4476 + 0.0001, time: 20.719879]
2023-06-02 16:55:18.240: epoch 15:	0.01966254  	0.05163598  	0.04459338  
2023-06-02 16:55:18.240: Find a better model.
2023-06-02 16:55:39.564: [iter 16 : loss : 1.1014 = 0.6527 + 0.4484 + 0.0002, time: 21.320438]
2023-06-02 16:55:39.833: epoch 16:	0.02278673  	0.05956198  	0.05145182  
2023-06-02 16:55:39.833: Find a better model.
2023-06-02 16:56:01.117: [iter 17 : loss : 1.0776 = 0.6273 + 0.4500 + 0.0003, time: 21.279037]
2023-06-02 16:56:01.416: epoch 17:	0.02526683  	0.06683154  	0.05673929  
2023-06-02 16:56:01.416: Find a better model.
2023-06-02 16:56:22.535: [iter 18 : loss : 1.0427 = 0.5902 + 0.4521 + 0.0005, time: 21.115677]
2023-06-02 16:56:22.810: epoch 18:	0.02680672  	0.07170538  	0.06034033  
2023-06-02 16:56:22.810: Find a better model.
2023-06-02 16:56:44.094: [iter 19 : loss : 0.9986 = 0.5433 + 0.4547 + 0.0007, time: 21.280102]
2023-06-02 16:56:44.371: epoch 19:	0.02840582  	0.07703543  	0.06356594  
2023-06-02 16:56:44.371: Find a better model.
2023-06-02 16:57:05.702: [iter 20 : loss : 0.9492 = 0.4903 + 0.4580 + 0.0009, time: 21.320041]
2023-06-02 16:57:05.979: epoch 20:	0.02925718  	0.07917137  	0.06524572  
2023-06-02 16:57:05.979: Find a better model.
2023-06-02 16:57:27.315: [iter 21 : loss : 0.8998 = 0.4373 + 0.4613 + 0.0012, time: 21.332001]
2023-06-02 16:57:27.597: epoch 21:	0.02955331  	0.08047117  	0.06600628  
2023-06-02 16:57:27.597: Find a better model.
2023-06-02 16:57:48.899: [iter 22 : loss : 0.8529 = 0.3875 + 0.4639 + 0.0015, time: 21.298369]
2023-06-02 16:57:49.166: epoch 22:	0.02964217  	0.08118006  	0.06648404  
2023-06-02 16:57:49.166: Find a better model.
2023-06-02 16:58:10.476: [iter 23 : loss : 0.8127 = 0.3450 + 0.4659 + 0.0018, time: 21.306520]
2023-06-02 16:58:10.744: epoch 23:	0.02988648  	0.08235303  	0.06672305  
2023-06-02 16:58:10.745: Find a better model.
2023-06-02 16:58:32.100: [iter 24 : loss : 0.7784 = 0.3091 + 0.4671 + 0.0021, time: 21.352008]
2023-06-02 16:58:32.367: epoch 24:	0.02997533  	0.08256202  	0.06691625  
2023-06-02 16:58:32.367: Find a better model.
2023-06-02 16:58:53.661: [iter 25 : loss : 0.7486 = 0.2787 + 0.4675 + 0.0024, time: 21.291847]
2023-06-02 16:58:53.926: epoch 25:	0.03005678  	0.08303062  	0.06701164  
2023-06-02 16:58:53.926: Find a better model.
2023-06-02 16:59:15.275: [iter 26 : loss : 0.7242 = 0.2539 + 0.4676 + 0.0027, time: 21.345007]
2023-06-02 16:59:15.549: epoch 26:	0.03012342  	0.08334870  	0.06726496  
2023-06-02 16:59:15.549: Find a better model.
2023-06-02 16:59:37.084: [iter 27 : loss : 0.7029 = 0.2327 + 0.4673 + 0.0030, time: 21.530222]
2023-06-02 16:59:37.354: epoch 27:	0.03019005  	0.08381764  	0.06740443  
2023-06-02 16:59:37.354: Find a better model.
2023-06-02 16:59:58.636: [iter 28 : loss : 0.6849 = 0.2149 + 0.4668 + 0.0032, time: 21.277109]
2023-06-02 16:59:58.900: epoch 28:	0.03020485  	0.08365431  	0.06738828  
2023-06-02 17:00:20.276: [iter 29 : loss : 0.6697 = 0.2000 + 0.4662 + 0.0034, time: 21.371240]
2023-06-02 17:00:20.549: epoch 29:	0.03031591  	0.08389746  	0.06756535  
2023-06-02 17:00:20.549: Find a better model.
2023-06-02 17:00:41.831: [iter 30 : loss : 0.6556 = 0.1863 + 0.4656 + 0.0037, time: 21.277806]
2023-06-02 17:00:42.100: epoch 30:	0.03037513  	0.08426638  	0.06755397  
2023-06-02 17:00:42.100: Find a better model.
2023-06-02 17:01:03.627: [iter 31 : loss : 0.6432 = 0.1743 + 0.4650 + 0.0039, time: 21.522158]
2023-06-02 17:01:03.892: epoch 31:	0.03053801  	0.08502508  	0.06791739  
2023-06-02 17:01:03.892: Find a better model.
2023-06-02 17:01:25.235: [iter 32 : loss : 0.6331 = 0.1647 + 0.4643 + 0.0041, time: 21.339027]
2023-06-02 17:01:25.511: epoch 32:	0.03056762  	0.08517096  	0.06801081  
2023-06-02 17:01:25.511: Find a better model.
2023-06-02 17:01:46.809: [iter 33 : loss : 0.6240 = 0.1561 + 0.4637 + 0.0043, time: 21.294019]
2023-06-02 17:01:47.074: epoch 33:	0.03062683  	0.08546129  	0.06827367  
2023-06-02 17:01:47.075: Find a better model.
2023-06-02 17:02:08.222: [iter 34 : loss : 0.6156 = 0.1481 + 0.4630 + 0.0045, time: 21.144025]
2023-06-02 17:02:08.503: epoch 34:	0.03064904  	0.08534639  	0.06821115  
2023-06-02 17:02:29.985: [iter 35 : loss : 0.6068 = 0.1398 + 0.4624 + 0.0047, time: 21.476990]
2023-06-02 17:02:30.249: epoch 35:	0.03074528  	0.08561233  	0.06850845  
2023-06-02 17:02:30.249: Find a better model.
2023-06-02 17:02:51.595: [iter 36 : loss : 0.6001 = 0.1334 + 0.4618 + 0.0049, time: 21.342229]
2023-06-02 17:02:51.860: epoch 36:	0.03063423  	0.08526807  	0.06844817  
2023-06-02 17:03:13.176: [iter 37 : loss : 0.5942 = 0.1279 + 0.4612 + 0.0051, time: 21.311018]
2023-06-02 17:03:13.453: epoch 37:	0.03061202  	0.08515146  	0.06842069  
2023-06-02 17:03:34.963: [iter 38 : loss : 0.5881 = 0.1222 + 0.4606 + 0.0053, time: 21.506500]
2023-06-02 17:03:35.227: epoch 38:	0.03064903  	0.08509369  	0.06828392  
2023-06-02 17:03:56.595: [iter 39 : loss : 0.5834 = 0.1177 + 0.4602 + 0.0054, time: 21.363216]
2023-06-02 17:03:56.859: epoch 39:	0.03048616  	0.08484697  	0.06825641  
2023-06-02 17:04:18.145: [iter 40 : loss : 0.5780 = 0.1128 + 0.4597 + 0.0056, time: 21.281631]
2023-06-02 17:04:18.425: epoch 40:	0.03049356  	0.08492112  	0.06838290  
2023-06-02 17:04:39.766: [iter 41 : loss : 0.5731 = 0.1080 + 0.4593 + 0.0058, time: 21.337086]
2023-06-02 17:04:40.032: epoch 41:	0.03052318  	0.08501641  	0.06845474  
2023-06-02 17:05:01.531: [iter 42 : loss : 0.5691 = 0.1043 + 0.4590 + 0.0059, time: 21.490050]
2023-06-02 17:05:01.797: epoch 42:	0.03045654  	0.08485416  	0.06834143  
2023-06-02 17:05:23.122: [iter 43 : loss : 0.5653 = 0.1006 + 0.4585 + 0.0061, time: 21.321093]
2023-06-02 17:05:23.403: epoch 43:	0.03039732  	0.08437286  	0.06818403  
2023-06-02 17:05:44.553: [iter 44 : loss : 0.5624 = 0.0980 + 0.4581 + 0.0062, time: 21.143365]
2023-06-02 17:05:44.817: epoch 44:	0.03033809  	0.08433922  	0.06820781  
2023-06-02 17:06:06.339: [iter 45 : loss : 0.5588 = 0.0946 + 0.4578 + 0.0064, time: 21.518553]
2023-06-02 17:06:06.607: epoch 45:	0.03030107  	0.08370230  	0.06807250  
2023-06-02 17:06:27.913: [iter 46 : loss : 0.5556 = 0.0915 + 0.4575 + 0.0065, time: 21.300356]
2023-06-02 17:06:28.177: epoch 46:	0.03028627  	0.08373723  	0.06807477  
2023-06-02 17:06:49.362: [iter 47 : loss : 0.5525 = 0.0886 + 0.4572 + 0.0067, time: 21.181571]
2023-06-02 17:06:49.629: epoch 47:	0.03026407  	0.08324918  	0.06792071  
2023-06-02 17:07:10.738: [iter 48 : loss : 0.5497 = 0.0861 + 0.4568 + 0.0068, time: 21.106022]
2023-06-02 17:07:11.001: epoch 48:	0.03010860  	0.08277173  	0.06777708  
2023-06-02 17:07:32.108: [iter 49 : loss : 0.5473 = 0.0838 + 0.4565 + 0.0070, time: 21.102017]
2023-06-02 17:07:32.369: epoch 49:	0.03004937  	0.08258881  	0.06759848  
2023-06-02 17:07:53.496: [iter 50 : loss : 0.5452 = 0.0818 + 0.4563 + 0.0071, time: 21.117057]
2023-06-02 17:07:53.762: epoch 50:	0.02989390  	0.08242683  	0.06744374  
2023-06-02 17:08:15.075: [iter 51 : loss : 0.5425 = 0.0792 + 0.4561 + 0.0072, time: 21.309052]
2023-06-02 17:08:15.343: epoch 51:	0.02981247  	0.08174393  	0.06714578  
2023-06-02 17:08:36.504: [iter 52 : loss : 0.5399 = 0.0769 + 0.4557 + 0.0073, time: 21.156259]
2023-06-02 17:08:36.769: epoch 52:	0.02975324  	0.08166645  	0.06712533  
2023-06-02 17:08:58.054: [iter 53 : loss : 0.5392 = 0.0762 + 0.4555 + 0.0075, time: 21.281032]
2023-06-02 17:08:58.318: epoch 53:	0.02970883  	0.08131137  	0.06698251  
2023-06-02 17:09:19.331: [iter 54 : loss : 0.5367 = 0.0739 + 0.4552 + 0.0076, time: 21.009058]
2023-06-02 17:09:19.602: epoch 54:	0.02964218  	0.08102411  	0.06684286  
2023-06-02 17:09:40.838: [iter 55 : loss : 0.5350 = 0.0722 + 0.4551 + 0.0077, time: 21.231042]
2023-06-02 17:09:41.105: epoch 55:	0.02954595  	0.08078904  	0.06674533  
2023-06-02 17:10:02.276: [iter 56 : loss : 0.5327 = 0.0700 + 0.4548 + 0.0079, time: 21.167035]
2023-06-02 17:10:02.544: epoch 56:	0.02941268  	0.08016493  	0.06653283  
2023-06-02 17:10:23.669: [iter 57 : loss : 0.5308 = 0.0682 + 0.4546 + 0.0080, time: 21.121027]
2023-06-02 17:10:23.932: epoch 57:	0.02943490  	0.08011403  	0.06655462  
2023-06-02 17:10:45.215: [iter 58 : loss : 0.5295 = 0.0669 + 0.4545 + 0.0081, time: 21.279001]
2023-06-02 17:10:45.490: epoch 58:	0.02949413  	0.08009285  	0.06641179  
2023-06-02 17:11:06.821: [iter 59 : loss : 0.5278 = 0.0653 + 0.4543 + 0.0082, time: 21.328453]
2023-06-02 17:11:07.083: epoch 59:	0.02941268  	0.07961950  	0.06621092  
2023-06-02 17:11:28.214: [iter 60 : loss : 0.5269 = 0.0644 + 0.4542 + 0.0083, time: 21.126529]
2023-06-02 17:11:28.489: epoch 60:	0.02942749  	0.07951717  	0.06610660  
2023-06-02 17:11:28.489: Early stopping is trigger at epoch: 60
2023-06-02 17:11:28.489: best_result@epoch 35:

2023-06-02 17:11:28.489: 		0.0307      	0.0856      	0.0685      
2023-06-02 17:15:18.678: my pid: 7648
2023-06-02 17:15:18.678: model: model.general_recommender.SGL
2023-06-02 17:15:18.678: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 17:15:18.678: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 17:15:22.780: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 17:15:43.952: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.169639]
2023-06-02 17:15:44.219: epoch 1:	0.00117708  	0.00238847  	0.00195853  
2023-06-02 17:15:44.219: Find a better model.
2023-06-02 17:16:05.385: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.160987]
2023-06-02 17:16:05.675: epoch 2:	0.00173231  	0.00331134  	0.00281544  
2023-06-02 17:16:05.675: Find a better model.
2023-06-02 17:16:26.767: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.088631]
2023-06-02 17:16:27.059: epoch 3:	0.00208025  	0.00380434  	0.00316227  
2023-06-02 17:16:27.059: Find a better model.
2023-06-02 17:16:48.152: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 21.089254]
2023-06-02 17:16:48.440: epoch 4:	0.00262067  	0.00586857  	0.00470534  
2023-06-02 17:16:48.440: Find a better model.
2023-06-02 17:17:09.720: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.276899]
2023-06-02 17:17:10.008: epoch 5:	0.00330175  	0.00698588  	0.00595037  
2023-06-02 17:17:10.008: Find a better model.
2023-06-02 17:17:31.151: [iter 6 : loss : 1.1346 = 0.6925 + 0.4422 + 0.0000, time: 21.139368]
2023-06-02 17:17:31.438: epoch 6:	0.00409387  	0.00935021  	0.00745241  
2023-06-02 17:17:31.438: Find a better model.
2023-06-02 17:17:52.543: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 21.101135]
2023-06-02 17:17:52.836: epoch 7:	0.00459727  	0.01091589  	0.00877803  
2023-06-02 17:17:52.837: Find a better model.
2023-06-02 17:18:13.785: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 20.944148]
2023-06-02 17:18:14.071: epoch 8:	0.00533756  	0.01303778  	0.01070128  
2023-06-02 17:18:14.071: Find a better model.
2023-06-02 17:18:35.124: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 21.050080]
2023-06-02 17:18:35.406: epoch 9:	0.00662568  	0.01718501  	0.01418958  
2023-06-02 17:18:35.406: Find a better model.
2023-06-02 17:18:56.345: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 20.934816]
2023-06-02 17:18:56.628: epoch 10:	0.00767690  	0.02035583  	0.01690010  
2023-06-02 17:18:56.628: Find a better model.
2023-06-02 17:19:17.519: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 20.888278]
2023-06-02 17:19:17.814: epoch 11:	0.00903164  	0.02334693  	0.02031161  
2023-06-02 17:19:17.814: Find a better model.
2023-06-02 17:19:38.715: [iter 12 : loss : 1.1322 = 0.6872 + 0.4450 + 0.0000, time: 20.896318]
2023-06-02 17:19:38.994: epoch 12:	0.01079358  	0.02894541  	0.02456461  
2023-06-02 17:19:38.995: Find a better model.
2023-06-02 17:19:59.718: [iter 13 : loss : 1.1298 = 0.6841 + 0.4457 + 0.0001, time: 20.719356]
2023-06-02 17:19:59.992: epoch 13:	0.01296269  	0.03447208  	0.03030593  
2023-06-02 17:19:59.992: Find a better model.
2023-06-02 17:20:20.684: [iter 14 : loss : 1.1255 = 0.6789 + 0.4465 + 0.0001, time: 20.688343]
2023-06-02 17:20:20.961: epoch 14:	0.01613865  	0.04324150  	0.03754946  
2023-06-02 17:20:20.961: Find a better model.
2023-06-02 17:20:41.661: [iter 15 : loss : 1.1176 = 0.6701 + 0.4473 + 0.0001, time: 20.695446]
2023-06-02 17:20:41.942: epoch 15:	0.01949971  	0.05120222  	0.04482744  
2023-06-02 17:20:41.942: Find a better model.
2023-06-02 17:21:03.466: [iter 16 : loss : 1.1033 = 0.6548 + 0.4483 + 0.0002, time: 21.519876]
2023-06-02 17:21:03.762: epoch 16:	0.02223153  	0.05883038  	0.05114396  
2023-06-02 17:21:03.762: Find a better model.
2023-06-02 17:21:25.029: [iter 17 : loss : 1.0805 = 0.6305 + 0.4497 + 0.0003, time: 21.263175]
2023-06-02 17:21:25.305: epoch 17:	0.02481525  	0.06540634  	0.05612709  
2023-06-02 17:21:25.305: Find a better model.
2023-06-02 17:21:46.677: [iter 18 : loss : 1.0465 = 0.5943 + 0.4518 + 0.0005, time: 21.367797]
2023-06-02 17:21:46.953: epoch 18:	0.02684375  	0.07105882  	0.06036521  
2023-06-02 17:21:46.953: Find a better model.
2023-06-02 17:22:08.251: [iter 19 : loss : 1.0029 = 0.5478 + 0.4545 + 0.0007, time: 21.292839]
2023-06-02 17:22:08.521: epoch 19:	0.02837621  	0.07535800  	0.06351069  
2023-06-02 17:22:08.522: Find a better model.
2023-06-02 17:22:29.893: [iter 20 : loss : 0.9537 = 0.4949 + 0.4578 + 0.0009, time: 21.367759]
2023-06-02 17:22:30.163: epoch 20:	0.02898326  	0.07726032  	0.06503663  
2023-06-02 17:22:30.163: Find a better model.
2023-06-02 17:22:51.392: [iter 21 : loss : 0.9038 = 0.4415 + 0.4611 + 0.0012, time: 21.225209]
2023-06-02 17:22:51.656: epoch 21:	0.02941266  	0.07956661  	0.06616322  
2023-06-02 17:22:51.656: Find a better model.
2023-06-02 17:23:12.852: [iter 22 : loss : 0.8564 = 0.3910 + 0.4639 + 0.0015, time: 21.191303]
2023-06-02 17:23:13.118: epoch 22:	0.02948670  	0.08034202  	0.06620823  
2023-06-02 17:23:13.118: Find a better model.
2023-06-02 17:23:34.602: [iter 23 : loss : 0.8155 = 0.3477 + 0.4660 + 0.0018, time: 21.480342]
2023-06-02 17:23:34.874: epoch 23:	0.02959034  	0.08081719  	0.06627443  
2023-06-02 17:23:34.875: Find a better model.
2023-06-02 17:23:56.053: [iter 24 : loss : 0.7805 = 0.3112 + 0.4671 + 0.0021, time: 21.174099]
2023-06-02 17:23:56.322: epoch 24:	0.02964956  	0.08107667  	0.06642281  
2023-06-02 17:23:56.322: Find a better model.
2023-06-02 17:24:17.648: [iter 25 : loss : 0.7506 = 0.2807 + 0.4675 + 0.0024, time: 21.322077]
2023-06-02 17:24:17.922: epoch 25:	0.02987165  	0.08173888  	0.06671188  
2023-06-02 17:24:17.922: Find a better model.
2023-06-02 17:24:39.216: [iter 26 : loss : 0.7258 = 0.2556 + 0.4676 + 0.0027, time: 21.290078]
2023-06-02 17:24:39.486: epoch 26:	0.02999751  	0.08250453  	0.06694185  
2023-06-02 17:24:39.486: Find a better model.
2023-06-02 17:25:00.795: [iter 27 : loss : 0.7045 = 0.2342 + 0.4673 + 0.0029, time: 21.305138]
2023-06-02 17:25:01.062: epoch 27:	0.02998270  	0.08207379  	0.06695608  
2023-06-02 17:25:22.404: [iter 28 : loss : 0.6861 = 0.2160 + 0.4669 + 0.0032, time: 21.336794]
2023-06-02 17:25:22.668: epoch 28:	0.02999012  	0.08231906  	0.06704562  
2023-06-02 17:25:44.141: [iter 29 : loss : 0.6707 = 0.2010 + 0.4663 + 0.0034, time: 21.468704]
2023-06-02 17:25:44.412: epoch 29:	0.03013818  	0.08253603  	0.06729552  
2023-06-02 17:25:44.413: Find a better model.
2023-06-02 17:26:05.798: [iter 30 : loss : 0.6564 = 0.1871 + 0.4656 + 0.0037, time: 21.380990]
2023-06-02 17:26:06.064: epoch 30:	0.03032326  	0.08314800  	0.06743843  
2023-06-02 17:26:06.064: Find a better model.
2023-06-02 17:26:27.571: [iter 31 : loss : 0.6443 = 0.1753 + 0.4651 + 0.0039, time: 21.502990]
2023-06-02 17:26:27.843: epoch 31:	0.03037508  	0.08328655  	0.06761258  
2023-06-02 17:26:27.844: Find a better model.
2023-06-02 17:26:49.540: [iter 32 : loss : 0.6339 = 0.1655 + 0.4643 + 0.0041, time: 21.693942]
2023-06-02 17:26:49.817: epoch 32:	0.03044170  	0.08316979  	0.06760013  
2023-06-02 17:27:11.330: [iter 33 : loss : 0.6245 = 0.1566 + 0.4636 + 0.0043, time: 21.507313]
2023-06-02 17:27:11.594: epoch 33:	0.03055274  	0.08288544  	0.06761869  
2023-06-02 17:27:32.979: [iter 34 : loss : 0.6158 = 0.1484 + 0.4629 + 0.0045, time: 21.379989]
2023-06-02 17:27:33.243: epoch 34:	0.03064898  	0.08329494  	0.06781135  
2023-06-02 17:27:33.243: Find a better model.
2023-06-02 17:27:54.740: [iter 35 : loss : 0.6074 = 0.1403 + 0.4623 + 0.0047, time: 21.492675]
2023-06-02 17:27:55.004: epoch 35:	0.03056755  	0.08313665  	0.06782550  
2023-06-02 17:28:16.494: [iter 36 : loss : 0.6011 = 0.1346 + 0.4616 + 0.0049, time: 21.486014]
2023-06-02 17:28:16.775: epoch 36:	0.03047131  	0.08288427  	0.06783814  
2023-06-02 17:28:38.110: [iter 37 : loss : 0.5948 = 0.1285 + 0.4612 + 0.0051, time: 21.332032]
2023-06-02 17:28:38.377: epoch 37:	0.03037507  	0.08242088  	0.06763241  
2023-06-02 17:28:59.930: [iter 38 : loss : 0.5889 = 0.1230 + 0.4606 + 0.0052, time: 21.548768]
2023-06-02 17:29:00.201: epoch 38:	0.03055275  	0.08266753  	0.06776443  
2023-06-02 17:29:21.632: [iter 39 : loss : 0.5834 = 0.1179 + 0.4601 + 0.0054, time: 21.426989]
2023-06-02 17:29:21.903: epoch 39:	0.03050833  	0.08267146  	0.06776571  
2023-06-02 17:29:43.090: [iter 40 : loss : 0.5785 = 0.1132 + 0.4597 + 0.0056, time: 21.183029]
2023-06-02 17:29:43.356: epoch 40:	0.03038988  	0.08233692  	0.06763356  
2023-06-02 17:30:04.855: [iter 41 : loss : 0.5737 = 0.1087 + 0.4592 + 0.0057, time: 21.496028]
2023-06-02 17:30:05.121: epoch 41:	0.03031585  	0.08224066  	0.06774438  
2023-06-02 17:30:26.508: [iter 42 : loss : 0.5694 = 0.1047 + 0.4588 + 0.0059, time: 21.383369]
2023-06-02 17:30:26.789: epoch 42:	0.03030845  	0.08200774  	0.06771374  
2023-06-02 17:30:48.269: [iter 43 : loss : 0.5654 = 0.1010 + 0.4584 + 0.0061, time: 21.476370]
2023-06-02 17:30:48.533: epoch 43:	0.03036028  	0.08248265  	0.06791795  
2023-06-02 17:31:09.901: [iter 44 : loss : 0.5629 = 0.0985 + 0.4581 + 0.0062, time: 21.364415]
2023-06-02 17:31:10.167: epoch 44:	0.03036028  	0.08249241  	0.06783547  
2023-06-02 17:31:31.497: [iter 45 : loss : 0.5589 = 0.0949 + 0.4577 + 0.0064, time: 21.324991]
2023-06-02 17:31:31.779: epoch 45:	0.03039729  	0.08256479  	0.06797066  
2023-06-02 17:31:53.252: [iter 46 : loss : 0.5557 = 0.0919 + 0.4573 + 0.0065, time: 21.468410]
2023-06-02 17:31:53.515: epoch 46:	0.03034547  	0.08231995  	0.06787452  
2023-06-02 17:32:14.858: [iter 47 : loss : 0.5526 = 0.0889 + 0.4570 + 0.0067, time: 21.337431]
2023-06-02 17:32:15.123: epoch 47:	0.03031586  	0.08250068  	0.06788596  
2023-06-02 17:32:36.433: [iter 48 : loss : 0.5499 = 0.0864 + 0.4567 + 0.0068, time: 21.307083]
2023-06-02 17:32:36.701: epoch 48:	0.03038989  	0.08263475  	0.06784170  
2023-06-02 17:32:58.254: [iter 49 : loss : 0.5477 = 0.0843 + 0.4564 + 0.0069, time: 21.549464]
2023-06-02 17:32:58.541: epoch 49:	0.03031585  	0.08254153  	0.06772829  
2023-06-02 17:33:19.863: [iter 50 : loss : 0.5451 = 0.0819 + 0.4561 + 0.0071, time: 21.315223]
2023-06-02 17:33:20.128: epoch 50:	0.03023441  	0.08201625  	0.06749914  
2023-06-02 17:33:41.575: [iter 51 : loss : 0.5427 = 0.0796 + 0.4559 + 0.0072, time: 21.444272]
2023-06-02 17:33:41.849: epoch 51:	0.03016779  	0.08174408  	0.06752115  
2023-06-02 17:34:03.230: [iter 52 : loss : 0.5400 = 0.0771 + 0.4556 + 0.0073, time: 21.377445]
2023-06-02 17:34:03.497: epoch 52:	0.03021961  	0.08150733  	0.06742450  
2023-06-02 17:34:24.808: [iter 53 : loss : 0.5391 = 0.0762 + 0.4554 + 0.0075, time: 21.307321]
2023-06-02 17:34:25.072: epoch 53:	0.03019739  	0.08180200  	0.06738533  
2023-06-02 17:34:46.203: [iter 54 : loss : 0.5368 = 0.0741 + 0.4551 + 0.0076, time: 21.127633]
2023-06-02 17:34:46.471: epoch 54:	0.03001972  	0.08104324  	0.06709010  
2023-06-02 17:35:07.782: [iter 55 : loss : 0.5347 = 0.0720 + 0.4549 + 0.0077, time: 21.307656]
2023-06-02 17:35:08.044: epoch 55:	0.02987164  	0.08038585  	0.06679372  
2023-06-02 17:35:29.592: [iter 56 : loss : 0.5327 = 0.0702 + 0.4547 + 0.0078, time: 21.545053]
2023-06-02 17:35:29.864: epoch 56:	0.02987165  	0.08028436  	0.06673122  
2023-06-02 17:35:51.147: [iter 57 : loss : 0.5311 = 0.0686 + 0.4546 + 0.0080, time: 21.279945]
2023-06-02 17:35:51.411: epoch 57:	0.02970878  	0.07968552  	0.06644815  
2023-06-02 17:36:12.783: [iter 58 : loss : 0.5296 = 0.0672 + 0.4544 + 0.0081, time: 21.366772]
2023-06-02 17:36:13.047: epoch 58:	0.02963474  	0.07929388  	0.06627865  
2023-06-02 17:36:34.533: [iter 59 : loss : 0.5282 = 0.0658 + 0.4542 + 0.0082, time: 21.482708]
2023-06-02 17:36:34.809: epoch 59:	0.02970137  	0.07954091  	0.06639922  
2023-06-02 17:36:34.810: Early stopping is trigger at epoch: 59
2023-06-02 17:36:34.810: best_result@epoch 34:

2023-06-02 17:36:34.810: 		0.0306      	0.0833      	0.0678      
2023-06-02 18:23:59.978: my pid: 14644
2023-06-02 18:23:59.978: model: model.general_recommender.SGL
2023-06-02 18:23:59.978: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 18:23:59.978: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 18:24:04.090: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 18:24:25.604: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.514634]
2023-06-02 18:24:25.873: epoch 1:	0.00139917  	0.00287086  	0.00244481  
2023-06-02 18:24:25.873: Find a better model.
2023-06-02 18:24:47.465: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.588845]
2023-06-02 18:24:47.776: epoch 2:	0.00171750  	0.00298597  	0.00265842  
2023-06-02 18:24:47.776: Find a better model.
2023-06-02 18:25:09.424: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 21.642323]
2023-06-02 18:25:09.738: epoch 3:	0.00219870  	0.00426145  	0.00354357  
2023-06-02 18:25:09.738: Find a better model.
2023-06-02 18:25:31.383: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.642326]
2023-06-02 18:25:31.680: epoch 4:	0.00241339  	0.00513144  	0.00455271  
2023-06-02 18:25:31.680: Find a better model.
2023-06-02 18:25:53.378: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.694587]
2023-06-02 18:25:53.673: epoch 5:	0.00270210  	0.00565110  	0.00499976  
2023-06-02 18:25:53.673: Find a better model.
2023-06-02 18:26:15.372: [iter 6 : loss : 1.1346 = 0.6925 + 0.4420 + 0.0000, time: 21.695385]
2023-06-02 18:26:15.674: epoch 6:	0.00353865  	0.00827462  	0.00653955  
2023-06-02 18:26:15.674: Find a better model.
2023-06-02 18:26:37.209: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 21.532507]
2023-06-02 18:26:37.521: epoch 7:	0.00434557  	0.01141325  	0.00885131  
2023-06-02 18:26:37.521: Find a better model.
2023-06-02 18:26:59.170: [iter 8 : loss : 1.1347 = 0.6919 + 0.4427 + 0.0000, time: 21.645099]
2023-06-02 18:26:59.470: epoch 8:	0.00552264  	0.01426379  	0.01152974  
2023-06-02 18:26:59.470: Find a better model.
2023-06-02 18:27:21.159: [iter 9 : loss : 1.1346 = 0.6915 + 0.4431 + 0.0000, time: 21.684476]
2023-06-02 18:27:21.463: epoch 9:	0.00689218  	0.01858383  	0.01505269  
2023-06-02 18:27:21.463: Find a better model.
2023-06-02 18:27:43.133: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 21.667049]
2023-06-02 18:27:43.438: epoch 10:	0.00801003  	0.02104526  	0.01731237  
2023-06-02 18:27:43.438: Find a better model.
2023-06-02 18:28:05.151: [iter 11 : loss : 1.1339 = 0.6897 + 0.4441 + 0.0000, time: 21.709878]
2023-06-02 18:28:05.464: epoch 11:	0.00955724  	0.02550761  	0.02113287  
2023-06-02 18:28:05.465: Find a better model.
2023-06-02 18:28:27.525: [iter 12 : loss : 1.1326 = 0.6878 + 0.4448 + 0.0000, time: 22.056206]
2023-06-02 18:28:27.820: epoch 12:	0.01100826  	0.02992963  	0.02516237  
2023-06-02 18:28:27.820: Find a better model.
2023-06-02 18:28:49.323: [iter 13 : loss : 1.1303 = 0.6848 + 0.4454 + 0.0001, time: 21.499065]
2023-06-02 18:28:49.620: epoch 13:	0.01334764  	0.03631549  	0.03115848  
2023-06-02 18:28:49.620: Find a better model.
2023-06-02 18:29:11.306: [iter 14 : loss : 1.1262 = 0.6799 + 0.4463 + 0.0001, time: 21.683090]
2023-06-02 18:29:11.604: epoch 14:	0.01608680  	0.04327502  	0.03734440  
2023-06-02 18:29:11.604: Find a better model.
2023-06-02 18:29:33.310: [iter 15 : loss : 1.1188 = 0.6716 + 0.4471 + 0.0001, time: 21.702270]
2023-06-02 18:29:33.601: epoch 15:	0.01912951  	0.05097740  	0.04441895  
2023-06-02 18:29:33.601: Find a better model.
2023-06-02 18:29:55.877: [iter 16 : loss : 1.1054 = 0.6572 + 0.4480 + 0.0002, time: 22.271809]
2023-06-02 18:29:56.169: epoch 16:	0.02232030  	0.05947834  	0.05129181  
2023-06-02 18:29:56.169: Find a better model.
2023-06-02 18:30:18.302: [iter 17 : loss : 1.0836 = 0.6340 + 0.4494 + 0.0003, time: 22.128083]
2023-06-02 18:30:18.593: epoch 17:	0.02492627  	0.06618433  	0.05688758  
2023-06-02 18:30:18.593: Find a better model.
2023-06-02 18:30:40.905: [iter 18 : loss : 1.0508 = 0.5991 + 0.4513 + 0.0004, time: 22.308520]
2023-06-02 18:30:41.203: epoch 18:	0.02665123  	0.07098890  	0.06074633  
2023-06-02 18:30:41.203: Find a better model.
2023-06-02 18:31:03.497: [iter 19 : loss : 1.0083 = 0.5538 + 0.4539 + 0.0006, time: 22.291085]
2023-06-02 18:31:03.786: epoch 19:	0.02792457  	0.07465506  	0.06344550  
2023-06-02 18:31:03.786: Find a better model.
2023-06-02 18:31:26.269: [iter 20 : loss : 0.9594 = 0.5013 + 0.4572 + 0.0009, time: 22.480328]
2023-06-02 18:31:26.553: epoch 20:	0.02852426  	0.07629994  	0.06473374  
2023-06-02 18:31:26.553: Find a better model.
2023-06-02 18:31:48.857: [iter 21 : loss : 0.9091 = 0.4474 + 0.4605 + 0.0012, time: 22.298756]
2023-06-02 18:31:49.141: epoch 21:	0.02907210  	0.07782388  	0.06540471  
2023-06-02 18:31:49.142: Find a better model.
2023-06-02 18:32:11.283: [iter 22 : loss : 0.8611 = 0.3963 + 0.4633 + 0.0015, time: 22.138244]
2023-06-02 18:32:11.565: epoch 22:	0.02917575  	0.07886107  	0.06590340  
2023-06-02 18:32:11.565: Find a better model.
2023-06-02 18:32:33.637: [iter 23 : loss : 0.8196 = 0.3522 + 0.4656 + 0.0018, time: 22.069230]
2023-06-02 18:32:33.921: epoch 23:	0.02940526  	0.07985550  	0.06609891  
2023-06-02 18:32:33.921: Find a better model.
2023-06-02 18:32:56.025: [iter 24 : loss : 0.7836 = 0.3148 + 0.4668 + 0.0021, time: 22.100214]
2023-06-02 18:32:56.326: epoch 24:	0.02959034  	0.08075626  	0.06661963  
2023-06-02 18:32:56.326: Find a better model.
2023-06-02 18:33:18.239: [iter 25 : loss : 0.7536 = 0.2839 + 0.4674 + 0.0024, time: 21.909384]
2023-06-02 18:33:18.525: epoch 25:	0.02968658  	0.08163990  	0.06696935  
2023-06-02 18:33:18.526: Find a better model.
2023-06-02 18:33:40.449: [iter 26 : loss : 0.7281 = 0.2581 + 0.4673 + 0.0026, time: 21.919030]
2023-06-02 18:33:40.725: epoch 26:	0.02975322  	0.08174653  	0.06744904  
2023-06-02 18:33:40.726: Find a better model.
2023-06-02 18:34:02.790: [iter 27 : loss : 0.7065 = 0.2364 + 0.4672 + 0.0029, time: 22.061049]
2023-06-02 18:34:03.072: epoch 27:	0.02990868  	0.08200186  	0.06740356  
2023-06-02 18:34:03.072: Find a better model.
2023-06-02 18:34:24.996: [iter 28 : loss : 0.6879 = 0.2180 + 0.4667 + 0.0032, time: 21.921288]
2023-06-02 18:34:25.275: epoch 28:	0.02996790  	0.08209726  	0.06748325  
2023-06-02 18:34:25.275: Find a better model.
2023-06-02 18:34:47.230: [iter 29 : loss : 0.6721 = 0.2026 + 0.4661 + 0.0034, time: 21.947328]
2023-06-02 18:34:47.507: epoch 29:	0.03003454  	0.08225871  	0.06760147  
2023-06-02 18:34:47.507: Find a better model.
2023-06-02 18:35:09.398: [iter 30 : loss : 0.6579 = 0.1889 + 0.4654 + 0.0036, time: 21.888036]
2023-06-02 18:35:09.674: epoch 30:	0.03002713  	0.08226572  	0.06768015  
2023-06-02 18:35:09.674: Find a better model.
2023-06-02 18:35:31.542: [iter 31 : loss : 0.6453 = 0.1767 + 0.4648 + 0.0039, time: 21.865009]
2023-06-02 18:35:31.816: epoch 31:	0.03007896  	0.08226407  	0.06776486  
2023-06-02 18:35:53.757: [iter 32 : loss : 0.6346 = 0.1665 + 0.4640 + 0.0041, time: 21.938581]
2023-06-02 18:35:54.029: epoch 32:	0.03018259  	0.08290532  	0.06813969  
2023-06-02 18:35:54.030: Find a better model.
2023-06-02 18:36:15.963: [iter 33 : loss : 0.6258 = 0.1580 + 0.4635 + 0.0043, time: 21.929555]
2023-06-02 18:36:16.233: epoch 33:	0.03019741  	0.08269579  	0.06811287  
2023-06-02 18:36:38.139: [iter 34 : loss : 0.6168 = 0.1496 + 0.4627 + 0.0045, time: 21.902063]
2023-06-02 18:36:38.419: epoch 34:	0.03023441  	0.08278149  	0.06804205  
2023-06-02 18:37:00.164: [iter 35 : loss : 0.6080 = 0.1413 + 0.4620 + 0.0047, time: 21.742365]
2023-06-02 18:37:00.438: epoch 35:	0.03008636  	0.08229172  	0.06801032  
2023-06-02 18:37:22.318: [iter 36 : loss : 0.6014 = 0.1350 + 0.4615 + 0.0049, time: 21.875037]
2023-06-02 18:37:22.584: epoch 36:	0.03025663  	0.08278336  	0.06809682  
2023-06-02 18:37:44.506: [iter 37 : loss : 0.5952 = 0.1291 + 0.4610 + 0.0050, time: 21.916989]
2023-06-02 18:37:44.774: epoch 37:	0.03014559  	0.08261920  	0.06809026  
2023-06-02 18:38:06.734: [iter 38 : loss : 0.5889 = 0.1232 + 0.4604 + 0.0052, time: 21.956126]
2023-06-02 18:38:07.002: epoch 38:	0.03022702  	0.08276156  	0.06811844  
2023-06-02 18:38:29.087: [iter 39 : loss : 0.5841 = 0.1190 + 0.4597 + 0.0054, time: 22.081015]
2023-06-02 18:38:29.370: epoch 39:	0.03026403  	0.08301813  	0.06843106  
2023-06-02 18:38:29.370: Find a better model.
2023-06-02 18:38:51.482: [iter 40 : loss : 0.5789 = 0.1140 + 0.4593 + 0.0056, time: 22.109922]
2023-06-02 18:38:51.749: epoch 40:	0.03015298  	0.08231156  	0.06824699  
2023-06-02 18:39:13.924: [iter 41 : loss : 0.5739 = 0.1091 + 0.4590 + 0.0057, time: 22.170076]
2023-06-02 18:39:14.191: epoch 41:	0.03012337  	0.08227149  	0.06826858  
2023-06-02 18:39:36.463: [iter 42 : loss : 0.5696 = 0.1052 + 0.4585 + 0.0059, time: 22.268059]
2023-06-02 18:39:36.729: epoch 42:	0.03020479  	0.08255726  	0.06829732  
2023-06-02 18:39:59.331: [iter 43 : loss : 0.5658 = 0.1017 + 0.4581 + 0.0061, time: 22.599204]
2023-06-02 18:39:59.598: epoch 43:	0.03004933  	0.08238009  	0.06816918  
2023-06-02 18:40:22.086: [iter 44 : loss : 0.5632 = 0.0992 + 0.4578 + 0.0062, time: 22.483727]
2023-06-02 18:40:22.365: epoch 44:	0.03013817  	0.08265428  	0.06833985  
2023-06-02 18:40:44.682: [iter 45 : loss : 0.5591 = 0.0954 + 0.4573 + 0.0064, time: 22.314091]
2023-06-02 18:40:44.949: epoch 45:	0.03015297  	0.08267134  	0.06835733  
2023-06-02 18:41:07.680: [iter 46 : loss : 0.5557 = 0.0921 + 0.4571 + 0.0065, time: 22.727996]
2023-06-02 18:41:07.945: epoch 46:	0.03014558  	0.08262739  	0.06831993  
2023-06-02 18:41:30.482: [iter 47 : loss : 0.5526 = 0.0892 + 0.4567 + 0.0066, time: 22.533063]
2023-06-02 18:41:30.746: epoch 47:	0.03008634  	0.08221351  	0.06804742  
2023-06-02 18:41:53.261: [iter 48 : loss : 0.5501 = 0.0868 + 0.4565 + 0.0068, time: 22.510086]
2023-06-02 18:41:53.528: epoch 48:	0.03008635  	0.08204818  	0.06809992  
2023-06-02 18:42:15.844: [iter 49 : loss : 0.5477 = 0.0846 + 0.4562 + 0.0069, time: 22.313457]
2023-06-02 18:42:16.111: epoch 49:	0.03006414  	0.08175314  	0.06811855  
2023-06-02 18:42:38.622: [iter 50 : loss : 0.5452 = 0.0823 + 0.4559 + 0.0071, time: 22.507433]
2023-06-02 18:42:38.903: epoch 50:	0.02998270  	0.08165964  	0.06791851  
2023-06-02 18:43:01.610: [iter 51 : loss : 0.5426 = 0.0798 + 0.4556 + 0.0072, time: 22.704104]
2023-06-02 18:43:01.874: epoch 51:	0.02984945  	0.08102642  	0.06776597  
2023-06-02 18:43:24.628: [iter 52 : loss : 0.5400 = 0.0774 + 0.4553 + 0.0073, time: 22.751264]
2023-06-02 18:43:24.894: epoch 52:	0.02979763  	0.08083279  	0.06745265  
2023-06-02 18:43:47.415: [iter 53 : loss : 0.5392 = 0.0766 + 0.4551 + 0.0075, time: 22.518065]
2023-06-02 18:43:47.681: epoch 53:	0.02959033  	0.08057998  	0.06723463  
2023-06-02 18:44:10.401: [iter 54 : loss : 0.5371 = 0.0745 + 0.4549 + 0.0076, time: 22.714153]
2023-06-02 18:44:10.665: epoch 54:	0.02960514  	0.08051585  	0.06710335  
2023-06-02 18:44:33.013: [iter 55 : loss : 0.5349 = 0.0725 + 0.4547 + 0.0077, time: 22.344726]
2023-06-02 18:44:33.283: epoch 55:	0.02956073  	0.08024664  	0.06704289  
2023-06-02 18:44:55.999: [iter 56 : loss : 0.5326 = 0.0704 + 0.4544 + 0.0078, time: 22.706866]
2023-06-02 18:44:56.265: epoch 56:	0.02959774  	0.08010971  	0.06698735  
2023-06-02 18:45:19.186: [iter 57 : loss : 0.5308 = 0.0686 + 0.4543 + 0.0079, time: 22.916988]
2023-06-02 18:45:19.463: epoch 57:	0.02952371  	0.07956581  	0.06678533  
2023-06-02 18:45:42.175: [iter 58 : loss : 0.5299 = 0.0677 + 0.4541 + 0.0081, time: 22.706997]
2023-06-02 18:45:42.448: epoch 58:	0.02949409  	0.07988875  	0.06677329  
2023-06-02 18:46:05.191: [iter 59 : loss : 0.5279 = 0.0658 + 0.4540 + 0.0082, time: 22.739459]
2023-06-02 18:46:05.465: epoch 59:	0.02961995  	0.08012616  	0.06691027  
2023-06-02 18:46:28.336: [iter 60 : loss : 0.5267 = 0.0647 + 0.4537 + 0.0083, time: 22.867870]
2023-06-02 18:46:28.602: epoch 60:	0.02958293  	0.07971343  	0.06667224  
2023-06-02 18:46:50.951: [iter 61 : loss : 0.5254 = 0.0634 + 0.4536 + 0.0084, time: 22.345296]
2023-06-02 18:46:51.219: epoch 61:	0.02958293  	0.07950687  	0.06657949  
2023-06-02 18:47:14.142: [iter 62 : loss : 0.5246 = 0.0627 + 0.4535 + 0.0085, time: 22.919079]
2023-06-02 18:47:14.422: epoch 62:	0.02936823  	0.07854521  	0.06615218  
2023-06-02 18:47:37.302: [iter 63 : loss : 0.5234 = 0.0614 + 0.4533 + 0.0086, time: 22.875410]
2023-06-02 18:47:37.569: epoch 63:	0.02930901  	0.07848999  	0.06632680  
2023-06-02 18:48:00.107: [iter 64 : loss : 0.5220 = 0.0601 + 0.4532 + 0.0087, time: 22.534461]
2023-06-02 18:48:00.384: epoch 64:	0.02927199  	0.07827944  	0.06614311  
2023-06-02 18:48:00.384: Early stopping is trigger at epoch: 64
2023-06-02 18:48:00.384: best_result@epoch 39:

2023-06-02 18:48:00.384: 		0.0303      	0.0830      	0.0684      
2023-06-02 19:41:24.256: my pid: 5816
2023-06-02 19:41:24.256: model: model.general_recommender.SGL
2023-06-02 19:41:24.256: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 19:41:24.256: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 19:41:28.385: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 19:41:49.557: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.169663]
2023-06-02 19:41:49.830: epoch 1:	0.00143619  	0.00279789  	0.00238986  
2023-06-02 19:41:49.830: Find a better model.
2023-06-02 19:42:10.931: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.098114]
2023-06-02 19:42:11.224: epoch 2:	0.00163607  	0.00299639  	0.00261196  
2023-06-02 19:42:11.225: Find a better model.
2023-06-02 19:42:32.162: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 20.933035]
2023-06-02 19:42:32.450: epoch 3:	0.00233195  	0.00473967  	0.00384713  
2023-06-02 19:42:32.450: Find a better model.
2023-06-02 19:42:53.383: [iter 4 : loss : 1.1344 = 0.6928 + 0.4415 + 0.0000, time: 20.929212]
2023-06-02 19:42:53.684: epoch 4:	0.00266509  	0.00578939  	0.00482899  
2023-06-02 19:42:53.684: Find a better model.
2023-06-02 19:43:14.756: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.068023]
2023-06-02 19:43:15.045: epoch 5:	0.00312408  	0.00735851  	0.00571394  
2023-06-02 19:43:15.045: Find a better model.
2023-06-02 19:43:35.924: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 20.875810]
2023-06-02 19:43:36.214: epoch 6:	0.00362748  	0.00846281  	0.00667769  
2023-06-02 19:43:36.214: Find a better model.
2023-06-02 19:43:57.139: [iter 7 : loss : 1.1348 = 0.6922 + 0.4425 + 0.0000, time: 20.920042]
2023-06-02 19:43:57.428: epoch 7:	0.00430856  	0.01082550  	0.00835015  
2023-06-02 19:43:57.428: Find a better model.
2023-06-02 19:44:18.357: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 20.925152]
2023-06-02 19:44:18.654: epoch 8:	0.00510067  	0.01332103  	0.01061002  
2023-06-02 19:44:18.654: Find a better model.
2023-06-02 19:44:39.302: [iter 9 : loss : 1.1348 = 0.6914 + 0.4433 + 0.0000, time: 20.645304]
2023-06-02 19:44:39.606: epoch 9:	0.00615929  	0.01527517  	0.01266936  
2023-06-02 19:44:39.606: Find a better model.
2023-06-02 19:45:00.320: [iter 10 : loss : 1.1344 = 0.6906 + 0.4438 + 0.0000, time: 20.710056]
2023-06-02 19:45:00.620: epoch 10:	0.00656645  	0.01766069  	0.01459780  
2023-06-02 19:45:00.620: Find a better model.
2023-06-02 19:45:21.496: [iter 11 : loss : 1.1337 = 0.6893 + 0.4444 + 0.0000, time: 20.871694]
2023-06-02 19:45:21.780: epoch 11:	0.00774352  	0.02087095  	0.01722411  
2023-06-02 19:45:21.780: Find a better model.
2023-06-02 19:45:42.488: [iter 12 : loss : 1.1323 = 0.6872 + 0.4450 + 0.0000, time: 20.704417]
2023-06-02 19:45:42.775: epoch 12:	0.00943880  	0.02573785  	0.02139386  
2023-06-02 19:45:42.775: Find a better model.
2023-06-02 19:46:03.468: [iter 13 : loss : 1.1301 = 0.6842 + 0.4458 + 0.0001, time: 20.689286]
2023-06-02 19:46:03.753: epoch 13:	0.01220015  	0.03302851  	0.02763989  
2023-06-02 19:46:03.753: Find a better model.
2023-06-02 19:46:24.459: [iter 14 : loss : 1.1259 = 0.6793 + 0.4465 + 0.0001, time: 20.700049]
2023-06-02 19:46:24.749: epoch 14:	0.01559081  	0.04168467  	0.03565203  
2023-06-02 19:46:24.749: Find a better model.
2023-06-02 19:46:45.449: [iter 15 : loss : 1.1183 = 0.6708 + 0.4474 + 0.0001, time: 20.695721]
2023-06-02 19:46:45.729: epoch 15:	0.01887783  	0.05029878  	0.04336440  
2023-06-02 19:46:45.729: Find a better model.
2023-06-02 19:47:07.056: [iter 16 : loss : 1.1047 = 0.6561 + 0.4484 + 0.0002, time: 21.322034]
2023-06-02 19:47:07.336: epoch 16:	0.02195018  	0.05881261  	0.05094745  
2023-06-02 19:47:07.336: Find a better model.
2023-06-02 19:47:28.632: [iter 17 : loss : 1.0825 = 0.6326 + 0.4496 + 0.0003, time: 21.292016]
2023-06-02 19:47:28.911: epoch 17:	0.02491148  	0.06556414  	0.05672606  
2023-06-02 19:47:28.911: Find a better model.
2023-06-02 19:47:50.219: [iter 18 : loss : 1.0496 = 0.5976 + 0.4516 + 0.0004, time: 21.303516]
2023-06-02 19:47:50.494: epoch 18:	0.02683633  	0.07090964  	0.06038854  
2023-06-02 19:47:50.494: Find a better model.
2023-06-02 19:48:11.636: [iter 19 : loss : 1.0073 = 0.5525 + 0.4542 + 0.0006, time: 21.137021]
2023-06-02 19:48:11.913: epoch 19:	0.02825036  	0.07437792  	0.06277794  
2023-06-02 19:48:11.913: Find a better model.
2023-06-02 19:48:33.039: [iter 20 : loss : 0.9586 = 0.5005 + 0.4572 + 0.0009, time: 21.121540]
2023-06-02 19:48:33.311: epoch 20:	0.02899808  	0.07741720  	0.06449346  
2023-06-02 19:48:33.311: Find a better model.
2023-06-02 19:48:54.401: [iter 21 : loss : 0.9090 = 0.4472 + 0.4605 + 0.0012, time: 21.086495]
2023-06-02 19:48:54.690: epoch 21:	0.02950149  	0.07891732  	0.06536771  
2023-06-02 19:48:54.690: Find a better model.
2023-06-02 19:49:15.968: [iter 22 : loss : 0.8615 = 0.3967 + 0.4634 + 0.0015, time: 21.274240]
2023-06-02 19:49:16.236: epoch 22:	0.02955332  	0.07981943  	0.06575204  
2023-06-02 19:49:16.236: Find a better model.
2023-06-02 19:49:37.409: [iter 23 : loss : 0.8201 = 0.3530 + 0.4654 + 0.0018, time: 21.168635]
2023-06-02 19:49:37.693: epoch 23:	0.02959033  	0.07996947  	0.06588715  
2023-06-02 19:49:37.693: Find a better model.
2023-06-02 19:49:58.969: [iter 24 : loss : 0.7846 = 0.3158 + 0.4668 + 0.0021, time: 21.271033]
2023-06-02 19:49:59.236: epoch 24:	0.02971620  	0.08066180  	0.06634295  
2023-06-02 19:49:59.242: Find a better model.
2023-06-02 19:50:20.356: [iter 25 : loss : 0.7541 = 0.2843 + 0.4674 + 0.0024, time: 21.111564]
2023-06-02 19:50:20.632: epoch 25:	0.02995310  	0.08114112  	0.06647313  
2023-06-02 19:50:20.632: Find a better model.
2023-06-02 19:50:41.734: [iter 26 : loss : 0.7290 = 0.2587 + 0.4676 + 0.0026, time: 21.096217]
2023-06-02 19:50:41.999: epoch 26:	0.03004935  	0.08105028  	0.06652433  
2023-06-02 19:51:03.357: [iter 27 : loss : 0.7069 = 0.2366 + 0.4674 + 0.0029, time: 21.354315]
2023-06-02 19:51:03.638: epoch 27:	0.03010858  	0.08146318  	0.06670494  
2023-06-02 19:51:03.638: Find a better model.
2023-06-02 19:51:24.988: [iter 28 : loss : 0.6884 = 0.2182 + 0.4670 + 0.0032, time: 21.345040]
2023-06-02 19:51:25.273: epoch 28:	0.03013819  	0.08164653  	0.06685538  
2023-06-02 19:51:25.273: Find a better model.
2023-06-02 19:51:46.538: [iter 29 : loss : 0.6730 = 0.2031 + 0.4665 + 0.0034, time: 21.260005]
2023-06-02 19:51:46.809: epoch 29:	0.03024183  	0.08160987  	0.06727230  
2023-06-02 19:52:08.109: [iter 30 : loss : 0.6585 = 0.1890 + 0.4659 + 0.0036, time: 21.297087]
2023-06-02 19:52:08.378: epoch 30:	0.03016040  	0.08158162  	0.06731749  
2023-06-02 19:52:29.703: [iter 31 : loss : 0.6459 = 0.1768 + 0.4652 + 0.0039, time: 21.322019]
2023-06-02 19:52:29.968: epoch 31:	0.03032327  	0.08195822  	0.06741688  
2023-06-02 19:52:29.968: Find a better model.
2023-06-02 19:52:51.325: [iter 32 : loss : 0.6350 = 0.1664 + 0.4645 + 0.0041, time: 21.353009]
2023-06-02 19:52:51.604: epoch 32:	0.03044171  	0.08196101  	0.06759217  
2023-06-02 19:52:51.604: Find a better model.
2023-06-02 19:53:12.893: [iter 33 : loss : 0.6261 = 0.1578 + 0.4639 + 0.0043, time: 21.283042]
2023-06-02 19:53:13.176: epoch 33:	0.03049353  	0.08178198  	0.06750944  
2023-06-02 19:53:34.490: [iter 34 : loss : 0.6170 = 0.1493 + 0.4633 + 0.0045, time: 21.309456]
2023-06-02 19:53:34.763: epoch 34:	0.03048612  	0.08179797  	0.06752211  
2023-06-02 19:53:56.069: [iter 35 : loss : 0.6087 = 0.1415 + 0.4625 + 0.0047, time: 21.301011]
2023-06-02 19:53:56.336: epoch 35:	0.03047132  	0.08198621  	0.06772089  
2023-06-02 19:53:56.336: Find a better model.
2023-06-02 19:54:17.462: [iter 36 : loss : 0.6019 = 0.1350 + 0.4621 + 0.0049, time: 21.121011]
2023-06-02 19:54:17.733: epoch 36:	0.03037508  	0.08198036  	0.06769514  
2023-06-02 19:54:38.899: [iter 37 : loss : 0.5956 = 0.1291 + 0.4614 + 0.0051, time: 21.163350]
2023-06-02 19:54:39.163: epoch 37:	0.03056016  	0.08201452  	0.06764383  
2023-06-02 19:54:39.163: Find a better model.
2023-06-02 19:55:00.438: [iter 38 : loss : 0.5896 = 0.1234 + 0.4610 + 0.0052, time: 21.271055]
2023-06-02 19:55:00.710: epoch 38:	0.03054535  	0.08195365  	0.06774363  
2023-06-02 19:55:21.863: [iter 39 : loss : 0.5844 = 0.1186 + 0.4604 + 0.0054, time: 21.149045]
2023-06-02 19:55:22.130: epoch 39:	0.03056756  	0.08195031  	0.06764270  
2023-06-02 19:55:43.469: [iter 40 : loss : 0.5791 = 0.1136 + 0.4599 + 0.0056, time: 21.335257]
2023-06-02 19:55:43.746: epoch 40:	0.03050093  	0.08150474  	0.06766601  
2023-06-02 19:56:05.083: [iter 41 : loss : 0.5742 = 0.1090 + 0.4595 + 0.0057, time: 21.332043]
2023-06-02 19:56:05.348: epoch 41:	0.03046391  	0.08156342  	0.06769143  
2023-06-02 19:56:26.418: [iter 42 : loss : 0.5700 = 0.1050 + 0.4592 + 0.0059, time: 21.065066]
2023-06-02 19:56:26.691: epoch 42:	0.03043430  	0.08133254  	0.06746230  
2023-06-02 19:56:47.857: [iter 43 : loss : 0.5661 = 0.1013 + 0.4588 + 0.0061, time: 21.161076]
2023-06-02 19:56:48.125: epoch 43:	0.03031585  	0.08107337  	0.06738081  
2023-06-02 19:57:09.458: [iter 44 : loss : 0.5632 = 0.0986 + 0.4583 + 0.0062, time: 21.327282]
2023-06-02 19:57:09.735: epoch 44:	0.03036768  	0.08125392  	0.06731852  
2023-06-02 19:57:30.624: [iter 45 : loss : 0.5593 = 0.0949 + 0.4580 + 0.0064, time: 20.885529]
2023-06-02 19:57:30.890: epoch 45:	0.03027144  	0.08103902  	0.06718013  
2023-06-02 19:57:52.011: [iter 46 : loss : 0.5565 = 0.0924 + 0.4576 + 0.0065, time: 21.117070]
2023-06-02 19:57:52.278: epoch 46:	0.03024183  	0.08080512  	0.06695516  
2023-06-02 19:58:13.421: [iter 47 : loss : 0.5528 = 0.0888 + 0.4573 + 0.0067, time: 21.139246]
2023-06-02 19:58:13.692: epoch 47:	0.03021962  	0.08048388  	0.06685736  
2023-06-02 19:58:35.009: [iter 48 : loss : 0.5505 = 0.0867 + 0.4569 + 0.0068, time: 21.313047]
2023-06-02 19:58:35.275: epoch 48:	0.03019001  	0.08023813  	0.06664331  
2023-06-02 19:58:56.404: [iter 49 : loss : 0.5479 = 0.0842 + 0.4567 + 0.0069, time: 21.125051]
2023-06-02 19:58:56.680: epoch 49:	0.03021222  	0.08020252  	0.06671616  
2023-06-02 19:59:17.981: [iter 50 : loss : 0.5458 = 0.0823 + 0.4564 + 0.0071, time: 21.296242]
2023-06-02 19:59:18.245: epoch 50:	0.03018260  	0.07988227  	0.06649921  
2023-06-02 19:59:39.369: [iter 51 : loss : 0.5430 = 0.0796 + 0.4562 + 0.0072, time: 21.120277]
2023-06-02 19:59:39.648: epoch 51:	0.03009376  	0.07957169  	0.06637772  
2023-06-02 20:00:00.795: [iter 52 : loss : 0.5403 = 0.0770 + 0.4559 + 0.0073, time: 21.141258]
2023-06-02 20:00:01.061: epoch 52:	0.03004195  	0.07948012  	0.06630889  
2023-06-02 20:00:22.388: [iter 53 : loss : 0.5392 = 0.0760 + 0.4557 + 0.0075, time: 21.322051]
2023-06-02 20:00:22.665: epoch 53:	0.03001233  	0.07962053  	0.06631227  
2023-06-02 20:00:43.744: [iter 54 : loss : 0.5372 = 0.0742 + 0.4554 + 0.0076, time: 21.075080]
2023-06-02 20:00:44.012: epoch 54:	0.02999011  	0.07913096  	0.06611652  
2023-06-02 20:01:05.170: [iter 55 : loss : 0.5351 = 0.0721 + 0.4553 + 0.0077, time: 21.154112]
2023-06-02 20:01:05.437: epoch 55:	0.02990868  	0.07870352  	0.06601093  
2023-06-02 20:01:26.579: [iter 56 : loss : 0.5331 = 0.0703 + 0.4550 + 0.0078, time: 21.134367]
2023-06-02 20:01:26.864: epoch 56:	0.02981243  	0.07884017  	0.06589340  
2023-06-02 20:01:47.933: [iter 57 : loss : 0.5312 = 0.0685 + 0.4548 + 0.0080, time: 21.064782]
2023-06-02 20:01:48.217: epoch 57:	0.02964216  	0.07840336  	0.06558714  
2023-06-02 20:02:09.311: [iter 58 : loss : 0.5298 = 0.0672 + 0.4546 + 0.0081, time: 21.089039]
2023-06-02 20:02:09.590: epoch 58:	0.02963476  	0.07813683  	0.06558259  
2023-06-02 20:02:30.719: [iter 59 : loss : 0.5285 = 0.0659 + 0.4544 + 0.0082, time: 21.125002]
2023-06-02 20:02:30.986: epoch 59:	0.02953852  	0.07800790  	0.06564496  
2023-06-02 20:02:52.077: [iter 60 : loss : 0.5275 = 0.0649 + 0.4544 + 0.0083, time: 21.086050]
2023-06-02 20:02:52.343: epoch 60:	0.02952371  	0.07774967  	0.06536616  
2023-06-02 20:03:13.299: [iter 61 : loss : 0.5258 = 0.0633 + 0.4542 + 0.0084, time: 20.953027]
2023-06-02 20:03:13.584: epoch 61:	0.02947929  	0.07731880  	0.06531590  
2023-06-02 20:03:34.897: [iter 62 : loss : 0.5249 = 0.0624 + 0.4540 + 0.0085, time: 21.308986]
2023-06-02 20:03:35.164: epoch 62:	0.02954591  	0.07706160  	0.06522659  
2023-06-02 20:03:35.164: Early stopping is trigger at epoch: 62
2023-06-02 20:03:35.164: best_result@epoch 37:

2023-06-02 20:03:35.164: 		0.0306      	0.0820      	0.0676      
2023-06-02 20:05:19.560: my pid: 3284
2023-06-02 20:05:19.560: model: model.general_recommender.SGL
2023-06-02 20:05:19.560: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 20:05:19.560: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 20:05:23.667: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 20:05:44.670: [iter 1 : loss : 1.1345 = 0.6931 + 0.4414 + 0.0000, time: 21.001924]
2023-06-02 20:05:44.957: epoch 1:	0.00152502  	0.00288674  	0.00249656  
2023-06-02 20:05:44.957: Find a better model.
2023-06-02 20:06:05.884: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 20.922617]
2023-06-02 20:06:06.191: epoch 2:	0.00173971  	0.00366615  	0.00283590  
2023-06-02 20:06:06.192: Find a better model.
2023-06-02 20:06:27.251: [iter 3 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 21.055371]
2023-06-02 20:06:27.540: epoch 3:	0.00206544  	0.00432867  	0.00328964  
2023-06-02 20:06:27.540: Find a better model.
2023-06-02 20:06:48.693: [iter 4 : loss : 1.1344 = 0.6928 + 0.4416 + 0.0000, time: 21.150190]
2023-06-02 20:06:48.986: epoch 4:	0.00250962  	0.00550617  	0.00454090  
2023-06-02 20:06:48.986: Find a better model.
2023-06-02 20:07:10.089: [iter 5 : loss : 1.1346 = 0.6927 + 0.4419 + 0.0000, time: 21.098513]
2023-06-02 20:07:10.382: epoch 5:	0.00295380  	0.00700134  	0.00564569  
2023-06-02 20:07:10.382: Find a better model.
2023-06-02 20:07:31.428: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.042938]
2023-06-02 20:07:31.733: epoch 6:	0.00356085  	0.00800331  	0.00659939  
2023-06-02 20:07:31.733: Find a better model.
2023-06-02 20:07:52.842: [iter 7 : loss : 1.1348 = 0.6922 + 0.4426 + 0.0000, time: 21.103064]
2023-06-02 20:07:53.133: epoch 7:	0.00433076  	0.01014128  	0.00838940  
2023-06-02 20:07:53.133: Find a better model.
2023-06-02 20:08:14.017: [iter 8 : loss : 1.1348 = 0.6919 + 0.4429 + 0.0000, time: 20.881052]
2023-06-02 20:08:14.302: epoch 8:	0.00517470  	0.01309659  	0.01049579  
2023-06-02 20:08:14.302: Find a better model.
2023-06-02 20:08:35.224: [iter 9 : loss : 1.1347 = 0.6914 + 0.4433 + 0.0000, time: 20.917222]
2023-06-02 20:08:35.507: epoch 9:	0.00575953  	0.01504536  	0.01227965  
2023-06-02 20:08:35.507: Find a better model.
2023-06-02 20:08:56.412: [iter 10 : loss : 1.1344 = 0.6905 + 0.4438 + 0.0000, time: 20.901302]
2023-06-02 20:08:56.702: epoch 10:	0.00660347  	0.01779297  	0.01441823  
2023-06-02 20:08:56.702: Find a better model.
2023-06-02 20:09:17.634: [iter 11 : loss : 1.1337 = 0.6892 + 0.4445 + 0.0000, time: 20.923392]
2023-06-02 20:09:17.921: epoch 11:	0.00815068  	0.02171610  	0.01764693  
2023-06-02 20:09:17.921: Find a better model.
2023-06-02 20:09:38.801: [iter 12 : loss : 1.1323 = 0.6872 + 0.4451 + 0.0000, time: 20.876161]
2023-06-02 20:09:39.083: epoch 12:	0.00983857  	0.02589430  	0.02203116  
2023-06-02 20:09:39.083: Find a better model.
2023-06-02 20:09:59.805: [iter 13 : loss : 1.1300 = 0.6842 + 0.4458 + 0.0001, time: 20.718271]
2023-06-02 20:10:00.092: epoch 13:	0.01213352  	0.03304674  	0.02805194  
2023-06-02 20:10:00.092: Find a better model.
2023-06-02 20:10:20.795: [iter 14 : loss : 1.1258 = 0.6792 + 0.4465 + 0.0001, time: 20.699454]
2023-06-02 20:10:21.069: epoch 14:	0.01516883  	0.04088388  	0.03522731  
2023-06-02 20:10:21.070: Find a better model.
2023-06-02 20:10:41.757: [iter 15 : loss : 1.1182 = 0.6707 + 0.4474 + 0.0001, time: 20.684019]
2023-06-02 20:10:42.031: epoch 15:	0.01852988  	0.04983681  	0.04351152  
2023-06-02 20:10:42.031: Find a better model.
2023-06-02 20:11:03.357: [iter 16 : loss : 1.1044 = 0.6559 + 0.4483 + 0.0002, time: 21.320727]
2023-06-02 20:11:03.639: epoch 16:	0.02163923  	0.05729849  	0.04996607  
2023-06-02 20:11:03.639: Find a better model.
2023-06-02 20:11:24.915: [iter 17 : loss : 1.0823 = 0.6323 + 0.4498 + 0.0003, time: 21.271428]
2023-06-02 20:11:25.195: epoch 17:	0.02465238  	0.06589282  	0.05581807  
2023-06-02 20:11:25.195: Find a better model.
2023-06-02 20:11:46.511: [iter 18 : loss : 1.0490 = 0.5969 + 0.4517 + 0.0004, time: 21.312179]
2023-06-02 20:11:46.805: epoch 18:	0.02638474  	0.07074106  	0.05965177  
2023-06-02 20:11:46.805: Find a better model.
2023-06-02 20:12:08.112: [iter 19 : loss : 1.0065 = 0.5515 + 0.4544 + 0.0006, time: 21.302774]
2023-06-02 20:12:08.389: epoch 19:	0.02807271  	0.07638997  	0.06315099  
2023-06-02 20:12:08.389: Find a better model.
2023-06-02 20:12:29.541: [iter 20 : loss : 0.9578 = 0.4992 + 0.4576 + 0.0009, time: 21.147413]
2023-06-02 20:12:29.826: epoch 20:	0.02877602  	0.07911299  	0.06492798  
2023-06-02 20:12:29.826: Find a better model.
2023-06-02 20:12:51.095: [iter 21 : loss : 0.9079 = 0.4457 + 0.4610 + 0.0012, time: 21.265606]
2023-06-02 20:12:51.387: epoch 21:	0.02931644  	0.07995868  	0.06544425  
2023-06-02 20:12:51.387: Find a better model.
2023-06-02 20:13:12.703: [iter 22 : loss : 0.8606 = 0.3953 + 0.4638 + 0.0015, time: 21.311834]
2023-06-02 20:13:12.974: epoch 22:	0.02950153  	0.08081042  	0.06573705  
2023-06-02 20:13:12.974: Find a better model.
2023-06-02 20:13:34.469: [iter 23 : loss : 0.8192 = 0.3516 + 0.4658 + 0.0018, time: 21.490527]
2023-06-02 20:13:34.757: epoch 23:	0.02959037  	0.08098278  	0.06595847  
2023-06-02 20:13:34.757: Find a better model.
2023-06-02 20:13:56.065: [iter 24 : loss : 0.7842 = 0.3149 + 0.4672 + 0.0021, time: 21.303466]
2023-06-02 20:13:56.336: epoch 24:	0.02973844  	0.08194052  	0.06627870  
2023-06-02 20:13:56.336: Find a better model.
2023-06-02 20:14:17.706: [iter 25 : loss : 0.7538 = 0.2837 + 0.4678 + 0.0024, time: 21.365479]
2023-06-02 20:14:17.976: epoch 25:	0.02978286  	0.08215056  	0.06665402  
2023-06-02 20:14:17.976: Find a better model.
2023-06-02 20:14:39.295: [iter 26 : loss : 0.7288 = 0.2582 + 0.4679 + 0.0026, time: 21.314322]
2023-06-02 20:14:39.566: epoch 26:	0.02990871  	0.08270545  	0.06696818  
2023-06-02 20:14:39.566: Find a better model.
2023-06-02 20:15:00.830: [iter 27 : loss : 0.7070 = 0.2365 + 0.4676 + 0.0029, time: 21.259044]
2023-06-02 20:15:01.099: epoch 27:	0.02999754  	0.08331264  	0.06719709  
2023-06-02 20:15:01.099: Find a better model.
2023-06-02 20:15:22.450: [iter 28 : loss : 0.6889 = 0.2185 + 0.4672 + 0.0032, time: 21.347285]
2023-06-02 20:15:22.737: epoch 28:	0.03013080  	0.08359859  	0.06755748  
2023-06-02 20:15:22.738: Find a better model.
2023-06-02 20:15:44.036: [iter 29 : loss : 0.6727 = 0.2027 + 0.4666 + 0.0034, time: 21.293502]
2023-06-02 20:15:44.304: epoch 29:	0.03010860  	0.08371604  	0.06773776  
2023-06-02 20:15:44.304: Find a better model.
2023-06-02 20:16:05.426: [iter 30 : loss : 0.6585 = 0.1889 + 0.4660 + 0.0036, time: 21.119104]
2023-06-02 20:16:05.694: epoch 30:	0.03025667  	0.08420382  	0.06788803  
2023-06-02 20:16:05.694: Find a better model.
2023-06-02 20:16:27.019: [iter 31 : loss : 0.6460 = 0.1768 + 0.4654 + 0.0039, time: 21.320341]
2023-06-02 20:16:27.286: epoch 31:	0.03029368  	0.08434158  	0.06807322  
2023-06-02 20:16:27.286: Find a better model.
2023-06-02 20:16:48.607: [iter 32 : loss : 0.6353 = 0.1666 + 0.4645 + 0.0041, time: 21.317045]
2023-06-02 20:16:48.882: epoch 32:	0.03039732  	0.08395270  	0.06810890  
2023-06-02 20:17:10.239: [iter 33 : loss : 0.6261 = 0.1579 + 0.4639 + 0.0043, time: 21.352920]
2023-06-02 20:17:10.506: epoch 33:	0.03033810  	0.08372565  	0.06818739  
2023-06-02 20:17:31.838: [iter 34 : loss : 0.6173 = 0.1496 + 0.4632 + 0.0045, time: 21.328077]
2023-06-02 20:17:32.104: epoch 34:	0.03030108  	0.08347041  	0.06814933  
2023-06-02 20:17:53.585: [iter 35 : loss : 0.6088 = 0.1415 + 0.4626 + 0.0047, time: 21.478153]
2023-06-02 20:17:53.862: epoch 35:	0.03027146  	0.08353593  	0.06823819  
2023-06-02 20:18:15.192: [iter 36 : loss : 0.6019 = 0.1350 + 0.4620 + 0.0049, time: 21.325674]
2023-06-02 20:18:15.457: epoch 36:	0.03032329  	0.08360161  	0.06824830  
2023-06-02 20:18:37.006: [iter 37 : loss : 0.5959 = 0.1293 + 0.4615 + 0.0050, time: 21.545073]
2023-06-02 20:18:37.272: epoch 37:	0.03031588  	0.08343505  	0.06821569  
2023-06-02 20:18:58.794: [iter 38 : loss : 0.5898 = 0.1236 + 0.4610 + 0.0052, time: 21.518286]
2023-06-02 20:18:59.062: epoch 38:	0.03044914  	0.08352030  	0.06844340  
2023-06-02 20:19:20.554: [iter 39 : loss : 0.5847 = 0.1189 + 0.4604 + 0.0054, time: 21.487383]
2023-06-02 20:19:20.829: epoch 39:	0.03045654  	0.08333307  	0.06835426  
2023-06-02 20:19:42.563: [iter 40 : loss : 0.5795 = 0.1139 + 0.4600 + 0.0056, time: 21.728899]
2023-06-02 20:19:42.840: epoch 40:	0.03035289  	0.08333507  	0.06836539  
2023-06-02 20:20:04.612: [iter 41 : loss : 0.5742 = 0.1090 + 0.4595 + 0.0057, time: 21.769104]
2023-06-02 20:20:04.886: epoch 41:	0.03050836  	0.08370765  	0.06856042  
2023-06-02 20:20:26.348: [iter 42 : loss : 0.5701 = 0.1051 + 0.4590 + 0.0059, time: 21.458178]
2023-06-02 20:20:26.615: epoch 42:	0.03054538  	0.08377853  	0.06853897  
2023-06-02 20:20:48.324: [iter 43 : loss : 0.5661 = 0.1014 + 0.4587 + 0.0061, time: 21.704529]
2023-06-02 20:20:48.588: epoch 43:	0.03053057  	0.08337463  	0.06860363  
2023-06-02 20:21:10.125: [iter 44 : loss : 0.5633 = 0.0990 + 0.4582 + 0.0062, time: 21.532475]
2023-06-02 20:21:10.396: epoch 44:	0.03055279  	0.08339258  	0.06861403  
2023-06-02 20:21:31.858: [iter 45 : loss : 0.5595 = 0.0952 + 0.4579 + 0.0064, time: 21.459652]
2023-06-02 20:21:32.124: epoch 45:	0.03050097  	0.08328464  	0.06852150  
2023-06-02 20:21:53.588: [iter 46 : loss : 0.5563 = 0.0922 + 0.4576 + 0.0065, time: 21.459056]
2023-06-02 20:21:53.865: epoch 46:	0.03050098  	0.08349337  	0.06859514  
2023-06-02 20:22:15.310: [iter 47 : loss : 0.5531 = 0.0892 + 0.4573 + 0.0067, time: 21.442738]
2023-06-02 20:22:15.576: epoch 47:	0.03043434  	0.08305454  	0.06843735  
2023-06-02 20:22:37.013: [iter 48 : loss : 0.5505 = 0.0868 + 0.4569 + 0.0068, time: 21.431848]
2023-06-02 20:22:37.284: epoch 48:	0.03036030  	0.08232467  	0.06817117  
2023-06-02 20:22:58.615: [iter 49 : loss : 0.5478 = 0.0843 + 0.4565 + 0.0069, time: 21.328867]
2023-06-02 20:22:58.885: epoch 49:	0.03012340  	0.08183610  	0.06790522  
2023-06-02 20:23:20.348: [iter 50 : loss : 0.5458 = 0.0825 + 0.4563 + 0.0071, time: 21.458511]
2023-06-02 20:23:20.625: epoch 50:	0.02999754  	0.08148386  	0.06767961  
2023-06-02 20:23:42.066: [iter 51 : loss : 0.5433 = 0.0800 + 0.4561 + 0.0072, time: 21.438496]
2023-06-02 20:23:42.332: epoch 51:	0.02995313  	0.08094625  	0.06744321  
2023-06-02 20:24:03.693: [iter 52 : loss : 0.5404 = 0.0773 + 0.4558 + 0.0073, time: 21.356112]
2023-06-02 20:24:03.965: epoch 52:	0.02984208  	0.08076177  	0.06730149  
2023-06-02 20:24:25.505: [iter 53 : loss : 0.5395 = 0.0763 + 0.4557 + 0.0075, time: 21.536451]
2023-06-02 20:24:25.785: epoch 53:	0.02983467  	0.08065791  	0.06713685  
2023-06-02 20:24:47.016: [iter 54 : loss : 0.5373 = 0.0744 + 0.4553 + 0.0076, time: 21.227061]
2023-06-02 20:24:47.282: epoch 54:	0.02970141  	0.08030712  	0.06698932  
2023-06-02 20:25:08.836: [iter 55 : loss : 0.5350 = 0.0723 + 0.4551 + 0.0077, time: 21.549819]
2023-06-02 20:25:09.127: epoch 55:	0.02959776  	0.08008713  	0.06682604  
2023-06-02 20:25:31.021: [iter 56 : loss : 0.5331 = 0.0704 + 0.4549 + 0.0078, time: 21.891174]
2023-06-02 20:25:31.287: epoch 56:	0.02953114  	0.07973870  	0.06654959  
2023-06-02 20:25:31.288: Early stopping is trigger at epoch: 56
2023-06-02 20:25:31.288: best_result@epoch 31:

2023-06-02 20:25:31.288: 		0.0303      	0.0843      	0.0681      
2023-06-02 20:26:56.842: my pid: 15864
2023-06-02 20:26:56.842: model: model.general_recommender.SGL
2023-06-02 20:26:56.842: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 20:26:56.842: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 20:27:01.097: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 20:27:22.205: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 21.107773]
2023-06-02 20:27:22.483: epoch 1:	0.00133254  	0.00242429  	0.00229936  
2023-06-02 20:27:22.484: Find a better model.
2023-06-02 20:27:43.610: [iter 2 : loss : 1.1340 = 0.6930 + 0.4410 + 0.0000, time: 21.123131]
2023-06-02 20:27:43.921: epoch 2:	0.00159165  	0.00301562  	0.00245519  
2023-06-02 20:27:43.921: Find a better model.
2023-06-02 20:28:05.406: [iter 3 : loss : 1.1342 = 0.6929 + 0.4412 + 0.0000, time: 21.481107]
2023-06-02 20:28:05.702: epoch 3:	0.00212467  	0.00433169  	0.00345513  
2023-06-02 20:28:05.702: Find a better model.
2023-06-02 20:28:26.826: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.121055]
2023-06-02 20:28:27.124: epoch 4:	0.00226533  	0.00527602  	0.00419133  
2023-06-02 20:28:27.124: Find a better model.
2023-06-02 20:28:48.396: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 21.268040]
2023-06-02 20:28:48.696: epoch 5:	0.00313888  	0.00706341  	0.00561087  
2023-06-02 20:28:48.696: Find a better model.
2023-06-02 20:29:09.978: [iter 6 : loss : 1.1347 = 0.6925 + 0.4422 + 0.0000, time: 21.277974]
2023-06-02 20:29:10.281: epoch 6:	0.00373853  	0.00858318  	0.00683319  
2023-06-02 20:29:10.281: Find a better model.
2023-06-02 20:29:31.806: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 21.520369]
2023-06-02 20:29:32.121: epoch 7:	0.00430115  	0.00994332  	0.00789069  
2023-06-02 20:29:32.121: Find a better model.
2023-06-02 20:29:53.420: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 21.294932]
2023-06-02 20:29:53.719: epoch 8:	0.00493781  	0.01233227  	0.00956739  
2023-06-02 20:29:53.719: Find a better model.
2023-06-02 20:30:14.947: [iter 9 : loss : 1.1347 = 0.6915 + 0.4433 + 0.0000, time: 21.224230]
2023-06-02 20:30:15.247: epoch 9:	0.00572992  	0.01495090  	0.01179039  
2023-06-02 20:30:15.247: Find a better model.
2023-06-02 20:30:36.395: [iter 10 : loss : 1.1345 = 0.6907 + 0.4438 + 0.0000, time: 21.145007]
2023-06-02 20:30:36.691: epoch 10:	0.00665529  	0.01726708  	0.01411907  
2023-06-02 20:30:36.692: Find a better model.
2023-06-02 20:30:57.792: [iter 11 : loss : 1.1338 = 0.6894 + 0.4443 + 0.0000, time: 21.095877]
2023-06-02 20:30:58.092: epoch 11:	0.00761767  	0.02003633  	0.01690375  
2023-06-02 20:30:58.093: Find a better model.
2023-06-02 20:31:19.146: [iter 12 : loss : 1.1323 = 0.6874 + 0.4449 + 0.0000, time: 21.050345]
2023-06-02 20:31:19.443: epoch 12:	0.00943880  	0.02516349  	0.02128600  
2023-06-02 20:31:19.444: Find a better model.
2023-06-02 20:31:40.551: [iter 13 : loss : 1.1303 = 0.6845 + 0.4457 + 0.0001, time: 21.104207]
2023-06-02 20:31:40.865: epoch 13:	0.01185222  	0.03139443  	0.02716189  
2023-06-02 20:31:40.865: Find a better model.
2023-06-02 20:32:02.117: [iter 14 : loss : 1.1262 = 0.6797 + 0.4464 + 0.0001, time: 21.247015]
2023-06-02 20:32:02.406: epoch 14:	0.01448035  	0.03835594  	0.03320266  
2023-06-02 20:32:02.407: Find a better model.
2023-06-02 20:32:23.602: [iter 15 : loss : 1.1189 = 0.6715 + 0.4473 + 0.0001, time: 21.191416]
2023-06-02 20:32:23.909: epoch 15:	0.01773032  	0.04676228  	0.04058402  
2023-06-02 20:32:23.909: Find a better model.
2023-06-02 20:32:45.756: [iter 16 : loss : 1.1056 = 0.6572 + 0.4482 + 0.0002, time: 21.842874]
2023-06-02 20:32:46.064: epoch 16:	0.02131349  	0.05634763  	0.04854847  
2023-06-02 20:32:46.064: Find a better model.
2023-06-02 20:33:07.880: [iter 17 : loss : 1.0842 = 0.6343 + 0.4496 + 0.0003, time: 21.812745]
2023-06-02 20:33:08.160: epoch 17:	0.02420076  	0.06379059  	0.05494552  
2023-06-02 20:33:08.160: Find a better model.
2023-06-02 20:33:29.806: [iter 18 : loss : 1.0519 = 0.6000 + 0.4515 + 0.0004, time: 21.641495]
2023-06-02 20:33:30.111: epoch 18:	0.02630331  	0.06923005  	0.05929542  
2023-06-02 20:33:30.111: Find a better model.
2023-06-02 20:33:51.804: [iter 19 : loss : 1.0099 = 0.5553 + 0.4539 + 0.0006, time: 21.689462]
2023-06-02 20:33:52.094: epoch 19:	0.02767288  	0.07379776  	0.06252405  
2023-06-02 20:33:52.094: Find a better model.
2023-06-02 20:34:13.934: [iter 20 : loss : 0.9615 = 0.5035 + 0.4571 + 0.0009, time: 21.836502]
2023-06-02 20:34:14.220: epoch 20:	0.02878337  	0.07643460  	0.06453244  
2023-06-02 20:34:14.220: Find a better model.
2023-06-02 20:34:35.731: [iter 21 : loss : 0.9118 = 0.4503 + 0.4603 + 0.0012, time: 21.506940]
2023-06-02 20:34:36.018: epoch 21:	0.02916834  	0.07757130  	0.06536126  
2023-06-02 20:34:36.018: Find a better model.
2023-06-02 20:34:57.626: [iter 22 : loss : 0.8639 = 0.3993 + 0.4632 + 0.0015, time: 21.605692]
2023-06-02 20:34:57.921: epoch 22:	0.02930161  	0.07825187  	0.06565889  
2023-06-02 20:34:57.921: Find a better model.
2023-06-02 20:35:19.420: [iter 23 : loss : 0.8222 = 0.3550 + 0.4654 + 0.0018, time: 21.494287]
2023-06-02 20:35:19.701: epoch 23:	0.02947189  	0.07824563  	0.06572178  
2023-06-02 20:35:41.276: [iter 24 : loss : 0.7866 = 0.3178 + 0.4667 + 0.0021, time: 21.570563]
2023-06-02 20:35:41.552: epoch 24:	0.02942007  	0.07849195  	0.06584495  
2023-06-02 20:35:41.552: Find a better model.
2023-06-02 20:36:03.257: [iter 25 : loss : 0.7556 = 0.2858 + 0.4675 + 0.0023, time: 21.700812]
2023-06-02 20:36:03.540: epoch 25:	0.02948670  	0.07896022  	0.06614305  
2023-06-02 20:36:03.540: Find a better model.
2023-06-02 20:36:24.790: [iter 26 : loss : 0.7305 = 0.2602 + 0.4676 + 0.0026, time: 21.246545]
2023-06-02 20:36:25.089: epoch 26:	0.02968660  	0.07991708  	0.06666608  
2023-06-02 20:36:25.089: Find a better model.
2023-06-02 20:36:46.659: [iter 27 : loss : 0.7083 = 0.2380 + 0.4675 + 0.0029, time: 21.564582]
2023-06-02 20:36:46.941: epoch 27:	0.02967917  	0.07985537  	0.06660365  
2023-06-02 20:37:08.914: [iter 28 : loss : 0.6898 = 0.2196 + 0.4671 + 0.0031, time: 21.968649]
2023-06-02 20:37:09.191: epoch 28:	0.02979023  	0.08007161  	0.06686625  
2023-06-02 20:37:09.191: Find a better model.
2023-06-02 20:37:30.998: [iter 29 : loss : 0.6740 = 0.2042 + 0.4664 + 0.0034, time: 21.803025]
2023-06-02 20:37:31.280: epoch 29:	0.02993088  	0.08035814  	0.06704807  
2023-06-02 20:37:31.280: Find a better model.
2023-06-02 20:37:54.414: [iter 30 : loss : 0.6592 = 0.1898 + 0.4658 + 0.0036, time: 23.127760]
2023-06-02 20:37:54.711: epoch 30:	0.02996049  	0.08054525  	0.06707602  
2023-06-02 20:37:54.711: Find a better model.
2023-06-02 20:38:17.935: [iter 31 : loss : 0.6468 = 0.1776 + 0.4653 + 0.0039, time: 23.220312]
2023-06-02 20:38:18.239: epoch 31:	0.02998271  	0.08089346  	0.06730738  
2023-06-02 20:38:18.239: Find a better model.
2023-06-02 20:38:40.972: [iter 32 : loss : 0.6357 = 0.1671 + 0.4646 + 0.0041, time: 22.728657]
2023-06-02 20:38:41.271: epoch 32:	0.02996050  	0.08107074  	0.06743778  
2023-06-02 20:38:41.271: Find a better model.
2023-06-02 20:39:04.336: [iter 33 : loss : 0.6270 = 0.1588 + 0.4639 + 0.0043, time: 23.060517]
2023-06-02 20:39:04.630: epoch 33:	0.03007895  	0.08068596  	0.06749202  
2023-06-02 20:39:27.021: [iter 34 : loss : 0.6177 = 0.1499 + 0.4633 + 0.0045, time: 22.387637]
2023-06-02 20:39:27.291: epoch 34:	0.03007153  	0.08060560  	0.06755154  
2023-06-02 20:39:48.934: [iter 35 : loss : 0.6088 = 0.1415 + 0.4627 + 0.0047, time: 21.639248]
2023-06-02 20:39:49.203: epoch 35:	0.03004931  	0.08033843  	0.06752533  
2023-06-02 20:40:10.982: [iter 36 : loss : 0.6024 = 0.1355 + 0.4620 + 0.0049, time: 21.773880]
2023-06-02 20:40:11.252: epoch 36:	0.03009374  	0.08017937  	0.06763473  
2023-06-02 20:40:32.958: [iter 37 : loss : 0.5960 = 0.1294 + 0.4615 + 0.0050, time: 21.703028]
2023-06-02 20:40:33.230: epoch 37:	0.03004932  	0.08034589  	0.06763478  
2023-06-02 20:40:55.607: [iter 38 : loss : 0.5898 = 0.1237 + 0.4609 + 0.0052, time: 22.372139]
2023-06-02 20:40:55.907: epoch 38:	0.02997528  	0.08036262  	0.06760804  
2023-06-02 20:41:18.845: [iter 39 : loss : 0.5848 = 0.1189 + 0.4604 + 0.0054, time: 22.933847]
2023-06-02 20:41:19.140: epoch 39:	0.02993826  	0.08040774  	0.06761225  
2023-06-02 20:41:42.421: [iter 40 : loss : 0.5793 = 0.1138 + 0.4600 + 0.0056, time: 23.276976]
2023-06-02 20:41:42.720: epoch 40:	0.02997528  	0.08033529  	0.06760214  
2023-06-02 20:42:05.861: [iter 41 : loss : 0.5744 = 0.1092 + 0.4595 + 0.0057, time: 23.135975]
2023-06-02 20:42:06.150: epoch 41:	0.02997528  	0.08023529  	0.06756128  
2023-06-02 20:42:29.143: [iter 42 : loss : 0.5704 = 0.1054 + 0.4591 + 0.0059, time: 22.988192]
2023-06-02 20:42:29.445: epoch 42:	0.02990864  	0.08003006  	0.06732994  
2023-06-02 20:42:52.666: [iter 43 : loss : 0.5665 = 0.1017 + 0.4587 + 0.0061, time: 23.216457]
2023-06-02 20:42:52.981: epoch 43:	0.02990865  	0.07986721  	0.06727011  
2023-06-02 20:43:14.495: [iter 44 : loss : 0.5638 = 0.0993 + 0.4583 + 0.0062, time: 21.511327]
2023-06-02 20:43:14.762: epoch 44:	0.02992345  	0.07973212  	0.06725156  
2023-06-02 20:43:36.509: [iter 45 : loss : 0.5596 = 0.0953 + 0.4579 + 0.0064, time: 21.743057]
2023-06-02 20:43:36.781: epoch 45:	0.02978279  	0.07923432  	0.06703138  
2023-06-02 20:43:58.562: [iter 46 : loss : 0.5566 = 0.0925 + 0.4576 + 0.0065, time: 21.778012]
2023-06-02 20:43:58.858: epoch 46:	0.02979760  	0.07907774  	0.06697006  
2023-06-02 20:44:21.307: [iter 47 : loss : 0.5534 = 0.0896 + 0.4572 + 0.0066, time: 22.445556]
2023-06-02 20:44:21.593: epoch 47:	0.02972356  	0.07881916  	0.06681899  
2023-06-02 20:44:44.625: [iter 48 : loss : 0.5508 = 0.0871 + 0.4570 + 0.0068, time: 23.026245]
2023-06-02 20:44:44.948: epoch 48:	0.02971615  	0.07856383  	0.06666887  
2023-06-02 20:45:08.242: [iter 49 : loss : 0.5480 = 0.0845 + 0.4566 + 0.0069, time: 23.289217]
2023-06-02 20:45:08.535: epoch 49:	0.02966433  	0.07851841  	0.06657969  
2023-06-02 20:45:24.537: my pid: 5316
2023-06-02 20:45:24.537: model: model.general_recommender.SGL
2023-06-02 20:45:24.537: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 20:45:24.537: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 20:45:29.202: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 20:45:51.661: [iter 1 : loss : 1.1344 = 0.6931 + 0.4414 + 0.0000, time: 22.458061]
2023-06-02 20:45:51.979: epoch 1:	0.00145099  	0.00305217  	0.00248638  
2023-06-02 20:45:51.979: Find a better model.
2023-06-02 20:46:14.238: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 22.255342]
2023-06-02 20:46:14.555: epoch 2:	0.00154723  	0.00343966  	0.00259538  
2023-06-02 20:46:14.555: Find a better model.
2023-06-02 20:46:35.858: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 21.299604]
2023-06-02 20:46:36.150: epoch 3:	0.00211727  	0.00448373  	0.00361433  
2023-06-02 20:46:36.150: Find a better model.
2023-06-02 20:46:57.674: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 21.521477]
2023-06-02 20:46:57.985: epoch 4:	0.00253183  	0.00514022  	0.00415657  
2023-06-02 20:46:57.986: Find a better model.
2023-06-02 20:47:20.356: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 22.366150]
2023-06-02 20:47:20.673: epoch 5:	0.00280575  	0.00613118  	0.00493031  
2023-06-02 20:47:20.673: Find a better model.
2023-06-02 20:47:43.079: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 22.401661]
2023-06-02 20:47:43.394: epoch 6:	0.00354605  	0.00821576  	0.00671523  
2023-06-02 20:47:43.395: Find a better model.
2023-06-02 20:48:05.895: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 22.495776]
2023-06-02 20:48:06.212: epoch 7:	0.00422712  	0.00953222  	0.00811874  
2023-06-02 20:48:06.212: Find a better model.
2023-06-02 20:48:28.445: [iter 8 : loss : 1.1347 = 0.6919 + 0.4428 + 0.0000, time: 22.228314]
2023-06-02 20:48:28.755: epoch 8:	0.00473793  	0.01126515  	0.00990216  
2023-06-02 20:48:28.755: Find a better model.
2023-06-02 20:48:51.248: [iter 9 : loss : 1.1346 = 0.6914 + 0.4432 + 0.0000, time: 22.488211]
2023-06-02 20:48:51.578: epoch 9:	0.00593721  	0.01560480  	0.01263500  
2023-06-02 20:48:51.578: Find a better model.
2023-06-02 20:49:12.587: [iter 10 : loss : 1.1344 = 0.6906 + 0.4437 + 0.0000, time: 21.005403]
2023-06-02 20:49:12.887: epoch 10:	0.00660347  	0.01653010  	0.01381708  
2023-06-02 20:49:12.887: Find a better model.
2023-06-02 20:49:33.792: [iter 11 : loss : 1.1337 = 0.6893 + 0.4443 + 0.0000, time: 20.901387]
2023-06-02 20:49:34.079: epoch 11:	0.00753624  	0.01996950  	0.01647684  
2023-06-02 20:49:34.079: Find a better model.
2023-06-02 20:49:55.204: [iter 12 : loss : 1.1323 = 0.6873 + 0.4449 + 0.0000, time: 21.119115]
2023-06-02 20:49:55.489: epoch 12:	0.00935737  	0.02529363  	0.02137206  
2023-06-02 20:49:55.489: Find a better model.
2023-06-02 20:50:16.550: [iter 13 : loss : 1.1301 = 0.6844 + 0.4457 + 0.0001, time: 21.057438]
2023-06-02 20:50:16.857: epoch 13:	0.01212613  	0.03281917  	0.02815359  
2023-06-02 20:50:16.866: Find a better model.
2023-06-02 20:50:37.765: [iter 14 : loss : 1.1259 = 0.6794 + 0.4464 + 0.0001, time: 20.895025]
2023-06-02 20:50:38.053: epoch 14:	0.01493933  	0.04088169  	0.03488972  
2023-06-02 20:50:38.053: Find a better model.
2023-06-02 20:50:59.422: [iter 15 : loss : 1.1184 = 0.6710 + 0.4473 + 0.0001, time: 21.364570]
2023-06-02 20:50:59.709: epoch 15:	0.01827075  	0.04938402  	0.04231596  
2023-06-02 20:50:59.709: Find a better model.
2023-06-02 20:51:21.538: [iter 16 : loss : 1.1048 = 0.6565 + 0.4482 + 0.0002, time: 21.825576]
2023-06-02 20:51:21.831: epoch 16:	0.02135051  	0.05746846  	0.04883623  
2023-06-02 20:51:21.832: Find a better model.
2023-06-02 20:51:44.474: [iter 17 : loss : 1.0830 = 0.6331 + 0.4496 + 0.0003, time: 22.635272]
2023-06-02 20:51:44.792: epoch 17:	0.02409712  	0.06372180  	0.05422757  
2023-06-02 20:51:44.792: Find a better model.
2023-06-02 20:52:07.791: [iter 18 : loss : 1.0501 = 0.5982 + 0.4515 + 0.0004, time: 22.994392]
2023-06-02 20:52:08.099: epoch 18:	0.02631809  	0.07061990  	0.05877967  
2023-06-02 20:52:08.099: Find a better model.
2023-06-02 20:52:30.902: [iter 19 : loss : 1.0078 = 0.5531 + 0.4540 + 0.0006, time: 22.797866]
2023-06-02 20:52:31.197: epoch 19:	0.02770251  	0.07465066  	0.06189366  
2023-06-02 20:52:31.197: Find a better model.
2023-06-02 20:52:53.909: [iter 20 : loss : 0.9594 = 0.5013 + 0.4571 + 0.0009, time: 22.708228]
2023-06-02 20:52:54.209: epoch 20:	0.02858349  	0.07750347  	0.06394735  
2023-06-02 20:52:54.209: Find a better model.
2023-06-02 20:53:16.308: [iter 21 : loss : 0.9095 = 0.4480 + 0.4604 + 0.0012, time: 22.094463]
2023-06-02 20:53:16.580: epoch 21:	0.02927940  	0.07908092  	0.06518829  
2023-06-02 20:53:16.580: Find a better model.
2023-06-02 20:53:38.086: [iter 22 : loss : 0.8618 = 0.3972 + 0.4631 + 0.0015, time: 21.501319]
2023-06-02 20:53:38.366: epoch 22:	0.02960513  	0.08002952  	0.06588812  
2023-06-02 20:53:38.366: Find a better model.
2023-06-02 20:54:00.340: [iter 23 : loss : 0.8204 = 0.3533 + 0.4653 + 0.0018, time: 21.969741]
2023-06-02 20:54:00.612: epoch 23:	0.02972360  	0.08079848  	0.06629328  
2023-06-02 20:54:00.612: Find a better model.
2023-06-02 20:54:23.530: [iter 24 : loss : 0.7848 = 0.3160 + 0.4667 + 0.0021, time: 22.915002]
2023-06-02 20:54:23.838: epoch 24:	0.02970879  	0.08139959  	0.06671702  
2023-06-02 20:54:23.838: Find a better model.
2023-06-02 20:54:46.495: [iter 25 : loss : 0.7542 = 0.2845 + 0.4673 + 0.0024, time: 22.647757]
2023-06-02 20:54:46.793: epoch 25:	0.03010857  	0.08253673  	0.06732198  
2023-06-02 20:54:46.793: Find a better model.
2023-06-02 20:55:09.480: [iter 26 : loss : 0.7289 = 0.2587 + 0.4676 + 0.0026, time: 22.682780]
2023-06-02 20:55:09.769: epoch 26:	0.03013819  	0.08257420  	0.06730812  
2023-06-02 20:55:09.769: Find a better model.
2023-06-02 20:55:32.514: [iter 27 : loss : 0.7072 = 0.2368 + 0.4675 + 0.0029, time: 22.739488]
2023-06-02 20:55:32.798: epoch 27:	0.03003455  	0.08245031  	0.06733803  
2023-06-02 20:55:54.422: [iter 28 : loss : 0.6887 = 0.2187 + 0.4669 + 0.0032, time: 21.621586]
2023-06-02 20:55:54.691: epoch 28:	0.03005676  	0.08278788  	0.06755877  
2023-06-02 20:55:54.691: Find a better model.
2023-06-02 20:56:16.666: [iter 29 : loss : 0.6730 = 0.2032 + 0.4664 + 0.0034, time: 21.970383]
2023-06-02 20:56:16.964: epoch 29:	0.03014559  	0.08277258  	0.06776780  
2023-06-02 20:56:40.113: [iter 30 : loss : 0.6583 = 0.1889 + 0.4658 + 0.0036, time: 23.145748]
2023-06-02 20:56:40.415: epoch 30:	0.03015298  	0.08280023  	0.06774825  
2023-06-02 20:56:40.415: Find a better model.
2023-06-02 20:57:03.457: [iter 31 : loss : 0.6462 = 0.1771 + 0.4652 + 0.0039, time: 23.037649]
2023-06-02 20:57:03.754: epoch 31:	0.03018260  	0.08273230  	0.06783696  
2023-06-02 20:57:26.803: [iter 32 : loss : 0.6352 = 0.1666 + 0.4645 + 0.0041, time: 23.044335]
2023-06-02 20:57:27.107: epoch 32:	0.03024184  	0.08331209  	0.06809960  
2023-06-02 20:57:27.107: Find a better model.
2023-06-02 20:57:49.922: [iter 33 : loss : 0.6258 = 0.1576 + 0.4639 + 0.0043, time: 22.810403]
2023-06-02 20:57:50.206: epoch 33:	0.03020483  	0.08250841  	0.06801768  
2023-06-02 20:58:11.807: [iter 34 : loss : 0.6172 = 0.1496 + 0.4632 + 0.0045, time: 21.597404]
2023-06-02 20:58:12.078: epoch 34:	0.03017522  	0.08274347  	0.06819756  
2023-06-02 20:58:33.576: [iter 35 : loss : 0.6085 = 0.1412 + 0.4626 + 0.0047, time: 21.494055]
2023-06-02 20:58:33.859: epoch 35:	0.03017521  	0.08265902  	0.06823789  
2023-06-02 20:58:55.566: [iter 36 : loss : 0.6020 = 0.1352 + 0.4620 + 0.0049, time: 21.703121]
2023-06-02 20:58:55.850: epoch 36:	0.03024924  	0.08300272  	0.06835296  
2023-06-02 20:59:17.556: [iter 37 : loss : 0.5958 = 0.1292 + 0.4615 + 0.0051, time: 21.701301]
2023-06-02 20:59:17.839: epoch 37:	0.03030846  	0.08270361  	0.06840444  
2023-06-02 20:59:39.326: [iter 38 : loss : 0.5894 = 0.1233 + 0.4608 + 0.0052, time: 21.481544]
2023-06-02 20:59:39.591: epoch 38:	0.03037508  	0.08272266  	0.06856638  
2023-06-02 21:00:01.174: [iter 39 : loss : 0.5843 = 0.1185 + 0.4603 + 0.0054, time: 21.579062]
2023-06-02 21:00:01.440: epoch 39:	0.03035288  	0.08241227  	0.06858557  
2023-06-02 21:00:22.942: [iter 40 : loss : 0.5793 = 0.1138 + 0.4599 + 0.0056, time: 21.498554]
2023-06-02 21:00:23.225: epoch 40:	0.03038249  	0.08263257  	0.06863645  
2023-06-02 21:00:44.736: [iter 41 : loss : 0.5744 = 0.1092 + 0.4595 + 0.0057, time: 21.506497]
2023-06-02 21:00:45.006: epoch 41:	0.03022702  	0.08189207  	0.06849539  
2023-06-02 21:01:06.564: [iter 42 : loss : 0.5701 = 0.1052 + 0.4590 + 0.0059, time: 21.554561]
2023-06-02 21:01:06.848: epoch 42:	0.03013819  	0.08175409  	0.06844192  
2023-06-02 21:01:28.310: [iter 43 : loss : 0.5658 = 0.1010 + 0.4587 + 0.0061, time: 21.457115]
2023-06-02 21:01:28.574: epoch 43:	0.03014558  	0.08170433  	0.06828883  
2023-06-02 21:01:50.347: [iter 44 : loss : 0.5633 = 0.0988 + 0.4583 + 0.0062, time: 21.768739]
2023-06-02 21:01:50.634: epoch 44:	0.03020481  	0.08171308  	0.06836081  
2023-06-02 21:02:12.340: [iter 45 : loss : 0.5594 = 0.0951 + 0.4579 + 0.0064, time: 21.701394]
2023-06-02 21:02:12.605: epoch 45:	0.03017519  	0.08234325  	0.06839721  
2023-06-02 21:02:34.943: [iter 46 : loss : 0.5567 = 0.0926 + 0.4576 + 0.0065, time: 22.333289]
2023-06-02 21:02:35.240: epoch 46:	0.03020480  	0.08261983  	0.06853036  
2023-06-02 21:02:57.863: [iter 47 : loss : 0.5531 = 0.0892 + 0.4573 + 0.0067, time: 22.617636]
2023-06-02 21:02:58.157: epoch 47:	0.03002712  	0.08177498  	0.06801542  
2023-06-02 21:03:20.968: [iter 48 : loss : 0.5505 = 0.0868 + 0.4569 + 0.0068, time: 22.808332]
2023-06-02 21:03:21.270: epoch 48:	0.03001231  	0.08164471  	0.06788571  
2023-06-02 21:03:43.962: [iter 49 : loss : 0.5480 = 0.0844 + 0.4566 + 0.0069, time: 22.687889]
2023-06-02 21:03:44.261: epoch 49:	0.02999750  	0.08131611  	0.06768319  
2023-06-02 21:04:06.781: [iter 50 : loss : 0.5458 = 0.0823 + 0.4564 + 0.0071, time: 22.515804]
2023-06-02 21:04:07.097: epoch 50:	0.02981242  	0.08061109  	0.06746994  
2023-06-02 21:04:28.696: [iter 51 : loss : 0.5428 = 0.0796 + 0.4561 + 0.0072, time: 21.595136]
2023-06-02 21:04:28.975: epoch 51:	0.02976060  	0.08045317  	0.06736466  
2023-06-02 21:04:50.646: [iter 52 : loss : 0.5406 = 0.0775 + 0.4558 + 0.0073, time: 21.667495]
2023-06-02 21:04:50.924: epoch 52:	0.02963474  	0.08024229  	0.06714375  
2023-06-02 21:05:12.419: [iter 53 : loss : 0.5399 = 0.0769 + 0.4556 + 0.0075, time: 21.490403]
2023-06-02 21:05:12.687: epoch 53:	0.02955330  	0.07974044  	0.06698639  
2023-06-02 21:05:34.065: [iter 54 : loss : 0.5371 = 0.0743 + 0.4552 + 0.0076, time: 21.373659]
2023-06-02 21:05:34.328: epoch 54:	0.02955329  	0.07947773  	0.06699158  
2023-06-02 21:05:56.611: [iter 55 : loss : 0.5353 = 0.0724 + 0.4551 + 0.0077, time: 22.278451]
2023-06-02 21:05:56.936: epoch 55:	0.02947927  	0.07935987  	0.06684864  
2023-06-02 21:06:19.902: [iter 56 : loss : 0.5332 = 0.0705 + 0.4549 + 0.0078, time: 22.961714]
2023-06-02 21:06:20.188: epoch 56:	0.02953849  	0.07942726  	0.06674357  
2023-06-02 21:06:43.270: [iter 57 : loss : 0.5313 = 0.0687 + 0.4547 + 0.0080, time: 23.078619]
2023-06-02 21:06:43.559: epoch 57:	0.02951628  	0.07943423  	0.06667855  
2023-06-02 21:06:43.559: Early stopping is trigger at epoch: 57
2023-06-02 21:06:43.559: best_result@epoch 32:

2023-06-02 21:06:43.559: 		0.0302      	0.0833      	0.0681      
2023-06-02 21:32:25.436: my pid: 6760
2023-06-02 21:32:25.436: model: model.general_recommender.SGL
2023-06-02 21:32:25.436: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-02 21:32:25.436: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-02 21:32:29.881: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-02 21:32:52.314: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 22.429161]
2023-06-02 21:32:52.634: epoch 1:	0.00118448  	0.00217694  	0.00181670  
2023-06-02 21:32:52.635: Find a better model.
2023-06-02 21:33:15.031: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 22.386681]
2023-06-02 21:33:15.351: epoch 2:	0.00186556  	0.00329996  	0.00273242  
2023-06-02 21:33:15.351: Find a better model.
2023-06-02 21:33:36.391: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.036210]
2023-06-02 21:33:36.690: epoch 3:	0.00208025  	0.00438991  	0.00338974  
2023-06-02 21:33:36.691: Find a better model.
2023-06-02 21:33:57.605: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 20.911109]
2023-06-02 21:33:57.898: epoch 4:	0.00233936  	0.00486474  	0.00395372  
2023-06-02 21:33:57.898: Find a better model.
2023-06-02 21:34:18.931: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.029265]
2023-06-02 21:34:19.220: epoch 5:	0.00284276  	0.00635210  	0.00501918  
2023-06-02 21:34:19.221: Find a better model.
2023-06-02 21:34:39.922: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.697523]
2023-06-02 21:34:40.213: epoch 6:	0.00338318  	0.00739066  	0.00625299  
2023-06-02 21:34:40.213: Find a better model.
2023-06-02 21:35:01.001: [iter 7 : loss : 1.1346 = 0.6923 + 0.4424 + 0.0000, time: 20.784205]
2023-06-02 21:35:01.293: epoch 7:	0.00397542  	0.00937399  	0.00799503  
2023-06-02 21:35:01.293: Find a better model.
2023-06-02 21:35:21.986: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.689730]
2023-06-02 21:35:22.274: epoch 8:	0.00498963  	0.01217237  	0.01022033  
2023-06-02 21:35:22.274: Find a better model.
2023-06-02 21:35:42.946: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.668081]
2023-06-02 21:35:43.234: epoch 9:	0.00593720  	0.01496360  	0.01228638  
2023-06-02 21:35:43.234: Find a better model.
2023-06-02 21:36:03.785: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.546553]
2023-06-02 21:36:04.070: epoch 10:	0.00755104  	0.01971685  	0.01591508  
2023-06-02 21:36:04.071: Find a better model.
2023-06-02 21:36:24.568: [iter 11 : loss : 1.1339 = 0.6897 + 0.4441 + 0.0000, time: 20.494001]
2023-06-02 21:36:24.848: epoch 11:	0.00867630  	0.02176307  	0.01842499  
2023-06-02 21:36:24.848: Find a better model.
2023-06-02 21:36:45.315: [iter 12 : loss : 1.1326 = 0.6878 + 0.4448 + 0.0000, time: 20.464081]
2023-06-02 21:36:45.594: epoch 12:	0.00996442  	0.02666098  	0.02252003  
2023-06-02 21:36:45.594: Find a better model.
2023-06-02 21:37:06.094: [iter 13 : loss : 1.1304 = 0.6849 + 0.4455 + 0.0001, time: 20.496800]
2023-06-02 21:37:06.373: epoch 13:	0.01216314  	0.03324915  	0.02774913  
2023-06-02 21:37:06.373: Find a better model.
2023-06-02 21:37:26.683: [iter 14 : loss : 1.1265 = 0.6801 + 0.4463 + 0.0001, time: 20.306077]
2023-06-02 21:37:26.959: epoch 14:	0.01528728  	0.04161604  	0.03503234  
2023-06-02 21:37:26.959: Find a better model.
2023-06-02 21:37:47.481: [iter 15 : loss : 1.1193 = 0.6720 + 0.4472 + 0.0001, time: 20.518250]
2023-06-02 21:37:47.758: epoch 15:	0.01870014  	0.05020384  	0.04262607  
2023-06-02 21:37:47.758: Find a better model.
2023-06-02 21:38:08.906: [iter 16 : loss : 1.1064 = 0.6581 + 0.4481 + 0.0002, time: 21.143468]
2023-06-02 21:38:09.181: epoch 16:	0.02167626  	0.05665514  	0.04913742  
2023-06-02 21:38:09.182: Find a better model.
2023-06-02 21:38:30.410: [iter 17 : loss : 1.0853 = 0.6356 + 0.4494 + 0.0003, time: 21.224514]
2023-06-02 21:38:30.694: epoch 17:	0.02425260  	0.06335493  	0.05506433  
2023-06-02 21:38:30.694: Find a better model.
2023-06-02 21:38:51.875: [iter 18 : loss : 1.0532 = 0.6014 + 0.4513 + 0.0004, time: 21.177169]
2023-06-02 21:38:52.150: epoch 18:	0.02634031  	0.06883982  	0.05901846  
2023-06-02 21:38:52.150: Find a better model.
2023-06-02 21:39:13.428: [iter 19 : loss : 1.0113 = 0.5569 + 0.4538 + 0.0006, time: 21.275107]
2023-06-02 21:39:13.707: epoch 19:	0.02783575  	0.07331603  	0.06174725  
2023-06-02 21:39:13.708: Find a better model.
2023-06-02 21:39:34.851: [iter 20 : loss : 0.9629 = 0.5051 + 0.4569 + 0.0009, time: 21.140011]
2023-06-02 21:39:35.126: epoch 20:	0.02845764  	0.07542300  	0.06345146  
2023-06-02 21:39:35.126: Find a better model.
2023-06-02 21:39:56.238: [iter 21 : loss : 0.9131 = 0.4517 + 0.4603 + 0.0012, time: 21.106258]
2023-06-02 21:39:56.508: epoch 21:	0.02906469  	0.07734424  	0.06471190  
2023-06-02 21:39:56.508: Find a better model.
2023-06-02 21:40:17.414: [iter 22 : loss : 0.8650 = 0.4005 + 0.4631 + 0.0015, time: 20.902436]
2023-06-02 21:40:17.693: epoch 22:	0.02933862  	0.07820343  	0.06504332  
2023-06-02 21:40:17.693: Find a better model.
2023-06-02 21:40:38.795: [iter 23 : loss : 0.8230 = 0.3559 + 0.4653 + 0.0018, time: 21.097496]
2023-06-02 21:40:39.062: epoch 23:	0.02934603  	0.07901935  	0.06527074  
2023-06-02 21:40:39.062: Find a better model.
2023-06-02 21:41:00.242: [iter 24 : loss : 0.7870 = 0.3182 + 0.4668 + 0.0021, time: 21.176567]
2023-06-02 21:41:00.499: epoch 24:	0.02950151  	0.07952352  	0.06563526  
2023-06-02 21:41:00.499: Find a better model.
2023-06-02 21:41:21.361: [iter 25 : loss : 0.7560 = 0.2863 + 0.4673 + 0.0023, time: 20.859098]
2023-06-02 21:41:21.640: epoch 25:	0.02963478  	0.08078822  	0.06596784  
2023-06-02 21:41:21.640: Find a better model.
2023-06-02 21:41:42.571: [iter 26 : loss : 0.7308 = 0.2606 + 0.4676 + 0.0026, time: 20.927043]
2023-06-02 21:41:42.840: epoch 26:	0.02968661  	0.08104330  	0.06610542  
2023-06-02 21:41:42.840: Find a better model.
2023-06-02 21:42:04.187: [iter 27 : loss : 0.7086 = 0.2383 + 0.4673 + 0.0029, time: 21.343162]
2023-06-02 21:42:04.457: epoch 27:	0.02970882  	0.08105437  	0.06618335  
2023-06-02 21:42:04.457: Find a better model.
2023-06-02 21:42:25.617: [iter 28 : loss : 0.6900 = 0.2199 + 0.4670 + 0.0031, time: 21.155272]
2023-06-02 21:42:25.882: epoch 28:	0.02959777  	0.08094150  	0.06610045  
2023-06-02 21:42:46.961: [iter 29 : loss : 0.6739 = 0.2042 + 0.4664 + 0.0034, time: 21.075355]
2023-06-02 21:42:47.225: epoch 29:	0.02956076  	0.08077157  	0.06607218  
2023-06-02 21:43:08.413: [iter 30 : loss : 0.6593 = 0.1899 + 0.4657 + 0.0036, time: 21.184446]
2023-06-02 21:43:08.683: epoch 30:	0.02967921  	0.08097152  	0.06609090  
2023-06-02 21:43:29.919: [iter 31 : loss : 0.6467 = 0.1778 + 0.4651 + 0.0038, time: 21.231150]
2023-06-02 21:43:30.181: epoch 31:	0.02976805  	0.08119195  	0.06642079  
2023-06-02 21:43:30.181: Find a better model.
2023-06-02 21:43:51.584: [iter 32 : loss : 0.6357 = 0.1672 + 0.4645 + 0.0041, time: 21.399060]
2023-06-02 21:43:51.850: epoch 32:	0.02979766  	0.08129639  	0.06652486  
2023-06-02 21:43:51.850: Find a better model.
2023-06-02 21:44:13.305: [iter 33 : loss : 0.6266 = 0.1586 + 0.4637 + 0.0043, time: 21.452450]
2023-06-02 21:44:13.568: epoch 33:	0.02983469  	0.08140498  	0.06669579  
2023-06-02 21:44:13.568: Find a better model.
2023-06-02 21:44:34.706: [iter 34 : loss : 0.6180 = 0.1504 + 0.4632 + 0.0045, time: 21.135256]
2023-06-02 21:44:34.969: epoch 34:	0.02990130  	0.08113890  	0.06672919  
2023-06-02 21:44:56.088: [iter 35 : loss : 0.6092 = 0.1420 + 0.4625 + 0.0047, time: 21.115084]
2023-06-02 21:44:56.353: epoch 35:	0.02994572  	0.08172323  	0.06673352  
2023-06-02 21:44:56.353: Find a better model.
2023-06-02 21:45:17.542: [iter 36 : loss : 0.6025 = 0.1358 + 0.4618 + 0.0049, time: 21.185037]
2023-06-02 21:45:17.807: epoch 36:	0.03010119  	0.08214553  	0.06701069  
2023-06-02 21:45:17.807: Find a better model.
2023-06-02 21:45:39.058: [iter 37 : loss : 0.5962 = 0.1298 + 0.4614 + 0.0050, time: 21.246990]
2023-06-02 21:45:39.320: epoch 37:	0.03004937  	0.08186727  	0.06708784  
2023-06-02 21:46:00.342: [iter 38 : loss : 0.5899 = 0.1239 + 0.4608 + 0.0052, time: 21.018150]
2023-06-02 21:46:00.606: epoch 38:	0.03010118  	0.08179147  	0.06715912  
2023-06-02 21:46:21.519: [iter 39 : loss : 0.5848 = 0.1191 + 0.4603 + 0.0054, time: 20.905118]
2023-06-02 21:46:21.786: epoch 39:	0.03019742  	0.08208784  	0.06729783  
2023-06-02 21:46:42.931: [iter 40 : loss : 0.5793 = 0.1139 + 0.4598 + 0.0056, time: 21.142094]
2023-06-02 21:46:43.194: epoch 40:	0.03013820  	0.08191409  	0.06724903  
2023-06-02 21:47:04.231: [iter 41 : loss : 0.5744 = 0.1093 + 0.4593 + 0.0057, time: 21.034299]
2023-06-02 21:47:04.494: epoch 41:	0.03019003  	0.08174264  	0.06728099  
2023-06-02 21:47:25.517: [iter 42 : loss : 0.5703 = 0.1056 + 0.4589 + 0.0059, time: 21.019436]
2023-06-02 21:47:25.786: epoch 42:	0.03011600  	0.08175179  	0.06742557  
2023-06-02 21:47:47.016: [iter 43 : loss : 0.5662 = 0.1016 + 0.4585 + 0.0060, time: 21.227086]
2023-06-02 21:47:47.280: epoch 43:	0.03017524  	0.08173909  	0.06731759  
2023-06-02 21:48:08.305: [iter 44 : loss : 0.5634 = 0.0991 + 0.4582 + 0.0062, time: 21.021416]
2023-06-02 21:48:08.567: epoch 44:	0.03007158  	0.08141063  	0.06723915  
2023-06-02 21:48:29.840: [iter 45 : loss : 0.5595 = 0.0955 + 0.4577 + 0.0063, time: 21.269826]
2023-06-02 21:48:30.103: epoch 45:	0.02997533  	0.08079883  	0.06696619  
2023-06-02 21:48:51.255: [iter 46 : loss : 0.5565 = 0.0925 + 0.4575 + 0.0065, time: 21.148004]
2023-06-02 21:48:51.518: epoch 46:	0.02993831  	0.08078293  	0.06690172  
2023-06-02 21:49:12.428: [iter 47 : loss : 0.5531 = 0.0894 + 0.4571 + 0.0066, time: 20.907310]
2023-06-02 21:49:12.697: epoch 47:	0.03005676  	0.08087875  	0.06698228  
2023-06-02 21:49:33.649: [iter 48 : loss : 0.5507 = 0.0871 + 0.4568 + 0.0068, time: 20.948234]
2023-06-02 21:49:33.911: epoch 48:	0.02992351  	0.08059748  	0.06688944  
2023-06-02 21:49:55.209: [iter 49 : loss : 0.5481 = 0.0847 + 0.4564 + 0.0069, time: 21.295087]
2023-06-02 21:49:55.479: epoch 49:	0.02993830  	0.08048422  	0.06682012  
2023-06-02 21:50:16.624: [iter 50 : loss : 0.5458 = 0.0826 + 0.4562 + 0.0071, time: 21.142164]
2023-06-02 21:50:16.887: epoch 50:	0.02974582  	0.08020647  	0.06652731  
2023-06-02 21:50:38.012: [iter 51 : loss : 0.5428 = 0.0797 + 0.4559 + 0.0072, time: 21.122301]
2023-06-02 21:50:38.275: epoch 51:	0.02974582  	0.08033860  	0.06651466  
2023-06-02 21:50:59.420: [iter 52 : loss : 0.5408 = 0.0777 + 0.4558 + 0.0073, time: 21.141673]
2023-06-02 21:50:59.693: epoch 52:	0.02971620  	0.08014594  	0.06643576  
2023-06-02 21:51:20.536: [iter 53 : loss : 0.5398 = 0.0769 + 0.4555 + 0.0075, time: 20.838741]
2023-06-02 21:51:20.804: epoch 53:	0.02972361  	0.08033928  	0.06652218  
2023-06-02 21:51:41.761: [iter 54 : loss : 0.5373 = 0.0745 + 0.4552 + 0.0076, time: 20.954686]
2023-06-02 21:51:42.026: epoch 54:	0.02968659  	0.07979819  	0.06627849  
2023-06-02 21:52:03.191: [iter 55 : loss : 0.5356 = 0.0729 + 0.4550 + 0.0077, time: 21.161783]
2023-06-02 21:52:03.455: epoch 55:	0.02967919  	0.07927691  	0.06595960  
2023-06-02 21:52:24.375: [iter 56 : loss : 0.5333 = 0.0708 + 0.4547 + 0.0078, time: 20.916073]
2023-06-02 21:52:24.652: epoch 56:	0.02959036  	0.07901797  	0.06591161  
2023-06-02 21:52:45.582: [iter 57 : loss : 0.5315 = 0.0690 + 0.4546 + 0.0079, time: 20.926984]
2023-06-02 21:52:45.845: epoch 57:	0.02956815  	0.07875190  	0.06595667  
2023-06-02 21:53:06.960: [iter 58 : loss : 0.5299 = 0.0674 + 0.4544 + 0.0081, time: 21.111126]
2023-06-02 21:53:07.221: epoch 58:	0.02942749  	0.07833499  	0.06567864  
2023-06-02 21:53:28.359: [iter 59 : loss : 0.5284 = 0.0659 + 0.4543 + 0.0082, time: 21.134890]
2023-06-02 21:53:28.635: epoch 59:	0.02950892  	0.07835007  	0.06569020  
2023-06-02 21:53:49.551: [iter 60 : loss : 0.5275 = 0.0650 + 0.4542 + 0.0083, time: 20.912606]
2023-06-02 21:53:49.816: epoch 60:	0.02930163  	0.07780144  	0.06534832  
2023-06-02 21:54:11.153: [iter 61 : loss : 0.5261 = 0.0637 + 0.4540 + 0.0084, time: 21.334301]
2023-06-02 21:54:11.416: epoch 61:	0.02930903  	0.07767280  	0.06524917  
2023-06-02 21:54:11.416: Early stopping is trigger at epoch: 61
2023-06-02 21:54:11.416: best_result@epoch 36:

2023-06-02 21:54:11.416: 		0.0301      	0.0821      	0.0670      
2023-06-03 18:14:46.048: my pid: 5576
2023-06-03 18:14:46.048: model: model.general_recommender.SGL
2023-06-03 18:14:46.048: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 18:14:46.048: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 18:14:50.082: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 18:15:11.537: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.454319]
2023-06-03 18:15:11.820: epoch 1:	0.00097720  	0.00185775  	0.00155657  
2023-06-03 18:15:11.820: Find a better model.
2023-06-03 18:15:33.174: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.350540]
2023-06-03 18:15:33.464: epoch 2:	0.00165087  	0.00339836  	0.00261015  
2023-06-03 18:15:33.464: Find a better model.
2023-06-03 18:15:54.735: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.266474]
2023-06-03 18:15:55.029: epoch 3:	0.00202103  	0.00400758  	0.00315942  
2023-06-03 18:15:55.029: Find a better model.
2023-06-03 18:16:16.484: [iter 4 : loss : 1.1342 = 0.6928 + 0.4413 + 0.0000, time: 21.450232]
2023-06-03 18:16:16.792: epoch 4:	0.00236157  	0.00475203  	0.00385363  
2023-06-03 18:16:16.792: Find a better model.
2023-06-03 18:16:38.076: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.281060]
2023-06-03 18:16:38.372: epoch 5:	0.00305745  	0.00627481  	0.00500220  
2023-06-03 18:16:38.372: Find a better model.
2023-06-03 18:16:59.478: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 21.103283]
2023-06-03 18:16:59.788: epoch 6:	0.00335357  	0.00691598  	0.00555971  
2023-06-03 18:16:59.788: Find a better model.
2023-06-03 18:17:20.855: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.062052]
2023-06-03 18:17:21.148: epoch 7:	0.00397542  	0.00925659  	0.00751291  
2023-06-03 18:17:21.148: Find a better model.
2023-06-03 18:17:42.454: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.302153]
2023-06-03 18:17:42.759: epoch 8:	0.00496001  	0.01290894  	0.00962634  
2023-06-03 18:17:42.759: Find a better model.
2023-06-03 18:18:03.829: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.066097]
2023-06-03 18:18:04.121: epoch 9:	0.00653684  	0.01754567  	0.01313285  
2023-06-03 18:18:04.121: Find a better model.
2023-06-03 18:18:25.240: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.116039]
2023-06-03 18:18:25.532: epoch 10:	0.00746961  	0.02105456  	0.01685553  
2023-06-03 18:18:25.532: Find a better model.
2023-06-03 18:18:46.508: [iter 11 : loss : 1.1340 = 0.6900 + 0.4439 + 0.0000, time: 20.972160]
2023-06-03 18:18:46.807: epoch 11:	0.00868369  	0.02373989  	0.01981260  
2023-06-03 18:18:46.807: Find a better model.
2023-06-03 18:19:07.823: [iter 12 : loss : 1.1329 = 0.6884 + 0.4445 + 0.0000, time: 21.011093]
2023-06-03 18:19:08.107: epoch 12:	0.01011989  	0.02761707  	0.02332894  
2023-06-03 18:19:08.107: Find a better model.
2023-06-03 18:19:29.060: [iter 13 : loss : 1.1309 = 0.6856 + 0.4452 + 0.0000, time: 20.949024]
2023-06-03 18:19:29.344: epoch 13:	0.01201508  	0.03313431  	0.02765623  
2023-06-03 18:19:29.344: Find a better model.
2023-06-03 18:19:50.410: [iter 14 : loss : 1.1273 = 0.6812 + 0.4460 + 0.0001, time: 21.062083]
2023-06-03 18:19:50.689: epoch 14:	0.01441371  	0.03944743  	0.03404998  
2023-06-03 18:19:50.689: Find a better model.
2023-06-03 18:20:11.639: [iter 15 : loss : 1.1209 = 0.6739 + 0.4469 + 0.0001, time: 20.946046]
2023-06-03 18:20:11.909: epoch 15:	0.01795982  	0.04838078  	0.04146821  
2023-06-03 18:20:11.909: Find a better model.
2023-06-03 18:20:33.616: [iter 16 : loss : 1.1091 = 0.6612 + 0.4477 + 0.0002, time: 21.702722]
2023-06-03 18:20:33.896: epoch 16:	0.02095071  	0.05566287  	0.04848100  
2023-06-03 18:20:33.896: Find a better model.
2023-06-03 18:20:55.625: [iter 17 : loss : 1.0896 = 0.6404 + 0.4490 + 0.0003, time: 21.725461]
2023-06-03 18:20:55.909: epoch 17:	0.02412670  	0.06307194  	0.05455368  
2023-06-03 18:20:55.909: Find a better model.
2023-06-03 18:21:17.610: [iter 18 : loss : 1.0593 = 0.6082 + 0.4508 + 0.0004, time: 21.696619]
2023-06-03 18:21:17.897: epoch 18:	0.02639211  	0.06917882  	0.05918448  
2023-06-03 18:21:17.897: Find a better model.
2023-06-03 18:21:39.603: [iter 19 : loss : 1.0189 = 0.5652 + 0.4532 + 0.0006, time: 21.703171]
2023-06-03 18:21:39.884: epoch 19:	0.02765808  	0.07335961  	0.06249635  
2023-06-03 18:21:39.884: Find a better model.
2023-06-03 18:22:01.400: [iter 20 : loss : 0.9712 = 0.5141 + 0.4563 + 0.0008, time: 21.511638]
2023-06-03 18:22:01.678: epoch 20:	0.02865754  	0.07716711  	0.06453286  
2023-06-03 18:22:01.678: Find a better model.
2023-06-03 18:22:23.204: [iter 21 : loss : 0.9209 = 0.4603 + 0.4594 + 0.0011, time: 21.523180]
2023-06-03 18:22:23.482: epoch 21:	0.02929421  	0.07867097  	0.06545639  
2023-06-03 18:22:23.482: Find a better model.
2023-06-03 18:22:45.191: [iter 22 : loss : 0.8721 = 0.4081 + 0.4625 + 0.0014, time: 21.704081]
2023-06-03 18:22:45.467: epoch 22:	0.02942747  	0.07981997  	0.06558856  
2023-06-03 18:22:45.467: Find a better model.
2023-06-03 18:23:07.162: [iter 23 : loss : 0.8290 = 0.3625 + 0.4647 + 0.0017, time: 21.690977]
2023-06-03 18:23:07.435: epoch 23:	0.02961256  	0.08030568  	0.06592403  
2023-06-03 18:23:07.435: Find a better model.
2023-06-03 18:23:29.166: [iter 24 : loss : 0.7919 = 0.3235 + 0.4663 + 0.0020, time: 21.727013]
2023-06-03 18:23:29.438: epoch 24:	0.02964218  	0.08043775  	0.06596896  
2023-06-03 18:23:29.438: Find a better model.
2023-06-03 18:23:51.154: [iter 25 : loss : 0.7602 = 0.2909 + 0.4671 + 0.0023, time: 21.712009]
2023-06-03 18:23:51.427: epoch 25:	0.02976064  	0.08091428  	0.06608167  
2023-06-03 18:23:51.427: Find a better model.
2023-06-03 18:24:13.160: [iter 26 : loss : 0.7339 = 0.2641 + 0.4672 + 0.0026, time: 21.729069]
2023-06-03 18:24:13.432: epoch 26:	0.02999755  	0.08178055  	0.06638026  
2023-06-03 18:24:13.432: Find a better model.
2023-06-03 18:24:35.133: [iter 27 : loss : 0.7112 = 0.2412 + 0.4672 + 0.0028, time: 21.696960]
2023-06-03 18:24:35.403: epoch 27:	0.03001236  	0.08164605  	0.06642511  
2023-06-03 18:24:57.143: [iter 28 : loss : 0.6922 = 0.2224 + 0.4668 + 0.0031, time: 21.736938]
2023-06-03 18:24:57.413: epoch 28:	0.03009379  	0.08215919  	0.06670205  
2023-06-03 18:24:57.413: Find a better model.
2023-06-03 18:25:19.138: [iter 29 : loss : 0.6755 = 0.2060 + 0.4661 + 0.0033, time: 21.721460]
2023-06-03 18:25:19.409: epoch 29:	0.03019003  	0.08263379  	0.06704989  
2023-06-03 18:25:19.409: Find a better model.
2023-06-03 18:25:41.135: [iter 30 : loss : 0.6609 = 0.1917 + 0.4657 + 0.0036, time: 21.722695]
2023-06-03 18:25:41.405: epoch 30:	0.03021964  	0.08277114  	0.06716432  
2023-06-03 18:25:41.405: Find a better model.
2023-06-03 18:26:03.292: [iter 31 : loss : 0.6480 = 0.1792 + 0.4649 + 0.0038, time: 21.883449]
2023-06-03 18:26:03.563: epoch 31:	0.03005677  	0.08261763  	0.06705444  
2023-06-03 18:26:25.130: [iter 32 : loss : 0.6371 = 0.1687 + 0.4643 + 0.0040, time: 21.562980]
2023-06-03 18:26:25.400: epoch 32:	0.02999013  	0.08239453  	0.06695764  
2023-06-03 18:26:47.113: [iter 33 : loss : 0.6276 = 0.1598 + 0.4636 + 0.0042, time: 21.709048]
2023-06-03 18:26:47.383: epoch 33:	0.03014561  	0.08282932  	0.06731400  
2023-06-03 18:26:47.383: Find a better model.
2023-06-03 18:27:09.120: [iter 34 : loss : 0.6183 = 0.1511 + 0.4628 + 0.0044, time: 21.732610]
2023-06-03 18:27:09.388: epoch 34:	0.03013079  	0.08272594  	0.06719422  
2023-06-03 18:27:31.086: [iter 35 : loss : 0.6101 = 0.1431 + 0.4623 + 0.0046, time: 21.693458]
2023-06-03 18:27:31.355: epoch 35:	0.03021222  	0.08250327  	0.06724670  
2023-06-03 18:27:53.250: [iter 36 : loss : 0.6031 = 0.1366 + 0.4617 + 0.0048, time: 21.890898]
2023-06-03 18:27:53.518: epoch 36:	0.03030847  	0.08264508  	0.06754591  
2023-06-03 18:28:15.251: [iter 37 : loss : 0.5965 = 0.1303 + 0.4612 + 0.0050, time: 21.729184]
2023-06-03 18:28:15.521: epoch 37:	0.03019002  	0.08241981  	0.06736427  
2023-06-03 18:28:37.228: [iter 38 : loss : 0.5903 = 0.1246 + 0.4605 + 0.0052, time: 21.703102]
2023-06-03 18:28:37.495: epoch 38:	0.03029367  	0.08277684  	0.06750585  
2023-06-03 18:28:59.249: [iter 39 : loss : 0.5853 = 0.1199 + 0.4601 + 0.0054, time: 21.750033]
2023-06-03 18:28:59.516: epoch 39:	0.03029367  	0.08280462  	0.06763562  
2023-06-03 18:29:21.259: [iter 40 : loss : 0.5797 = 0.1146 + 0.4596 + 0.0055, time: 21.740426]
2023-06-03 18:29:21.531: epoch 40:	0.03035290  	0.08280433  	0.06773388  
2023-06-03 18:29:43.252: [iter 41 : loss : 0.5744 = 0.1097 + 0.4590 + 0.0057, time: 21.716614]
2023-06-03 18:29:43.520: epoch 41:	0.03017521  	0.08275144  	0.06764489  
2023-06-03 18:30:05.209: [iter 42 : loss : 0.5705 = 0.1059 + 0.4587 + 0.0059, time: 21.685217]
2023-06-03 18:30:05.479: epoch 42:	0.03028627  	0.08269802  	0.06766207  
2023-06-03 18:30:27.216: [iter 43 : loss : 0.5662 = 0.1019 + 0.4583 + 0.0060, time: 21.732929]
2023-06-03 18:30:27.484: epoch 43:	0.03014560  	0.08200741  	0.06729201  
2023-06-03 18:30:49.216: [iter 44 : loss : 0.5635 = 0.0994 + 0.4579 + 0.0062, time: 21.728364]
2023-06-03 18:30:49.488: epoch 44:	0.03013079  	0.08209465  	0.06731115  
2023-06-03 18:31:11.193: [iter 45 : loss : 0.5595 = 0.0956 + 0.4575 + 0.0063, time: 21.699300]
2023-06-03 18:31:11.460: epoch 45:	0.03007897  	0.08174894  	0.06725241  
2023-06-03 18:31:33.185: [iter 46 : loss : 0.5565 = 0.0928 + 0.4572 + 0.0065, time: 21.720098]
2023-06-03 18:31:33.456: epoch 46:	0.02990870  	0.08102848  	0.06689838  
2023-06-03 18:31:55.379: [iter 47 : loss : 0.5532 = 0.0898 + 0.4568 + 0.0066, time: 21.919104]
2023-06-03 18:31:55.648: epoch 47:	0.02987909  	0.08098539  	0.06676131  
2023-06-03 18:32:17.679: [iter 48 : loss : 0.5506 = 0.0873 + 0.4565 + 0.0068, time: 22.026671]
2023-06-03 18:32:17.963: epoch 48:	0.02970141  	0.08026855  	0.06644707  
2023-06-03 18:32:40.676: [iter 49 : loss : 0.5481 = 0.0851 + 0.4562 + 0.0069, time: 22.710266]
2023-06-03 18:32:40.978: epoch 49:	0.02965698  	0.08019159  	0.06639950  
2023-06-03 18:33:03.381: my pid: 2100
2023-06-03 18:33:03.381: model: model.general_recommender.SGL
2023-06-03 18:33:03.381: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 18:33:03.381: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 18:33:07.739: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 18:33:29.178: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.436582]
2023-06-03 18:33:29.437: epoch 1:	0.00118448  	0.00235553  	0.00202550  
2023-06-03 18:33:29.437: Find a better model.
2023-06-03 18:33:50.322: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.880551]
2023-06-03 18:33:50.613: epoch 2:	0.00175452  	0.00295176  	0.00257103  
2023-06-03 18:33:50.613: Find a better model.
2023-06-03 18:34:11.513: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.895106]
2023-06-03 18:34:11.817: epoch 3:	0.00190258  	0.00367540  	0.00296255  
2023-06-03 18:34:11.818: Find a better model.
2023-06-03 18:34:32.507: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.685042]
2023-06-03 18:34:32.810: epoch 4:	0.00230234  	0.00403839  	0.00364342  
2023-06-03 18:34:32.810: Find a better model.
2023-06-03 18:34:53.499: [iter 5 : loss : 1.1345 = 0.6927 + 0.4418 + 0.0000, time: 20.684531]
2023-06-03 18:34:53.810: epoch 5:	0.00267989  	0.00633098  	0.00468608  
2023-06-03 18:34:53.810: Find a better model.
2023-06-03 18:35:14.488: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.672637]
2023-06-03 18:35:14.794: epoch 6:	0.00348682  	0.00871261  	0.00649859  
2023-06-03 18:35:14.794: Find a better model.
2023-06-03 18:35:35.471: [iter 7 : loss : 1.1347 = 0.6923 + 0.4425 + 0.0000, time: 20.674023]
2023-06-03 18:35:35.786: epoch 7:	0.00369411  	0.00897768  	0.00703101  
2023-06-03 18:35:35.786: Find a better model.
2023-06-03 18:35:56.638: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.847324]
2023-06-03 18:35:56.947: epoch 8:	0.00444181  	0.01066499  	0.00830655  
2023-06-03 18:35:56.947: Find a better model.
2023-06-03 18:36:17.849: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.897491]
2023-06-03 18:36:18.148: epoch 9:	0.00570031  	0.01516679  	0.01134446  
2023-06-03 18:36:18.148: Find a better model.
2023-06-03 18:36:39.062: [iter 10 : loss : 1.1345 = 0.6908 + 0.4437 + 0.0000, time: 20.910037]
2023-06-03 18:36:39.363: epoch 10:	0.00611487  	0.01646584  	0.01252020  
2023-06-03 18:36:39.363: Find a better model.
2023-06-03 18:37:00.232: [iter 11 : loss : 1.1338 = 0.6895 + 0.4442 + 0.0000, time: 20.864259]
2023-06-03 18:37:00.513: epoch 11:	0.00719570  	0.01917313  	0.01519826  
2023-06-03 18:37:00.513: Find a better model.
2023-06-03 18:37:21.224: [iter 12 : loss : 1.1325 = 0.6876 + 0.4449 + 0.0000, time: 20.707594]
2023-06-03 18:37:21.521: epoch 12:	0.00860967  	0.02291186  	0.01890688  
2023-06-03 18:37:21.521: Find a better model.
2023-06-03 18:37:42.395: [iter 13 : loss : 1.1304 = 0.6847 + 0.4456 + 0.0001, time: 20.870163]
2023-06-03 18:37:42.690: epoch 13:	0.01097125  	0.02894369  	0.02444356  
2023-06-03 18:37:42.690: Find a better model.
2023-06-03 18:38:03.423: [iter 14 : loss : 1.1265 = 0.6800 + 0.4464 + 0.0001, time: 20.729191]
2023-06-03 18:38:03.718: epoch 14:	0.01422862  	0.03828558  	0.03157021  
2023-06-03 18:38:03.718: Find a better model.
2023-06-03 18:38:24.466: [iter 15 : loss : 1.1195 = 0.6721 + 0.4473 + 0.0001, time: 20.743998]
2023-06-03 18:38:24.766: epoch 15:	0.01732315  	0.04617191  	0.03931201  
2023-06-03 18:38:24.766: Find a better model.
2023-06-03 18:38:46.399: [iter 16 : loss : 1.1066 = 0.6583 + 0.4481 + 0.0002, time: 21.629509]
2023-06-03 18:38:46.687: epoch 16:	0.02048433  	0.05488509  	0.04710793  
2023-06-03 18:38:46.687: Find a better model.
2023-06-03 18:39:07.994: [iter 17 : loss : 1.0859 = 0.6361 + 0.4495 + 0.0003, time: 21.303223]
2023-06-03 18:39:08.287: epoch 17:	0.02376397  	0.06328420  	0.05340233  
2023-06-03 18:39:08.287: Find a better model.
2023-06-03 18:39:29.588: [iter 18 : loss : 1.0544 = 0.6027 + 0.4513 + 0.0004, time: 21.296131]
2023-06-03 18:39:29.886: epoch 18:	0.02609601  	0.06973618  	0.05796287  
2023-06-03 18:39:29.886: Find a better model.
2023-06-03 18:39:51.132: [iter 19 : loss : 1.0131 = 0.5588 + 0.4537 + 0.0006, time: 21.242079]
2023-06-03 18:39:51.418: epoch 19:	0.02760627  	0.07356747  	0.06092753  
2023-06-03 18:39:51.419: Find a better model.
2023-06-03 18:40:12.757: [iter 20 : loss : 0.9653 = 0.5077 + 0.4568 + 0.0009, time: 21.334488]
2023-06-03 18:40:13.047: epoch 20:	0.02851687  	0.07674540  	0.06293846  
2023-06-03 18:40:13.048: Find a better model.
2023-06-03 18:40:34.350: [iter 21 : loss : 0.9159 = 0.4548 + 0.4599 + 0.0011, time: 21.299102]
2023-06-03 18:40:34.640: epoch 21:	0.02897588  	0.07837598  	0.06385195  
2023-06-03 18:40:34.641: Find a better model.
2023-06-03 18:40:55.932: [iter 22 : loss : 0.8680 = 0.4037 + 0.4629 + 0.0014, time: 21.288025]
2023-06-03 18:40:56.217: epoch 22:	0.02913876  	0.07936127  	0.06432685  
2023-06-03 18:40:56.217: Find a better model.
2023-06-03 18:41:17.337: [iter 23 : loss : 0.8259 = 0.3592 + 0.4650 + 0.0017, time: 21.114972]
2023-06-03 18:41:17.627: epoch 23:	0.02936825  	0.07998521  	0.06465719  
2023-06-03 18:41:17.627: Find a better model.
2023-06-03 18:41:38.933: [iter 24 : loss : 0.7896 = 0.3211 + 0.4665 + 0.0020, time: 21.301095]
2023-06-03 18:41:39.216: epoch 24:	0.02945710  	0.08089735  	0.06491483  
2023-06-03 18:41:39.216: Find a better model.
2023-06-03 18:42:00.486: [iter 25 : loss : 0.7587 = 0.2892 + 0.4672 + 0.0023, time: 21.266078]
2023-06-03 18:42:00.783: epoch 25:	0.02967919  	0.08115634  	0.06519366  
2023-06-03 18:42:00.783: Find a better model.
2023-06-03 18:42:21.918: [iter 26 : loss : 0.7331 = 0.2630 + 0.4675 + 0.0026, time: 21.131805]
2023-06-03 18:42:22.198: epoch 26:	0.02980504  	0.08126450  	0.06559569  
2023-06-03 18:42:22.198: Find a better model.
2023-06-03 18:42:43.507: [iter 27 : loss : 0.7104 = 0.2404 + 0.4671 + 0.0029, time: 21.303807]
2023-06-03 18:42:43.792: epoch 27:	0.02979024  	0.08175191  	0.06579538  
2023-06-03 18:42:43.792: Find a better model.
2023-06-03 18:43:05.119: [iter 28 : loss : 0.6917 = 0.2217 + 0.4669 + 0.0031, time: 21.323265]
2023-06-03 18:43:05.398: epoch 28:	0.02983467  	0.08226059  	0.06607831  
2023-06-03 18:43:05.398: Find a better model.
2023-06-03 18:43:26.492: [iter 29 : loss : 0.6753 = 0.2056 + 0.4663 + 0.0034, time: 21.089961]
2023-06-03 18:43:26.778: epoch 29:	0.02980505  	0.08236666  	0.06618603  
2023-06-03 18:43:26.778: Find a better model.
2023-06-03 18:43:47.891: [iter 30 : loss : 0.6605 = 0.1911 + 0.4658 + 0.0036, time: 21.109146]
2023-06-03 18:43:48.163: epoch 30:	0.02978284  	0.08217998  	0.06603362  
2023-06-03 18:44:09.451: [iter 31 : loss : 0.6479 = 0.1789 + 0.4652 + 0.0038, time: 21.283434]
2023-06-03 18:44:09.723: epoch 31:	0.02976803  	0.08187252  	0.06604598  
2023-06-03 18:44:31.069: [iter 32 : loss : 0.6369 = 0.1682 + 0.4646 + 0.0040, time: 21.342042]
2023-06-03 18:44:31.340: epoch 32:	0.02989389  	0.08217698  	0.06629296  
2023-06-03 18:44:52.652: [iter 33 : loss : 0.6277 = 0.1597 + 0.4638 + 0.0043, time: 21.306580]
2023-06-03 18:44:52.925: epoch 33:	0.02981245  	0.08220906  	0.06644638  
2023-06-03 18:45:14.066: [iter 34 : loss : 0.6185 = 0.1508 + 0.4632 + 0.0045, time: 21.137362]
2023-06-03 18:45:14.337: epoch 34:	0.02984207  	0.08228223  	0.06649566  
2023-06-03 18:45:35.627: [iter 35 : loss : 0.6100 = 0.1428 + 0.4626 + 0.0047, time: 21.283056]
2023-06-03 18:45:35.903: epoch 35:	0.02982726  	0.08251672  	0.06653447  
2023-06-03 18:45:35.903: Find a better model.
2023-06-03 18:45:57.247: [iter 36 : loss : 0.6033 = 0.1364 + 0.4620 + 0.0048, time: 21.340097]
2023-06-03 18:45:57.517: epoch 36:	0.02975322  	0.08230535  	0.06656734  
2023-06-03 18:46:18.805: [iter 37 : loss : 0.5970 = 0.1304 + 0.4615 + 0.0050, time: 21.283125]
2023-06-03 18:46:19.076: epoch 37:	0.02987168  	0.08221470  	0.06667283  
2023-06-03 18:46:40.427: [iter 38 : loss : 0.5904 = 0.1242 + 0.4610 + 0.0052, time: 21.346434]
2023-06-03 18:46:40.699: epoch 38:	0.02987167  	0.08232363  	0.06682046  
2023-06-03 18:47:02.080: [iter 39 : loss : 0.5854 = 0.1196 + 0.4604 + 0.0054, time: 21.376642]
2023-06-03 18:47:02.351: epoch 39:	0.02970139  	0.08156691  	0.06657714  
2023-06-03 18:47:23.617: [iter 40 : loss : 0.5800 = 0.1145 + 0.4599 + 0.0056, time: 21.261473]
2023-06-03 18:47:23.894: epoch 40:	0.02963477  	0.08142383  	0.06665508  
2023-06-03 18:47:45.379: [iter 41 : loss : 0.5751 = 0.1099 + 0.4595 + 0.0057, time: 21.482240]
2023-06-03 18:47:45.651: epoch 41:	0.02964216  	0.08121605  	0.06653219  
2023-06-03 18:48:07.194: [iter 42 : loss : 0.5707 = 0.1057 + 0.4591 + 0.0059, time: 21.539157]
2023-06-03 18:48:07.467: epoch 42:	0.02960515  	0.08108228  	0.06656548  
2023-06-03 18:48:28.777: [iter 43 : loss : 0.5668 = 0.1020 + 0.4587 + 0.0060, time: 21.306511]
2023-06-03 18:48:29.046: epoch 43:	0.02968658  	0.08125035  	0.06646502  
2023-06-03 18:48:50.343: [iter 44 : loss : 0.5639 = 0.0993 + 0.4584 + 0.0062, time: 21.292999]
2023-06-03 18:48:50.613: epoch 44:	0.02949410  	0.08067056  	0.06627428  
2023-06-03 18:49:12.019: [iter 45 : loss : 0.5600 = 0.0956 + 0.4580 + 0.0064, time: 21.403742]
2023-06-03 18:49:12.281: epoch 45:	0.02943486  	0.08043423  	0.06621134  
2023-06-03 18:49:33.621: [iter 46 : loss : 0.5568 = 0.0926 + 0.4577 + 0.0065, time: 21.336069]
2023-06-03 18:49:33.900: epoch 46:	0.02953111  	0.08052726  	0.06624389  
2023-06-03 18:49:55.417: [iter 47 : loss : 0.5536 = 0.0897 + 0.4573 + 0.0066, time: 21.511982]
2023-06-03 18:49:55.681: epoch 47:	0.02953851  	0.08016929  	0.06614508  
2023-06-03 18:50:17.406: [iter 48 : loss : 0.5510 = 0.0872 + 0.4570 + 0.0068, time: 21.719669]
2023-06-03 18:50:17.691: epoch 48:	0.02934602  	0.07926679  	0.06577051  
2023-06-03 18:50:50.747: my pid: 16164
2023-06-03 18:50:50.747: model: model.general_recommender.SGL
2023-06-03 18:50:50.747: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 18:50:50.747: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 18:50:55.069: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 18:51:16.140: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.069865]
2023-06-03 18:51:16.414: epoch 1:	0.00115487  	0.00228493  	0.00190254  
2023-06-03 18:51:16.414: Find a better model.
2023-06-03 18:51:37.363: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.945834]
2023-06-03 18:51:37.656: epoch 2:	0.00166568  	0.00371545  	0.00289654  
2023-06-03 18:51:37.656: Find a better model.
2023-06-03 18:51:58.551: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.890792]
2023-06-03 18:51:58.848: epoch 3:	0.00216168  	0.00515186  	0.00374820  
2023-06-03 18:51:58.848: Find a better model.
2023-06-03 18:52:19.539: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.686996]
2023-06-03 18:52:19.846: epoch 4:	0.00239858  	0.00474883  	0.00393039  
2023-06-03 18:52:40.705: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.855642]
2023-06-03 18:52:40.992: epoch 5:	0.00287978  	0.00620720  	0.00517905  
2023-06-03 18:52:40.992: Find a better model.
2023-06-03 18:53:01.706: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.711093]
2023-06-03 18:53:01.999: epoch 6:	0.00331655  	0.00705786  	0.00590326  
2023-06-03 18:53:01.999: Find a better model.
2023-06-03 18:53:22.707: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.703029]
2023-06-03 18:53:23.004: epoch 7:	0.00387178  	0.00933125  	0.00724821  
2023-06-03 18:53:23.004: Find a better model.
2023-06-03 18:53:43.921: [iter 8 : loss : 1.1347 = 0.6920 + 0.4428 + 0.0000, time: 20.914165]
2023-06-03 18:53:44.216: epoch 8:	0.00461948  	0.01189526  	0.00976882  
2023-06-03 18:53:44.216: Find a better model.
2023-06-03 18:54:04.907: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.687442]
2023-06-03 18:54:05.202: epoch 9:	0.00548562  	0.01473232  	0.01190058  
2023-06-03 18:54:05.202: Find a better model.
2023-06-03 18:54:25.876: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.670312]
2023-06-03 18:54:26.168: epoch 10:	0.00594461  	0.01562324  	0.01311378  
2023-06-03 18:54:26.168: Find a better model.
2023-06-03 18:54:46.908: [iter 11 : loss : 1.1338 = 0.6895 + 0.4443 + 0.0000, time: 20.735766]
2023-06-03 18:54:47.198: epoch 11:	0.00713648  	0.01898755  	0.01574626  
2023-06-03 18:54:47.198: Find a better model.
2023-06-03 18:55:07.857: [iter 12 : loss : 1.1324 = 0.6875 + 0.4449 + 0.0000, time: 20.655805]
2023-06-03 18:55:08.148: epoch 12:	0.00854304  	0.02387229  	0.01960167  
2023-06-03 18:55:08.148: Find a better model.
2023-06-03 18:55:28.676: [iter 13 : loss : 1.1305 = 0.6848 + 0.4456 + 0.0001, time: 20.523421]
2023-06-03 18:55:28.967: epoch 13:	0.01115633  	0.03030804  	0.02594843  
2023-06-03 18:55:28.967: Find a better model.
2023-06-03 18:55:49.492: [iter 14 : loss : 1.1267 = 0.6802 + 0.4464 + 0.0001, time: 20.522000]
2023-06-03 18:55:49.785: epoch 14:	0.01363637  	0.03695428  	0.03217587  
2023-06-03 18:55:49.785: Find a better model.
2023-06-03 18:56:10.236: [iter 15 : loss : 1.1198 = 0.6724 + 0.4473 + 0.0001, time: 20.446880]
2023-06-03 18:56:10.514: epoch 15:	0.01707144  	0.04473484  	0.03922755  
2023-06-03 18:56:10.514: Find a better model.
2023-06-03 18:56:31.433: [iter 16 : loss : 1.1072 = 0.6589 + 0.4481 + 0.0002, time: 20.914405]
2023-06-03 18:56:31.712: epoch 16:	0.02054354  	0.05390240  	0.04723354  
2023-06-03 18:56:31.712: Find a better model.
2023-06-03 18:56:52.591: [iter 17 : loss : 1.0868 = 0.6370 + 0.4495 + 0.0003, time: 20.875695]
2023-06-03 18:56:52.868: epoch 17:	0.02343821  	0.06131557  	0.05332965  
2023-06-03 18:56:52.868: Find a better model.
2023-06-03 18:57:13.763: [iter 18 : loss : 1.0554 = 0.6037 + 0.4513 + 0.0004, time: 20.890687]
2023-06-03 18:57:14.042: epoch 18:	0.02576286  	0.06813243  	0.05813864  
2023-06-03 18:57:14.042: Find a better model.
2023-06-03 18:57:34.824: [iter 19 : loss : 1.0144 = 0.5600 + 0.4538 + 0.0006, time: 20.777505]
2023-06-03 18:57:35.103: epoch 19:	0.02742856  	0.07214731  	0.06120544  
2023-06-03 18:57:35.103: Find a better model.
2023-06-03 18:57:56.144: [iter 20 : loss : 0.9663 = 0.5086 + 0.4569 + 0.0009, time: 21.036525]
2023-06-03 18:57:56.419: epoch 20:	0.02868715  	0.07595537  	0.06354933  
2023-06-03 18:57:56.419: Find a better model.
2023-06-03 18:58:17.332: [iter 21 : loss : 0.9167 = 0.4552 + 0.4603 + 0.0011, time: 20.909419]
2023-06-03 18:58:17.610: epoch 21:	0.02903510  	0.07768217  	0.06423256  
2023-06-03 18:58:17.610: Find a better model.
2023-06-03 18:58:38.558: [iter 22 : loss : 0.8685 = 0.4038 + 0.4633 + 0.0014, time: 20.944129]
2023-06-03 18:58:38.843: epoch 22:	0.02919798  	0.07849911  	0.06435721  
2023-06-03 18:58:38.843: Find a better model.
2023-06-03 18:58:59.543: [iter 23 : loss : 0.8263 = 0.3591 + 0.4655 + 0.0017, time: 20.695075]
2023-06-03 18:58:59.826: epoch 23:	0.02928681  	0.07837690  	0.06438054  
2023-06-03 18:59:20.742: [iter 24 : loss : 0.7898 = 0.3207 + 0.4671 + 0.0020, time: 20.911351]
2023-06-03 18:59:21.020: epoch 24:	0.02941267  	0.07946658  	0.06452433  
2023-06-03 18:59:21.021: Find a better model.
2023-06-03 18:59:41.924: [iter 25 : loss : 0.7586 = 0.2887 + 0.4676 + 0.0023, time: 20.899562]
2023-06-03 18:59:42.195: epoch 25:	0.02947190  	0.07962080  	0.06471816  
2023-06-03 18:59:42.196: Find a better model.
2023-06-03 19:00:03.338: [iter 26 : loss : 0.7329 = 0.2624 + 0.4680 + 0.0026, time: 21.139089]
2023-06-03 19:00:03.612: epoch 26:	0.02964958  	0.08059543  	0.06536014  
2023-06-03 19:00:03.612: Find a better model.
2023-06-03 19:00:24.594: [iter 27 : loss : 0.7106 = 0.2400 + 0.4677 + 0.0029, time: 20.977440]
2023-06-03 19:00:24.867: epoch 27:	0.02966440  	0.08078269  	0.06538285  
2023-06-03 19:00:24.867: Find a better model.
2023-06-03 19:00:46.114: [iter 28 : loss : 0.6918 = 0.2215 + 0.4672 + 0.0031, time: 21.243352]
2023-06-03 19:00:46.386: epoch 28:	0.02966439  	0.08067436  	0.06533509  
2023-06-03 19:01:07.318: [iter 29 : loss : 0.6754 = 0.2053 + 0.4668 + 0.0034, time: 20.927652]
2023-06-03 19:01:07.592: epoch 29:	0.02970140  	0.08078935  	0.06544703  
2023-06-03 19:01:07.592: Find a better model.
2023-06-03 19:01:28.543: [iter 30 : loss : 0.6608 = 0.1911 + 0.4661 + 0.0036, time: 20.947152]
2023-06-03 19:01:28.821: epoch 30:	0.02973101  	0.08081128  	0.06546312  
2023-06-03 19:01:28.822: Find a better model.
2023-06-03 19:01:50.089: [iter 31 : loss : 0.6481 = 0.1788 + 0.4655 + 0.0038, time: 21.262797]
2023-06-03 19:01:50.346: epoch 31:	0.02964958  	0.08073504  	0.06566742  
2023-06-03 19:02:11.517: [iter 32 : loss : 0.6372 = 0.1683 + 0.4648 + 0.0040, time: 21.166129]
2023-06-03 19:02:11.800: epoch 32:	0.02988649  	0.08108429  	0.06594680  
2023-06-03 19:02:11.800: Find a better model.
2023-06-03 19:02:32.864: [iter 33 : loss : 0.6282 = 0.1598 + 0.4642 + 0.0043, time: 21.060399]
2023-06-03 19:02:33.133: epoch 33:	0.03001233  	0.08103635  	0.06601738  
2023-06-03 19:02:54.482: [iter 34 : loss : 0.6190 = 0.1510 + 0.4636 + 0.0045, time: 21.344000]
2023-06-03 19:02:54.768: epoch 34:	0.03007155  	0.08127919  	0.06620996  
2023-06-03 19:02:54.768: Find a better model.
2023-06-03 19:03:15.719: [iter 35 : loss : 0.6102 = 0.1428 + 0.4628 + 0.0047, time: 20.946117]
2023-06-03 19:03:15.992: epoch 35:	0.03012337  	0.08154313  	0.06624331  
2023-06-03 19:03:15.992: Find a better model.
2023-06-03 19:03:37.077: [iter 36 : loss : 0.6036 = 0.1366 + 0.4622 + 0.0048, time: 21.081100]
2023-06-03 19:03:37.347: epoch 36:	0.03001232  	0.08139106  	0.06615445  
2023-06-03 19:03:58.483: [iter 37 : loss : 0.5971 = 0.1304 + 0.4617 + 0.0050, time: 21.131474]
2023-06-03 19:03:58.771: epoch 37:	0.03012337  	0.08173350  	0.06636000  
2023-06-03 19:03:58.771: Find a better model.
2023-06-03 19:04:19.839: [iter 38 : loss : 0.5909 = 0.1246 + 0.4611 + 0.0052, time: 21.063155]
2023-06-03 19:04:20.117: epoch 38:	0.03009376  	0.08150285  	0.06635991  
2023-06-03 19:04:41.049: [iter 39 : loss : 0.5859 = 0.1199 + 0.4606 + 0.0054, time: 20.927604]
2023-06-03 19:04:41.318: epoch 39:	0.03010856  	0.08187474  	0.06645507  
2023-06-03 19:04:41.318: Find a better model.
2023-06-03 19:05:02.469: [iter 40 : loss : 0.5802 = 0.1145 + 0.4601 + 0.0055, time: 21.148143]
2023-06-03 19:05:02.751: epoch 40:	0.03000492  	0.08146684  	0.06636077  
2023-06-03 19:05:23.825: [iter 41 : loss : 0.5751 = 0.1098 + 0.4596 + 0.0057, time: 21.065900]
2023-06-03 19:05:24.094: epoch 41:	0.02998272  	0.08137307  	0.06637105  
2023-06-03 19:05:45.062: [iter 42 : loss : 0.5709 = 0.1058 + 0.4592 + 0.0059, time: 20.964648]
2023-06-03 19:05:45.330: epoch 42:	0.02993090  	0.08099122  	0.06636746  
2023-06-03 19:06:06.458: [iter 43 : loss : 0.5669 = 0.1020 + 0.4588 + 0.0060, time: 21.124359]
2023-06-03 19:06:06.729: epoch 43:	0.02999011  	0.08119388  	0.06629021  
2023-06-03 19:06:27.762: [iter 44 : loss : 0.5643 = 0.0997 + 0.4584 + 0.0062, time: 21.024975]
2023-06-03 19:06:28.033: epoch 44:	0.02998272  	0.08080631  	0.06611212  
2023-06-03 19:06:49.218: [iter 45 : loss : 0.5600 = 0.0956 + 0.4580 + 0.0063, time: 21.182359]
2023-06-03 19:06:49.487: epoch 45:	0.02983466  	0.08081092  	0.06599715  
2023-06-03 19:07:10.613: [iter 46 : loss : 0.5570 = 0.0928 + 0.4577 + 0.0065, time: 21.121527]
2023-06-03 19:07:10.884: epoch 46:	0.02975322  	0.08029395  	0.06575084  
2023-06-03 19:07:31.788: [iter 47 : loss : 0.5539 = 0.0899 + 0.4574 + 0.0066, time: 20.900155]
2023-06-03 19:07:32.059: epoch 47:	0.02970140  	0.07995471  	0.06569615  
2023-06-03 19:07:53.010: [iter 48 : loss : 0.5515 = 0.0877 + 0.4571 + 0.0068, time: 20.947134]
2023-06-03 19:07:53.281: epoch 48:	0.02956814  	0.07951244  	0.06538830  
2023-06-03 19:08:14.361: [iter 49 : loss : 0.5487 = 0.0851 + 0.4567 + 0.0069, time: 21.077178]
2023-06-03 19:08:14.628: epoch 49:	0.02950891  	0.07897729  	0.06524237  
2023-06-03 19:08:35.560: [iter 50 : loss : 0.5463 = 0.0828 + 0.4565 + 0.0070, time: 20.929104]
2023-06-03 19:08:35.838: epoch 50:	0.02947190  	0.07903662  	0.06515472  
2023-06-03 19:08:56.977: [iter 51 : loss : 0.5434 = 0.0800 + 0.4561 + 0.0072, time: 21.136006]
2023-06-03 19:08:57.247: epoch 51:	0.02939786  	0.07881484  	0.06497925  
2023-06-03 19:09:18.532: [iter 52 : loss : 0.5409 = 0.0777 + 0.4560 + 0.0073, time: 21.282050]
2023-06-03 19:09:18.810: epoch 52:	0.02936084  	0.07858507  	0.06482802  
2023-06-03 19:09:39.930: [iter 53 : loss : 0.5401 = 0.0769 + 0.4558 + 0.0074, time: 21.116013]
2023-06-03 19:09:40.203: epoch 53:	0.02950150  	0.07892503  	0.06486598  
2023-06-03 19:10:01.537: [iter 54 : loss : 0.5380 = 0.0750 + 0.4554 + 0.0076, time: 21.330207]
2023-06-03 19:10:01.814: epoch 54:	0.02944968  	0.07858160  	0.06479605  
2023-06-03 19:10:23.118: [iter 55 : loss : 0.5356 = 0.0727 + 0.4552 + 0.0077, time: 21.300125]
2023-06-03 19:10:23.431: epoch 55:	0.02932382  	0.07820971  	0.06447525  
2023-06-03 19:10:45.398: [iter 56 : loss : 0.5337 = 0.0708 + 0.4550 + 0.0078, time: 21.962999]
2023-06-03 19:10:45.697: epoch 56:	0.02930162  	0.07811639  	0.06435926  
2023-06-03 19:10:54.583: my pid: 2052
2023-06-03 19:10:54.583: model: model.general_recommender.SGL
2023-06-03 19:10:54.583: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 19:10:54.583: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 19:10:58.958: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 19:11:20.830: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 21.870681]
2023-06-03 19:11:21.096: epoch 1:	0.00125111  	0.00247722  	0.00205225  
2023-06-03 19:11:21.096: Find a better model.
2023-06-03 19:11:41.948: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 20.848636]
2023-06-03 19:11:42.242: epoch 2:	0.00168789  	0.00378389  	0.00315659  
2023-06-03 19:11:42.242: Find a better model.
2023-06-03 19:12:03.118: [iter 3 : loss : 1.1341 = 0.6929 + 0.4412 + 0.0000, time: 20.872078]
2023-06-03 19:12:03.411: epoch 3:	0.00186556  	0.00432073  	0.00327114  
2023-06-03 19:12:03.411: Find a better model.
2023-06-03 19:12:24.170: [iter 4 : loss : 1.1343 = 0.6928 + 0.4415 + 0.0000, time: 20.754990]
2023-06-03 19:12:24.468: epoch 4:	0.00234676  	0.00479000  	0.00384169  
2023-06-03 19:12:24.468: Find a better model.
2023-06-03 19:12:45.130: [iter 5 : loss : 1.1344 = 0.6927 + 0.4418 + 0.0000, time: 20.658140]
2023-06-03 19:12:45.421: epoch 5:	0.00284276  	0.00605042  	0.00474088  
2023-06-03 19:12:45.421: Find a better model.
2023-06-03 19:13:06.148: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.723462]
2023-06-03 19:13:06.443: epoch 6:	0.00338318  	0.00700927  	0.00595354  
2023-06-03 19:13:06.443: Find a better model.
2023-06-03 19:13:27.125: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.678907]
2023-06-03 19:13:27.416: epoch 7:	0.00407166  	0.00968357  	0.00757574  
2023-06-03 19:13:27.416: Find a better model.
2023-06-03 19:13:48.112: [iter 8 : loss : 1.1348 = 0.6919 + 0.4428 + 0.0000, time: 20.691884]
2023-06-03 19:13:48.400: epoch 8:	0.00466390  	0.01199285  	0.00957031  
2023-06-03 19:13:48.400: Find a better model.
2023-06-03 19:14:09.084: [iter 9 : loss : 1.1347 = 0.6915 + 0.4432 + 0.0000, time: 20.681085]
2023-06-03 19:14:09.371: epoch 9:	0.00541159  	0.01427385  	0.01157928  
2023-06-03 19:14:09.371: Find a better model.
2023-06-03 19:14:30.139: [iter 10 : loss : 1.1344 = 0.6907 + 0.4437 + 0.0000, time: 20.765097]
2023-06-03 19:14:30.426: epoch 10:	0.00589278  	0.01632822  	0.01247539  
2023-06-03 19:14:30.426: Find a better model.
2023-06-03 19:14:51.079: [iter 11 : loss : 1.1337 = 0.6894 + 0.4443 + 0.0000, time: 20.648420]
2023-06-03 19:14:51.360: epoch 11:	0.00681815  	0.01969219  	0.01528492  
2023-06-03 19:14:51.360: Find a better model.
2023-06-03 19:15:11.908: [iter 12 : loss : 1.1324 = 0.6875 + 0.4449 + 0.0000, time: 20.545363]
2023-06-03 19:15:12.191: epoch 12:	0.00860227  	0.02477727  	0.01988442  
2023-06-03 19:15:12.191: Find a better model.
2023-06-03 19:15:32.665: [iter 13 : loss : 1.1304 = 0.6847 + 0.4456 + 0.0001, time: 20.469931]
2023-06-03 19:15:32.951: epoch 13:	0.01068994  	0.03157759  	0.02555854  
2023-06-03 19:15:32.951: Find a better model.
2023-06-03 19:15:53.475: [iter 14 : loss : 1.1266 = 0.6801 + 0.4463 + 0.0001, time: 20.519134]
2023-06-03 19:15:53.769: epoch 14:	0.01385847  	0.03958497  	0.03271140  
2023-06-03 19:15:53.769: Find a better model.
2023-06-03 19:16:14.261: [iter 15 : loss : 1.1197 = 0.6723 + 0.4473 + 0.0001, time: 20.487087]
2023-06-03 19:16:14.540: epoch 15:	0.01692341  	0.04697929  	0.03993493  
2023-06-03 19:16:14.540: Find a better model.
2023-06-03 19:16:35.634: [iter 16 : loss : 1.1070 = 0.6587 + 0.4481 + 0.0002, time: 21.089597]
2023-06-03 19:16:35.918: epoch 16:	0.02066204  	0.05644321  	0.04796189  
2023-06-03 19:16:35.918: Find a better model.
2023-06-03 19:16:56.999: [iter 17 : loss : 1.0866 = 0.6368 + 0.4495 + 0.0003, time: 21.077434]
2023-06-03 19:16:57.278: epoch 17:	0.02323836  	0.06301881  	0.05372354  
2023-06-03 19:16:57.278: Find a better model.
2023-06-03 19:17:18.449: [iter 18 : loss : 1.0553 = 0.6035 + 0.4514 + 0.0004, time: 21.166435]
2023-06-03 19:17:18.725: epoch 18:	0.02562961  	0.06932240  	0.05844679  
2023-06-03 19:17:18.725: Find a better model.
2023-06-03 19:17:39.791: [iter 19 : loss : 1.0144 = 0.5600 + 0.4538 + 0.0006, time: 21.062069]
2023-06-03 19:17:40.069: epoch 19:	0.02728054  	0.07406927  	0.06179293  
2023-06-03 19:17:40.069: Find a better model.
2023-06-03 19:18:01.218: [iter 20 : loss : 0.9665 = 0.5088 + 0.4568 + 0.0009, time: 21.144873]
2023-06-03 19:18:01.502: epoch 20:	0.02845765  	0.07752444  	0.06410614  
2023-06-03 19:18:01.502: Find a better model.
2023-06-03 19:18:22.598: [iter 21 : loss : 0.9169 = 0.4557 + 0.4601 + 0.0011, time: 21.092709]
2023-06-03 19:18:22.878: epoch 21:	0.02896108  	0.07928316  	0.06487911  
2023-06-03 19:18:22.878: Find a better model.
2023-06-03 19:18:43.976: [iter 22 : loss : 0.8691 = 0.4046 + 0.4630 + 0.0014, time: 21.093944]
2023-06-03 19:18:44.258: epoch 22:	0.02940528  	0.08026599  	0.06558540  
2023-06-03 19:18:44.258: Find a better model.
2023-06-03 19:19:05.364: [iter 23 : loss : 0.8268 = 0.3599 + 0.4651 + 0.0017, time: 21.101488]
2023-06-03 19:19:05.641: epoch 23:	0.02961998  	0.08131760  	0.06603507  
2023-06-03 19:19:05.641: Find a better model.
2023-06-03 19:19:26.760: [iter 24 : loss : 0.7906 = 0.3218 + 0.4667 + 0.0020, time: 21.114041]
2023-06-03 19:19:27.029: epoch 24:	0.02964959  	0.08204899  	0.06632143  
2023-06-03 19:19:27.029: Find a better model.
2023-06-03 19:19:47.967: [iter 25 : loss : 0.7593 = 0.2895 + 0.4675 + 0.0023, time: 20.934389]
2023-06-03 19:19:48.238: epoch 25:	0.02992351  	0.08283177  	0.06675274  
2023-06-03 19:19:48.238: Find a better model.
2023-06-03 19:20:09.182: [iter 26 : loss : 0.7334 = 0.2631 + 0.4677 + 0.0026, time: 20.940314]
2023-06-03 19:20:09.457: epoch 26:	0.03012339  	0.08345950  	0.06704055  
2023-06-03 19:20:09.457: Find a better model.
2023-06-03 19:20:30.518: [iter 27 : loss : 0.7111 = 0.2407 + 0.4675 + 0.0029, time: 21.056980]
2023-06-03 19:20:30.796: epoch 27:	0.03011599  	0.08372698  	0.06713815  
2023-06-03 19:20:30.796: Find a better model.
2023-06-03 19:20:51.740: [iter 28 : loss : 0.6922 = 0.2219 + 0.4672 + 0.0031, time: 20.940319]
2023-06-03 19:20:52.012: epoch 28:	0.03016041  	0.08373278  	0.06727297  
2023-06-03 19:20:52.012: Find a better model.
2023-06-03 19:21:13.128: [iter 29 : loss : 0.6759 = 0.2060 + 0.4665 + 0.0034, time: 21.112083]
2023-06-03 19:21:13.398: epoch 29:	0.03023444  	0.08394111  	0.06728974  
2023-06-03 19:21:13.398: Find a better model.
2023-06-03 19:21:34.518: [iter 30 : loss : 0.6610 = 0.1913 + 0.4661 + 0.0036, time: 21.117180]
2023-06-03 19:21:34.796: epoch 30:	0.03037510  	0.08475994  	0.06763605  
2023-06-03 19:21:34.796: Find a better model.
2023-06-03 19:21:55.897: [iter 31 : loss : 0.6483 = 0.1791 + 0.4653 + 0.0038, time: 21.096419]
2023-06-03 19:21:56.166: epoch 31:	0.03040471  	0.08440814  	0.06763277  
2023-06-03 19:22:17.346: [iter 32 : loss : 0.6374 = 0.1687 + 0.4647 + 0.0040, time: 21.175391]
2023-06-03 19:22:17.614: epoch 32:	0.03048613  	0.08463182  	0.06778897  
2023-06-03 19:22:38.674: [iter 33 : loss : 0.6278 = 0.1596 + 0.4640 + 0.0042, time: 21.056063]
2023-06-03 19:22:38.936: epoch 33:	0.03053796  	0.08487499  	0.06812701  
2023-06-03 19:22:38.936: Find a better model.
2023-06-03 19:22:59.898: [iter 34 : loss : 0.6191 = 0.1513 + 0.4634 + 0.0044, time: 20.957511]
2023-06-03 19:23:00.170: epoch 34:	0.03047132  	0.08457816  	0.06791591  
2023-06-03 19:23:21.493: [iter 35 : loss : 0.6101 = 0.1429 + 0.4626 + 0.0046, time: 21.318197]
2023-06-03 19:23:21.778: epoch 35:	0.03039730  	0.08445871  	0.06790929  
2023-06-03 19:23:42.850: [iter 36 : loss : 0.6034 = 0.1364 + 0.4622 + 0.0048, time: 21.068197]
2023-06-03 19:23:43.115: epoch 36:	0.03049355  	0.08466523  	0.06805920  
2023-06-03 19:24:04.055: [iter 37 : loss : 0.5974 = 0.1307 + 0.4617 + 0.0050, time: 20.935031]
2023-06-03 19:24:04.323: epoch 37:	0.03041212  	0.08438407  	0.06787714  
2023-06-03 19:24:25.441: [iter 38 : loss : 0.5906 = 0.1244 + 0.4609 + 0.0052, time: 21.114057]
2023-06-03 19:24:25.715: epoch 38:	0.03034548  	0.08412577  	0.06794237  
2023-06-03 19:24:46.429: [iter 39 : loss : 0.5856 = 0.1197 + 0.4605 + 0.0054, time: 20.710650]
2023-06-03 19:24:46.694: epoch 39:	0.03040470  	0.08417667  	0.06795299  
2023-06-03 19:25:08.131: [iter 40 : loss : 0.5800 = 0.1145 + 0.4600 + 0.0055, time: 21.432717]
2023-06-03 19:25:08.399: epoch 40:	0.03050093  	0.08450413  	0.06804847  
2023-06-03 19:25:32.127: my pid: 12996
2023-06-03 19:25:32.127: model: model.general_recommender.SGL
2023-06-03 19:25:32.127: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 19:25:32.127: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 19:25:36.407: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 19:25:58.603: [iter 1 : loss : 1.1344 = 0.6931 + 0.4413 + 0.0000, time: 22.196599]
2023-06-03 19:25:58.877: epoch 1:	0.00146580  	0.00305013  	0.00238767  
2023-06-03 19:25:58.878: Find a better model.
2023-06-03 19:26:20.020: [iter 2 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 21.138770]
2023-06-03 19:26:20.312: epoch 2:	0.00165087  	0.00300768  	0.00260583  
2023-06-03 19:26:41.225: [iter 3 : loss : 1.1341 = 0.6929 + 0.4411 + 0.0000, time: 20.909353]
2023-06-03 19:26:41.517: epoch 3:	0.00204324  	0.00359939  	0.00307318  
2023-06-03 19:26:41.517: Find a better model.
2023-06-03 19:27:02.508: [iter 4 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 20.986867]
2023-06-03 19:27:02.813: epoch 4:	0.00232455  	0.00508259  	0.00407583  
2023-06-03 19:27:02.813: Find a better model.
2023-06-03 19:27:23.573: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.755924]
2023-06-03 19:27:23.875: epoch 5:	0.00288718  	0.00652283  	0.00503758  
2023-06-03 19:27:23.875: Find a better model.
2023-06-03 19:27:44.653: [iter 6 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 20.774694]
2023-06-03 19:27:44.954: epoch 6:	0.00360527  	0.00839662  	0.00653703  
2023-06-03 19:27:44.954: Find a better model.
2023-06-03 19:28:05.400: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 20.443214]
2023-06-03 19:28:05.690: epoch 7:	0.00425674  	0.01087602  	0.00874199  
2023-06-03 19:28:05.690: Find a better model.
2023-06-03 19:28:26.155: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.462059]
2023-06-03 19:28:26.443: epoch 8:	0.00514509  	0.01359256  	0.01089599  
2023-06-03 19:28:26.443: Find a better model.
2023-06-03 19:28:46.953: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.505551]
2023-06-03 19:28:47.240: epoch 9:	0.00587058  	0.01670792  	0.01328709  
2023-06-03 19:28:47.240: Find a better model.
2023-06-03 19:29:07.794: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 20.550129]
2023-06-03 19:29:08.078: epoch 10:	0.00609267  	0.01687275  	0.01390026  
2023-06-03 19:29:08.078: Find a better model.
2023-06-03 19:29:28.641: [iter 11 : loss : 1.1338 = 0.6896 + 0.4442 + 0.0000, time: 20.558943]
2023-06-03 19:29:28.931: epoch 11:	0.00723272  	0.02023965  	0.01624602  
2023-06-03 19:29:28.931: Find a better model.
2023-06-03 19:29:49.523: [iter 12 : loss : 1.1326 = 0.6876 + 0.4449 + 0.0000, time: 20.586931]
2023-06-03 19:29:49.816: epoch 12:	0.00897982  	0.02470885  	0.02075714  
2023-06-03 19:29:49.816: Find a better model.
2023-06-03 19:30:10.323: [iter 13 : loss : 1.1305 = 0.6849 + 0.4456 + 0.0001, time: 20.503325]
2023-06-03 19:30:10.605: epoch 13:	0.01113411  	0.03091511  	0.02606416  
2023-06-03 19:30:10.605: Find a better model.
2023-06-03 19:30:31.168: [iter 14 : loss : 1.1268 = 0.6804 + 0.4463 + 0.0001, time: 20.559446]
2023-06-03 19:30:31.444: epoch 14:	0.01437669  	0.03857692  	0.03341741  
2023-06-03 19:30:31.444: Find a better model.
2023-06-03 19:30:52.154: [iter 15 : loss : 1.1200 = 0.6727 + 0.4472 + 0.0001, time: 20.704115]
2023-06-03 19:30:52.433: epoch 15:	0.01796723  	0.04826491  	0.04113805  
2023-06-03 19:30:52.433: Find a better model.
2023-06-03 19:31:13.531: [iter 16 : loss : 1.1076 = 0.6593 + 0.4481 + 0.0002, time: 21.095110]
2023-06-03 19:31:13.817: epoch 16:	0.02133572  	0.05715797  	0.04837108  
2023-06-03 19:31:13.817: Find a better model.
2023-06-03 19:31:34.875: [iter 17 : loss : 1.0873 = 0.6377 + 0.4494 + 0.0003, time: 21.054077]
2023-06-03 19:31:35.154: epoch 17:	0.02420079  	0.06496228  	0.05483730  
2023-06-03 19:31:35.154: Find a better model.
2023-06-03 19:31:56.323: [iter 18 : loss : 1.0563 = 0.6046 + 0.4513 + 0.0004, time: 21.165133]
2023-06-03 19:31:56.602: epoch 18:	0.02632551  	0.07088365  	0.05933413  
2023-06-03 19:31:56.602: Find a better model.
2023-06-03 19:32:17.703: [iter 19 : loss : 1.0154 = 0.5611 + 0.4537 + 0.0006, time: 21.097144]
2023-06-03 19:32:17.983: epoch 19:	0.02745821  	0.07393150  	0.06235560  
2023-06-03 19:32:17.983: Find a better model.
2023-06-03 19:32:39.245: [iter 20 : loss : 0.9676 = 0.5099 + 0.4569 + 0.0008, time: 21.258077]
2023-06-03 19:32:39.530: epoch 20:	0.02838362  	0.07659568  	0.06410180  
2023-06-03 19:32:39.530: Find a better model.
2023-06-03 19:33:00.441: [iter 21 : loss : 0.9178 = 0.4565 + 0.4602 + 0.0011, time: 20.906751]
2023-06-03 19:33:00.706: epoch 21:	0.02898328  	0.07783792  	0.06514879  
2023-06-03 19:33:00.707: Find a better model.
2023-06-03 19:33:21.682: [iter 22 : loss : 0.8697 = 0.4050 + 0.4633 + 0.0014, time: 20.972046]
2023-06-03 19:33:21.960: epoch 22:	0.02922020  	0.07884058  	0.06552358  
2023-06-03 19:33:21.960: Find a better model.
2023-06-03 19:33:43.073: [iter 23 : loss : 0.8276 = 0.3604 + 0.4655 + 0.0017, time: 21.109094]
2023-06-03 19:33:43.349: epoch 23:	0.02927942  	0.07932019  	0.06590520  
2023-06-03 19:33:43.349: Find a better model.
2023-06-03 19:34:04.413: [iter 24 : loss : 0.7912 = 0.3223 + 0.4670 + 0.0020, time: 21.061039]
2023-06-03 19:34:04.690: epoch 24:	0.02941268  	0.08020300  	0.06604805  
2023-06-03 19:34:04.690: Find a better model.
2023-06-03 19:34:25.617: [iter 25 : loss : 0.7599 = 0.2899 + 0.4677 + 0.0023, time: 20.923613]
2023-06-03 19:34:25.891: epoch 25:	0.02950152  	0.08052044  	0.06643915  
2023-06-03 19:34:25.891: Find a better model.
2023-06-03 19:34:47.051: [iter 26 : loss : 0.7340 = 0.2636 + 0.4678 + 0.0026, time: 21.155480]
2023-06-03 19:34:47.323: epoch 26:	0.02969400  	0.08090246  	0.06663731  
2023-06-03 19:34:47.323: Find a better model.
2023-06-03 19:35:08.461: [iter 27 : loss : 0.7115 = 0.2409 + 0.4677 + 0.0029, time: 21.133489]
2023-06-03 19:35:08.741: epoch 27:	0.02989390  	0.08119068  	0.06675201  
2023-06-03 19:35:08.741: Find a better model.
2023-06-03 19:35:30.009: [iter 28 : loss : 0.6927 = 0.2223 + 0.4672 + 0.0031, time: 21.258438]
2023-06-03 19:35:30.277: epoch 28:	0.02983466  	0.08129267  	0.06676494  
2023-06-03 19:35:30.277: Find a better model.
2023-06-03 19:35:51.412: [iter 29 : loss : 0.6763 = 0.2063 + 0.4667 + 0.0034, time: 21.132159]
2023-06-03 19:35:51.687: epoch 29:	0.02982725  	0.08181427  	0.06721336  
2023-06-03 19:35:51.687: Find a better model.
2023-06-03 19:36:12.640: [iter 30 : loss : 0.6617 = 0.1920 + 0.4661 + 0.0036, time: 20.947574]
2023-06-03 19:36:12.918: epoch 30:	0.02983466  	0.08165047  	0.06719224  
2023-06-03 19:36:33.992: [iter 31 : loss : 0.6490 = 0.1797 + 0.4654 + 0.0038, time: 21.070165]
2023-06-03 19:36:34.264: epoch 31:	0.02981246  	0.08204712  	0.06717334  
2023-06-03 19:36:34.264: Find a better model.
2023-06-03 19:36:55.385: [iter 32 : loss : 0.6379 = 0.1691 + 0.4648 + 0.0040, time: 21.116874]
2023-06-03 19:36:55.654: epoch 32:	0.02989390  	0.08237404  	0.06741903  
2023-06-03 19:36:55.655: Find a better model.
2023-06-03 19:37:16.590: [iter 33 : loss : 0.6286 = 0.1602 + 0.4641 + 0.0042, time: 20.931146]
2023-06-03 19:37:16.861: epoch 33:	0.02993091  	0.08215328  	0.06725670  
2023-06-03 19:37:38.017: [iter 34 : loss : 0.6198 = 0.1519 + 0.4635 + 0.0044, time: 21.151761]
2023-06-03 19:37:38.285: epoch 34:	0.02992351  	0.08238247  	0.06735475  
2023-06-03 19:37:38.285: Find a better model.
2023-06-03 19:37:59.400: [iter 35 : loss : 0.6108 = 0.1434 + 0.4627 + 0.0046, time: 21.110620]
2023-06-03 19:37:59.674: epoch 35:	0.02992350  	0.08260395  	0.06741497  
2023-06-03 19:37:59.674: Find a better model.
2023-06-03 19:38:20.717: [iter 36 : loss : 0.6037 = 0.1367 + 0.4621 + 0.0048, time: 21.038563]
2023-06-03 19:38:20.987: epoch 36:	0.03007156  	0.08283360  	0.06754989  
2023-06-03 19:38:20.987: Find a better model.
2023-06-03 19:38:42.114: [iter 37 : loss : 0.5978 = 0.1310 + 0.4617 + 0.0050, time: 21.123575]
2023-06-03 19:38:42.383: epoch 37:	0.03010859  	0.08271944  	0.06774892  
2023-06-03 19:39:03.770: [iter 38 : loss : 0.5914 = 0.1251 + 0.4611 + 0.0052, time: 21.383878]
2023-06-03 19:39:04.037: epoch 38:	0.03012339  	0.08243845  	0.06784680  
2023-06-03 19:39:25.163: [iter 39 : loss : 0.5862 = 0.1203 + 0.4605 + 0.0054, time: 21.123290]
2023-06-03 19:39:25.437: epoch 39:	0.03017522  	0.08294825  	0.06795052  
2023-06-03 19:39:25.438: Find a better model.
2023-06-03 19:39:46.711: [iter 40 : loss : 0.5807 = 0.1151 + 0.4601 + 0.0055, time: 21.270093]
2023-06-03 19:39:46.980: epoch 40:	0.03019743  	0.08252411  	0.06787971  
2023-06-03 19:40:08.127: [iter 41 : loss : 0.5758 = 0.1105 + 0.4596 + 0.0057, time: 21.143280]
2023-06-03 19:40:08.398: epoch 41:	0.03017522  	0.08246139  	0.06790933  
2023-06-03 19:40:29.535: [iter 42 : loss : 0.5713 = 0.1063 + 0.4591 + 0.0059, time: 21.133045]
2023-06-03 19:40:29.812: epoch 42:	0.03019002  	0.08261779  	0.06797846  
2023-06-03 19:40:50.970: [iter 43 : loss : 0.5674 = 0.1026 + 0.4587 + 0.0060, time: 21.155045]
2023-06-03 19:40:51.240: epoch 43:	0.03013819  	0.08217924  	0.06765234  
2023-06-03 19:41:12.486: [iter 44 : loss : 0.5645 = 0.1000 + 0.4584 + 0.0062, time: 21.242902]
2023-06-03 19:41:12.769: epoch 44:	0.03008638  	0.08216849  	0.06758917  
2023-06-03 19:41:33.703: [iter 45 : loss : 0.5605 = 0.0961 + 0.4580 + 0.0063, time: 20.930290]
2023-06-03 19:41:33.971: epoch 45:	0.03002715  	0.08189837  	0.06739120  
2023-06-03 19:41:55.108: [iter 46 : loss : 0.5575 = 0.0933 + 0.4577 + 0.0065, time: 21.132050]
2023-06-03 19:41:55.375: epoch 46:	0.03004197  	0.08177731  	0.06725814  
2023-06-03 19:42:16.535: [iter 47 : loss : 0.5540 = 0.0901 + 0.4573 + 0.0066, time: 21.157290]
2023-06-03 19:42:16.811: epoch 47:	0.02995313  	0.08165980  	0.06723168  
2023-06-03 19:42:37.681: [iter 48 : loss : 0.5515 = 0.0877 + 0.4570 + 0.0068, time: 20.865974]
2023-06-03 19:42:37.953: epoch 48:	0.02999014  	0.08182544  	0.06740137  
2023-06-03 19:42:58.886: [iter 49 : loss : 0.5487 = 0.0851 + 0.4567 + 0.0069, time: 20.929583]
2023-06-03 19:42:59.156: epoch 49:	0.02981987  	0.08120918  	0.06713542  
2023-06-03 19:43:20.281: [iter 50 : loss : 0.5465 = 0.0831 + 0.4564 + 0.0070, time: 21.120492]
2023-06-03 19:43:20.548: epoch 50:	0.02981987  	0.08085834  	0.06690076  
2023-06-03 19:43:41.462: [iter 51 : loss : 0.5437 = 0.0804 + 0.4561 + 0.0072, time: 20.910187]
2023-06-03 19:43:41.734: epoch 51:	0.02968661  	0.08038125  	0.06657106  
2023-06-03 19:44:02.644: [iter 52 : loss : 0.5413 = 0.0781 + 0.4559 + 0.0073, time: 20.902129]
2023-06-03 19:44:02.921: epoch 52:	0.02970141  	0.08011787  	0.06652814  
2023-06-03 19:44:24.039: [iter 53 : loss : 0.5404 = 0.0773 + 0.4557 + 0.0074, time: 21.115152]
2023-06-03 19:44:24.311: epoch 53:	0.02967920  	0.07994796  	0.06633075  
2023-06-03 19:44:45.261: [iter 54 : loss : 0.5381 = 0.0751 + 0.4554 + 0.0076, time: 20.946554]
2023-06-03 19:44:45.531: epoch 54:	0.02976803  	0.07989730  	0.06640986  
2023-06-03 19:45:06.675: [iter 55 : loss : 0.5360 = 0.0730 + 0.4552 + 0.0077, time: 21.140719]
2023-06-03 19:45:06.954: epoch 55:	0.02957555  	0.07911346  	0.06602029  
2023-06-03 19:45:28.168: [iter 56 : loss : 0.5338 = 0.0711 + 0.4549 + 0.0078, time: 21.211472]
2023-06-03 19:45:28.435: epoch 56:	0.02958294  	0.07900444  	0.06592343  
2023-06-03 19:45:49.392: [iter 57 : loss : 0.5319 = 0.0692 + 0.4548 + 0.0079, time: 20.952256]
2023-06-03 19:45:49.660: epoch 57:	0.02950891  	0.07892431  	0.06600019  
2023-06-03 19:46:10.598: [iter 58 : loss : 0.5309 = 0.0681 + 0.4547 + 0.0080, time: 20.935044]
2023-06-03 19:46:10.867: epoch 58:	0.02946449  	0.07869282  	0.06581724  
2023-06-03 19:46:31.826: [iter 59 : loss : 0.5292 = 0.0666 + 0.4545 + 0.0082, time: 20.954533]
2023-06-03 19:46:32.096: epoch 59:	0.02939785  	0.07821900  	0.06552286  
2023-06-03 19:46:52.968: [iter 60 : loss : 0.5276 = 0.0651 + 0.4543 + 0.0083, time: 20.868949]
2023-06-03 19:46:53.237: epoch 60:	0.02936084  	0.07800592  	0.06549026  
2023-06-03 19:47:14.358: [iter 61 : loss : 0.5263 = 0.0638 + 0.4541 + 0.0084, time: 21.118240]
2023-06-03 19:47:14.626: epoch 61:	0.02930161  	0.07757020  	0.06531586  
2023-06-03 19:47:35.617: [iter 62 : loss : 0.5256 = 0.0632 + 0.4540 + 0.0085, time: 20.987583]
2023-06-03 19:47:35.891: epoch 62:	0.02927941  	0.07775398  	0.06524162  
2023-06-03 19:47:56.808: [iter 63 : loss : 0.5243 = 0.0619 + 0.4538 + 0.0086, time: 20.914101]
2023-06-03 19:47:57.078: epoch 63:	0.02923498  	0.07750703  	0.06503596  
2023-06-03 19:48:18.145: [iter 64 : loss : 0.5228 = 0.0604 + 0.4536 + 0.0087, time: 21.063524]
2023-06-03 19:48:18.419: epoch 64:	0.02917575  	0.07726789  	0.06494868  
2023-06-03 19:48:18.419: Early stopping is trigger at epoch: 64
2023-06-03 19:48:18.419: best_result@epoch 39:

2023-06-03 19:48:18.419: 		0.0302      	0.0829      	0.0680      
2023-06-03 19:49:22.421: my pid: 12460
2023-06-03 19:49:22.421: model: model.general_recommender.SGL
2023-06-03 19:49:22.421: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-03 19:49:22.421: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-03 19:49:26.448: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-03 19:49:47.384: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.935179]
2023-06-03 19:49:47.662: epoch 1:	0.00133254  	0.00246236  	0.00203011  
2023-06-03 19:49:47.662: Find a better model.
2023-06-03 19:50:08.963: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.297653]
2023-06-03 19:50:09.264: epoch 2:	0.00149541  	0.00344904  	0.00255212  
2023-06-03 19:50:09.264: Find a better model.
2023-06-03 19:50:30.523: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.256256]
2023-06-03 19:50:30.837: epoch 3:	0.00207285  	0.00460897  	0.00341093  
2023-06-03 19:50:30.837: Find a better model.
2023-06-03 19:50:52.167: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.326151]
2023-06-03 19:50:52.468: epoch 4:	0.00243559  	0.00550925  	0.00407362  
2023-06-03 19:50:52.468: Find a better model.
2023-06-03 19:51:13.744: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.272629]
2023-06-03 19:51:14.052: epoch 5:	0.00286497  	0.00627007  	0.00480228  
2023-06-03 19:51:14.052: Find a better model.
2023-06-03 19:51:35.344: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 21.287085]
2023-06-03 19:51:35.644: epoch 6:	0.00336838  	0.00795605  	0.00598359  
2023-06-03 19:51:35.644: Find a better model.
2023-06-03 19:51:57.122: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.474311]
2023-06-03 19:51:57.422: epoch 7:	0.00432336  	0.01079184  	0.00843039  
2023-06-03 19:51:57.422: Find a better model.
2023-06-03 19:52:18.569: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 21.143624]
2023-06-03 19:52:18.878: epoch 8:	0.00509327  	0.01276874  	0.01023484  
2023-06-03 19:52:18.879: Find a better model.
2023-06-03 19:52:40.096: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 21.214028]
2023-06-03 19:52:40.397: epoch 9:	0.00585577  	0.01458428  	0.01233231  
2023-06-03 19:52:40.397: Find a better model.
2023-06-03 19:53:01.508: [iter 10 : loss : 1.1345 = 0.6909 + 0.4436 + 0.0000, time: 21.108097]
2023-06-03 19:53:01.804: epoch 10:	0.00688478  	0.01861026  	0.01524156  
2023-06-03 19:53:01.804: Find a better model.
2023-06-03 19:53:22.897: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 21.087626]
2023-06-03 19:53:23.196: epoch 11:	0.00842459  	0.02308074  	0.01907433  
2023-06-03 19:53:23.196: Find a better model.
2023-06-03 19:53:44.298: [iter 12 : loss : 1.1329 = 0.6881 + 0.4448 + 0.0000, time: 21.098252]
2023-06-03 19:53:44.595: epoch 12:	0.00936477  	0.02646174  	0.02212179  
2023-06-03 19:53:44.595: Find a better model.
2023-06-03 19:54:05.878: [iter 13 : loss : 1.1308 = 0.6853 + 0.4454 + 0.0001, time: 21.280266]
2023-06-03 19:54:06.170: epoch 13:	0.01163012  	0.03285191  	0.02784305  
2023-06-03 19:54:06.170: Find a better model.
2023-06-03 19:54:27.278: [iter 14 : loss : 1.1271 = 0.6809 + 0.4461 + 0.0001, time: 21.103283]
2023-06-03 19:54:27.575: epoch 14:	0.01452476  	0.04013906  	0.03459727  
2023-06-03 19:54:27.575: Find a better model.
2023-06-03 19:54:48.484: [iter 15 : loss : 1.1207 = 0.6735 + 0.4470 + 0.0001, time: 20.905597]
2023-06-03 19:54:48.775: epoch 15:	0.01764890  	0.04826979  	0.04150040  
2023-06-03 19:54:48.775: Find a better model.
2023-06-03 19:55:10.454: [iter 16 : loss : 1.1088 = 0.6608 + 0.4478 + 0.0002, time: 21.674792]
2023-06-03 19:55:10.739: epoch 16:	0.02095814  	0.05640918  	0.04901357  
2023-06-03 19:55:10.739: Find a better model.
2023-06-03 19:55:32.297: [iter 17 : loss : 1.0894 = 0.6400 + 0.4491 + 0.0003, time: 21.553607]
2023-06-03 19:55:32.586: epoch 17:	0.02387504  	0.06402837  	0.05539097  
2023-06-03 19:55:32.586: Find a better model.
2023-06-03 19:55:54.071: [iter 18 : loss : 1.0592 = 0.6079 + 0.4509 + 0.0004, time: 21.481930]
2023-06-03 19:55:54.363: epoch 18:	0.02614783  	0.07050516  	0.06001712  
2023-06-03 19:55:54.363: Find a better model.
2023-06-03 19:56:16.023: [iter 19 : loss : 1.0190 = 0.5652 + 0.4532 + 0.0006, time: 21.655198]
2023-06-03 19:56:16.308: epoch 19:	0.02783578  	0.07607925  	0.06373230  
2023-06-03 19:56:16.308: Find a better model.
2023-06-03 19:56:37.842: [iter 20 : loss : 0.9717 = 0.5146 + 0.4562 + 0.0008, time: 21.529645]
2023-06-03 19:56:38.131: epoch 20:	0.02916095  	0.07914251  	0.06584164  
2023-06-03 19:56:38.131: Find a better model.
2023-06-03 19:56:59.640: [iter 21 : loss : 0.9219 = 0.4613 + 0.4596 + 0.0011, time: 21.506276]
2023-06-03 19:56:59.929: epoch 21:	0.02955332  	0.07991882  	0.06641505  
2023-06-03 19:56:59.929: Find a better model.
2023-06-03 19:57:21.447: [iter 22 : loss : 0.8733 = 0.4093 + 0.4625 + 0.0014, time: 21.513562]
2023-06-03 19:57:21.729: epoch 22:	0.02967920  	0.08101510  	0.06671865  
2023-06-03 19:57:21.729: Find a better model.
2023-06-03 19:57:43.202: [iter 23 : loss : 0.8303 = 0.3637 + 0.4649 + 0.0017, time: 21.468769]
2023-06-03 19:57:43.480: epoch 23:	0.02994571  	0.08185773  	0.06686572  
2023-06-03 19:57:43.481: Find a better model.
2023-06-03 19:58:04.984: [iter 24 : loss : 0.7932 = 0.3249 + 0.4664 + 0.0020, time: 21.499235]
2023-06-03 19:58:05.263: epoch 24:	0.03006416  	0.08242111  	0.06702207  
2023-06-03 19:58:05.264: Find a better model.
2023-06-03 19:58:26.777: [iter 25 : loss : 0.7616 = 0.2921 + 0.4672 + 0.0023, time: 21.509050]
2023-06-03 19:58:27.055: epoch 25:	0.03033066  	0.08335976  	0.06738242  
2023-06-03 19:58:27.055: Find a better model.
2023-06-03 19:58:48.626: [iter 26 : loss : 0.7351 = 0.2651 + 0.4674 + 0.0026, time: 21.567445]
2023-06-03 19:58:48.917: epoch 26:	0.03036028  	0.08348957  	0.06787501  
2023-06-03 19:58:48.917: Find a better model.
2023-06-03 19:59:10.620: [iter 27 : loss : 0.7124 = 0.2422 + 0.4673 + 0.0028, time: 21.698940]
2023-06-03 19:59:10.902: epoch 27:	0.03047133  	0.08380394  	0.06828825  
2023-06-03 19:59:10.902: Find a better model.
2023-06-03 19:59:32.550: [iter 28 : loss : 0.6932 = 0.2231 + 0.4670 + 0.0031, time: 21.643590]
2023-06-03 19:59:32.847: epoch 28:	0.03041210  	0.08374534  	0.06845310  
2023-06-03 19:59:54.377: [iter 29 : loss : 0.6767 = 0.2069 + 0.4664 + 0.0033, time: 21.527285]
2023-06-03 19:59:54.651: epoch 29:	0.03061938  	0.08405983  	0.06883352  
2023-06-03 19:59:54.651: Find a better model.
2023-06-03 20:00:16.344: [iter 30 : loss : 0.6617 = 0.1923 + 0.4658 + 0.0036, time: 21.689538]
2023-06-03 20:00:16.620: epoch 30:	0.03052314  	0.08422689  	0.06879093  
2023-06-03 20:00:16.620: Find a better model.
2023-06-03 20:00:38.332: [iter 31 : loss : 0.6488 = 0.1798 + 0.4651 + 0.0038, time: 21.707421]
2023-06-03 20:00:38.607: epoch 31:	0.03048613  	0.08417530  	0.06895126  
2023-06-03 20:01:00.343: [iter 32 : loss : 0.6378 = 0.1693 + 0.4644 + 0.0040, time: 21.731123]
2023-06-03 20:01:00.603: epoch 32:	0.03055276  	0.08423506  	0.06911172  
2023-06-03 20:01:00.603: Find a better model.
2023-06-03 20:01:22.509: [iter 33 : loss : 0.6283 = 0.1604 + 0.4637 + 0.0042, time: 21.902196]
2023-06-03 20:01:22.788: epoch 33:	0.03059717  	0.08434865  	0.06928100  
2023-06-03 20:01:22.788: Find a better model.
2023-06-03 20:01:44.507: [iter 34 : loss : 0.6193 = 0.1516 + 0.4633 + 0.0044, time: 21.716199]
2023-06-03 20:01:44.781: epoch 34:	0.03067120  	0.08421659  	0.06935919  
2023-06-03 20:02:06.506: [iter 35 : loss : 0.6106 = 0.1434 + 0.4626 + 0.0046, time: 21.720133]
2023-06-03 20:02:06.779: epoch 35:	0.03066380  	0.08445240  	0.06943780  
2023-06-03 20:02:06.779: Find a better model.
2023-06-03 20:02:28.493: [iter 36 : loss : 0.6035 = 0.1368 + 0.4618 + 0.0048, time: 21.710193]
2023-06-03 20:02:28.769: epoch 36:	0.03070821  	0.08439957  	0.06948678  
2023-06-03 20:02:50.510: [iter 37 : loss : 0.5972 = 0.1308 + 0.4614 + 0.0050, time: 21.737240]
2023-06-03 20:02:50.782: epoch 37:	0.03068601  	0.08442967  	0.06943728  
2023-06-03 20:03:12.667: [iter 38 : loss : 0.5908 = 0.1248 + 0.4608 + 0.0052, time: 21.880392]
2023-06-03 20:03:12.952: epoch 38:	0.03069340  	0.08446121  	0.06928805  
2023-06-03 20:03:12.952: Find a better model.
2023-06-03 20:03:34.709: [iter 39 : loss : 0.5856 = 0.1200 + 0.4603 + 0.0054, time: 21.752123]
2023-06-03 20:03:34.991: epoch 39:	0.03072302  	0.08469119  	0.06950776  
2023-06-03 20:03:34.991: Find a better model.
2023-06-03 20:03:56.679: [iter 40 : loss : 0.5804 = 0.1151 + 0.4597 + 0.0055, time: 21.684403]
2023-06-03 20:03:56.958: epoch 40:	0.03073783  	0.08487152  	0.06958567  
2023-06-03 20:03:56.958: Find a better model.
2023-06-03 20:04:18.677: [iter 41 : loss : 0.5753 = 0.1102 + 0.4593 + 0.0057, time: 21.714045]
2023-06-03 20:04:18.953: epoch 41:	0.03067120  	0.08453602  	0.06940594  
2023-06-03 20:04:40.667: [iter 42 : loss : 0.5709 = 0.1062 + 0.4589 + 0.0059, time: 21.710158]
2023-06-03 20:04:40.947: epoch 42:	0.03067861  	0.08440629  	0.06949586  
2023-06-03 20:05:02.628: [iter 43 : loss : 0.5668 = 0.1022 + 0.4585 + 0.0060, time: 21.678458]
2023-06-03 20:05:02.912: epoch 43:	0.03078966  	0.08485176  	0.06961351  
2023-06-03 20:05:24.627: [iter 44 : loss : 0.5639 = 0.0996 + 0.4581 + 0.0062, time: 21.711982]
2023-06-03 20:05:24.908: epoch 44:	0.03078227  	0.08475785  	0.06946269  
2023-06-03 20:05:46.610: [iter 45 : loss : 0.5602 = 0.0961 + 0.4578 + 0.0063, time: 21.698016]
2023-06-03 20:05:46.888: epoch 45:	0.03070823  	0.08469576  	0.06953708  
2023-06-03 20:06:08.603: [iter 46 : loss : 0.5573 = 0.0933 + 0.4575 + 0.0065, time: 21.712225]
2023-06-03 20:06:08.883: epoch 46:	0.03058977  	0.08400172  	0.06927145  
2023-06-03 20:06:30.587: [iter 47 : loss : 0.5536 = 0.0900 + 0.4570 + 0.0066, time: 21.699053]
2023-06-03 20:06:30.867: epoch 47:	0.03047871  	0.08379107  	0.06910837  
2023-06-03 20:06:52.391: [iter 48 : loss : 0.5510 = 0.0874 + 0.4568 + 0.0068, time: 21.521050]
2023-06-03 20:06:52.664: epoch 48:	0.03041950  	0.08365915  	0.06891732  
2023-06-03 20:07:14.362: [iter 49 : loss : 0.5484 = 0.0852 + 0.4564 + 0.0069, time: 21.694370]
2023-06-03 20:07:14.630: epoch 49:	0.03030105  	0.08267933  	0.06866520  
2023-06-03 20:07:35.955: [iter 50 : loss : 0.5464 = 0.0831 + 0.4563 + 0.0070, time: 21.322323]
2023-06-03 20:07:36.223: epoch 50:	0.03015298  	0.08222111  	0.06845871  
2023-06-03 20:07:57.739: [iter 51 : loss : 0.5435 = 0.0803 + 0.4560 + 0.0072, time: 21.511358]
2023-06-03 20:07:58.013: epoch 51:	0.03016038  	0.08182706  	0.06839582  
2023-06-03 20:08:19.534: [iter 52 : loss : 0.5407 = 0.0777 + 0.4557 + 0.0073, time: 21.517492]
2023-06-03 20:08:19.802: epoch 52:	0.03009375  	0.08158661  	0.06827615  
2023-06-03 20:08:41.338: [iter 53 : loss : 0.5398 = 0.0770 + 0.4554 + 0.0074, time: 21.530540]
2023-06-03 20:08:41.606: epoch 53:	0.03002712  	0.08136123  	0.06801485  
2023-06-03 20:09:03.122: [iter 54 : loss : 0.5376 = 0.0749 + 0.4552 + 0.0076, time: 21.511980]
2023-06-03 20:09:03.395: epoch 54:	0.02996049  	0.08103528  	0.06787350  
2023-06-03 20:09:25.135: [iter 55 : loss : 0.5358 = 0.0730 + 0.4551 + 0.0077, time: 21.735998]
2023-06-03 20:09:25.405: epoch 55:	0.02986424  	0.08077592  	0.06755020  
2023-06-03 20:09:47.112: [iter 56 : loss : 0.5336 = 0.0710 + 0.4548 + 0.0078, time: 21.703166]
2023-06-03 20:09:47.380: epoch 56:	0.02991607  	0.08062903  	0.06750144  
2023-06-03 20:10:08.914: [iter 57 : loss : 0.5318 = 0.0692 + 0.4546 + 0.0079, time: 21.530090]
2023-06-03 20:10:09.181: epoch 57:	0.02980502  	0.08023700  	0.06723038  
2023-06-03 20:10:30.509: [iter 58 : loss : 0.5304 = 0.0680 + 0.4544 + 0.0080, time: 21.325055]
2023-06-03 20:10:30.776: epoch 58:	0.02971618  	0.08014404  	0.06707077  
2023-06-03 20:10:51.906: [iter 59 : loss : 0.5284 = 0.0661 + 0.4542 + 0.0082, time: 21.125246]
2023-06-03 20:10:52.173: epoch 59:	0.02976060  	0.08012702  	0.06708319  
2023-06-03 20:11:13.480: [iter 60 : loss : 0.5275 = 0.0651 + 0.4541 + 0.0083, time: 21.303381]
2023-06-03 20:11:13.750: epoch 60:	0.02980502  	0.07997244  	0.06706930  
2023-06-03 20:11:35.278: [iter 61 : loss : 0.5261 = 0.0638 + 0.4539 + 0.0084, time: 21.523509]
2023-06-03 20:11:35.546: epoch 61:	0.02961993  	0.07913443  	0.06668469  
2023-06-03 20:11:56.675: [iter 62 : loss : 0.5251 = 0.0629 + 0.4537 + 0.0085, time: 21.125028]
2023-06-03 20:11:56.947: epoch 62:	0.02952369  	0.07887571  	0.06649323  
2023-06-03 20:12:18.461: [iter 63 : loss : 0.5238 = 0.0615 + 0.4537 + 0.0086, time: 21.510629]
2023-06-03 20:12:18.728: epoch 63:	0.02946447  	0.07854488  	0.06636015  
2023-06-03 20:12:40.255: [iter 64 : loss : 0.5228 = 0.0605 + 0.4536 + 0.0087, time: 21.524032]
2023-06-03 20:12:40.521: epoch 64:	0.02930160  	0.07829228  	0.06628669  
2023-06-03 20:13:02.044: [iter 65 : loss : 0.5213 = 0.0591 + 0.4534 + 0.0088, time: 21.520068]
2023-06-03 20:13:02.312: epoch 65:	0.02928679  	0.07778060  	0.06612551  
2023-06-03 20:13:02.312: Early stopping is trigger at epoch: 65
2023-06-03 20:13:02.312: best_result@epoch 40:

2023-06-03 20:13:02.312: 		0.0307      	0.0849      	0.0696      
2023-06-04 19:32:48.864: my pid: 7924
2023-06-04 19:32:48.864: model: model.general_recommender.SGL
2023-06-04 19:32:48.864: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-04 19:32:48.864: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-04 19:32:52.880: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-04 19:33:14.154: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.272757]
2023-06-04 19:33:14.422: epoch 1:	0.00131774  	0.00271001  	0.00221953  
2023-06-04 19:33:14.422: Find a better model.
2023-06-04 19:33:35.585: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.160384]
2023-06-04 19:33:35.869: epoch 2:	0.00142138  	0.00310625  	0.00259956  
2023-06-04 19:33:35.869: Find a better model.
2023-06-04 19:33:56.986: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.114022]
2023-06-04 19:33:57.285: epoch 3:	0.00203583  	0.00375826  	0.00331528  
2023-06-04 19:33:57.285: Find a better model.
2023-06-04 19:34:18.559: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.270528]
2023-06-04 19:34:18.850: epoch 4:	0.00208025  	0.00460798  	0.00345129  
2023-06-04 19:34:18.850: Find a better model.
2023-06-04 19:34:40.146: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.293020]
2023-06-04 19:34:40.440: epoch 5:	0.00282795  	0.00685647  	0.00512996  
2023-06-04 19:34:40.440: Find a better model.
2023-06-04 19:35:01.726: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.282443]
2023-06-04 19:35:02.013: epoch 6:	0.00331655  	0.00722213  	0.00584395  
2023-06-04 19:35:02.013: Find a better model.
2023-06-04 19:35:23.151: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.134834]
2023-06-04 19:35:23.441: epoch 7:	0.00396062  	0.00925946  	0.00724987  
2023-06-04 19:35:23.441: Find a better model.
2023-06-04 19:35:44.727: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.283202]
2023-06-04 19:35:45.006: epoch 8:	0.00473793  	0.01148590  	0.00928387  
2023-06-04 19:35:45.006: Find a better model.
2023-06-04 19:36:06.097: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.086286]
2023-06-04 19:36:06.387: epoch 9:	0.00569291  	0.01434855  	0.01180723  
2023-06-04 19:36:06.387: Find a better model.
2023-06-04 19:36:27.317: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.927396]
2023-06-04 19:36:27.598: epoch 10:	0.00641839  	0.01677264  	0.01389488  
2023-06-04 19:36:27.598: Find a better model.
2023-06-04 19:36:48.688: [iter 11 : loss : 1.1340 = 0.6901 + 0.4439 + 0.0000, time: 21.084981]
2023-06-04 19:36:48.965: epoch 11:	0.00765469  	0.02075414  	0.01751116  
2023-06-04 19:36:48.965: Find a better model.
2023-06-04 19:37:09.735: [iter 12 : loss : 1.1329 = 0.6884 + 0.4445 + 0.0000, time: 20.767038]
2023-06-04 19:37:10.017: epoch 12:	0.00909826  	0.02473247  	0.02026121  
2023-06-04 19:37:10.017: Find a better model.
2023-06-04 19:37:31.099: [iter 13 : loss : 1.1310 = 0.6858 + 0.4452 + 0.0000, time: 21.077362]
2023-06-04 19:37:31.380: epoch 13:	0.01124516  	0.03043299  	0.02551535  
2023-06-04 19:37:31.380: Find a better model.
2023-06-04 19:37:52.492: [iter 14 : loss : 1.1277 = 0.6817 + 0.4459 + 0.0001, time: 21.109070]
2023-06-04 19:37:52.766: epoch 14:	0.01436929  	0.03888014  	0.03314532  
2023-06-04 19:37:52.766: Find a better model.
2023-06-04 19:38:13.677: [iter 15 : loss : 1.1218 = 0.6749 + 0.4468 + 0.0001, time: 20.907553]
2023-06-04 19:38:13.953: epoch 15:	0.01744161  	0.04741444  	0.04080715  
2023-06-04 19:38:13.953: Find a better model.
2023-06-04 19:38:35.290: [iter 16 : loss : 1.1105 = 0.6629 + 0.4475 + 0.0002, time: 21.333090]
2023-06-04 19:38:35.553: epoch 16:	0.02055095  	0.05508477  	0.04738060  
2023-06-04 19:38:35.553: Find a better model.
2023-06-04 19:38:57.061: [iter 17 : loss : 1.0923 = 0.6433 + 0.4488 + 0.0002, time: 21.505227]
2023-06-04 19:38:57.344: epoch 17:	0.02340121  	0.06144528  	0.05320843  
2023-06-04 19:38:57.344: Find a better model.
2023-06-04 19:39:18.840: [iter 18 : loss : 1.0636 = 0.6127 + 0.4505 + 0.0004, time: 21.492980]
2023-06-04 19:39:19.118: epoch 18:	0.02557776  	0.06750793  	0.05790353  
2023-06-04 19:39:19.118: Find a better model.
2023-06-04 19:39:40.702: [iter 19 : loss : 1.0245 = 0.5712 + 0.4528 + 0.0006, time: 21.578049]
2023-06-04 19:39:40.979: epoch 19:	0.02700660  	0.07211833  	0.06128674  
2023-06-04 19:39:40.980: Find a better model.
2023-06-04 19:40:02.640: [iter 20 : loss : 0.9776 = 0.5212 + 0.4556 + 0.0008, time: 21.657029]
2023-06-04 19:40:02.914: epoch 20:	0.02808008  	0.07503753  	0.06362958  
2023-06-04 19:40:02.914: Find a better model.
2023-06-04 19:40:24.626: [iter 21 : loss : 0.9280 = 0.4679 + 0.4590 + 0.0011, time: 21.708601]
2023-06-04 19:40:24.897: epoch 21:	0.02858349  	0.07731204  	0.06492503  
2023-06-04 19:40:24.897: Find a better model.
2023-06-04 19:40:46.611: [iter 22 : loss : 0.8790 = 0.4156 + 0.4620 + 0.0014, time: 21.710061]
2023-06-04 19:40:46.881: epoch 22:	0.02885742  	0.07753455  	0.06510875  
2023-06-04 19:40:46.881: Find a better model.
2023-06-04 19:41:08.612: [iter 23 : loss : 0.8357 = 0.3695 + 0.4646 + 0.0017, time: 21.728034]
2023-06-04 19:41:08.882: epoch 23:	0.02916837  	0.07890698  	0.06570272  
2023-06-04 19:41:08.882: Find a better model.
2023-06-04 19:41:30.620: [iter 24 : loss : 0.7975 = 0.3296 + 0.4659 + 0.0020, time: 21.734779]
2023-06-04 19:41:30.887: epoch 24:	0.02930163  	0.07950851  	0.06595673  
2023-06-04 19:41:30.887: Find a better model.
2023-06-04 19:41:52.583: [iter 25 : loss : 0.7651 = 0.2960 + 0.4668 + 0.0023, time: 21.691990]
2023-06-04 19:41:52.850: epoch 25:	0.02953853  	0.08020329  	0.06639762  
2023-06-04 19:41:52.850: Find a better model.
2023-06-04 19:42:14.555: [iter 26 : loss : 0.7382 = 0.2684 + 0.4673 + 0.0025, time: 21.701135]
2023-06-04 19:42:14.821: epoch 26:	0.02952372  	0.08060872  	0.06664848  
2023-06-04 19:42:14.821: Find a better model.
2023-06-04 19:42:36.565: [iter 27 : loss : 0.7153 = 0.2454 + 0.4671 + 0.0028, time: 21.740191]
2023-06-04 19:42:36.833: epoch 27:	0.02947932  	0.08078586  	0.06658376  
2023-06-04 19:42:36.833: Find a better model.
2023-06-04 19:42:58.567: [iter 28 : loss : 0.6957 = 0.2259 + 0.4668 + 0.0031, time: 21.731018]
2023-06-04 19:42:58.832: epoch 28:	0.02951633  	0.08101753  	0.06665196  
2023-06-04 19:42:58.832: Find a better model.
2023-06-04 19:43:20.539: [iter 29 : loss : 0.6788 = 0.2093 + 0.4662 + 0.0033, time: 21.703003]
2023-06-04 19:43:20.804: epoch 29:	0.02961998  	0.08117295  	0.06683126  
2023-06-04 19:43:20.804: Find a better model.
2023-06-04 19:43:42.742: [iter 30 : loss : 0.6634 = 0.1943 + 0.4656 + 0.0035, time: 21.934184]
2023-06-04 19:43:43.010: epoch 30:	0.02952373  	0.08092472  	0.06675700  
2023-06-04 19:44:04.733: [iter 31 : loss : 0.6505 = 0.1817 + 0.4650 + 0.0038, time: 21.719710]
2023-06-04 19:44:05.000: epoch 31:	0.02966438  	0.08096861  	0.06690536  
2023-06-04 19:44:26.719: [iter 32 : loss : 0.6393 = 0.1710 + 0.4644 + 0.0040, time: 21.714115]
2023-06-04 19:44:26.985: epoch 32:	0.02974581  	0.08115783  	0.06692924  
2023-06-04 19:44:48.729: [iter 33 : loss : 0.6298 = 0.1619 + 0.4636 + 0.0042, time: 21.739037]
2023-06-04 19:44:48.995: epoch 33:	0.02984945  	0.08101454  	0.06698468  
2023-06-04 19:45:10.891: [iter 34 : loss : 0.6202 = 0.1530 + 0.4629 + 0.0044, time: 21.891192]
2023-06-04 19:45:11.157: epoch 34:	0.02970140  	0.08114535  	0.06692550  
2023-06-04 19:45:32.909: [iter 35 : loss : 0.6116 = 0.1447 + 0.4623 + 0.0046, time: 21.748002]
2023-06-04 19:45:33.173: epoch 35:	0.02970880  	0.08093160  	0.06701524  
2023-06-04 19:45:55.085: [iter 36 : loss : 0.6045 = 0.1380 + 0.4617 + 0.0048, time: 21.908873]
2023-06-04 19:45:55.355: epoch 36:	0.02984206  	0.08111232  	0.06708989  
2023-06-04 19:46:17.129: [iter 37 : loss : 0.5980 = 0.1318 + 0.4612 + 0.0050, time: 21.769538]
2023-06-04 19:46:17.400: epoch 37:	0.02980504  	0.08074357  	0.06710686  
2023-06-04 19:46:39.061: [iter 38 : loss : 0.5915 = 0.1257 + 0.4606 + 0.0052, time: 21.658099]
2023-06-04 19:46:39.331: epoch 38:	0.02976062  	0.08057408  	0.06711011  
2023-06-04 19:47:01.085: [iter 39 : loss : 0.5862 = 0.1208 + 0.4601 + 0.0053, time: 21.749412]
2023-06-04 19:47:01.355: epoch 39:	0.02985685  	0.08090627  	0.06706026  
2023-06-04 19:47:22.878: [iter 40 : loss : 0.5807 = 0.1156 + 0.4596 + 0.0055, time: 21.520067]
2023-06-04 19:47:23.144: epoch 40:	0.02964216  	0.08022750  	0.06680503  
2023-06-04 19:47:44.697: [iter 41 : loss : 0.5759 = 0.1111 + 0.4591 + 0.0057, time: 21.550143]
2023-06-04 19:47:44.964: epoch 41:	0.02965696  	0.07984599  	0.06679703  
2023-06-04 19:48:06.667: [iter 42 : loss : 0.5715 = 0.1069 + 0.4587 + 0.0058, time: 21.698032]
2023-06-04 19:48:06.931: epoch 42:	0.02969398  	0.07985071  	0.06680358  
2023-06-04 19:48:28.642: [iter 43 : loss : 0.5674 = 0.1031 + 0.4583 + 0.0060, time: 21.707123]
2023-06-04 19:48:28.907: epoch 43:	0.02982723  	0.08019858  	0.06698626  
2023-06-04 19:48:50.619: [iter 44 : loss : 0.5647 = 0.1005 + 0.4580 + 0.0062, time: 21.708241]
2023-06-04 19:48:50.885: epoch 44:	0.02973839  	0.07983810  	0.06693088  
2023-06-04 19:49:12.447: [iter 45 : loss : 0.5607 = 0.0969 + 0.4576 + 0.0063, time: 21.559249]
2023-06-04 19:49:12.712: epoch 45:	0.02961994  	0.07954783  	0.06670393  
2023-06-04 19:49:34.213: [iter 46 : loss : 0.5573 = 0.0936 + 0.4572 + 0.0065, time: 21.498168]
2023-06-04 19:49:34.479: epoch 46:	0.02963475  	0.07959597  	0.06667572  
2023-06-04 19:49:56.058: [iter 47 : loss : 0.5539 = 0.0903 + 0.4569 + 0.0066, time: 21.575461]
2023-06-04 19:49:56.336: epoch 47:	0.02949408  	0.07877316  	0.06628765  
2023-06-04 19:50:17.842: [iter 48 : loss : 0.5515 = 0.0881 + 0.4566 + 0.0067, time: 21.501988]
2023-06-04 19:50:18.108: epoch 48:	0.02941266  	0.07834407  	0.06610210  
2023-06-04 19:50:39.798: [iter 49 : loss : 0.5488 = 0.0857 + 0.4563 + 0.0069, time: 21.685030]
2023-06-04 19:50:40.066: epoch 49:	0.02935342  	0.07807782  	0.06605164  
2023-06-04 19:51:01.595: [iter 50 : loss : 0.5466 = 0.0836 + 0.4560 + 0.0070, time: 21.525335]
2023-06-04 19:51:01.863: epoch 50:	0.02938304  	0.07800260  	0.06594677  
2023-06-04 19:51:23.189: [iter 51 : loss : 0.5435 = 0.0807 + 0.4557 + 0.0072, time: 21.322039]
2023-06-04 19:51:23.459: epoch 51:	0.02933121  	0.07773200  	0.06581314  
2023-06-04 19:51:45.020: [iter 52 : loss : 0.5411 = 0.0784 + 0.4555 + 0.0073, time: 21.556079]
2023-06-04 19:51:45.299: epoch 52:	0.02937562  	0.07750293  	0.06563031  
2023-06-04 19:52:06.982: [iter 53 : loss : 0.5402 = 0.0776 + 0.4552 + 0.0074, time: 21.680255]
2023-06-04 19:52:07.263: epoch 53:	0.02927199  	0.07729198  	0.06541828  
2023-06-04 19:52:28.599: [iter 54 : loss : 0.5380 = 0.0755 + 0.4550 + 0.0076, time: 21.332002]
2023-06-04 19:52:28.865: epoch 54:	0.02915353  	0.07706910  	0.06524067  
2023-06-04 19:52:28.865: Early stopping is trigger at epoch: 54
2023-06-04 19:52:28.865: best_result@epoch 29:

2023-06-04 19:52:28.865: 		0.0296      	0.0812      	0.0668      
2023-06-04 19:55:12.596: my pid: 13360
2023-06-04 19:55:12.596: model: model.general_recommender.SGL
2023-06-04 19:55:12.596: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-04 19:55:12.596: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-04 19:55:16.761: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-04 19:55:37.755: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.993643]
2023-06-04 19:55:38.019: epoch 1:	0.00134735  	0.00257531  	0.00218738  
2023-06-04 19:55:38.019: Find a better model.
2023-06-04 19:55:59.173: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.151170]
2023-06-04 19:55:59.461: epoch 2:	0.00165087  	0.00333653  	0.00281177  
2023-06-04 19:55:59.462: Find a better model.
2023-06-04 19:56:20.619: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.153258]
2023-06-04 19:56:20.916: epoch 3:	0.00208025  	0.00488818  	0.00377989  
2023-06-04 19:56:20.916: Find a better model.
2023-06-04 19:56:41.940: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.021216]
2023-06-04 19:56:42.229: epoch 4:	0.00228754  	0.00482927  	0.00426536  
2023-06-04 19:57:03.166: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 20.933903]
2023-06-04 19:57:03.456: epoch 5:	0.00313148  	0.00693002  	0.00532840  
2023-06-04 19:57:03.456: Find a better model.
2023-06-04 19:57:24.308: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.847932]
2023-06-04 19:57:24.605: epoch 6:	0.00345721  	0.00852081  	0.00692000  
2023-06-04 19:57:24.605: Find a better model.
2023-06-04 19:57:45.537: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.926699]
2023-06-04 19:57:45.827: epoch 7:	0.00393841  	0.00974718  	0.00777298  
2023-06-04 19:57:45.827: Find a better model.
2023-06-04 19:58:06.503: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.673347]
2023-06-04 19:58:06.789: epoch 8:	0.00464909  	0.01282058  	0.00977537  
2023-06-04 19:58:06.790: Find a better model.
2023-06-04 19:58:27.716: [iter 9 : loss : 1.1347 = 0.6915 + 0.4431 + 0.0000, time: 20.922032]
2023-06-04 19:58:28.000: epoch 9:	0.00537458  	0.01509071  	0.01143250  
2023-06-04 19:58:28.000: Find a better model.
2023-06-04 19:58:48.694: [iter 10 : loss : 1.1344 = 0.6908 + 0.4435 + 0.0000, time: 20.690022]
2023-06-04 19:58:48.979: epoch 10:	0.00586318  	0.01641391  	0.01295427  
2023-06-04 19:58:48.979: Find a better model.
2023-06-04 19:59:09.700: [iter 11 : loss : 1.1339 = 0.6896 + 0.4442 + 0.0000, time: 20.716353]
2023-06-04 19:59:09.987: epoch 11:	0.00681815  	0.02017409  	0.01583621  
2023-06-04 19:59:09.987: Find a better model.
2023-06-04 19:59:30.692: [iter 12 : loss : 1.1325 = 0.6877 + 0.4447 + 0.0000, time: 20.701493]
2023-06-04 19:59:30.975: epoch 12:	0.00847642  	0.02460138  	0.01999370  
2023-06-04 19:59:30.975: Find a better model.
2023-06-04 19:59:51.858: [iter 13 : loss : 1.1306 = 0.6850 + 0.4455 + 0.0001, time: 20.878531]
2023-06-04 19:59:52.135: epoch 13:	0.01048264  	0.03017343  	0.02494429  
2023-06-04 19:59:52.136: Find a better model.
2023-06-04 20:00:13.052: [iter 14 : loss : 1.1269 = 0.6806 + 0.4462 + 0.0001, time: 20.912535]
2023-06-04 20:00:13.330: epoch 14:	0.01351792  	0.03931411  	0.03269330  
2023-06-04 20:00:13.330: Find a better model.
2023-06-04 20:00:34.073: [iter 15 : loss : 1.1203 = 0.6731 + 0.4471 + 0.0001, time: 20.738434]
2023-06-04 20:00:34.347: epoch 15:	0.01681233  	0.04759725  	0.04057015  
2023-06-04 20:00:34.347: Find a better model.
2023-06-04 20:00:55.696: [iter 16 : loss : 1.1082 = 0.6600 + 0.4480 + 0.0002, time: 21.346191]
2023-06-04 20:00:55.973: epoch 16:	0.02020301  	0.05547488  	0.04782236  
2023-06-04 20:00:55.973: Find a better model.
2023-06-04 20:01:17.608: [iter 17 : loss : 1.0882 = 0.6386 + 0.4494 + 0.0003, time: 21.630017]
2023-06-04 20:01:17.884: epoch 17:	0.02329017  	0.06369410  	0.05443561  
2023-06-04 20:01:17.884: Find a better model.
2023-06-04 20:01:39.419: [iter 18 : loss : 1.0576 = 0.6060 + 0.4512 + 0.0004, time: 21.531253]
2023-06-04 20:01:39.697: epoch 18:	0.02555558  	0.06947871  	0.05884578  
2023-06-04 20:01:39.697: Find a better model.
2023-06-04 20:02:01.220: [iter 19 : loss : 1.0171 = 0.5628 + 0.4537 + 0.0006, time: 21.518046]
2023-06-04 20:02:01.512: epoch 19:	0.02688074  	0.07213537  	0.06185064  
2023-06-04 20:02:01.512: Find a better model.
2023-06-04 20:02:22.995: [iter 20 : loss : 0.9696 = 0.5120 + 0.4567 + 0.0008, time: 21.476921]
2023-06-04 20:02:23.275: epoch 20:	0.02816152  	0.07645144  	0.06461927  
2023-06-04 20:02:23.275: Find a better model.
2023-06-04 20:02:44.616: [iter 21 : loss : 0.9201 = 0.4589 + 0.4600 + 0.0011, time: 21.337015]
2023-06-04 20:02:44.891: epoch 21:	0.02861312  	0.07729000  	0.06518541  
2023-06-04 20:02:44.891: Find a better model.
2023-06-04 20:03:06.370: [iter 22 : loss : 0.8716 = 0.4073 + 0.4628 + 0.0014, time: 21.475510]
2023-06-04 20:03:06.644: epoch 22:	0.02894628  	0.07810245  	0.06579858  
2023-06-04 20:03:06.645: Find a better model.
2023-06-04 20:03:28.193: [iter 23 : loss : 0.8293 = 0.3624 + 0.4652 + 0.0017, time: 21.545242]
2023-06-04 20:03:28.462: epoch 23:	0.02929423  	0.07940497  	0.06624874  
2023-06-04 20:03:28.462: Find a better model.
2023-06-04 20:03:49.805: [iter 24 : loss : 0.7929 = 0.3241 + 0.4669 + 0.0020, time: 21.339466]
2023-06-04 20:03:50.071: epoch 24:	0.02933125  	0.08002876  	0.06647063  
2023-06-04 20:03:50.071: Find a better model.
2023-06-04 20:04:11.368: [iter 25 : loss : 0.7614 = 0.2916 + 0.4674 + 0.0023, time: 21.293936]
2023-06-04 20:04:11.643: epoch 25:	0.02944971  	0.08109242  	0.06681139  
2023-06-04 20:04:11.643: Find a better model.
2023-06-04 20:04:32.963: [iter 26 : loss : 0.7352 = 0.2648 + 0.4678 + 0.0026, time: 21.315040]
2023-06-04 20:04:33.233: epoch 26:	0.02966440  	0.08176585  	0.06704460  
2023-06-04 20:04:33.234: Find a better model.
2023-06-04 20:04:54.549: [iter 27 : loss : 0.7128 = 0.2422 + 0.4678 + 0.0028, time: 21.311435]
2023-06-04 20:04:54.826: epoch 27:	0.02964960  	0.08210376  	0.06721611  
2023-06-04 20:04:54.826: Find a better model.
2023-06-04 20:05:16.193: [iter 28 : loss : 0.6937 = 0.2233 + 0.4673 + 0.0031, time: 21.362997]
2023-06-04 20:05:16.463: epoch 28:	0.02972363  	0.08236409  	0.06745837  
2023-06-04 20:05:16.463: Find a better model.
2023-06-04 20:05:37.977: [iter 29 : loss : 0.6774 = 0.2073 + 0.4668 + 0.0033, time: 21.511282]
2023-06-04 20:05:38.243: epoch 29:	0.02973844  	0.08298222  	0.06774317  
2023-06-04 20:05:38.243: Find a better model.
2023-06-04 20:05:59.759: [iter 30 : loss : 0.6623 = 0.1927 + 0.4660 + 0.0036, time: 21.511550]
2023-06-04 20:06:00.031: epoch 30:	0.02988649  	0.08366469  	0.06791066  
2023-06-04 20:06:00.031: Find a better model.
2023-06-04 20:06:21.361: [iter 31 : loss : 0.6495 = 0.1801 + 0.4655 + 0.0038, time: 21.324863]
2023-06-04 20:06:21.642: epoch 31:	0.02993831  	0.08368441  	0.06796592  
2023-06-04 20:06:21.642: Find a better model.
2023-06-04 20:06:43.159: [iter 32 : loss : 0.6385 = 0.1696 + 0.4648 + 0.0040, time: 21.513489]
2023-06-04 20:06:43.445: epoch 32:	0.03003456  	0.08350354  	0.06825355  
2023-06-04 20:07:04.897: [iter 33 : loss : 0.6290 = 0.1607 + 0.4641 + 0.0042, time: 21.447515]
2023-06-04 20:07:05.165: epoch 33:	0.03001235  	0.08334466  	0.06825117  
2023-06-04 20:07:26.353: [iter 34 : loss : 0.6200 = 0.1520 + 0.4635 + 0.0044, time: 21.184309]
2023-06-04 20:07:26.627: epoch 34:	0.03007158  	0.08354558  	0.06839946  
2023-06-04 20:07:48.119: [iter 35 : loss : 0.6111 = 0.1438 + 0.4627 + 0.0046, time: 21.487329]
2023-06-04 20:07:48.384: epoch 35:	0.02985689  	0.08314597  	0.06832173  
2023-06-04 20:08:09.907: [iter 36 : loss : 0.6043 = 0.1373 + 0.4622 + 0.0048, time: 21.518441]
2023-06-04 20:08:10.171: epoch 36:	0.02988651  	0.08308766  	0.06813524  
2023-06-04 20:08:31.340: [iter 37 : loss : 0.5980 = 0.1313 + 0.4617 + 0.0050, time: 21.164038]
2023-06-04 20:08:31.618: epoch 37:	0.02982728  	0.08261961  	0.06809944  
2023-06-04 20:08:53.082: [iter 38 : loss : 0.5913 = 0.1251 + 0.4610 + 0.0052, time: 21.459637]
2023-06-04 20:08:53.349: epoch 38:	0.02976805  	0.08253011  	0.06808450  
2023-06-04 20:09:14.858: [iter 39 : loss : 0.5864 = 0.1205 + 0.4605 + 0.0054, time: 21.505352]
2023-06-04 20:09:15.126: epoch 39:	0.02979766  	0.08240659  	0.06818760  
2023-06-04 20:09:36.442: [iter 40 : loss : 0.5809 = 0.1152 + 0.4601 + 0.0055, time: 21.310807]
2023-06-04 20:09:36.712: epoch 40:	0.02981247  	0.08269487  	0.06829847  
2023-06-04 20:09:58.042: [iter 41 : loss : 0.5759 = 0.1105 + 0.4596 + 0.0057, time: 21.324585]
2023-06-04 20:09:58.307: epoch 41:	0.02963479  	0.08248779  	0.06819372  
2023-06-04 20:10:19.870: [iter 42 : loss : 0.5715 = 0.1064 + 0.4592 + 0.0059, time: 21.558032]
2023-06-04 20:10:20.136: epoch 42:	0.02966440  	0.08259202  	0.06824362  
2023-06-04 20:10:41.255: [iter 43 : loss : 0.5676 = 0.1027 + 0.4589 + 0.0060, time: 21.115994]
2023-06-04 20:10:41.539: epoch 43:	0.02970141  	0.08247886  	0.06821671  
2023-06-04 20:11:02.863: [iter 44 : loss : 0.5650 = 0.1003 + 0.4585 + 0.0062, time: 21.321516]
2023-06-04 20:11:03.138: epoch 44:	0.02967921  	0.08224113  	0.06819423  
2023-06-04 20:11:24.620: [iter 45 : loss : 0.5606 = 0.0962 + 0.4581 + 0.0063, time: 21.477096]
2023-06-04 20:11:24.883: epoch 45:	0.02975324  	0.08246774  	0.06810640  
2023-06-04 20:11:46.232: [iter 46 : loss : 0.5575 = 0.0933 + 0.4577 + 0.0065, time: 21.345171]
2023-06-04 20:11:46.516: epoch 46:	0.02979026  	0.08250831  	0.06815363  
2023-06-04 20:12:07.844: [iter 47 : loss : 0.5543 = 0.0902 + 0.4575 + 0.0066, time: 21.324013]
2023-06-04 20:12:08.112: epoch 47:	0.02965700  	0.08179008  	0.06795650  
2023-06-04 20:12:29.457: [iter 48 : loss : 0.5518 = 0.0879 + 0.4571 + 0.0068, time: 21.340181]
2023-06-04 20:12:29.724: epoch 48:	0.02959036  	0.08193469  	0.06794787  
2023-06-04 20:12:51.011: [iter 49 : loss : 0.5488 = 0.0853 + 0.4566 + 0.0069, time: 21.282694]
2023-06-04 20:12:51.298: epoch 49:	0.02958296  	0.08165208  	0.06789210  
2023-06-04 20:13:12.611: [iter 50 : loss : 0.5467 = 0.0832 + 0.4564 + 0.0070, time: 21.310045]
2023-06-04 20:13:12.896: epoch 50:	0.02948671  	0.08105073  	0.06753013  
2023-06-04 20:13:34.379: [iter 51 : loss : 0.5441 = 0.0807 + 0.4562 + 0.0072, time: 21.477932]
2023-06-04 20:13:34.649: epoch 51:	0.02942008  	0.08085243  	0.06745682  
2023-06-04 20:13:55.985: [iter 52 : loss : 0.5411 = 0.0779 + 0.4559 + 0.0073, time: 21.331497]
2023-06-04 20:13:56.268: epoch 52:	0.02939787  	0.08089354  	0.06744013  
2023-06-04 20:14:17.603: [iter 53 : loss : 0.5403 = 0.0771 + 0.4557 + 0.0074, time: 21.330007]
2023-06-04 20:14:17.872: epoch 53:	0.02933864  	0.08058096  	0.06731807  
2023-06-04 20:14:39.335: [iter 54 : loss : 0.5381 = 0.0750 + 0.4555 + 0.0076, time: 21.459299]
2023-06-04 20:14:39.609: epoch 54:	0.02925719  	0.08009674  	0.06701920  
2023-06-04 20:15:00.824: [iter 55 : loss : 0.5361 = 0.0731 + 0.4553 + 0.0077, time: 21.210431]
2023-06-04 20:15:01.088: epoch 55:	0.02928680  	0.08018206  	0.06696714  
2023-06-04 20:15:22.341: [iter 56 : loss : 0.5341 = 0.0712 + 0.4550 + 0.0078, time: 21.249028]
2023-06-04 20:15:22.620: epoch 56:	0.02932382  	0.08018149  	0.06691651  
2023-06-04 20:15:22.620: Early stopping is trigger at epoch: 56
2023-06-04 20:15:22.620: best_result@epoch 31:

2023-06-04 20:15:22.620: 		0.0299      	0.0837      	0.0680      
2023-06-04 20:19:28.662: my pid: 13604
2023-06-04 20:19:28.662: model: model.general_recommender.SGL
2023-06-04 20:19:28.662: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-04 20:19:28.662: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-04 20:19:32.769: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-04 20:19:53.703: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 20.932712]
2023-06-04 20:19:53.981: epoch 1:	0.00116968  	0.00239628  	0.00200123  
2023-06-04 20:19:53.981: Find a better model.
2023-06-04 20:20:15.237: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.251151]
2023-06-04 20:20:15.525: epoch 2:	0.00203583  	0.00378073  	0.00307750  
2023-06-04 20:20:15.525: Find a better model.
2023-06-04 20:20:36.855: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.326137]
2023-06-04 20:20:37.149: epoch 3:	0.00197661  	0.00415934  	0.00384676  
2023-06-04 20:20:37.149: Find a better model.
2023-06-04 20:20:58.254: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.101771]
2023-06-04 20:20:58.547: epoch 4:	0.00228754  	0.00432986  	0.00357868  
2023-06-04 20:20:58.547: Find a better model.
2023-06-04 20:21:19.818: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.267137]
2023-06-04 20:21:20.109: epoch 5:	0.00282795  	0.00562049  	0.00482064  
2023-06-04 20:21:20.110: Find a better model.
2023-06-04 20:21:41.222: [iter 6 : loss : 1.1346 = 0.6925 + 0.4420 + 0.0000, time: 21.109039]
2023-06-04 20:21:41.512: epoch 6:	0.00316849  	0.00747804  	0.00596360  
2023-06-04 20:21:41.512: Find a better model.
2023-06-04 20:22:02.624: [iter 7 : loss : 1.1347 = 0.6923 + 0.4424 + 0.0000, time: 21.108096]
2023-06-04 20:22:02.915: epoch 7:	0.00359787  	0.00831012  	0.00693833  
2023-06-04 20:22:02.915: Find a better model.
2023-06-04 20:22:23.997: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 21.078153]
2023-06-04 20:22:24.282: epoch 8:	0.00433817  	0.01074012  	0.00873781  
2023-06-04 20:22:24.283: Find a better model.
2023-06-04 20:22:45.390: [iter 9 : loss : 1.1346 = 0.6915 + 0.4430 + 0.0000, time: 21.104045]
2023-06-04 20:22:45.683: epoch 9:	0.00490820  	0.01295017  	0.01028293  
2023-06-04 20:22:45.683: Find a better model.
2023-06-04 20:23:06.805: [iter 10 : loss : 1.1345 = 0.6908 + 0.4436 + 0.0000, time: 21.111037]
2023-06-04 20:23:07.093: epoch 10:	0.00527094  	0.01445080  	0.01142647  
2023-06-04 20:23:07.093: Find a better model.
2023-06-04 20:23:28.209: [iter 11 : loss : 1.1338 = 0.6897 + 0.4442 + 0.0000, time: 21.112133]
2023-06-04 20:23:28.491: epoch 11:	0.00621111  	0.01692450  	0.01366240  
2023-06-04 20:23:28.492: Find a better model.
2023-06-04 20:23:49.419: [iter 12 : loss : 1.1326 = 0.6878 + 0.4447 + 0.0000, time: 20.923255]
2023-06-04 20:23:49.714: epoch 12:	0.00803964  	0.02208019  	0.01793426  
2023-06-04 20:23:49.714: Find a better model.
2023-06-04 20:24:10.543: [iter 13 : loss : 1.1306 = 0.6851 + 0.4455 + 0.0001, time: 20.824513]
2023-06-04 20:24:10.831: epoch 13:	0.01031978  	0.02902015  	0.02433409  
2023-06-04 20:24:10.831: Find a better model.
2023-06-04 20:24:31.598: [iter 14 : loss : 1.1270 = 0.6807 + 0.4462 + 0.0001, time: 20.764194]
2023-06-04 20:24:31.880: epoch 14:	0.01311074  	0.03625536  	0.03086201  
2023-06-04 20:24:31.881: Find a better model.
2023-06-04 20:24:52.750: [iter 15 : loss : 1.1206 = 0.6733 + 0.4472 + 0.0001, time: 20.865215]
2023-06-04 20:24:53.027: epoch 15:	0.01655323  	0.04466304  	0.03842710  
2023-06-04 20:24:53.027: Find a better model.
2023-06-04 20:25:14.525: [iter 16 : loss : 1.1084 = 0.6604 + 0.4479 + 0.0002, time: 21.494359]
2023-06-04 20:25:14.817: epoch 16:	0.02002534  	0.05357501  	0.04572890  
2023-06-04 20:25:14.817: Find a better model.
2023-06-04 20:25:36.343: [iter 17 : loss : 1.0889 = 0.6393 + 0.4493 + 0.0003, time: 21.521027]
2023-06-04 20:25:36.624: epoch 17:	0.02294961  	0.06013469  	0.05174245  
2023-06-04 20:25:36.624: Find a better model.
2023-06-04 20:25:58.334: [iter 18 : loss : 1.0583 = 0.6068 + 0.4511 + 0.0004, time: 21.707456]
2023-06-04 20:25:58.613: epoch 18:	0.02532608  	0.06600971  	0.05648487  
2023-06-04 20:25:58.614: Find a better model.
2023-06-04 20:26:20.144: [iter 19 : loss : 1.0180 = 0.5637 + 0.4536 + 0.0006, time: 21.525092]
2023-06-04 20:26:20.427: epoch 19:	0.02712505  	0.07110029  	0.06000450  
2023-06-04 20:26:20.428: Find a better model.
2023-06-04 20:26:41.735: [iter 20 : loss : 0.9701 = 0.5125 + 0.4568 + 0.0008, time: 21.304028]
2023-06-04 20:26:42.016: epoch 20:	0.02810230  	0.07432910  	0.06218071  
2023-06-04 20:26:42.016: Find a better model.
2023-06-04 20:27:03.304: [iter 21 : loss : 0.9205 = 0.4592 + 0.4602 + 0.0011, time: 21.283032]
2023-06-04 20:27:03.578: epoch 21:	0.02847984  	0.07568946  	0.06294127  
2023-06-04 20:27:03.578: Find a better model.
2023-06-04 20:27:25.095: [iter 22 : loss : 0.8722 = 0.4076 + 0.4632 + 0.0014, time: 21.511953]
2023-06-04 20:27:25.368: epoch 22:	0.02879080  	0.07659762  	0.06357120  
2023-06-04 20:27:25.368: Find a better model.
2023-06-04 20:27:46.855: [iter 23 : loss : 0.8295 = 0.3623 + 0.4654 + 0.0017, time: 21.483064]
2023-06-04 20:27:47.125: epoch 23:	0.02899807  	0.07730944  	0.06415216  
2023-06-04 20:27:47.126: Find a better model.
2023-06-04 20:28:08.478: [iter 24 : loss : 0.7929 = 0.3238 + 0.4671 + 0.0020, time: 21.349103]
2023-06-04 20:28:08.764: epoch 24:	0.02912394  	0.07846390  	0.06442727  
2023-06-04 20:28:08.764: Find a better model.
2023-06-04 20:28:30.280: [iter 25 : loss : 0.7615 = 0.2915 + 0.4677 + 0.0023, time: 21.510948]
2023-06-04 20:28:30.550: epoch 25:	0.02928682  	0.07907386  	0.06474296  
2023-06-04 20:28:30.550: Find a better model.
2023-06-04 20:28:52.274: [iter 26 : loss : 0.7356 = 0.2651 + 0.4679 + 0.0026, time: 21.720059]
2023-06-04 20:28:52.542: epoch 26:	0.02949411  	0.07938623  	0.06500308  
2023-06-04 20:28:52.542: Find a better model.
2023-06-04 20:29:14.131: [iter 27 : loss : 0.7127 = 0.2420 + 0.4678 + 0.0028, time: 21.584997]
2023-06-04 20:29:14.402: epoch 27:	0.02945708  	0.07929908  	0.06504361  
2023-06-04 20:29:36.240: [iter 28 : loss : 0.6937 = 0.2233 + 0.4673 + 0.0031, time: 21.833234]
2023-06-04 20:29:36.508: epoch 28:	0.02959034  	0.07984236  	0.06548729  
2023-06-04 20:29:36.508: Find a better model.
2023-06-04 20:29:58.287: [iter 29 : loss : 0.6773 = 0.2072 + 0.4668 + 0.0033, time: 21.775016]
2023-06-04 20:29:58.574: epoch 29:	0.02977541  	0.08076932  	0.06598925  
2023-06-04 20:29:58.574: Find a better model.
2023-06-04 20:30:20.202: [iter 30 : loss : 0.6626 = 0.1928 + 0.4661 + 0.0036, time: 21.625092]
2023-06-04 20:30:20.470: epoch 30:	0.02983464  	0.08065139  	0.06608361  
2023-06-04 20:30:42.067: [iter 31 : loss : 0.6496 = 0.1803 + 0.4655 + 0.0038, time: 21.593318]
2023-06-04 20:30:42.336: epoch 31:	0.02991607  	0.08123709  	0.06639298  
2023-06-04 20:30:42.336: Find a better model.
2023-06-04 20:31:04.031: [iter 32 : loss : 0.6384 = 0.1696 + 0.4648 + 0.0040, time: 21.689375]
2023-06-04 20:31:04.299: epoch 32:	0.02987905  	0.08109068  	0.06639568  
2023-06-04 20:31:26.042: [iter 33 : loss : 0.6291 = 0.1607 + 0.4642 + 0.0042, time: 21.739374]
2023-06-04 20:31:26.309: epoch 33:	0.02985683  	0.08084334  	0.06634477  
2023-06-04 20:31:48.042: [iter 34 : loss : 0.6198 = 0.1520 + 0.4634 + 0.0044, time: 21.727996]
2023-06-04 20:31:48.328: epoch 34:	0.02997530  	0.08117660  	0.06655251  
2023-06-04 20:32:10.024: [iter 35 : loss : 0.6112 = 0.1438 + 0.4628 + 0.0046, time: 21.692092]
2023-06-04 20:32:10.289: epoch 35:	0.02989386  	0.08096855  	0.06634650  
2023-06-04 20:32:31.977: [iter 36 : loss : 0.6046 = 0.1376 + 0.4622 + 0.0048, time: 21.682441]
2023-06-04 20:32:32.242: epoch 36:	0.02982724  	0.08083689  	0.06636491  
2023-06-04 20:32:54.165: [iter 37 : loss : 0.5981 = 0.1314 + 0.4617 + 0.0050, time: 21.919124]
2023-06-04 20:32:54.433: epoch 37:	0.02984945  	0.08068397  	0.06631977  
2023-06-04 20:33:16.198: [iter 38 : loss : 0.5916 = 0.1253 + 0.4611 + 0.0052, time: 21.760035]
2023-06-04 20:33:16.463: epoch 38:	0.02994569  	0.08045962  	0.06617250  
2023-06-04 20:33:38.154: [iter 39 : loss : 0.5863 = 0.1204 + 0.4606 + 0.0054, time: 21.687738]
2023-06-04 20:33:38.420: epoch 39:	0.03005674  	0.08058959  	0.06652181  
2023-06-04 20:33:59.976: [iter 40 : loss : 0.5808 = 0.1152 + 0.4601 + 0.0055, time: 21.550938]
2023-06-04 20:34:00.257: epoch 40:	0.02990868  	0.08006789  	0.06636233  
2023-06-04 20:34:22.148: [iter 41 : loss : 0.5760 = 0.1107 + 0.4596 + 0.0057, time: 21.887696]
2023-06-04 20:34:22.412: epoch 41:	0.02984945  	0.07997018  	0.06644878  
2023-06-04 20:34:44.157: [iter 42 : loss : 0.5715 = 0.1064 + 0.4593 + 0.0059, time: 21.740037]
2023-06-04 20:34:44.420: epoch 42:	0.02982724  	0.07970989  	0.06643584  
2023-06-04 20:35:06.194: [iter 43 : loss : 0.5677 = 0.1029 + 0.4588 + 0.0060, time: 21.769081]
2023-06-04 20:35:06.460: epoch 43:	0.02977541  	0.07996522  	0.06643613  
2023-06-04 20:35:28.138: [iter 44 : loss : 0.5645 = 0.0999 + 0.4584 + 0.0062, time: 21.674027]
2023-06-04 20:35:28.402: epoch 44:	0.02964214  	0.07938650  	0.06617466  
2023-06-04 20:35:49.919: [iter 45 : loss : 0.5607 = 0.0964 + 0.4580 + 0.0063, time: 21.512043]
2023-06-04 20:35:50.186: epoch 45:	0.02976799  	0.07934300  	0.06628122  
2023-06-04 20:36:11.924: [iter 46 : loss : 0.5578 = 0.0937 + 0.4577 + 0.0065, time: 21.733154]
2023-06-04 20:36:12.188: epoch 46:	0.02970877  	0.07913584  	0.06607103  
2023-06-04 20:36:33.686: [iter 47 : loss : 0.5542 = 0.0902 + 0.4574 + 0.0066, time: 21.495020]
2023-06-04 20:36:33.949: epoch 47:	0.02951629  	0.07878377  	0.06574425  
2023-06-04 20:36:55.491: [iter 48 : loss : 0.5517 = 0.0880 + 0.4570 + 0.0068, time: 21.538031]
2023-06-04 20:36:55.768: epoch 48:	0.02956071  	0.07861200  	0.06571192  
2023-06-04 20:37:17.484: [iter 49 : loss : 0.5491 = 0.0854 + 0.4568 + 0.0069, time: 21.711040]
2023-06-04 20:37:17.763: epoch 49:	0.02942745  	0.07830644  	0.06542078  
2023-06-04 20:37:39.302: [iter 50 : loss : 0.5467 = 0.0832 + 0.4565 + 0.0070, time: 21.536063]
2023-06-04 20:37:39.567: epoch 50:	0.02953109  	0.07849572  	0.06553566  
2023-06-04 20:38:01.109: [iter 51 : loss : 0.5438 = 0.0804 + 0.4561 + 0.0072, time: 21.537458]
2023-06-04 20:38:01.376: epoch 51:	0.02936083  	0.07775562  	0.06524390  
2023-06-04 20:38:22.895: [iter 52 : loss : 0.5413 = 0.0781 + 0.4559 + 0.0073, time: 21.515028]
2023-06-04 20:38:23.160: epoch 52:	0.02929420  	0.07757281  	0.06533129  
2023-06-04 20:38:44.842: [iter 53 : loss : 0.5403 = 0.0772 + 0.4557 + 0.0074, time: 21.678313]
2023-06-04 20:38:45.108: epoch 53:	0.02922016  	0.07751088  	0.06517348  
2023-06-04 20:39:06.650: [iter 54 : loss : 0.5380 = 0.0749 + 0.4555 + 0.0076, time: 21.538734]
2023-06-04 20:39:06.924: epoch 54:	0.02913132  	0.07711101  	0.06488567  
2023-06-04 20:39:28.447: [iter 55 : loss : 0.5359 = 0.0731 + 0.4551 + 0.0077, time: 21.519996]
2023-06-04 20:39:28.733: epoch 55:	0.02914613  	0.07712958  	0.06500988  
2023-06-04 20:39:50.229: [iter 56 : loss : 0.5337 = 0.0709 + 0.4550 + 0.0078, time: 21.493148]
2023-06-04 20:39:50.495: epoch 56:	0.02911651  	0.07737546  	0.06506915  
2023-06-04 20:39:50.496: Early stopping is trigger at epoch: 56
2023-06-04 20:39:50.496: best_result@epoch 31:

2023-06-04 20:39:50.496: 		0.0299      	0.0812      	0.0664      
2023-06-04 20:42:30.927: my pid: 14116
2023-06-04 20:42:30.928: model: model.general_recommender.SGL
2023-06-04 20:42:30.928: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-04 20:42:30.928: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-04 20:42:35.098: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-04 20:42:56.176: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.077378]
2023-06-04 20:42:56.446: epoch 1:	0.00148060  	0.00328196  	0.00256696  
2023-06-04 20:42:56.446: Find a better model.
2023-06-04 20:43:17.395: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 20.945049]
2023-06-04 20:43:17.681: epoch 2:	0.00157684  	0.00337158  	0.00271587  
2023-06-04 20:43:17.681: Find a better model.
2023-06-04 20:43:38.853: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.168025]
2023-06-04 20:43:39.147: epoch 3:	0.00220610  	0.00506938  	0.00370112  
2023-06-04 20:43:39.147: Find a better model.
2023-06-04 20:44:00.404: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.252708]
2023-06-04 20:44:00.690: epoch 4:	0.00247261  	0.00541289  	0.00402794  
2023-06-04 20:44:00.690: Find a better model.
2023-06-04 20:44:21.811: [iter 5 : loss : 1.1344 = 0.6927 + 0.4417 + 0.0000, time: 21.117278]
2023-06-04 20:44:22.100: epoch 5:	0.00267989  	0.00612855  	0.00466047  
2023-06-04 20:44:22.100: Find a better model.
2023-06-04 20:44:43.180: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 21.075437]
2023-06-04 20:44:43.467: epoch 6:	0.00310927  	0.00760012  	0.00584339  
2023-06-04 20:44:43.467: Find a better model.
2023-06-04 20:45:04.529: [iter 7 : loss : 1.1347 = 0.6923 + 0.4423 + 0.0000, time: 21.059283]
2023-06-04 20:45:04.814: epoch 7:	0.00393101  	0.00991382  	0.00839197  
2023-06-04 20:45:04.814: Find a better model.
2023-06-04 20:45:25.762: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.945388]
2023-06-04 20:45:26.055: epoch 8:	0.00449363  	0.01201707  	0.00928528  
2023-06-04 20:45:26.055: Find a better model.
2023-06-04 20:45:46.987: [iter 9 : loss : 1.1346 = 0.6915 + 0.4431 + 0.0000, time: 20.928025]
2023-06-04 20:45:47.269: epoch 9:	0.00492300  	0.01320091  	0.01045080  
2023-06-04 20:45:47.269: Find a better model.
2023-06-04 20:46:08.311: [iter 10 : loss : 1.1344 = 0.6908 + 0.4436 + 0.0000, time: 21.036477]
2023-06-04 20:46:08.593: epoch 10:	0.00572992  	0.01514250  	0.01219411  
2023-06-04 20:46:08.594: Find a better model.
2023-06-04 20:46:29.338: [iter 11 : loss : 1.1337 = 0.6896 + 0.4441 + 0.0000, time: 20.741018]
2023-06-04 20:46:29.618: epoch 11:	0.00644060  	0.01732255  	0.01383494  
2023-06-04 20:46:29.618: Find a better model.
2023-06-04 20:46:50.535: [iter 12 : loss : 1.1326 = 0.6877 + 0.4448 + 0.0000, time: 20.912780]
2023-06-04 20:46:50.837: epoch 12:	0.00758806  	0.02114508  	0.01719967  
2023-06-04 20:46:50.837: Find a better model.
2023-06-04 20:47:11.742: [iter 13 : loss : 1.1306 = 0.6851 + 0.4455 + 0.0001, time: 20.901649]
2023-06-04 20:47:12.032: epoch 13:	0.01019393  	0.02835114  	0.02323133  
2023-06-04 20:47:12.032: Find a better model.
2023-06-04 20:47:32.936: [iter 14 : loss : 1.1270 = 0.6807 + 0.4462 + 0.0001, time: 20.898321]
2023-06-04 20:47:33.210: epoch 14:	0.01282943  	0.03607630  	0.02987723  
2023-06-04 20:47:33.210: Find a better model.
2023-06-04 20:47:53.954: [iter 15 : loss : 1.1204 = 0.6732 + 0.4471 + 0.0001, time: 20.741355]
2023-06-04 20:47:54.250: epoch 15:	0.01639035  	0.04481193  	0.03811975  
2023-06-04 20:47:54.250: Find a better model.
2023-06-04 20:48:15.720: [iter 16 : loss : 1.1083 = 0.6602 + 0.4479 + 0.0002, time: 21.465572]
2023-06-04 20:48:16.020: epoch 16:	0.01989207  	0.05375932  	0.04573433  
2023-06-04 20:48:16.020: Find a better model.
2023-06-04 20:48:37.670: [iter 17 : loss : 1.0885 = 0.6390 + 0.4492 + 0.0003, time: 21.646667]
2023-06-04 20:48:37.959: epoch 17:	0.02311990  	0.06183255  	0.05212213  
2023-06-04 20:48:37.960: Find a better model.
2023-06-04 20:48:59.481: [iter 18 : loss : 1.0580 = 0.6065 + 0.4511 + 0.0004, time: 21.517539]
2023-06-04 20:48:59.776: epoch 18:	0.02565921  	0.06786474  	0.05695136  
2023-06-04 20:48:59.776: Find a better model.
2023-06-04 20:49:21.655: [iter 19 : loss : 1.0176 = 0.5634 + 0.4536 + 0.0006, time: 21.876062]
2023-06-04 20:49:21.946: epoch 19:	0.02755445  	0.07275152  	0.06035885  
2023-06-04 20:49:21.946: Find a better model.
2023-06-04 20:49:43.454: [iter 20 : loss : 0.9700 = 0.5123 + 0.4568 + 0.0008, time: 21.503060]
2023-06-04 20:49:43.730: epoch 20:	0.02832439  	0.07515836  	0.06249321  
2023-06-04 20:49:43.730: Find a better model.
2023-06-04 20:50:05.636: [iter 21 : loss : 0.9202 = 0.4589 + 0.4602 + 0.0011, time: 21.902377]
2023-06-04 20:50:05.926: epoch 21:	0.02903510  	0.07776194  	0.06409771  
2023-06-04 20:50:05.926: Find a better model.
2023-06-04 20:50:27.664: [iter 22 : loss : 0.8720 = 0.4073 + 0.4632 + 0.0014, time: 21.733835]
2023-06-04 20:50:27.954: epoch 22:	0.02917578  	0.07904491  	0.06455804  
2023-06-04 20:50:27.955: Find a better model.
2023-06-04 20:50:49.639: [iter 23 : loss : 0.8294 = 0.3622 + 0.4655 + 0.0017, time: 21.680514]
2023-06-04 20:50:49.934: epoch 23:	0.02930163  	0.07978340  	0.06510385  
2023-06-04 20:50:49.934: Find a better model.
2023-06-04 20:51:11.384: [iter 24 : loss : 0.7930 = 0.3239 + 0.4671 + 0.0020, time: 21.445168]
2023-06-04 20:51:11.658: epoch 24:	0.02944970  	0.08061005  	0.06528141  
2023-06-04 20:51:11.659: Find a better model.
2023-06-04 20:51:32.849: [iter 25 : loss : 0.7615 = 0.2914 + 0.4678 + 0.0023, time: 21.186507]
2023-06-04 20:51:33.122: epoch 25:	0.02975322  	0.08208969  	0.06601647  
2023-06-04 20:51:33.122: Find a better model.
2023-06-04 20:51:54.430: [iter 26 : loss : 0.7352 = 0.2646 + 0.4680 + 0.0026, time: 21.303015]
2023-06-04 20:51:54.696: epoch 26:	0.02989390  	0.08259712  	0.06629486  
2023-06-04 20:51:54.696: Find a better model.
2023-06-04 20:52:16.040: [iter 27 : loss : 0.7128 = 0.2421 + 0.4679 + 0.0028, time: 21.340494]
2023-06-04 20:52:16.303: epoch 27:	0.03001235  	0.08293689  	0.06640899  
2023-06-04 20:52:16.304: Find a better model.
2023-06-04 20:52:37.604: [iter 28 : loss : 0.6936 = 0.2230 + 0.4675 + 0.0031, time: 21.296597]
2023-06-04 20:52:37.870: epoch 28:	0.03021963  	0.08402108  	0.06689624  
2023-06-04 20:52:37.871: Find a better model.
2023-06-04 20:52:59.190: [iter 29 : loss : 0.6776 = 0.2073 + 0.4669 + 0.0033, time: 21.315053]
2023-06-04 20:52:59.457: epoch 29:	0.03030108  	0.08429184  	0.06712916  
2023-06-04 20:52:59.458: Find a better model.
2023-06-04 20:53:20.975: [iter 30 : loss : 0.6623 = 0.1925 + 0.4663 + 0.0036, time: 21.514026]
2023-06-04 20:53:21.241: epoch 30:	0.03030107  	0.08418539  	0.06724525  
2023-06-04 20:53:42.398: [iter 31 : loss : 0.6497 = 0.1803 + 0.4656 + 0.0038, time: 21.153064]
2023-06-04 20:53:42.663: epoch 31:	0.03036770  	0.08389236  	0.06729011  
2023-06-04 20:54:04.004: [iter 32 : loss : 0.6387 = 0.1699 + 0.4648 + 0.0040, time: 21.337046]
2023-06-04 20:54:04.273: epoch 32:	0.03050837  	0.08410921  	0.06763400  
2023-06-04 20:54:25.726: [iter 33 : loss : 0.6289 = 0.1605 + 0.4642 + 0.0042, time: 21.449023]
2023-06-04 20:54:26.004: epoch 33:	0.03050836  	0.08368654  	0.06768107  
2023-06-04 20:54:47.361: [iter 34 : loss : 0.6201 = 0.1521 + 0.4636 + 0.0044, time: 21.352789]
2023-06-04 20:54:47.626: epoch 34:	0.03043432  	0.08351958  	0.06758216  
2023-06-04 20:55:09.192: [iter 35 : loss : 0.6114 = 0.1439 + 0.4628 + 0.0046, time: 21.561380]
2023-06-04 20:55:09.478: epoch 35:	0.03044912  	0.08343714  	0.06763770  
2023-06-04 20:55:30.953: [iter 36 : loss : 0.6044 = 0.1373 + 0.4623 + 0.0048, time: 21.472116]
2023-06-04 20:55:31.220: epoch 36:	0.03036769  	0.08312915  	0.06753542  
2023-06-04 20:55:52.799: [iter 37 : loss : 0.5978 = 0.1312 + 0.4616 + 0.0050, time: 21.576019]
2023-06-04 20:55:53.080: epoch 37:	0.03038989  	0.08337380  	0.06777522  
2023-06-04 20:56:14.684: [iter 38 : loss : 0.5917 = 0.1254 + 0.4611 + 0.0052, time: 21.600391]
2023-06-04 20:56:14.966: epoch 38:	0.03044911  	0.08328933  	0.06762729  
2023-06-04 20:56:36.683: [iter 39 : loss : 0.5864 = 0.1205 + 0.4606 + 0.0054, time: 21.713716]
2023-06-04 20:56:36.965: epoch 39:	0.03051574  	0.08336667  	0.06772199  
2023-06-04 20:56:58.566: [iter 40 : loss : 0.5809 = 0.1153 + 0.4601 + 0.0055, time: 21.597096]
2023-06-04 20:56:58.832: epoch 40:	0.03064900  	0.08383059  	0.06771763  
2023-06-04 20:57:20.536: [iter 41 : loss : 0.5759 = 0.1107 + 0.4595 + 0.0057, time: 21.700019]
2023-06-04 20:57:20.802: epoch 41:	0.03054536  	0.08340364  	0.06767818  
2023-06-04 20:57:42.658: [iter 42 : loss : 0.5717 = 0.1066 + 0.4593 + 0.0059, time: 21.853030]
2023-06-04 20:57:42.941: epoch 42:	0.03043432  	0.08308440  	0.06746992  
2023-06-04 20:58:04.455: [iter 43 : loss : 0.5674 = 0.1025 + 0.4589 + 0.0060, time: 21.511506]
2023-06-04 20:58:04.721: epoch 43:	0.03050834  	0.08354473  	0.06755929  
2023-06-04 20:58:26.534: [iter 44 : loss : 0.5647 = 0.1001 + 0.4584 + 0.0062, time: 21.809096]
2023-06-04 20:58:26.801: epoch 44:	0.03044912  	0.08310033  	0.06743413  
2023-06-04 20:58:48.686: [iter 45 : loss : 0.5604 = 0.0960 + 0.4581 + 0.0063, time: 21.879175]
2023-06-04 20:58:48.967: epoch 45:	0.03050834  	0.08322024  	0.06743317  
2023-06-04 20:59:10.702: [iter 46 : loss : 0.5574 = 0.0933 + 0.4577 + 0.0065, time: 21.730019]
2023-06-04 20:59:10.979: epoch 46:	0.03046393  	0.08307619  	0.06736754  
2023-06-04 20:59:32.860: [iter 47 : loss : 0.5543 = 0.0904 + 0.4574 + 0.0066, time: 21.877218]
2023-06-04 20:59:33.141: epoch 47:	0.03038989  	0.08276267  	0.06712753  
2023-06-04 20:59:59.184: my pid: 4084
2023-06-04 20:59:59.184: model: model.general_recommender.SGL
2023-06-04 20:59:59.185: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-04 20:59:59.185: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-04 21:00:03.481: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-04 21:00:25.279: [iter 1 : loss : 1.1343 = 0.6931 + 0.4413 + 0.0000, time: 21.797591]
2023-06-04 21:00:25.545: epoch 1:	0.00123630  	0.00223099  	0.00183067  
2023-06-04 21:00:25.546: Find a better model.
2023-06-04 21:00:46.880: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.331312]
2023-06-04 21:00:47.176: epoch 2:	0.00176192  	0.00308313  	0.00265128  
2023-06-04 21:00:47.176: Find a better model.
2023-06-04 21:01:08.421: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.240251]
2023-06-04 21:01:08.711: epoch 3:	0.00208765  	0.00401357  	0.00308850  
2023-06-04 21:01:08.711: Find a better model.
2023-06-04 21:01:30.035: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.320576]
2023-06-04 21:01:30.325: epoch 4:	0.00216909  	0.00490258  	0.00379989  
2023-06-04 21:01:30.325: Find a better model.
2023-06-04 21:01:51.416: [iter 5 : loss : 1.1344 = 0.6927 + 0.4416 + 0.0000, time: 21.088057]
2023-06-04 21:01:51.705: epoch 5:	0.00256885  	0.00558320  	0.00451249  
2023-06-04 21:01:51.705: Find a better model.
2023-06-04 21:02:13.002: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 21.293264]
2023-06-04 21:02:13.291: epoch 6:	0.00309446  	0.00711338  	0.00547966  
2023-06-04 21:02:13.291: Find a better model.
2023-06-04 21:02:34.439: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 21.143539]
2023-06-04 21:02:34.736: epoch 7:	0.00362748  	0.00833921  	0.00699639  
2023-06-04 21:02:34.736: Find a better model.
2023-06-04 21:02:56.020: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 21.281329]
2023-06-04 21:02:56.305: epoch 8:	0.00439739  	0.01126788  	0.00874842  
2023-06-04 21:02:56.305: Find a better model.
2023-06-04 21:03:17.126: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 20.817123]
2023-06-04 21:03:17.410: epoch 9:	0.00518210  	0.01356109  	0.01072247  
2023-06-04 21:03:17.410: Find a better model.
2023-06-04 21:03:38.111: [iter 10 : loss : 1.1345 = 0.6910 + 0.4435 + 0.0000, time: 20.697047]
2023-06-04 21:03:38.395: epoch 10:	0.00633696  	0.01757422  	0.01383409  
2023-06-04 21:03:38.395: Find a better model.
2023-06-04 21:03:58.914: [iter 11 : loss : 1.1340 = 0.6900 + 0.4440 + 0.0000, time: 20.514759]
2023-06-04 21:03:59.193: epoch 11:	0.00745481  	0.02076773  	0.01687998  
2023-06-04 21:03:59.193: Find a better model.
2023-06-04 21:04:19.658: [iter 12 : loss : 1.1328 = 0.6882 + 0.4446 + 0.0000, time: 20.461035]
2023-06-04 21:04:19.945: epoch 12:	0.00845421  	0.02367060  	0.01926976  
2023-06-04 21:04:19.946: Find a better model.
2023-06-04 21:04:40.444: [iter 13 : loss : 1.1309 = 0.6855 + 0.4454 + 0.0000, time: 20.494086]
2023-06-04 21:04:40.720: epoch 13:	0.01000884  	0.02787895  	0.02407049  
2023-06-04 21:04:40.720: Find a better model.
2023-06-04 21:05:01.293: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 20.569194]
2023-06-04 21:05:01.568: epoch 14:	0.01352533  	0.03688197  	0.03223373  
2023-06-04 21:05:01.568: Find a better model.
2023-06-04 21:05:22.006: [iter 15 : loss : 1.1213 = 0.6741 + 0.4470 + 0.0001, time: 20.434179]
2023-06-04 21:05:22.278: epoch 15:	0.01645697  	0.04482881  	0.03904617  
2023-06-04 21:05:22.278: Find a better model.
2023-06-04 21:05:43.249: [iter 16 : loss : 1.1097 = 0.6617 + 0.4479 + 0.0002, time: 20.966079]
2023-06-04 21:05:43.525: epoch 16:	0.01995129  	0.05359112  	0.04662109  
2023-06-04 21:05:43.525: Find a better model.
2023-06-04 21:06:04.652: [iter 17 : loss : 1.0907 = 0.6413 + 0.4491 + 0.0002, time: 21.123245]
2023-06-04 21:06:04.939: epoch 17:	0.02351224  	0.06323377  	0.05441879  
2023-06-04 21:06:04.939: Find a better model.
2023-06-04 21:06:26.248: [iter 18 : loss : 1.0611 = 0.6098 + 0.4509 + 0.0004, time: 21.303150]
2023-06-04 21:06:26.524: epoch 18:	0.02549632  	0.06879053  	0.05871054  
2023-06-04 21:06:26.524: Find a better model.
2023-06-04 21:06:47.783: [iter 19 : loss : 1.0212 = 0.5673 + 0.4533 + 0.0006, time: 21.255250]
2023-06-04 21:06:48.065: epoch 19:	0.02737675  	0.07411635  	0.06219130  
2023-06-04 21:06:48.066: Find a better model.
2023-06-04 21:07:09.399: [iter 20 : loss : 0.9742 = 0.5169 + 0.4565 + 0.0008, time: 21.330564]
2023-06-04 21:07:09.664: epoch 20:	0.02834658  	0.07671427  	0.06424841  
2023-06-04 21:07:09.664: Find a better model.
2023-06-04 21:07:30.784: [iter 21 : loss : 0.9245 = 0.4635 + 0.4599 + 0.0011, time: 21.114509]
2023-06-04 21:07:31.064: epoch 21:	0.02864272  	0.07732572  	0.06486298  
2023-06-04 21:07:31.064: Find a better model.
2023-06-04 21:07:52.193: [iter 22 : loss : 0.8757 = 0.4114 + 0.4629 + 0.0014, time: 21.125135]
2023-06-04 21:07:52.462: epoch 22:	0.02895366  	0.07892740  	0.06538083  
2023-06-04 21:07:52.462: Find a better model.
2023-06-04 21:08:13.751: [iter 23 : loss : 0.8328 = 0.3659 + 0.4652 + 0.0017, time: 21.285457]
2023-06-04 21:08:14.024: epoch 23:	0.02904990  	0.07982054  	0.06604371  
2023-06-04 21:08:14.025: Find a better model.
2023-06-04 21:08:35.195: [iter 24 : loss : 0.7956 = 0.3269 + 0.4668 + 0.0020, time: 21.166127]
2023-06-04 21:08:35.466: epoch 24:	0.02924239  	0.08070387  	0.06657964  
2023-06-04 21:08:35.466: Find a better model.
2023-06-04 21:08:56.819: [iter 25 : loss : 0.7638 = 0.2941 + 0.4675 + 0.0023, time: 21.349030]
2023-06-04 21:08:57.093: epoch 25:	0.02934604  	0.08057535  	0.06651566  
2023-06-04 21:09:18.380: [iter 26 : loss : 0.7374 = 0.2670 + 0.4678 + 0.0026, time: 21.283173]
2023-06-04 21:09:18.650: epoch 26:	0.02953852  	0.08093236  	0.06669993  
2023-06-04 21:09:18.650: Find a better model.
2023-06-04 21:09:39.925: [iter 27 : loss : 0.7146 = 0.2441 + 0.4677 + 0.0028, time: 21.271517]
2023-06-04 21:09:40.194: epoch 27:	0.02961995  	0.08103421  	0.06682995  
2023-06-04 21:09:40.194: Find a better model.
2023-06-04 21:10:01.531: [iter 28 : loss : 0.6951 = 0.2247 + 0.4673 + 0.0031, time: 21.334150]
2023-06-04 21:10:01.801: epoch 28:	0.02964957  	0.08127686  	0.06707800  
2023-06-04 21:10:01.801: Find a better model.
2023-06-04 21:10:22.949: [iter 29 : loss : 0.6786 = 0.2086 + 0.4666 + 0.0033, time: 21.142855]
2023-06-04 21:10:23.214: epoch 29:	0.02972360  	0.08150651  	0.06713993  
2023-06-04 21:10:23.214: Find a better model.
2023-06-04 21:10:44.294: [iter 30 : loss : 0.6636 = 0.1940 + 0.4661 + 0.0036, time: 21.075636]
2023-06-04 21:10:44.558: epoch 30:	0.02976801  	0.08184615  	0.06750403  
2023-06-04 21:10:44.558: Find a better model.
2023-06-04 21:11:05.707: [iter 31 : loss : 0.6508 = 0.1816 + 0.4654 + 0.0038, time: 21.144549]
2023-06-04 21:11:05.979: epoch 31:	0.02984944  	0.08219304  	0.06768423  
2023-06-04 21:11:05.979: Find a better model.
2023-06-04 21:11:27.310: [iter 32 : loss : 0.6395 = 0.1708 + 0.4646 + 0.0040, time: 21.327752]
2023-06-04 21:11:27.575: epoch 32:	0.03006413  	0.08253770  	0.06794889  
2023-06-04 21:11:27.575: Find a better model.
2023-06-04 21:11:48.897: [iter 33 : loss : 0.6300 = 0.1618 + 0.4640 + 0.0042, time: 21.316547]
2023-06-04 21:11:49.162: epoch 33:	0.03008635  	0.08259848  	0.06805347  
2023-06-04 21:11:49.162: Find a better model.
2023-06-04 21:12:10.270: [iter 34 : loss : 0.6205 = 0.1527 + 0.4634 + 0.0044, time: 21.104080]
2023-06-04 21:12:10.534: epoch 34:	0.03001972  	0.08249855  	0.06797463  
2023-06-04 21:12:31.694: [iter 35 : loss : 0.6120 = 0.1447 + 0.4627 + 0.0046, time: 21.156048]
2023-06-04 21:12:31.968: epoch 35:	0.03011596  	0.08291230  	0.06800014  
2023-06-04 21:12:31.968: Find a better model.
2023-06-04 21:12:53.252: [iter 36 : loss : 0.6045 = 0.1376 + 0.4621 + 0.0048, time: 21.280259]
2023-06-04 21:12:53.514: epoch 36:	0.02996791  	0.08262450  	0.06790055  
2023-06-04 21:13:14.885: [iter 37 : loss : 0.5987 = 0.1320 + 0.4616 + 0.0050, time: 21.367016]
2023-06-04 21:13:15.150: epoch 37:	0.02994570  	0.08298393  	0.06797467  
2023-06-04 21:13:15.150: Find a better model.
2023-06-04 21:13:36.470: [iter 38 : loss : 0.5920 = 0.1259 + 0.4609 + 0.0052, time: 21.315986]
2023-06-04 21:13:36.733: epoch 38:	0.03006415  	0.08294331  	0.06805788  
2023-06-04 21:13:57.926: [iter 39 : loss : 0.5869 = 0.1211 + 0.4604 + 0.0053, time: 21.188274]
2023-06-04 21:13:58.191: epoch 39:	0.03005674  	0.08301548  	0.06808630  
2023-06-04 21:13:58.191: Find a better model.
2023-06-04 21:14:19.486: [iter 40 : loss : 0.5814 = 0.1158 + 0.4600 + 0.0055, time: 21.290179]
2023-06-04 21:14:19.749: epoch 40:	0.03005674  	0.08283225  	0.06802759  
2023-06-04 21:14:41.242: [iter 41 : loss : 0.5765 = 0.1112 + 0.4596 + 0.0057, time: 21.489122]
2023-06-04 21:14:41.506: epoch 41:	0.02999752  	0.08238595  	0.06797639  
2023-06-04 21:15:03.010: [iter 42 : loss : 0.5720 = 0.1070 + 0.4591 + 0.0058, time: 21.500031]
2023-06-04 21:15:03.274: epoch 42:	0.03003455  	0.08230925  	0.06789012  
2023-06-04 21:15:24.798: [iter 43 : loss : 0.5677 = 0.1029 + 0.4588 + 0.0060, time: 21.521142]
2023-06-04 21:15:25.068: epoch 43:	0.02992349  	0.08190727  	0.06773783  
2023-06-04 21:15:46.393: [iter 44 : loss : 0.5648 = 0.1004 + 0.4583 + 0.0062, time: 21.320095]
2023-06-04 21:15:46.657: epoch 44:	0.02983465  	0.08192991  	0.06747568  
2023-06-04 21:16:08.419: [iter 45 : loss : 0.5608 = 0.0964 + 0.4581 + 0.0063, time: 21.757627]
2023-06-04 21:16:08.682: epoch 45:	0.02979764  	0.08137887  	0.06736636  
2023-06-04 21:16:30.201: [iter 46 : loss : 0.5576 = 0.0935 + 0.4576 + 0.0065, time: 21.515952]
2023-06-04 21:16:30.467: epoch 46:	0.02975322  	0.08110285  	0.06712046  
2023-06-04 21:16:51.978: [iter 47 : loss : 0.5544 = 0.0905 + 0.4573 + 0.0066, time: 21.507371]
2023-06-04 21:16:52.241: epoch 47:	0.02975322  	0.08114871  	0.06714335  
2023-06-04 21:17:13.559: [iter 48 : loss : 0.5518 = 0.0881 + 0.4569 + 0.0067, time: 21.314266]
2023-06-04 21:17:13.822: epoch 48:	0.02971620  	0.08060805  	0.06711549  
2023-06-04 21:17:35.185: [iter 49 : loss : 0.5491 = 0.0855 + 0.4566 + 0.0069, time: 21.359122]
2023-06-04 21:17:35.451: epoch 49:	0.02970880  	0.08039955  	0.06693667  
2023-06-04 21:17:56.955: [iter 50 : loss : 0.5466 = 0.0832 + 0.4564 + 0.0070, time: 21.499032]
2023-06-04 21:17:57.220: epoch 50:	0.02956075  	0.07987240  	0.06663413  
2023-06-04 21:18:18.379: [iter 51 : loss : 0.5439 = 0.0806 + 0.4561 + 0.0072, time: 21.153405]
2023-06-04 21:18:18.642: epoch 51:	0.02950892  	0.07987080  	0.06647956  
2023-06-04 21:18:39.998: [iter 52 : loss : 0.5416 = 0.0785 + 0.4558 + 0.0073, time: 21.352986]
2023-06-04 21:18:40.262: epoch 52:	0.02954594  	0.07951082  	0.06633354  
2023-06-04 21:19:01.761: [iter 53 : loss : 0.5406 = 0.0776 + 0.4556 + 0.0074, time: 21.495088]
2023-06-04 21:19:02.027: epoch 53:	0.02951632  	0.07980164  	0.06628564  
2023-06-04 21:19:23.361: [iter 54 : loss : 0.5377 = 0.0747 + 0.4554 + 0.0076, time: 21.330267]
2023-06-04 21:19:23.624: epoch 54:	0.02945709  	0.07945441  	0.06598867  
2023-06-04 21:19:44.762: [iter 55 : loss : 0.5360 = 0.0732 + 0.4552 + 0.0077, time: 21.133484]
2023-06-04 21:19:45.026: epoch 55:	0.02924240  	0.07859526  	0.06550737  
2023-06-04 21:20:06.516: [iter 56 : loss : 0.5341 = 0.0714 + 0.4549 + 0.0078, time: 21.486074]
2023-06-04 21:20:06.780: epoch 56:	0.02907953  	0.07807928  	0.06534748  
2023-06-04 21:20:28.091: [iter 57 : loss : 0.5322 = 0.0695 + 0.4548 + 0.0079, time: 21.307070]
2023-06-04 21:20:28.354: epoch 57:	0.02899810  	0.07787546  	0.06526553  
2023-06-04 21:20:49.496: [iter 58 : loss : 0.5307 = 0.0681 + 0.4546 + 0.0080, time: 21.139016]
2023-06-04 21:20:49.761: epoch 58:	0.02902030  	0.07766546  	0.06513233  
2023-06-04 21:21:10.898: [iter 59 : loss : 0.5292 = 0.0667 + 0.4544 + 0.0082, time: 21.133026]
2023-06-04 21:21:11.160: epoch 59:	0.02893887  	0.07743806  	0.06507040  
2023-06-04 21:21:32.307: [iter 60 : loss : 0.5279 = 0.0654 + 0.4543 + 0.0083, time: 21.142233]
2023-06-04 21:21:32.567: epoch 60:	0.02895367  	0.07756010  	0.06495983  
2023-06-04 21:21:53.684: [iter 61 : loss : 0.5267 = 0.0642 + 0.4541 + 0.0084, time: 21.112433]
2023-06-04 21:21:53.956: epoch 61:	0.02896108  	0.07727098  	0.06490558  
2023-06-04 21:22:15.277: [iter 62 : loss : 0.5253 = 0.0629 + 0.4539 + 0.0085, time: 21.316118]
2023-06-04 21:22:15.538: epoch 62:	0.02890185  	0.07720955  	0.06491196  
2023-06-04 21:22:36.666: [iter 63 : loss : 0.5241 = 0.0616 + 0.4539 + 0.0086, time: 21.123394]
2023-06-04 21:22:36.940: epoch 63:	0.02878341  	0.07663643  	0.06458902  
2023-06-04 21:22:58.252: [iter 64 : loss : 0.5230 = 0.0607 + 0.4537 + 0.0087, time: 21.307657]
2023-06-04 21:22:58.515: epoch 64:	0.02866494  	0.07646418  	0.06441101  
2023-06-04 21:22:58.515: Early stopping is trigger at epoch: 64
2023-06-04 21:22:58.515: best_result@epoch 39:

2023-06-04 21:22:58.515: 		0.0301      	0.0830      	0.0681      
2023-06-05 10:41:35.231: my pid: 14684
2023-06-05 10:41:35.231: model: model.general_recommender.SGL
2023-06-05 10:41:35.231: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 10:41:35.231: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 10:41:39.352: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 10:42:00.835: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.483041]
2023-06-05 10:42:01.111: epoch 1:	0.00149541  	0.00331865  	0.00249180  
2023-06-05 10:42:01.111: Find a better model.
2023-06-05 10:42:23.006: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.890250]
2023-06-05 10:42:23.301: epoch 2:	0.00184335  	0.00355939  	0.00286668  
2023-06-05 10:42:23.301: Find a better model.
2023-06-05 10:42:45.225: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.921047]
2023-06-05 10:42:45.520: epoch 3:	0.00181374  	0.00381438  	0.00313240  
2023-06-05 10:42:45.520: Find a better model.
2023-06-05 10:43:07.570: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 22.046007]
2023-06-05 10:43:07.863: epoch 4:	0.00225792  	0.00498765  	0.00388928  
2023-06-05 10:43:07.863: Find a better model.
2023-06-05 10:43:29.800: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.933243]
2023-06-05 10:43:30.101: epoch 5:	0.00248001  	0.00510824  	0.00423793  
2023-06-05 10:43:30.101: Find a better model.
2023-06-05 10:43:51.832: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 21.728014]
2023-06-05 10:43:52.123: epoch 6:	0.00310927  	0.00688189  	0.00551158  
2023-06-05 10:43:52.123: Find a better model.
2023-06-05 10:44:13.821: [iter 7 : loss : 1.1345 = 0.6923 + 0.4422 + 0.0000, time: 21.693891]
2023-06-05 10:44:14.122: epoch 7:	0.00374593  	0.00913534  	0.00725597  
2023-06-05 10:44:14.122: Find a better model.
2023-06-05 10:44:35.949: [iter 8 : loss : 1.1346 = 0.6920 + 0.4425 + 0.0000, time: 21.822683]
2023-06-05 10:44:36.240: epoch 8:	0.00465649  	0.01205722  	0.00915721  
2023-06-05 10:44:36.241: Find a better model.
2023-06-05 10:44:57.788: [iter 9 : loss : 1.1346 = 0.6916 + 0.4429 + 0.0000, time: 21.543993]
2023-06-05 10:44:58.081: epoch 9:	0.00545601  	0.01437823  	0.01117027  
2023-06-05 10:44:58.081: Find a better model.
2023-06-05 10:45:19.733: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 21.648122]
2023-06-05 10:45:20.030: epoch 10:	0.00647022  	0.01813536  	0.01450604  
2023-06-05 10:45:20.030: Find a better model.
2023-06-05 10:45:41.738: [iter 11 : loss : 1.1340 = 0.6901 + 0.4439 + 0.0000, time: 21.703558]
2023-06-05 10:45:42.041: epoch 11:	0.00797301  	0.02256414  	0.01814627  
2023-06-05 10:45:42.042: Find a better model.
2023-06-05 10:46:03.731: [iter 12 : loss : 1.1330 = 0.6885 + 0.4444 + 0.0000, time: 21.685208]
2023-06-05 10:46:04.028: epoch 12:	0.00920190  	0.02556888  	0.02140248  
2023-06-05 10:46:04.029: Find a better model.
2023-06-05 10:46:25.751: [iter 13 : loss : 1.1312 = 0.6859 + 0.4452 + 0.0000, time: 21.718992]
2023-06-05 10:46:26.050: epoch 13:	0.01116372  	0.03077773  	0.02599533  
2023-06-05 10:46:26.050: Find a better model.
2023-06-05 10:46:47.717: [iter 14 : loss : 1.1277 = 0.6816 + 0.4460 + 0.0001, time: 21.662528]
2023-06-05 10:46:48.010: epoch 14:	0.01393991  	0.03707684  	0.03249415  
2023-06-05 10:46:48.010: Find a better model.
2023-06-05 10:47:09.717: [iter 15 : loss : 1.1216 = 0.6747 + 0.4468 + 0.0001, time: 21.703476]
2023-06-05 10:47:10.001: epoch 15:	0.01693819  	0.04479190  	0.03957368  
2023-06-05 10:47:10.001: Find a better model.
2023-06-05 10:47:32.131: [iter 16 : loss : 1.1103 = 0.6625 + 0.4477 + 0.0002, time: 22.125094]
2023-06-05 10:47:32.415: epoch 16:	0.02052876  	0.05427679  	0.04736171  
2023-06-05 10:47:32.415: Find a better model.
2023-06-05 10:47:54.672: [iter 17 : loss : 1.0916 = 0.6424 + 0.4490 + 0.0002, time: 22.252330]
2023-06-05 10:47:54.967: epoch 17:	0.02380841  	0.06317627  	0.05395497  
2023-06-05 10:47:54.967: Find a better model.
2023-06-05 10:48:17.250: [iter 18 : loss : 1.0623 = 0.6111 + 0.4508 + 0.0004, time: 22.277148]
2023-06-05 10:48:17.529: epoch 18:	0.02595535  	0.06844798  	0.05837818  
2023-06-05 10:48:17.529: Find a better model.
2023-06-05 10:48:39.697: [iter 19 : loss : 1.0226 = 0.5689 + 0.4531 + 0.0006, time: 22.164175]
2023-06-05 10:48:39.992: epoch 19:	0.02751002  	0.07243815  	0.06171928  
2023-06-05 10:48:39.992: Find a better model.
2023-06-05 10:49:02.407: [iter 20 : loss : 0.9756 = 0.5186 + 0.4562 + 0.0008, time: 22.410166]
2023-06-05 10:49:02.705: epoch 20:	0.02819853  	0.07405736  	0.06328839  
2023-06-05 10:49:02.705: Find a better model.
2023-06-05 10:49:24.862: [iter 21 : loss : 0.9258 = 0.4652 + 0.4595 + 0.0011, time: 22.153126]
2023-06-05 10:49:25.140: epoch 21:	0.02876857  	0.07551992  	0.06377744  
2023-06-05 10:49:25.141: Find a better model.
2023-06-05 10:49:47.202: [iter 22 : loss : 0.8771 = 0.4132 + 0.4625 + 0.0014, time: 22.058262]
2023-06-05 10:49:47.484: epoch 22:	0.02910170  	0.07683279  	0.06447178  
2023-06-05 10:49:47.484: Find a better model.
2023-06-05 10:50:09.864: [iter 23 : loss : 0.8340 = 0.3675 + 0.4649 + 0.0017, time: 22.376955]
2023-06-05 10:50:10.165: epoch 23:	0.02915353  	0.07748742  	0.06471343  
2023-06-05 10:50:10.165: Find a better model.
2023-06-05 10:50:32.282: [iter 24 : loss : 0.7966 = 0.3282 + 0.4664 + 0.0020, time: 22.112176]
2023-06-05 10:50:32.585: epoch 24:	0.02935342  	0.07786959  	0.06499156  
2023-06-05 10:50:32.585: Find a better model.
2023-06-05 10:50:54.836: [iter 25 : loss : 0.7646 = 0.2952 + 0.4672 + 0.0023, time: 22.247492]
2023-06-05 10:50:55.113: epoch 25:	0.02949408  	0.07830234  	0.06527472  
2023-06-05 10:50:55.114: Find a better model.
2023-06-05 10:51:17.421: [iter 26 : loss : 0.7378 = 0.2678 + 0.4675 + 0.0026, time: 22.302531]
2023-06-05 10:51:17.697: epoch 26:	0.02944227  	0.07886854  	0.06550897  
2023-06-05 10:51:17.697: Find a better model.
2023-06-05 10:51:40.016: [iter 27 : loss : 0.7147 = 0.2446 + 0.4673 + 0.0028, time: 22.315700]
2023-06-05 10:51:40.292: epoch 27:	0.02950150  	0.07886157  	0.06550054  
2023-06-05 10:52:02.435: [iter 28 : loss : 0.6952 = 0.2252 + 0.4669 + 0.0031, time: 22.139927]
2023-06-05 10:52:02.717: epoch 28:	0.02964217  	0.07934628  	0.06584417  
2023-06-05 10:52:02.717: Find a better model.
2023-06-05 10:52:25.012: [iter 29 : loss : 0.6786 = 0.2091 + 0.4662 + 0.0033, time: 22.289992]
2023-06-05 10:52:25.302: epoch 29:	0.02982724  	0.08036692  	0.06615464  
2023-06-05 10:52:25.302: Find a better model.
2023-06-05 10:52:47.540: [iter 30 : loss : 0.6636 = 0.1944 + 0.4657 + 0.0036, time: 22.234057]
2023-06-05 10:52:47.809: epoch 30:	0.02976061  	0.08029694  	0.06614944  
2023-06-05 10:53:09.962: [iter 31 : loss : 0.6506 = 0.1817 + 0.4651 + 0.0038, time: 22.148581]
2023-06-05 10:53:10.231: epoch 31:	0.02979023  	0.08024772  	0.06623111  
2023-06-05 10:53:32.546: [iter 32 : loss : 0.6396 = 0.1711 + 0.4646 + 0.0040, time: 22.311329]
2023-06-05 10:53:32.815: epoch 32:	0.02973101  	0.08055273  	0.06634196  
2023-06-05 10:53:32.815: Find a better model.
2023-06-05 10:53:55.137: [iter 33 : loss : 0.6297 = 0.1617 + 0.4638 + 0.0042, time: 22.318258]
2023-06-05 10:53:55.409: epoch 33:	0.02979762  	0.08042504  	0.06646413  
2023-06-05 10:54:17.411: [iter 34 : loss : 0.6205 = 0.1530 + 0.4631 + 0.0044, time: 21.998450]
2023-06-05 10:54:17.685: epoch 34:	0.02981244  	0.08010953  	0.06655128  
2023-06-05 10:54:39.652: [iter 35 : loss : 0.6117 = 0.1447 + 0.4624 + 0.0046, time: 21.963010]
2023-06-05 10:54:39.934: epoch 35:	0.02992347  	0.08027320  	0.06683517  
2023-06-05 10:55:01.663: [iter 36 : loss : 0.6046 = 0.1379 + 0.4619 + 0.0048, time: 21.725329]
2023-06-05 10:55:01.950: epoch 36:	0.02987906  	0.08002397  	0.06682143  
2023-06-05 10:55:23.642: [iter 37 : loss : 0.5983 = 0.1320 + 0.4613 + 0.0050, time: 21.686011]
2023-06-05 10:55:23.924: epoch 37:	0.02995309  	0.07991662  	0.06676896  
2023-06-05 10:55:45.649: [iter 38 : loss : 0.5918 = 0.1258 + 0.4608 + 0.0052, time: 21.721875]
2023-06-05 10:55:45.929: epoch 38:	0.02988647  	0.07950439  	0.06676526  
2023-06-05 10:56:07.817: [iter 39 : loss : 0.5867 = 0.1212 + 0.4602 + 0.0053, time: 21.883509]
2023-06-05 10:56:08.091: epoch 39:	0.03000492  	0.07945839  	0.06680643  
2023-06-05 10:56:29.801: [iter 40 : loss : 0.5812 = 0.1160 + 0.4597 + 0.0055, time: 21.705665]
2023-06-05 10:56:30.079: epoch 40:	0.02999753  	0.07958525  	0.06679979  
2023-06-05 10:56:52.102: [iter 41 : loss : 0.5759 = 0.1111 + 0.4591 + 0.0057, time: 22.019107]
2023-06-05 10:56:52.371: epoch 41:	0.02987907  	0.07932273  	0.06673688  
2023-06-05 10:57:14.362: [iter 42 : loss : 0.5718 = 0.1071 + 0.4589 + 0.0058, time: 21.987900]
2023-06-05 10:57:14.635: epoch 42:	0.02979762  	0.07857184  	0.06647819  
2023-06-05 10:57:36.591: [iter 43 : loss : 0.5678 = 0.1033 + 0.4585 + 0.0060, time: 21.953441]
2023-06-05 10:57:36.867: epoch 43:	0.02984205  	0.07851935  	0.06635322  
2023-06-05 10:57:58.769: [iter 44 : loss : 0.5648 = 0.1005 + 0.4581 + 0.0062, time: 21.897469]
2023-06-05 10:57:59.048: epoch 44:	0.02996050  	0.07834870  	0.06643953  
2023-06-05 10:58:20.769: [iter 45 : loss : 0.5606 = 0.0965 + 0.4577 + 0.0063, time: 21.718057]
2023-06-05 10:58:21.050: epoch 45:	0.02980503  	0.07835964  	0.06628073  
2023-06-05 10:58:42.772: [iter 46 : loss : 0.5574 = 0.0935 + 0.4574 + 0.0065, time: 21.718222]
2023-06-05 10:58:43.052: epoch 46:	0.02978283  	0.07837913  	0.06613278  
2023-06-05 10:59:04.793: [iter 47 : loss : 0.5541 = 0.0904 + 0.4571 + 0.0066, time: 21.738299]
2023-06-05 10:59:05.071: epoch 47:	0.02970139  	0.07837280  	0.06610361  
2023-06-05 10:59:27.143: [iter 48 : loss : 0.5516 = 0.0881 + 0.4567 + 0.0068, time: 22.068026]
2023-06-05 10:59:27.413: epoch 48:	0.02973101  	0.07873537  	0.06613978  
2023-06-05 10:59:49.341: [iter 49 : loss : 0.5488 = 0.0855 + 0.4564 + 0.0069, time: 21.924376]
2023-06-05 10:59:49.615: epoch 49:	0.02955334  	0.07773422  	0.06569694  
2023-06-05 11:00:11.515: [iter 50 : loss : 0.5465 = 0.0833 + 0.4561 + 0.0070, time: 21.896592]
2023-06-05 11:00:11.786: epoch 50:	0.02963477  	0.07786838  	0.06570920  
2023-06-05 11:00:33.718: [iter 51 : loss : 0.5438 = 0.0808 + 0.4559 + 0.0072, time: 21.927090]
2023-06-05 11:00:33.991: epoch 51:	0.02954594  	0.07782274  	0.06553362  
2023-06-05 11:00:55.681: [iter 52 : loss : 0.5412 = 0.0783 + 0.4556 + 0.0073, time: 21.686404]
2023-06-05 11:00:55.964: epoch 52:	0.02949411  	0.07768618  	0.06556479  
2023-06-05 11:01:17.670: [iter 53 : loss : 0.5405 = 0.0775 + 0.4555 + 0.0074, time: 21.703793]
2023-06-05 11:01:17.948: epoch 53:	0.02939046  	0.07743753  	0.06528983  
2023-06-05 11:01:39.736: [iter 54 : loss : 0.5379 = 0.0751 + 0.4552 + 0.0076, time: 21.782145]
2023-06-05 11:01:40.007: epoch 54:	0.02936085  	0.07720922  	0.06517112  
2023-06-05 11:02:01.907: [iter 55 : loss : 0.5357 = 0.0730 + 0.4549 + 0.0077, time: 21.896746]
2023-06-05 11:02:02.181: epoch 55:	0.02924240  	0.07660253  	0.06506292  
2023-06-05 11:02:23.943: [iter 56 : loss : 0.5340 = 0.0714 + 0.4547 + 0.0078, time: 21.757121]
2023-06-05 11:02:24.216: epoch 56:	0.02908693  	0.07633527  	0.06483777  
2023-06-05 11:02:46.109: [iter 57 : loss : 0.5321 = 0.0696 + 0.4546 + 0.0079, time: 21.890531]
2023-06-05 11:02:46.383: epoch 57:	0.02899069  	0.07595723  	0.06456599  
2023-06-05 11:02:46.383: Early stopping is trigger at epoch: 57
2023-06-05 11:02:46.383: best_result@epoch 32:

2023-06-05 11:02:46.383: 		0.0297      	0.0806      	0.0663      
2023-06-05 11:03:07.792: my pid: 7936
2023-06-05 11:03:07.792: model: model.general_recommender.SGL
2023-06-05 11:03:07.792: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 11:03:07.792: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 11:03:11.866: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 11:03:33.159: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.292725]
2023-06-05 11:03:33.431: epoch 1:	0.00145099  	0.00325606  	0.00242279  
2023-06-05 11:03:33.431: Find a better model.
2023-06-05 11:03:54.945: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.510324]
2023-06-05 11:03:55.224: epoch 2:	0.00176192  	0.00298717  	0.00264615  
2023-06-05 11:04:16.731: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.502298]
2023-06-05 11:04:17.035: epoch 3:	0.00163607  	0.00352593  	0.00263063  
2023-06-05 11:04:17.035: Find a better model.
2023-06-05 11:04:38.923: [iter 4 : loss : 1.1340 = 0.6928 + 0.4412 + 0.0000, time: 21.883878]
2023-06-05 11:04:39.216: epoch 4:	0.00227273  	0.00480561  	0.00388199  
2023-06-05 11:04:39.216: Find a better model.
2023-06-05 11:05:00.526: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.307642]
2023-06-05 11:05:00.817: epoch 5:	0.00250963  	0.00550808  	0.00462481  
2023-06-05 11:05:00.817: Find a better model.
2023-06-05 11:05:22.287: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.466004]
2023-06-05 11:05:22.579: epoch 6:	0.00298342  	0.00725341  	0.00532330  
2023-06-05 11:05:22.579: Find a better model.
2023-06-05 11:05:43.878: [iter 7 : loss : 1.1344 = 0.6924 + 0.4421 + 0.0000, time: 21.295824]
2023-06-05 11:05:44.167: epoch 7:	0.00393841  	0.01045730  	0.00782869  
2023-06-05 11:05:44.167: Find a better model.
2023-06-05 11:06:05.453: [iter 8 : loss : 1.1345 = 0.6921 + 0.4424 + 0.0000, time: 21.282401]
2023-06-05 11:06:05.740: epoch 8:	0.00446402  	0.01182510  	0.00905696  
2023-06-05 11:06:05.740: Find a better model.
2023-06-05 11:06:26.680: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 20.936495]
2023-06-05 11:06:26.976: epoch 9:	0.00567810  	0.01582120  	0.01256912  
2023-06-05 11:06:26.976: Find a better model.
2023-06-05 11:06:48.040: [iter 10 : loss : 1.1343 = 0.6911 + 0.4432 + 0.0000, time: 21.061093]
2023-06-05 11:06:48.324: epoch 10:	0.00624813  	0.01764989  	0.01405995  
2023-06-05 11:06:48.324: Find a better model.
2023-06-05 11:07:09.503: [iter 11 : loss : 1.1340 = 0.6903 + 0.4437 + 0.0000, time: 21.176067]
2023-06-05 11:07:09.786: epoch 11:	0.00751403  	0.02120644  	0.01697244  
2023-06-05 11:07:09.786: Find a better model.
2023-06-05 11:07:31.013: [iter 12 : loss : 1.1332 = 0.6889 + 0.4443 + 0.0000, time: 21.224068]
2023-06-05 11:07:31.297: epoch 12:	0.00909086  	0.02564302  	0.02088613  
2023-06-05 11:07:31.297: Find a better model.
2023-06-05 11:07:52.641: [iter 13 : loss : 1.1315 = 0.6865 + 0.4450 + 0.0000, time: 21.339570]
2023-06-05 11:07:52.940: epoch 13:	0.01078617  	0.03033598  	0.02562525  
2023-06-05 11:07:52.940: Find a better model.
2023-06-05 11:08:13.997: [iter 14 : loss : 1.1283 = 0.6825 + 0.4457 + 0.0001, time: 21.053057]
2023-06-05 11:08:14.282: epoch 14:	0.01342168  	0.03627261  	0.03184107  
2023-06-05 11:08:14.282: Find a better model.
2023-06-05 11:08:35.421: [iter 15 : loss : 1.1227 = 0.6761 + 0.4465 + 0.0001, time: 21.134126]
2023-06-05 11:08:35.702: epoch 15:	0.01636814  	0.04393560  	0.03796174  
2023-06-05 11:08:35.702: Find a better model.
2023-06-05 11:08:57.393: [iter 16 : loss : 1.1123 = 0.6649 + 0.4473 + 0.0001, time: 21.687397]
2023-06-05 11:08:57.674: epoch 16:	0.01963294  	0.05234865  	0.04515050  
2023-06-05 11:08:57.674: Find a better model.
2023-06-05 11:09:19.375: [iter 17 : loss : 1.0951 = 0.6463 + 0.4486 + 0.0002, time: 21.696261]
2023-06-05 11:09:19.658: epoch 17:	0.02314209  	0.06144200  	0.05245206  
2023-06-05 11:09:19.658: Find a better model.
2023-06-05 11:09:41.152: [iter 18 : loss : 1.0675 = 0.6170 + 0.4502 + 0.0004, time: 21.491677]
2023-06-05 11:09:41.435: epoch 18:	0.02539269  	0.06710039  	0.05728559  
2023-06-05 11:09:41.435: Find a better model.
2023-06-05 11:10:03.361: [iter 19 : loss : 1.0296 = 0.5766 + 0.4524 + 0.0005, time: 21.922944]
2023-06-05 11:10:03.645: epoch 19:	0.02714726  	0.07184992  	0.06117341  
2023-06-05 11:10:03.645: Find a better model.
2023-06-05 11:10:25.346: [iter 20 : loss : 0.9830 = 0.5271 + 0.4552 + 0.0008, time: 21.698081]
2023-06-05 11:10:25.634: epoch 20:	0.02837619  	0.07546358  	0.06351335  
2023-06-05 11:10:25.634: Find a better model.
2023-06-05 11:10:47.358: [iter 21 : loss : 0.9337 = 0.4740 + 0.4587 + 0.0010, time: 21.720540]
2023-06-05 11:10:47.639: epoch 21:	0.02908690  	0.07794006  	0.06478246  
2023-06-05 11:10:47.639: Find a better model.
2023-06-05 11:11:09.357: [iter 22 : loss : 0.8840 = 0.4209 + 0.4617 + 0.0013, time: 21.714753]
2023-06-05 11:11:09.640: epoch 22:	0.02924238  	0.07894745  	0.06517439  
2023-06-05 11:11:09.641: Find a better model.
2023-06-05 11:11:31.316: [iter 23 : loss : 0.8397 = 0.3738 + 0.4643 + 0.0016, time: 21.672520]
2023-06-05 11:11:31.589: epoch 23:	0.02937564  	0.07981101  	0.06546673  
2023-06-05 11:11:31.589: Find a better model.
2023-06-05 11:11:53.329: [iter 24 : loss : 0.8012 = 0.3332 + 0.4661 + 0.0019, time: 21.735945]
2023-06-05 11:11:53.601: epoch 24:	0.02944227  	0.08058774  	0.06562325  
2023-06-05 11:11:53.601: Find a better model.
2023-06-05 11:12:15.513: [iter 25 : loss : 0.7682 = 0.2991 + 0.4669 + 0.0022, time: 21.906303]
2023-06-05 11:12:15.788: epoch 25:	0.02967917  	0.08116920  	0.06600010  
2023-06-05 11:12:15.788: Find a better model.
2023-06-05 11:12:37.544: [iter 26 : loss : 0.7408 = 0.2711 + 0.4672 + 0.0025, time: 21.752494]
2023-06-05 11:12:37.814: epoch 26:	0.02993088  	0.08182049  	0.06635725  
2023-06-05 11:12:37.814: Find a better model.
2023-06-05 11:12:59.715: [iter 27 : loss : 0.7172 = 0.2474 + 0.4671 + 0.0028, time: 21.896517]
2023-06-05 11:13:00.001: epoch 27:	0.02996789  	0.08211358  	0.06660306  
2023-06-05 11:13:00.001: Find a better model.
2023-06-05 11:13:21.898: [iter 28 : loss : 0.6975 = 0.2275 + 0.4669 + 0.0030, time: 21.893065]
2023-06-05 11:13:22.168: epoch 28:	0.03001972  	0.08228544  	0.06667098  
2023-06-05 11:13:22.168: Find a better model.
2023-06-05 11:13:43.913: [iter 29 : loss : 0.6804 = 0.2109 + 0.4662 + 0.0033, time: 21.741765]
2023-06-05 11:13:44.189: epoch 29:	0.03001972  	0.08248150  	0.06699933  
2023-06-05 11:13:44.189: Find a better model.
2023-06-05 11:14:05.718: [iter 30 : loss : 0.6651 = 0.1958 + 0.4657 + 0.0035, time: 21.524082]
2023-06-05 11:14:05.999: epoch 30:	0.03008634  	0.08259530  	0.06708942  
2023-06-05 11:14:05.999: Find a better model.
2023-06-05 11:14:27.728: [iter 31 : loss : 0.6518 = 0.1830 + 0.4650 + 0.0038, time: 21.726702]
2023-06-05 11:14:28.003: epoch 31:	0.03010115  	0.08317610  	0.06736393  
2023-06-05 11:14:28.003: Find a better model.
2023-06-05 11:14:49.667: [iter 32 : loss : 0.6405 = 0.1721 + 0.4644 + 0.0040, time: 21.659664]
2023-06-05 11:14:49.951: epoch 32:	0.03022702  	0.08311676  	0.06749191  
2023-06-05 11:15:11.667: [iter 33 : loss : 0.6304 = 0.1626 + 0.4636 + 0.0042, time: 21.712052]
2023-06-05 11:15:11.952: epoch 33:	0.03012337  	0.08277356  	0.06748371  
2023-06-05 11:15:34.015: [iter 34 : loss : 0.6216 = 0.1540 + 0.4632 + 0.0044, time: 22.059197]
2023-06-05 11:15:34.289: epoch 34:	0.03019000  	0.08288959  	0.06747493  
2023-06-05 11:15:56.039: [iter 35 : loss : 0.6120 = 0.1452 + 0.4623 + 0.0046, time: 21.746129]
2023-06-05 11:15:56.306: epoch 35:	0.03023443  	0.08288671  	0.06754848  
2023-06-05 11:16:18.236: [iter 36 : loss : 0.6054 = 0.1390 + 0.4617 + 0.0048, time: 21.926928]
2023-06-05 11:16:18.505: epoch 36:	0.03030846  	0.08291620  	0.06745887  
2023-06-05 11:16:40.599: [iter 37 : loss : 0.5988 = 0.1326 + 0.4612 + 0.0050, time: 22.090143]
2023-06-05 11:16:40.870: epoch 37:	0.03041209  	0.08325628  	0.06755825  
2023-06-05 11:16:40.870: Find a better model.
2023-06-05 11:17:02.600: [iter 38 : loss : 0.5920 = 0.1263 + 0.4606 + 0.0052, time: 21.726175]
2023-06-05 11:17:02.868: epoch 38:	0.03034546  	0.08270195  	0.06742010  
2023-06-05 11:17:24.825: [iter 39 : loss : 0.5869 = 0.1215 + 0.4601 + 0.0053, time: 21.954049]
2023-06-05 11:17:25.102: epoch 39:	0.03029364  	0.08260087  	0.06753669  
2023-06-05 11:17:46.971: [iter 40 : loss : 0.5812 = 0.1162 + 0.4595 + 0.0055, time: 21.865202]
2023-06-05 11:17:47.242: epoch 40:	0.03037507  	0.08292166  	0.06767989  
2023-06-05 11:18:08.988: [iter 41 : loss : 0.5765 = 0.1116 + 0.4592 + 0.0057, time: 21.742049]
2023-06-05 11:18:09.258: epoch 41:	0.03033065  	0.08276600  	0.06752794  
2023-06-05 11:18:31.353: [iter 42 : loss : 0.5718 = 0.1072 + 0.4588 + 0.0058, time: 22.090540]
2023-06-05 11:18:31.628: epoch 42:	0.03034545  	0.08304702  	0.06749260  
2023-06-05 11:18:53.822: [iter 43 : loss : 0.5679 = 0.1036 + 0.4583 + 0.0060, time: 22.191143]
2023-06-05 11:18:54.097: epoch 43:	0.03013816  	0.08227339  	0.06711966  
2023-06-05 11:19:16.001: [iter 44 : loss : 0.5649 = 0.1008 + 0.4580 + 0.0061, time: 21.899667]
2023-06-05 11:19:16.271: epoch 44:	0.03016777  	0.08201868  	0.06703372  
2023-06-05 11:19:38.365: [iter 45 : loss : 0.5610 = 0.0971 + 0.4576 + 0.0063, time: 22.091014]
2023-06-05 11:19:38.634: epoch 45:	0.03016777  	0.08158097  	0.06685249  
2023-06-05 11:20:00.799: [iter 46 : loss : 0.5579 = 0.0943 + 0.4572 + 0.0064, time: 22.162491]
2023-06-05 11:20:01.072: epoch 46:	0.03018997  	0.08151278  	0.06679889  
2023-06-05 11:20:23.169: [iter 47 : loss : 0.5545 = 0.0910 + 0.4569 + 0.0066, time: 22.094166]
2023-06-05 11:20:23.439: epoch 47:	0.03016036  	0.08141306  	0.06668600  
2023-06-05 11:20:45.591: [iter 48 : loss : 0.5519 = 0.0886 + 0.4566 + 0.0067, time: 22.147785]
2023-06-05 11:20:45.859: epoch 48:	0.03010114  	0.08108512  	0.06670015  
2023-06-05 11:21:07.942: [iter 49 : loss : 0.5490 = 0.0860 + 0.4562 + 0.0069, time: 22.078733]
2023-06-05 11:21:08.219: epoch 49:	0.03012334  	0.08079596  	0.06657442  
2023-06-05 11:21:30.354: [iter 50 : loss : 0.5467 = 0.0837 + 0.4560 + 0.0070, time: 22.130079]
2023-06-05 11:21:30.625: epoch 50:	0.03001969  	0.08066655  	0.06635933  
2023-06-05 11:21:52.528: [iter 51 : loss : 0.5440 = 0.0812 + 0.4557 + 0.0072, time: 21.899008]
2023-06-05 11:21:52.804: epoch 51:	0.02992345  	0.08035016  	0.06628860  
2023-06-05 11:22:14.703: [iter 52 : loss : 0.5413 = 0.0787 + 0.4554 + 0.0073, time: 21.895920]
2023-06-05 11:22:14.986: epoch 52:	0.02989384  	0.08016557  	0.06611480  
2023-06-05 11:22:37.509: [iter 53 : loss : 0.5399 = 0.0773 + 0.4552 + 0.0074, time: 22.518943]
2023-06-05 11:22:37.780: epoch 53:	0.02976059  	0.07939613  	0.06573170  
2023-06-05 11:22:59.731: [iter 54 : loss : 0.5379 = 0.0754 + 0.4549 + 0.0075, time: 21.947355]
2023-06-05 11:23:00.008: epoch 54:	0.02979020  	0.07931891  	0.06575416  
2023-06-05 11:23:22.087: [iter 55 : loss : 0.5361 = 0.0737 + 0.4547 + 0.0077, time: 22.074408]
2023-06-05 11:23:22.362: epoch 55:	0.02987904  	0.07954733  	0.06588680  
2023-06-05 11:23:44.491: [iter 56 : loss : 0.5339 = 0.0715 + 0.4546 + 0.0078, time: 22.125043]
2023-06-05 11:23:44.766: epoch 56:	0.02984202  	0.07951374  	0.06587721  
2023-06-05 11:24:06.688: [iter 57 : loss : 0.5320 = 0.0697 + 0.4544 + 0.0079, time: 21.919332]
2023-06-05 11:24:06.969: epoch 57:	0.02982721  	0.07946856  	0.06584056  
2023-06-05 11:24:28.901: [iter 58 : loss : 0.5307 = 0.0685 + 0.4542 + 0.0080, time: 21.928523]
2023-06-05 11:24:29.173: epoch 58:	0.02977539  	0.07930209  	0.06565531  
2023-06-05 11:24:51.061: [iter 59 : loss : 0.5288 = 0.0666 + 0.4540 + 0.0081, time: 21.884049]
2023-06-05 11:24:51.330: epoch 59:	0.02993086  	0.07968951  	0.06586174  
2023-06-05 11:25:13.296: [iter 60 : loss : 0.5277 = 0.0657 + 0.4538 + 0.0082, time: 21.961564]
2023-06-05 11:25:13.568: epoch 60:	0.02985683  	0.07921661  	0.06568941  
2023-06-05 11:25:35.484: [iter 61 : loss : 0.5262 = 0.0642 + 0.4537 + 0.0084, time: 21.911590]
2023-06-05 11:25:35.755: epoch 61:	0.02975318  	0.07896082  	0.06537475  
2023-06-05 11:25:57.824: [iter 62 : loss : 0.5252 = 0.0632 + 0.4535 + 0.0085, time: 22.064932]
2023-06-05 11:25:58.098: epoch 62:	0.02973097  	0.07876463  	0.06524419  
2023-06-05 11:25:58.098: Early stopping is trigger at epoch: 62
2023-06-05 11:25:58.098: best_result@epoch 37:

2023-06-05 11:25:58.098: 		0.0304      	0.0833      	0.0676      
2023-06-05 11:27:36.465: my pid: 13200
2023-06-05 11:27:36.466: model: model.general_recommender.SGL
2023-06-05 11:27:36.466: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 11:27:36.466: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 11:27:40.484: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 11:28:02.249: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.764471]
2023-06-05 11:28:02.523: epoch 1:	0.00115487  	0.00232851  	0.00186714  
2023-06-05 11:28:02.523: Find a better model.
2023-06-05 11:28:24.245: [iter 2 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 21.718150]
2023-06-05 11:28:24.541: epoch 2:	0.00142878  	0.00338766  	0.00258692  
2023-06-05 11:28:24.541: Find a better model.
2023-06-05 11:28:46.225: [iter 3 : loss : 1.1338 = 0.6929 + 0.4408 + 0.0000, time: 21.679644]
2023-06-05 11:28:46.522: epoch 3:	0.00162867  	0.00362424  	0.00279571  
2023-06-05 11:28:46.522: Find a better model.
2023-06-05 11:29:08.218: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 21.693051]
2023-06-05 11:29:08.511: epoch 4:	0.00197661  	0.00451018  	0.00358466  
2023-06-05 11:29:08.511: Find a better model.
2023-06-05 11:29:30.212: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.697042]
2023-06-05 11:29:30.513: epoch 5:	0.00227273  	0.00534887  	0.00438236  
2023-06-05 11:29:30.513: Find a better model.
2023-06-05 11:29:52.210: [iter 6 : loss : 1.1342 = 0.6926 + 0.4416 + 0.0000, time: 21.694217]
2023-06-05 11:29:52.509: epoch 6:	0.00273912  	0.00638239  	0.00520926  
2023-06-05 11:29:52.510: Find a better model.
2023-06-05 11:30:14.401: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 21.886595]
2023-06-05 11:30:14.693: epoch 7:	0.00361268  	0.00852531  	0.00679747  
2023-06-05 11:30:14.693: Find a better model.
2023-06-05 11:30:36.600: [iter 8 : loss : 1.1344 = 0.6921 + 0.4422 + 0.0000, time: 21.902726]
2023-06-05 11:30:36.889: epoch 8:	0.00410867  	0.00978613  	0.00809953  
2023-06-05 11:30:36.889: Find a better model.
2023-06-05 11:30:58.792: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 21.900473]
2023-06-05 11:30:59.089: epoch 9:	0.00515990  	0.01299061  	0.01072751  
2023-06-05 11:30:59.089: Find a better model.
2023-06-05 11:31:20.786: [iter 10 : loss : 1.1343 = 0.6912 + 0.4430 + 0.0000, time: 21.692061]
2023-06-05 11:31:21.083: epoch 10:	0.00638138  	0.01711810  	0.01404906  
2023-06-05 11:31:21.083: Find a better model.
2023-06-05 11:31:42.809: [iter 11 : loss : 1.1339 = 0.6905 + 0.4435 + 0.0000, time: 21.723331]
2023-06-05 11:31:43.098: epoch 11:	0.00800262  	0.02200286  	0.01835925  
2023-06-05 11:31:43.098: Find a better model.
2023-06-05 11:32:04.765: [iter 12 : loss : 1.1332 = 0.6892 + 0.4440 + 0.0000, time: 21.661495]
2023-06-05 11:32:05.061: epoch 12:	0.00950542  	0.02687310  	0.02222764  
2023-06-05 11:32:05.061: Find a better model.
2023-06-05 11:32:26.771: [iter 13 : loss : 1.1318 = 0.6870 + 0.4447 + 0.0000, time: 21.707131]
2023-06-05 11:32:27.063: epoch 13:	0.01134880  	0.03192703  	0.02666388  
2023-06-05 11:32:27.063: Find a better model.
2023-06-05 11:32:48.582: [iter 14 : loss : 1.1288 = 0.6833 + 0.4454 + 0.0001, time: 21.514506]
2023-06-05 11:32:48.861: epoch 14:	0.01345130  	0.03621569  	0.03139073  
2023-06-05 11:32:48.861: Find a better model.
2023-06-05 11:33:10.546: [iter 15 : loss : 1.1235 = 0.6771 + 0.4463 + 0.0001, time: 21.680504]
2023-06-05 11:33:10.825: epoch 15:	0.01675311  	0.04532518  	0.03826951  
2023-06-05 11:33:10.825: Find a better model.
2023-06-05 11:33:32.916: [iter 16 : loss : 1.1135 = 0.6663 + 0.4471 + 0.0001, time: 22.087056]
2023-06-05 11:33:33.198: epoch 16:	0.02021782  	0.05539519  	0.04644444  
2023-06-05 11:33:33.198: Find a better model.
2023-06-05 11:33:55.521: [iter 17 : loss : 1.0971 = 0.6484 + 0.4484 + 0.0002, time: 22.318723]
2023-06-05 11:33:55.798: epoch 17:	0.02309767  	0.06135898  	0.05248258  
2023-06-05 11:33:55.798: Find a better model.
2023-06-05 11:34:18.095: [iter 18 : loss : 1.0700 = 0.6198 + 0.4499 + 0.0003, time: 22.292235]
2023-06-05 11:34:18.373: epoch 18:	0.02561478  	0.06690869  	0.05760854  
2023-06-05 11:34:18.373: Find a better model.
2023-06-05 11:34:40.502: [iter 19 : loss : 1.0326 = 0.5799 + 0.4522 + 0.0005, time: 22.126112]
2023-06-05 11:34:40.780: epoch 19:	0.02714725  	0.07093606  	0.06120452  
2023-06-05 11:34:40.780: Find a better model.
2023-06-05 11:35:02.684: [iter 20 : loss : 0.9865 = 0.5306 + 0.4552 + 0.0007, time: 21.900099]
2023-06-05 11:35:02.973: epoch 20:	0.02794681  	0.07374897  	0.06304331  
2023-06-05 11:35:02.973: Find a better model.
2023-06-05 11:35:25.087: [iter 21 : loss : 0.9364 = 0.4769 + 0.4585 + 0.0010, time: 22.111349]
2023-06-05 11:35:25.364: epoch 21:	0.02853168  	0.07582844  	0.06417428  
2023-06-05 11:35:25.364: Find a better model.
2023-06-05 11:35:47.466: [iter 22 : loss : 0.8866 = 0.4236 + 0.4617 + 0.0013, time: 22.097584]
2023-06-05 11:35:47.738: epoch 22:	0.02894624  	0.07720454  	0.06482856  
2023-06-05 11:35:47.738: Find a better model.
2023-06-05 11:36:09.849: [iter 23 : loss : 0.8422 = 0.3764 + 0.4642 + 0.0016, time: 22.106768]
2023-06-05 11:36:10.126: epoch 23:	0.02905730  	0.07803541  	0.06499725  
2023-06-05 11:36:10.126: Find a better model.
2023-06-05 11:36:32.246: [iter 24 : loss : 0.8034 = 0.3355 + 0.4660 + 0.0019, time: 22.116394]
2023-06-05 11:36:32.517: epoch 24:	0.02945709  	0.07851561  	0.06528451  
2023-06-05 11:36:32.517: Find a better model.
2023-06-05 11:36:54.481: [iter 25 : loss : 0.7702 = 0.3012 + 0.4668 + 0.0022, time: 21.959510]
2023-06-05 11:36:54.753: epoch 25:	0.02942747  	0.07892521  	0.06535762  
2023-06-05 11:36:54.753: Find a better model.
2023-06-05 11:37:16.832: [iter 26 : loss : 0.7424 = 0.2728 + 0.4671 + 0.0025, time: 22.074131]
2023-06-05 11:37:17.110: epoch 26:	0.02956814  	0.07942416  	0.06576792  
2023-06-05 11:37:17.110: Find a better model.
2023-06-05 11:37:39.056: [iter 27 : loss : 0.7188 = 0.2489 + 0.4671 + 0.0028, time: 21.942050]
2023-06-05 11:37:39.326: epoch 27:	0.02970880  	0.07994813  	0.06604969  
2023-06-05 11:37:39.326: Find a better model.
2023-06-05 11:38:01.453: [iter 28 : loss : 0.6986 = 0.2289 + 0.4667 + 0.0030, time: 22.122562]
2023-06-05 11:38:01.724: epoch 28:	0.02956814  	0.07983013  	0.06591050  
2023-06-05 11:38:23.826: [iter 29 : loss : 0.6813 = 0.2120 + 0.4660 + 0.0033, time: 22.096462]
2023-06-05 11:38:24.099: epoch 29:	0.02964216  	0.08035804  	0.06620777  
2023-06-05 11:38:24.100: Find a better model.
2023-06-05 11:38:46.451: [iter 30 : loss : 0.6659 = 0.1970 + 0.4654 + 0.0035, time: 22.347980]
2023-06-05 11:38:46.730: epoch 30:	0.02963476  	0.08033479  	0.06621238  
2023-06-05 11:39:09.032: [iter 31 : loss : 0.6527 = 0.1840 + 0.4650 + 0.0037, time: 22.298043]
2023-06-05 11:39:09.309: epoch 31:	0.02983463  	0.08072832  	0.06658631  
2023-06-05 11:39:09.309: Find a better model.
2023-06-05 11:39:31.410: [iter 32 : loss : 0.6412 = 0.1731 + 0.4641 + 0.0040, time: 22.096335]
2023-06-05 11:39:31.681: epoch 32:	0.02985686  	0.08104599  	0.06657366  
2023-06-05 11:39:31.682: Find a better model.
2023-06-05 11:39:54.018: [iter 33 : loss : 0.6312 = 0.1637 + 0.4634 + 0.0042, time: 22.333214]
2023-06-05 11:39:54.293: epoch 33:	0.02993089  	0.08102744  	0.06652207  
2023-06-05 11:40:16.788: [iter 34 : loss : 0.6218 = 0.1546 + 0.4629 + 0.0044, time: 22.491248]
2023-06-05 11:40:17.070: epoch 34:	0.02995309  	0.08041502  	0.06645690  
2023-06-05 11:40:39.193: [iter 35 : loss : 0.6126 = 0.1459 + 0.4621 + 0.0046, time: 22.118306]
2023-06-05 11:40:39.464: epoch 35:	0.03000491  	0.08084238  	0.06667855  
2023-06-05 11:41:01.793: [iter 36 : loss : 0.6057 = 0.1394 + 0.4616 + 0.0048, time: 22.324954]
2023-06-05 11:41:02.073: epoch 36:	0.03010115  	0.08094132  	0.06675228  
2023-06-05 11:41:24.368: [iter 37 : loss : 0.5990 = 0.1331 + 0.4609 + 0.0050, time: 22.292092]
2023-06-05 11:41:24.636: epoch 37:	0.02994570  	0.08064173  	0.06668863  
2023-06-05 11:41:46.943: [iter 38 : loss : 0.5924 = 0.1269 + 0.4604 + 0.0051, time: 22.302856]
2023-06-05 11:41:47.210: epoch 38:	0.02999011  	0.08071826  	0.06670469  
2023-06-05 11:42:09.540: [iter 39 : loss : 0.5872 = 0.1221 + 0.4598 + 0.0053, time: 22.326213]
2023-06-05 11:42:09.809: epoch 39:	0.03006415  	0.08084400  	0.06680957  
2023-06-05 11:42:31.961: [iter 40 : loss : 0.5813 = 0.1166 + 0.4593 + 0.0055, time: 22.148568]
2023-06-05 11:42:32.230: epoch 40:	0.02995311  	0.08075262  	0.06663854  
2023-06-05 11:42:54.521: [iter 41 : loss : 0.5765 = 0.1118 + 0.4590 + 0.0056, time: 22.285408]
2023-06-05 11:42:54.789: epoch 41:	0.03001233  	0.08074433  	0.06668979  
2023-06-05 11:43:16.927: [iter 42 : loss : 0.5722 = 0.1079 + 0.4585 + 0.0058, time: 22.135050]
2023-06-05 11:43:17.194: epoch 42:	0.02993089  	0.08041926  	0.06658852  
2023-06-05 11:43:39.371: [iter 43 : loss : 0.5677 = 0.1038 + 0.4580 + 0.0060, time: 22.173001]
2023-06-05 11:43:39.641: epoch 43:	0.02990868  	0.08019052  	0.06642568  
2023-06-05 11:44:01.912: [iter 44 : loss : 0.5650 = 0.1012 + 0.4577 + 0.0061, time: 22.266288]
2023-06-05 11:44:02.186: epoch 44:	0.02994570  	0.08020139  	0.06660637  
2023-06-05 11:44:24.320: [iter 45 : loss : 0.5610 = 0.0974 + 0.4573 + 0.0063, time: 22.129343]
2023-06-05 11:44:24.592: epoch 45:	0.02999752  	0.08048992  	0.06677345  
2023-06-05 11:44:46.694: [iter 46 : loss : 0.5577 = 0.0943 + 0.4570 + 0.0064, time: 22.099404]
2023-06-05 11:44:46.977: epoch 46:	0.02996790  	0.08025350  	0.06664092  
2023-06-05 11:45:09.111: [iter 47 : loss : 0.5544 = 0.0912 + 0.4566 + 0.0066, time: 22.129963]
2023-06-05 11:45:09.379: epoch 47:	0.02993089  	0.08036003  	0.06678957  
2023-06-05 11:45:31.476: [iter 48 : loss : 0.5517 = 0.0887 + 0.4563 + 0.0067, time: 22.092919]
2023-06-05 11:45:31.748: epoch 48:	0.02981244  	0.07970579  	0.06639706  
2023-06-05 11:45:53.862: [iter 49 : loss : 0.5488 = 0.0859 + 0.4560 + 0.0069, time: 22.108668]
2023-06-05 11:45:54.135: epoch 49:	0.02983465  	0.07966748  	0.06656193  
2023-06-05 11:46:16.266: [iter 50 : loss : 0.5464 = 0.0836 + 0.4558 + 0.0070, time: 22.127160]
2023-06-05 11:46:16.533: epoch 50:	0.02966437  	0.07916167  	0.06620009  
2023-06-05 11:46:38.645: [iter 51 : loss : 0.5437 = 0.0811 + 0.4555 + 0.0071, time: 22.108551]
2023-06-05 11:46:38.912: epoch 51:	0.02961994  	0.07924596  	0.06626193  
2023-06-05 11:47:01.074: [iter 52 : loss : 0.5411 = 0.0787 + 0.4552 + 0.0073, time: 22.157633]
2023-06-05 11:47:01.344: epoch 52:	0.02959773  	0.07903011  	0.06593615  
2023-06-05 11:47:23.443: [iter 53 : loss : 0.5400 = 0.0777 + 0.4549 + 0.0074, time: 22.095053]
2023-06-05 11:47:23.713: epoch 53:	0.02950150  	0.07862329  	0.06573384  
2023-06-05 11:47:45.836: [iter 54 : loss : 0.5379 = 0.0757 + 0.4547 + 0.0075, time: 22.119022]
2023-06-05 11:47:46.110: epoch 54:	0.02939044  	0.07821819  	0.06558897  
2023-06-05 11:48:08.224: [iter 55 : loss : 0.5356 = 0.0735 + 0.4545 + 0.0076, time: 22.111054]
2023-06-05 11:48:08.495: epoch 55:	0.02932381  	0.07796712  	0.06543187  
2023-06-05 11:48:30.822: [iter 56 : loss : 0.5335 = 0.0715 + 0.4542 + 0.0078, time: 22.323358]
2023-06-05 11:48:31.092: epoch 56:	0.02921276  	0.07735934  	0.06522736  
2023-06-05 11:48:52.990: [iter 57 : loss : 0.5316 = 0.0696 + 0.4541 + 0.0079, time: 21.894705]
2023-06-05 11:48:53.258: epoch 57:	0.02921276  	0.07747941  	0.06521289  
2023-06-05 11:48:53.258: Early stopping is trigger at epoch: 57
2023-06-05 11:48:53.258: best_result@epoch 32:

2023-06-05 11:48:53.259: 		0.0299      	0.0810      	0.0666      
2023-06-05 14:33:26.500: my pid: 148
2023-06-05 14:33:26.500: model: model.general_recommender.SGL
2023-06-05 14:33:26.500: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 14:33:26.500: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 14:33:30.555: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 14:33:51.559: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 21.004147]
2023-06-05 14:33:51.835: epoch 1:	0.00120669  	0.00241532  	0.00189338  
2023-06-05 14:33:51.835: Find a better model.
2023-06-05 14:34:13.377: [iter 2 : loss : 1.1338 = 0.6930 + 0.4408 + 0.0000, time: 21.536073]
2023-06-05 14:34:13.669: epoch 2:	0.00179153  	0.00332253  	0.00294850  
2023-06-05 14:34:13.669: Find a better model.
2023-06-05 14:34:34.755: [iter 3 : loss : 1.1340 = 0.6929 + 0.4411 + 0.0000, time: 21.082340]
2023-06-05 14:34:35.060: epoch 3:	0.00205804  	0.00447957  	0.00345028  
2023-06-05 14:34:35.060: Find a better model.
2023-06-05 14:34:56.704: [iter 4 : loss : 1.1342 = 0.6928 + 0.4414 + 0.0000, time: 21.641150]
2023-06-05 14:34:56.999: epoch 4:	0.00199141  	0.00433370  	0.00366472  
2023-06-05 14:35:18.166: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.164088]
2023-06-05 14:35:18.458: epoch 5:	0.00280574  	0.00650740  	0.00489819  
2023-06-05 14:35:18.458: Find a better model.
2023-06-05 14:35:39.391: [iter 6 : loss : 1.1345 = 0.6925 + 0.4420 + 0.0000, time: 20.929334]
2023-06-05 14:35:39.690: epoch 6:	0.00294640  	0.00738273  	0.00549240  
2023-06-05 14:35:39.690: Find a better model.
2023-06-05 14:36:00.577: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.883097]
2023-06-05 14:36:00.867: epoch 7:	0.00357566  	0.00907046  	0.00679777  
2023-06-05 14:36:00.867: Find a better model.
2023-06-05 14:36:21.558: [iter 8 : loss : 1.1347 = 0.6920 + 0.4427 + 0.0000, time: 20.687814]
2023-06-05 14:36:21.846: epoch 8:	0.00437518  	0.01090658  	0.00868914  
2023-06-05 14:36:21.846: Find a better model.
2023-06-05 14:36:42.516: [iter 9 : loss : 1.1347 = 0.6916 + 0.4431 + 0.0000, time: 20.667034]
2023-06-05 14:36:42.800: epoch 9:	0.00508586  	0.01361679  	0.01032700  
2023-06-05 14:36:42.800: Find a better model.
2023-06-05 14:37:03.328: [iter 10 : loss : 1.1344 = 0.6909 + 0.4435 + 0.0000, time: 20.522107]
2023-06-05 14:37:03.609: epoch 10:	0.00578174  	0.01630088  	0.01221930  
2023-06-05 14:37:03.609: Find a better model.
2023-06-05 14:37:24.300: [iter 11 : loss : 1.1339 = 0.6898 + 0.4441 + 0.0000, time: 20.686490]
2023-06-05 14:37:24.593: epoch 11:	0.00635177  	0.01811464  	0.01392359  
2023-06-05 14:37:24.594: Find a better model.
2023-06-05 14:37:45.121: [iter 12 : loss : 1.1327 = 0.6880 + 0.4447 + 0.0000, time: 20.523107]
2023-06-05 14:37:45.404: epoch 12:	0.00770651  	0.02250686  	0.01762819  
2023-06-05 14:37:45.404: Find a better model.
2023-06-05 14:38:05.952: [iter 13 : loss : 1.1308 = 0.6854 + 0.4454 + 0.0000, time: 20.544149]
2023-06-05 14:38:06.234: epoch 13:	0.00960907  	0.02861613  	0.02261373  
2023-06-05 14:38:06.234: Find a better model.
2023-06-05 14:38:26.539: [iter 14 : loss : 1.1274 = 0.6812 + 0.4461 + 0.0001, time: 20.302068]
2023-06-05 14:38:26.817: epoch 14:	0.01251109  	0.03610916  	0.02926908  
2023-06-05 14:38:26.817: Find a better model.
2023-06-05 14:38:47.304: [iter 15 : loss : 1.1213 = 0.6741 + 0.4470 + 0.0001, time: 20.484125]
2023-06-05 14:38:47.591: epoch 15:	0.01586473  	0.04412491  	0.03671411  
2023-06-05 14:38:47.591: Find a better model.
2023-06-05 14:39:08.661: [iter 16 : loss : 1.1097 = 0.6618 + 0.4478 + 0.0002, time: 21.065080]
2023-06-05 14:39:08.932: epoch 16:	0.01998831  	0.05417323  	0.04532740  
2023-06-05 14:39:08.932: Find a better model.
2023-06-05 14:39:31.634: [iter 17 : loss : 1.0910 = 0.6416 + 0.4492 + 0.0002, time: 22.698380]
2023-06-05 14:39:31.901: epoch 17:	0.02305326  	0.06199444  	0.05188913  
2023-06-05 14:39:31.901: Find a better model.
2023-06-05 14:39:53.074: [iter 18 : loss : 1.0615 = 0.6101 + 0.4510 + 0.0004, time: 21.168956]
2023-06-05 14:39:53.344: epoch 18:	0.02527424  	0.06725007  	0.05649508  
2023-06-05 14:39:53.344: Find a better model.
2023-06-05 14:40:14.625: [iter 19 : loss : 1.0220 = 0.5681 + 0.4534 + 0.0006, time: 21.277003]
2023-06-05 14:40:14.897: epoch 19:	0.02688815  	0.07217493  	0.05994288  
2023-06-05 14:40:14.897: Find a better model.
2023-06-05 14:40:36.045: [iter 20 : loss : 0.9752 = 0.5178 + 0.4566 + 0.0008, time: 21.143387]
2023-06-05 14:40:36.305: epoch 20:	0.02793200  	0.07487879  	0.06197916  
2023-06-05 14:40:36.305: Find a better model.
2023-06-05 14:40:57.427: [iter 21 : loss : 0.9255 = 0.4646 + 0.4598 + 0.0011, time: 21.117607]
2023-06-05 14:40:57.693: epoch 21:	0.02854649  	0.07726089  	0.06325764  
2023-06-05 14:40:57.693: Find a better model.
2023-06-05 14:41:18.799: [iter 22 : loss : 0.8769 = 0.4127 + 0.4628 + 0.0014, time: 21.101970]
2023-06-05 14:41:19.063: epoch 22:	0.02885002  	0.07763418  	0.06393061  
2023-06-05 14:41:19.063: Find a better model.
2023-06-05 14:41:40.193: [iter 23 : loss : 0.8341 = 0.3671 + 0.4653 + 0.0017, time: 21.125018]
2023-06-05 14:41:40.457: epoch 23:	0.02899808  	0.07845768  	0.06407572  
2023-06-05 14:41:40.457: Find a better model.
2023-06-05 14:42:01.417: [iter 24 : loss : 0.7966 = 0.3278 + 0.4669 + 0.0020, time: 20.956411]
2023-06-05 14:42:01.682: epoch 24:	0.02913874  	0.07911447  	0.06442919  
2023-06-05 14:42:01.682: Find a better model.
2023-06-05 14:42:22.816: [iter 25 : loss : 0.7647 = 0.2950 + 0.4675 + 0.0023, time: 21.131753]
2023-06-05 14:42:23.076: epoch 25:	0.02919057  	0.07937714  	0.06469231  
2023-06-05 14:42:23.076: Find a better model.
2023-06-05 14:42:44.162: [iter 26 : loss : 0.7382 = 0.2678 + 0.4679 + 0.0025, time: 21.082028]
2023-06-05 14:42:44.423: epoch 26:	0.02938307  	0.08014722  	0.06489766  
2023-06-05 14:42:44.423: Find a better model.
2023-06-05 14:43:05.607: [iter 27 : loss : 0.7154 = 0.2449 + 0.4677 + 0.0028, time: 21.179304]
2023-06-05 14:43:05.866: epoch 27:	0.02952374  	0.08014750  	0.06507169  
2023-06-05 14:43:05.866: Find a better model.
2023-06-05 14:43:26.974: [iter 28 : loss : 0.6962 = 0.2257 + 0.4675 + 0.0031, time: 21.104019]
2023-06-05 14:43:27.233: epoch 28:	0.02960519  	0.08126120  	0.06542286  
2023-06-05 14:43:27.233: Find a better model.
2023-06-05 14:43:48.346: [iter 29 : loss : 0.6794 = 0.2092 + 0.4669 + 0.0033, time: 21.109024]
2023-06-05 14:43:48.615: epoch 29:	0.02972363  	0.08155019  	0.06571183  
2023-06-05 14:43:48.615: Find a better model.
2023-06-05 14:44:09.943: [iter 30 : loss : 0.6641 = 0.1944 + 0.4662 + 0.0036, time: 21.325513]
2023-06-05 14:44:10.201: epoch 30:	0.02973104  	0.08207005  	0.06593128  
2023-06-05 14:44:10.201: Find a better model.
2023-06-05 14:44:31.344: [iter 31 : loss : 0.6511 = 0.1817 + 0.4656 + 0.0038, time: 21.137532]
2023-06-05 14:44:31.611: epoch 31:	0.02971624  	0.08206218  	0.06597695  
2023-06-05 14:44:52.927: [iter 32 : loss : 0.6400 = 0.1711 + 0.4648 + 0.0040, time: 21.311330]
2023-06-05 14:44:53.186: epoch 32:	0.02990132  	0.08230572  	0.06636674  
2023-06-05 14:44:53.186: Find a better model.
2023-06-05 14:45:14.357: [iter 33 : loss : 0.6299 = 0.1616 + 0.4641 + 0.0042, time: 21.166964]
2023-06-05 14:45:14.626: epoch 33:	0.02979027  	0.08192020  	0.06626831  
2023-06-05 14:45:35.946: [iter 34 : loss : 0.6210 = 0.1531 + 0.4635 + 0.0044, time: 21.315746]
2023-06-05 14:45:36.205: epoch 34:	0.02975325  	0.08168517  	0.06626009  
2023-06-05 14:45:57.340: [iter 35 : loss : 0.6119 = 0.1445 + 0.4628 + 0.0046, time: 21.131958]
2023-06-05 14:45:57.610: epoch 35:	0.02975324  	0.08144440  	0.06619086  
2023-06-05 14:46:18.901: [iter 36 : loss : 0.6052 = 0.1382 + 0.4622 + 0.0048, time: 21.287208]
2023-06-05 14:46:19.161: epoch 36:	0.02979026  	0.08188639  	0.06645174  
2023-06-05 14:46:40.333: [iter 37 : loss : 0.5989 = 0.1322 + 0.4617 + 0.0050, time: 21.168174]
2023-06-05 14:46:40.602: epoch 37:	0.02980505  	0.08176957  	0.06644335  
2023-06-05 14:47:01.883: [iter 38 : loss : 0.5922 = 0.1259 + 0.4611 + 0.0052, time: 21.276694]
2023-06-05 14:47:02.143: epoch 38:	0.02973102  	0.08186996  	0.06634331  
2023-06-05 14:47:23.482: [iter 39 : loss : 0.5872 = 0.1212 + 0.4606 + 0.0053, time: 21.335356]
2023-06-05 14:47:23.741: epoch 39:	0.02985688  	0.08170658  	0.06626810  
2023-06-05 14:47:44.878: [iter 40 : loss : 0.5814 = 0.1158 + 0.4601 + 0.0055, time: 21.132533]
2023-06-05 14:47:45.137: epoch 40:	0.02985688  	0.08164790  	0.06641986  
2023-06-05 14:48:06.321: [iter 41 : loss : 0.5764 = 0.1110 + 0.4597 + 0.0057, time: 21.180410]
2023-06-05 14:48:06.588: epoch 41:	0.02978284  	0.08160789  	0.06636029  
2023-06-05 14:48:28.065: [iter 42 : loss : 0.5722 = 0.1070 + 0.4593 + 0.0058, time: 21.472165]
2023-06-05 14:48:28.325: epoch 42:	0.02968660  	0.08132739  	0.06621080  
2023-06-05 14:48:49.643: [iter 43 : loss : 0.5681 = 0.1032 + 0.4589 + 0.0060, time: 21.312135]
2023-06-05 14:48:49.901: epoch 43:	0.02971621  	0.08115073  	0.06606529  
2023-06-05 14:49:11.237: [iter 44 : loss : 0.5653 = 0.1007 + 0.4585 + 0.0062, time: 21.333098]
2023-06-05 14:49:11.498: epoch 44:	0.02976064  	0.08119931  	0.06598587  
2023-06-05 14:49:32.840: [iter 45 : loss : 0.5612 = 0.0967 + 0.4581 + 0.0063, time: 21.333470]
2023-06-05 14:49:33.098: epoch 45:	0.02978285  	0.08109432  	0.06586698  
2023-06-05 14:49:54.269: [iter 46 : loss : 0.5575 = 0.0934 + 0.4576 + 0.0065, time: 21.165770]
2023-06-05 14:49:54.545: epoch 46:	0.02976064  	0.08118755  	0.06580187  
2023-06-05 14:50:15.837: [iter 47 : loss : 0.5545 = 0.0904 + 0.4574 + 0.0066, time: 21.287775]
2023-06-05 14:50:16.096: epoch 47:	0.02974582  	0.08095387  	0.06578340  
2023-06-05 14:50:37.446: [iter 48 : loss : 0.5521 = 0.0882 + 0.4572 + 0.0067, time: 21.346769]
2023-06-05 14:50:37.710: epoch 48:	0.02975323  	0.08093128  	0.06578766  
2023-06-05 14:50:58.804: [iter 49 : loss : 0.5493 = 0.0856 + 0.4568 + 0.0069, time: 21.091065]
2023-06-05 14:50:59.064: epoch 49:	0.02964958  	0.08072529  	0.06560705  
2023-06-05 14:51:20.195: [iter 50 : loss : 0.5474 = 0.0839 + 0.4565 + 0.0070, time: 21.127283]
2023-06-05 14:51:20.458: epoch 50:	0.02970140  	0.08051790  	0.06561930  
2023-06-05 14:51:41.787: [iter 51 : loss : 0.5442 = 0.0808 + 0.4562 + 0.0072, time: 21.326051]
2023-06-05 14:51:42.048: epoch 51:	0.02952372  	0.08004638  	0.06541599  
2023-06-05 14:52:03.174: [iter 52 : loss : 0.5415 = 0.0783 + 0.4559 + 0.0073, time: 21.123331]
2023-06-05 14:52:03.432: epoch 52:	0.02966437  	0.08039051  	0.06572263  
2023-06-05 14:52:24.580: [iter 53 : loss : 0.5404 = 0.0773 + 0.4557 + 0.0074, time: 21.144101]
2023-06-05 14:52:24.839: epoch 53:	0.02964957  	0.08050542  	0.06569218  
2023-06-05 14:52:45.961: [iter 54 : loss : 0.5383 = 0.0753 + 0.4554 + 0.0076, time: 21.117410]
2023-06-05 14:52:46.221: epoch 54:	0.02961255  	0.07994372  	0.06549440  
2023-06-05 14:53:07.395: [iter 55 : loss : 0.5362 = 0.0733 + 0.4552 + 0.0077, time: 21.169980]
2023-06-05 14:53:07.658: epoch 55:	0.02947928  	0.07948896  	0.06524529  
2023-06-05 14:53:28.951: [iter 56 : loss : 0.5341 = 0.0713 + 0.4551 + 0.0078, time: 21.289714]
2023-06-05 14:53:29.213: epoch 56:	0.02933122  	0.07888451  	0.06494214  
2023-06-05 14:53:50.384: [iter 57 : loss : 0.5320 = 0.0693 + 0.4549 + 0.0079, time: 21.166014]
2023-06-05 14:53:50.652: epoch 57:	0.02928680  	0.07890147  	0.06487366  
2023-06-05 14:53:50.652: Early stopping is trigger at epoch: 57
2023-06-05 14:53:50.652: best_result@epoch 32:

2023-06-05 14:53:50.652: 		0.0299      	0.0823      	0.0664      
2023-06-05 15:25:34.231: my pid: 3472
2023-06-05 15:25:34.232: model: model.general_recommender.SGL
2023-06-05 15:25:34.232: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 15:25:34.232: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 15:25:38.368: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 15:25:59.163: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.794889]
2023-06-05 15:25:59.436: epoch 1:	0.00122890  	0.00216991  	0.00187332  
2023-06-05 15:25:59.436: Find a better model.
2023-06-05 15:26:20.549: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 21.109494]
2023-06-05 15:26:20.846: epoch 2:	0.00175452  	0.00344043  	0.00278641  
2023-06-05 15:26:20.846: Find a better model.
2023-06-05 15:26:41.761: [iter 3 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 20.912054]
2023-06-05 15:26:42.063: epoch 3:	0.00194700  	0.00418832  	0.00323903  
2023-06-05 15:26:42.063: Find a better model.
2023-06-05 15:27:03.138: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.072469]
2023-06-05 15:27:03.432: epoch 4:	0.00215428  	0.00438038  	0.00337343  
2023-06-05 15:27:03.432: Find a better model.
2023-06-05 15:27:24.480: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.044139]
2023-06-05 15:27:24.776: epoch 5:	0.00245040  	0.00511253  	0.00405294  
2023-06-05 15:27:24.776: Find a better model.
2023-06-05 15:27:45.689: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.909328]
2023-06-05 15:27:46.000: epoch 6:	0.00315369  	0.00743910  	0.00582560  
2023-06-05 15:27:46.000: Find a better model.
2023-06-05 15:28:07.071: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 21.067024]
2023-06-05 15:28:07.366: epoch 7:	0.00362008  	0.00885998  	0.00723560  
2023-06-05 15:28:07.366: Find a better model.
2023-06-05 15:28:28.529: [iter 8 : loss : 1.1346 = 0.6920 + 0.4426 + 0.0000, time: 21.159046]
2023-06-05 15:28:28.821: epoch 8:	0.00419011  	0.01078786  	0.00872972  
2023-06-05 15:28:28.821: Find a better model.
2023-06-05 15:28:49.651: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.827003]
2023-06-05 15:28:49.946: epoch 9:	0.00490079  	0.01349663  	0.01042035  
2023-06-05 15:28:49.946: Find a better model.
2023-06-05 15:29:10.878: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.929040]
2023-06-05 15:29:11.165: epoch 10:	0.00579655  	0.01641091  	0.01310133  
2023-06-05 15:29:11.166: Find a better model.
2023-06-05 15:29:32.027: [iter 11 : loss : 1.1339 = 0.6899 + 0.4440 + 0.0000, time: 20.857026]
2023-06-05 15:29:32.322: epoch 11:	0.00655905  	0.01886783  	0.01502436  
2023-06-05 15:29:32.322: Find a better model.
2023-06-05 15:29:54.478: [iter 12 : loss : 1.1328 = 0.6881 + 0.4446 + 0.0000, time: 22.152270]
2023-06-05 15:29:54.755: epoch 12:	0.00785457  	0.02209222  	0.01827489  
2023-06-05 15:29:54.755: Find a better model.
2023-06-05 15:30:15.655: [iter 13 : loss : 1.1309 = 0.6855 + 0.4453 + 0.0000, time: 20.896173]
2023-06-05 15:30:15.932: epoch 13:	0.00976454  	0.02746569  	0.02342567  
2023-06-05 15:30:15.932: Find a better model.
2023-06-05 15:30:36.659: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 20.723062]
2023-06-05 15:30:36.929: epoch 14:	0.01261473  	0.03516808  	0.02978373  
2023-06-05 15:30:36.929: Find a better model.
2023-06-05 15:30:57.633: [iter 15 : loss : 1.1214 = 0.6743 + 0.4470 + 0.0001, time: 20.700158]
2023-06-05 15:30:57.901: epoch 15:	0.01605720  	0.04369726  	0.03736795  
2023-06-05 15:30:57.901: Find a better model.
2023-06-05 15:31:19.609: [iter 16 : loss : 1.1100 = 0.6620 + 0.4478 + 0.0002, time: 21.704165]
2023-06-05 15:31:19.875: epoch 16:	0.01961813  	0.05305700  	0.04545015  
2023-06-05 15:31:19.875: Find a better model.
2023-06-05 15:31:41.388: [iter 17 : loss : 1.0912 = 0.6419 + 0.4491 + 0.0002, time: 21.508773]
2023-06-05 15:31:41.659: epoch 17:	0.02262385  	0.06095141  	0.05193071  
2023-06-05 15:31:41.660: Find a better model.
2023-06-05 15:32:03.023: [iter 18 : loss : 1.0618 = 0.6106 + 0.4508 + 0.0004, time: 21.358796]
2023-06-05 15:32:03.294: epoch 18:	0.02524462  	0.06657001  	0.05705891  
2023-06-05 15:32:03.294: Find a better model.
2023-06-05 15:32:24.575: [iter 19 : loss : 1.0224 = 0.5688 + 0.4531 + 0.0006, time: 21.278476]
2023-06-05 15:32:24.848: epoch 19:	0.02690297  	0.07157881  	0.06099945  
2023-06-05 15:32:24.848: Find a better model.
2023-06-05 15:32:46.583: [iter 20 : loss : 0.9757 = 0.5188 + 0.4562 + 0.0008, time: 21.730058]
2023-06-05 15:32:46.855: epoch 20:	0.02772473  	0.07400857  	0.06271195  
2023-06-05 15:32:46.855: Find a better model.
2023-06-05 15:33:08.544: [iter 21 : loss : 0.9262 = 0.4658 + 0.4593 + 0.0011, time: 21.685090]
2023-06-05 15:33:08.816: epoch 21:	0.02848727  	0.07668573  	0.06408800  
2023-06-05 15:33:08.816: Find a better model.
2023-06-05 15:33:30.349: [iter 22 : loss : 0.8777 = 0.4140 + 0.4623 + 0.0014, time: 21.529024]
2023-06-05 15:33:30.614: epoch 22:	0.02896849  	0.07839156  	0.06513202  
2023-06-05 15:33:30.615: Find a better model.
2023-06-05 15:33:51.936: [iter 23 : loss : 0.8348 = 0.3685 + 0.4647 + 0.0017, time: 21.317570]
2023-06-05 15:33:52.204: epoch 23:	0.02902771  	0.07897689  	0.06534886  
2023-06-05 15:33:52.204: Find a better model.
2023-06-05 15:34:13.741: [iter 24 : loss : 0.7975 = 0.3293 + 0.4662 + 0.0020, time: 21.532501]
2023-06-05 15:34:14.017: epoch 24:	0.02921279  	0.07979659  	0.06583824  
2023-06-05 15:34:14.017: Find a better model.
2023-06-05 15:34:35.519: [iter 25 : loss : 0.7651 = 0.2958 + 0.4671 + 0.0023, time: 21.497188]
2023-06-05 15:34:35.780: epoch 25:	0.02937565  	0.08024438  	0.06615587  
2023-06-05 15:34:35.780: Find a better model.
2023-06-05 15:34:57.316: [iter 26 : loss : 0.7387 = 0.2686 + 0.4675 + 0.0025, time: 21.532403]
2023-06-05 15:34:57.574: epoch 26:	0.02942749  	0.08039329  	0.06636053  
2023-06-05 15:34:57.575: Find a better model.
2023-06-05 15:35:18.897: [iter 27 : loss : 0.7155 = 0.2452 + 0.4675 + 0.0028, time: 21.318498]
2023-06-05 15:35:19.160: epoch 27:	0.02966438  	0.08114790  	0.06663758  
2023-06-05 15:35:19.160: Find a better model.
2023-06-05 15:35:40.714: [iter 28 : loss : 0.6961 = 0.2259 + 0.4671 + 0.0031, time: 21.549120]
2023-06-05 15:35:40.991: epoch 28:	0.02976802  	0.08077761  	0.06673752  
2023-06-05 15:36:02.508: [iter 29 : loss : 0.6795 = 0.2096 + 0.4666 + 0.0033, time: 21.513442]
2023-06-05 15:36:02.771: epoch 29:	0.02978283  	0.08106846  	0.06679157  
2023-06-05 15:36:24.294: [iter 30 : loss : 0.6643 = 0.1947 + 0.4660 + 0.0036, time: 21.518404]
2023-06-05 15:36:24.551: epoch 30:	0.02974582  	0.08082730  	0.06701697  
2023-06-05 15:36:46.060: [iter 31 : loss : 0.6511 = 0.1819 + 0.4654 + 0.0038, time: 21.504952]
2023-06-05 15:36:46.320: epoch 31:	0.02984206  	0.08107420  	0.06723190  
2023-06-05 15:37:07.883: [iter 32 : loss : 0.6401 = 0.1713 + 0.4648 + 0.0040, time: 21.560712]
2023-06-05 15:37:08.149: epoch 32:	0.02975324  	0.08055186  	0.06713919  
2023-06-05 15:37:29.475: [iter 33 : loss : 0.6302 = 0.1619 + 0.4641 + 0.0042, time: 21.322035]
2023-06-05 15:37:29.738: epoch 33:	0.02974583  	0.08040330  	0.06715387  
2023-06-05 15:37:51.459: [iter 34 : loss : 0.6210 = 0.1532 + 0.4634 + 0.0044, time: 21.717013]
2023-06-05 15:37:51.719: epoch 34:	0.02979026  	0.08044493  	0.06728961  
2023-06-05 15:38:13.264: [iter 35 : loss : 0.6120 = 0.1447 + 0.4628 + 0.0046, time: 21.540267]
2023-06-05 15:38:13.523: epoch 35:	0.02964218  	0.07990330  	0.06713530  
2023-06-05 15:38:35.056: [iter 36 : loss : 0.6052 = 0.1382 + 0.4621 + 0.0048, time: 21.529990]
2023-06-05 15:38:35.317: epoch 36:	0.02967920  	0.08038305  	0.06729214  
2023-06-05 15:38:56.824: [iter 37 : loss : 0.5985 = 0.1318 + 0.4617 + 0.0050, time: 21.502512]
2023-06-05 15:38:57.096: epoch 37:	0.02977544  	0.08030816  	0.06733788  
2023-06-05 15:39:18.643: [iter 38 : loss : 0.5924 = 0.1260 + 0.4612 + 0.0052, time: 21.541853]
2023-06-05 15:39:18.902: epoch 38:	0.02987168  	0.08051502  	0.06751274  
2023-06-05 15:39:40.615: [iter 39 : loss : 0.5873 = 0.1213 + 0.4607 + 0.0053, time: 21.708099]
2023-06-05 15:39:40.876: epoch 39:	0.02981246  	0.08047324  	0.06741340  
2023-06-05 15:40:02.635: [iter 40 : loss : 0.5818 = 0.1161 + 0.4601 + 0.0055, time: 21.755460]
2023-06-05 15:40:02.892: epoch 40:	0.02984947  	0.08046734  	0.06754363  
2023-06-05 15:40:24.409: [iter 41 : loss : 0.5765 = 0.1112 + 0.4596 + 0.0057, time: 21.512992]
2023-06-05 15:40:24.669: epoch 41:	0.02979025  	0.08012538  	0.06742689  
2023-06-05 15:40:46.401: [iter 42 : loss : 0.5722 = 0.1071 + 0.4593 + 0.0058, time: 21.727530]
2023-06-05 15:40:46.663: epoch 42:	0.02979024  	0.08001295  	0.06746750  
2023-06-05 15:41:08.414: [iter 43 : loss : 0.5682 = 0.1033 + 0.4589 + 0.0060, time: 21.746136]
2023-06-05 15:41:08.671: epoch 43:	0.02972363  	0.07985862  	0.06734568  
2023-06-05 15:41:30.398: [iter 44 : loss : 0.5655 = 0.1009 + 0.4585 + 0.0062, time: 21.724033]
2023-06-05 15:41:30.657: epoch 44:	0.02962738  	0.07958893  	0.06729697  
2023-06-05 15:41:52.376: [iter 45 : loss : 0.5612 = 0.0969 + 0.4580 + 0.0063, time: 21.715189]
2023-06-05 15:41:52.633: epoch 45:	0.02961257  	0.07948422  	0.06716827  
2023-06-05 15:42:14.376: [iter 46 : loss : 0.5579 = 0.0938 + 0.4577 + 0.0065, time: 21.739190]
2023-06-05 15:42:14.635: epoch 46:	0.02954594  	0.07942133  	0.06717510  
2023-06-05 15:42:35.955: [iter 47 : loss : 0.5548 = 0.0908 + 0.4574 + 0.0066, time: 21.315634]
2023-06-05 15:42:36.214: epoch 47:	0.02953854  	0.07909890  	0.06697276  
2023-06-05 15:42:57.774: [iter 48 : loss : 0.5521 = 0.0883 + 0.4570 + 0.0067, time: 21.556063]
2023-06-05 15:42:58.039: epoch 48:	0.02942009  	0.07896014  	0.06677552  
2023-06-05 15:43:19.557: [iter 49 : loss : 0.5495 = 0.0859 + 0.4567 + 0.0069, time: 21.512990]
2023-06-05 15:43:19.817: epoch 49:	0.02942748  	0.07889786  	0.06677966  
2023-06-05 15:43:41.343: [iter 50 : loss : 0.5471 = 0.0836 + 0.4565 + 0.0070, time: 21.521058]
2023-06-05 15:43:41.602: epoch 50:	0.02944968  	0.07856303  	0.06669669  
2023-06-05 15:44:03.145: [iter 51 : loss : 0.5443 = 0.0809 + 0.4562 + 0.0072, time: 21.539181]
2023-06-05 15:44:03.406: epoch 51:	0.02932382  	0.07843549  	0.06648836  
2023-06-05 15:44:24.932: [iter 52 : loss : 0.5418 = 0.0787 + 0.4559 + 0.0073, time: 21.522029]
2023-06-05 15:44:25.196: epoch 52:	0.02929422  	0.07824935  	0.06637838  
2023-06-05 15:44:25.196: Early stopping is trigger at epoch: 52
2023-06-05 15:44:25.196: best_result@epoch 27:

2023-06-05 15:44:25.196: 		0.0297      	0.0811      	0.0666      
2023-06-05 15:45:08.304: my pid: 10868
2023-06-05 15:45:08.304: model: model.general_recommender.SGL
2023-06-05 15:45:08.304: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 15:45:08.304: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 15:45:12.386: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 15:45:33.362: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.975652]
2023-06-05 15:45:33.637: epoch 1:	0.00140657  	0.00299336  	0.00225154  
2023-06-05 15:45:33.637: Find a better model.
2023-06-05 15:45:54.577: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 20.936331]
2023-06-05 15:45:54.871: epoch 2:	0.00152502  	0.00307791  	0.00236942  
2023-06-05 15:45:54.871: Find a better model.
2023-06-05 15:46:15.947: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.072992]
2023-06-05 15:46:16.241: epoch 3:	0.00199141  	0.00403465  	0.00328491  
2023-06-05 15:46:16.241: Find a better model.
2023-06-05 15:46:37.333: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 21.088877]
2023-06-05 15:46:37.631: epoch 4:	0.00230234  	0.00499849  	0.00364493  
2023-06-05 15:46:37.632: Find a better model.
2023-06-05 15:46:58.749: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.113079]
2023-06-05 15:46:59.054: epoch 5:	0.00256144  	0.00571936  	0.00456596  
2023-06-05 15:46:59.055: Find a better model.
2023-06-05 15:47:19.949: [iter 6 : loss : 1.1345 = 0.6925 + 0.4419 + 0.0000, time: 20.891152]
2023-06-05 15:47:20.242: epoch 6:	0.00323512  	0.00779840  	0.00599702  
2023-06-05 15:47:20.242: Find a better model.
2023-06-05 15:47:41.109: [iter 7 : loss : 1.1346 = 0.6923 + 0.4423 + 0.0000, time: 20.863069]
2023-06-05 15:47:41.402: epoch 7:	0.00353124  	0.00879145  	0.00688616  
2023-06-05 15:47:41.402: Find a better model.
2023-06-05 15:48:02.089: [iter 8 : loss : 1.1347 = 0.6920 + 0.4426 + 0.0000, time: 20.684626]
2023-06-05 15:48:02.387: epoch 8:	0.00428635  	0.01190176  	0.00898705  
2023-06-05 15:48:02.387: Find a better model.
2023-06-05 15:48:23.313: [iter 9 : loss : 1.1347 = 0.6916 + 0.4430 + 0.0000, time: 20.920164]
2023-06-05 15:48:23.603: epoch 9:	0.00533756  	0.01555538  	0.01163929  
2023-06-05 15:48:23.603: Find a better model.
2023-06-05 15:48:44.466: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.859361]
2023-06-05 15:48:44.762: epoch 10:	0.00627033  	0.01755355  	0.01418677  
2023-06-05 15:48:44.762: Find a better model.
2023-06-05 15:49:05.460: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 20.694059]
2023-06-05 15:49:05.749: epoch 11:	0.00698842  	0.02043721  	0.01652805  
2023-06-05 15:49:05.749: Find a better model.
2023-06-05 15:49:26.464: [iter 12 : loss : 1.1328 = 0.6882 + 0.4446 + 0.0000, time: 20.710432]
2023-06-05 15:49:26.748: epoch 12:	0.00830614  	0.02462903  	0.02018881  
2023-06-05 15:49:26.749: Find a better model.
2023-06-05 15:49:47.492: [iter 13 : loss : 1.1309 = 0.6855 + 0.4453 + 0.0000, time: 20.740079]
2023-06-05 15:49:47.777: epoch 13:	0.01045303  	0.03098409  	0.02597199  
2023-06-05 15:49:47.777: Find a better model.
2023-06-05 15:50:08.238: [iter 14 : loss : 1.1275 = 0.6813 + 0.4461 + 0.0001, time: 20.457116]
2023-06-05 15:50:08.508: epoch 14:	0.01348091  	0.03968832  	0.03322466  
2023-06-05 15:50:08.508: Find a better model.
2023-06-05 15:50:30.632: [iter 15 : loss : 1.1213 = 0.6742 + 0.4470 + 0.0001, time: 22.119805]
2023-06-05 15:50:30.904: epoch 15:	0.01661246  	0.04788867  	0.04041047  
2023-06-05 15:50:30.904: Find a better model.
2023-06-05 15:50:52.192: [iter 16 : loss : 1.1098 = 0.6618 + 0.4478 + 0.0002, time: 21.283403]
2023-06-05 15:50:52.465: epoch 16:	0.02030665  	0.05613949  	0.04781137  
2023-06-05 15:50:52.465: Find a better model.
2023-06-05 15:51:13.603: [iter 17 : loss : 1.0907 = 0.6414 + 0.4491 + 0.0002, time: 21.134879]
2023-06-05 15:51:13.871: epoch 17:	0.02347527  	0.06462469  	0.05462125  
2023-06-05 15:51:13.871: Find a better model.
2023-06-05 15:51:34.962: [iter 18 : loss : 1.0611 = 0.6098 + 0.4510 + 0.0004, time: 21.086070]
2023-06-05 15:51:35.233: epoch 18:	0.02579247  	0.07022664  	0.05918452  
2023-06-05 15:51:35.233: Find a better model.
2023-06-05 15:51:56.360: [iter 19 : loss : 1.0214 = 0.5675 + 0.4534 + 0.0006, time: 21.123471]
2023-06-05 15:51:56.629: epoch 19:	0.02763587  	0.07543287  	0.06293450  
2023-06-05 15:51:56.629: Find a better model.
2023-06-05 15:52:17.757: [iter 20 : loss : 0.9744 = 0.5171 + 0.4564 + 0.0008, time: 21.124436]
2023-06-05 15:52:18.038: epoch 20:	0.02853166  	0.07837433  	0.06481727  
2023-06-05 15:52:18.038: Find a better model.
2023-06-05 15:52:39.180: [iter 21 : loss : 0.9246 = 0.4638 + 0.4598 + 0.0011, time: 21.136098]
2023-06-05 15:52:39.445: epoch 21:	0.02921276  	0.08049099  	0.06623847  
2023-06-05 15:52:39.445: Find a better model.
2023-06-05 15:53:00.737: [iter 22 : loss : 0.8760 = 0.4118 + 0.4629 + 0.0014, time: 21.288604]
2023-06-05 15:53:01.017: epoch 22:	0.02942746  	0.08123405  	0.06670728  
2023-06-05 15:53:01.017: Find a better model.
2023-06-05 15:53:22.354: [iter 23 : loss : 0.8331 = 0.3662 + 0.4653 + 0.0017, time: 21.334099]
2023-06-05 15:53:22.616: epoch 23:	0.02951631  	0.08178050  	0.06687248  
2023-06-05 15:53:22.616: Find a better model.
2023-06-05 15:53:43.740: [iter 24 : loss : 0.7960 = 0.3272 + 0.4669 + 0.0020, time: 21.120061]
2023-06-05 15:53:44.016: epoch 24:	0.02992350  	0.08282618  	0.06755547  
2023-06-05 15:53:44.016: Find a better model.
2023-06-05 15:54:05.312: [iter 25 : loss : 0.7642 = 0.2943 + 0.4676 + 0.0023, time: 21.292147]
2023-06-05 15:54:05.574: epoch 25:	0.02984947  	0.08263189  	0.06765788  
2023-06-05 15:54:26.711: [iter 26 : loss : 0.7375 = 0.2672 + 0.4678 + 0.0026, time: 21.133048]
2023-06-05 15:54:26.975: epoch 26:	0.02988649  	0.08250578  	0.06784988  
2023-06-05 15:54:48.285: [iter 27 : loss : 0.7147 = 0.2441 + 0.4678 + 0.0028, time: 21.298652]
2023-06-05 15:54:48.547: epoch 27:	0.03006416  	0.08296926  	0.06814086  
2023-06-05 15:54:48.547: Find a better model.
2023-06-05 15:55:09.685: [iter 28 : loss : 0.6954 = 0.2250 + 0.4673 + 0.0031, time: 21.134109]
2023-06-05 15:55:09.942: epoch 28:	0.03019001  	0.08347118  	0.06845637  
2023-06-05 15:55:09.942: Find a better model.
2023-06-05 15:55:31.155: [iter 29 : loss : 0.6786 = 0.2084 + 0.4669 + 0.0033, time: 21.209066]
2023-06-05 15:55:31.416: epoch 29:	0.03019741  	0.08340262  	0.06844648  
2023-06-05 15:55:52.688: [iter 30 : loss : 0.6637 = 0.1939 + 0.4662 + 0.0036, time: 21.268084]
2023-06-05 15:55:52.950: epoch 30:	0.03024924  	0.08369029  	0.06850066  
2023-06-05 15:55:52.950: Find a better model.
2023-06-05 15:56:14.298: [iter 31 : loss : 0.6508 = 0.1814 + 0.4656 + 0.0038, time: 21.343176]
2023-06-05 15:56:14.559: epoch 31:	0.03024183  	0.08382922  	0.06867889  
2023-06-05 15:56:14.559: Find a better model.
2023-06-05 15:56:35.855: [iter 32 : loss : 0.6395 = 0.1705 + 0.4650 + 0.0040, time: 21.292062]
2023-06-05 15:56:36.120: epoch 32:	0.03027884  	0.08391957  	0.06868462  
2023-06-05 15:56:36.120: Find a better model.
2023-06-05 15:56:57.451: [iter 33 : loss : 0.6300 = 0.1615 + 0.4643 + 0.0042, time: 21.327751]
2023-06-05 15:56:57.711: epoch 33:	0.03032326  	0.08374045  	0.06878663  
2023-06-05 15:57:18.879: [iter 34 : loss : 0.6210 = 0.1530 + 0.4636 + 0.0044, time: 21.164110]
2023-06-05 15:57:19.143: epoch 34:	0.03044912  	0.08422443  	0.06902976  
2023-06-05 15:57:19.143: Find a better model.
2023-06-05 15:57:40.431: [iter 35 : loss : 0.6119 = 0.1445 + 0.4628 + 0.0046, time: 21.284397]
2023-06-05 15:57:40.689: epoch 35:	0.03036768  	0.08359939  	0.06880026  
2023-06-05 15:58:02.039: [iter 36 : loss : 0.6049 = 0.1378 + 0.4623 + 0.0048, time: 21.345969]
2023-06-05 15:58:02.297: epoch 36:	0.03037508  	0.08329434  	0.06869353  
2023-06-05 15:58:23.468: [iter 37 : loss : 0.5987 = 0.1319 + 0.4618 + 0.0050, time: 21.167031]
2023-06-05 15:58:23.727: epoch 37:	0.03036029  	0.08335288  	0.06864315  
2023-06-05 15:58:45.012: [iter 38 : loss : 0.5923 = 0.1259 + 0.4613 + 0.0052, time: 21.281052]
2023-06-05 15:58:45.269: epoch 38:	0.03033067  	0.08354945  	0.06869659  
2023-06-05 15:59:06.639: [iter 39 : loss : 0.5869 = 0.1209 + 0.4606 + 0.0053, time: 21.366404]
2023-06-05 15:59:06.898: epoch 39:	0.03031586  	0.08334623  	0.06863965  
2023-06-05 15:59:28.017: [iter 40 : loss : 0.5813 = 0.1156 + 0.4601 + 0.0055, time: 21.116071]
2023-06-05 15:59:28.276: epoch 40:	0.03034547  	0.08344969  	0.06885976  
2023-06-05 15:59:49.417: [iter 41 : loss : 0.5760 = 0.1108 + 0.4596 + 0.0057, time: 21.136079]
2023-06-05 15:59:49.676: epoch 41:	0.03032326  	0.08322323  	0.06883908  
2023-06-05 16:00:10.980: [iter 42 : loss : 0.5718 = 0.1066 + 0.4593 + 0.0058, time: 21.300658]
2023-06-05 16:00:11.237: epoch 42:	0.03018260  	0.08274423  	0.06857457  
2023-06-05 16:00:32.403: [iter 43 : loss : 0.5679 = 0.1030 + 0.4589 + 0.0060, time: 21.163109]
2023-06-05 16:00:32.660: epoch 43:	0.03018260  	0.08246408  	0.06860552  
2023-06-05 16:00:53.813: [iter 44 : loss : 0.5649 = 0.1003 + 0.4585 + 0.0062, time: 21.147733]
2023-06-05 16:00:54.077: epoch 44:	0.03023441  	0.08234092  	0.06858449  
2023-06-05 16:01:15.403: [iter 45 : loss : 0.5607 = 0.0964 + 0.4581 + 0.0063, time: 21.322544]
2023-06-05 16:01:15.660: epoch 45:	0.03007155  	0.08193447  	0.06842736  
2023-06-05 16:01:36.762: [iter 46 : loss : 0.5577 = 0.0936 + 0.4577 + 0.0065, time: 21.097194]
2023-06-05 16:01:37.033: epoch 46:	0.02993089  	0.08155661  	0.06824006  
2023-06-05 16:01:58.394: [iter 47 : loss : 0.5546 = 0.0905 + 0.4575 + 0.0066, time: 21.356282]
2023-06-05 16:01:58.652: epoch 47:	0.02999753  	0.08177188  	0.06845452  
2023-06-05 16:02:19.967: [iter 48 : loss : 0.5517 = 0.0879 + 0.4571 + 0.0067, time: 21.311021]
2023-06-05 16:02:20.219: epoch 48:	0.02986426  	0.08138980  	0.06822100  
2023-06-05 16:02:41.338: [iter 49 : loss : 0.5489 = 0.0853 + 0.4567 + 0.0069, time: 21.115425]
2023-06-05 16:02:41.597: epoch 49:	0.02995310  	0.08149819  	0.06843252  
2023-06-05 16:03:02.743: [iter 50 : loss : 0.5470 = 0.0834 + 0.4565 + 0.0070, time: 21.141081]
2023-06-05 16:03:03.018: epoch 50:	0.02976060  	0.08094912  	0.06822846  
2023-06-05 16:03:24.308: [iter 51 : loss : 0.5440 = 0.0807 + 0.4561 + 0.0072, time: 21.286104]
2023-06-05 16:03:24.561: epoch 51:	0.02970878  	0.08089664  	0.06815987  
2023-06-05 16:03:45.704: [iter 52 : loss : 0.5413 = 0.0781 + 0.4559 + 0.0073, time: 21.139361]
2023-06-05 16:03:45.960: epoch 52:	0.02960514  	0.08055428  	0.06798358  
2023-06-05 16:04:07.169: [iter 53 : loss : 0.5406 = 0.0773 + 0.4558 + 0.0074, time: 21.205074]
2023-06-05 16:04:07.427: epoch 53:	0.02962734  	0.08016209  	0.06785432  
2023-06-05 16:04:28.700: [iter 54 : loss : 0.5382 = 0.0752 + 0.4554 + 0.0076, time: 21.269559]
2023-06-05 16:04:28.961: epoch 54:	0.02951629  	0.07995296  	0.06764112  
2023-06-05 16:04:49.961: [iter 55 : loss : 0.5359 = 0.0731 + 0.4552 + 0.0077, time: 20.996019]
2023-06-05 16:04:50.223: epoch 55:	0.02947187  	0.07942168  	0.06733719  
2023-06-05 16:05:11.491: [iter 56 : loss : 0.5343 = 0.0715 + 0.4550 + 0.0078, time: 21.264107]
2023-06-05 16:05:11.749: epoch 56:	0.02941266  	0.07948763  	0.06736105  
2023-06-05 16:05:32.875: [iter 57 : loss : 0.5321 = 0.0694 + 0.4549 + 0.0079, time: 21.122081]
2023-06-05 16:05:33.139: epoch 57:	0.02939784  	0.07944794  	0.06735787  
2023-06-05 16:05:54.266: [iter 58 : loss : 0.5304 = 0.0677 + 0.4547 + 0.0080, time: 21.123068]
2023-06-05 16:05:54.523: epoch 58:	0.02929420  	0.07913294  	0.06719157  
2023-06-05 16:06:15.517: [iter 59 : loss : 0.5293 = 0.0666 + 0.4545 + 0.0081, time: 20.988923]
2023-06-05 16:06:15.776: epoch 59:	0.02920536  	0.07879079  	0.06692611  
2023-06-05 16:06:15.777: Early stopping is trigger at epoch: 59
2023-06-05 16:06:15.777: best_result@epoch 34:

2023-06-05 16:06:15.777: 		0.0304      	0.0842      	0.0690      
2023-06-05 16:07:32.091: my pid: 9132
2023-06-05 16:07:32.091: model: model.general_recommender.SGL
2023-06-05 16:07:32.091: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 16:07:32.092: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 16:07:36.166: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 16:07:57.097: [iter 1 : loss : 1.1343 = 0.6931 + 0.4412 + 0.0000, time: 20.930521]
2023-06-05 16:07:57.370: epoch 1:	0.00141398  	0.00265888  	0.00216035  
2023-06-05 16:07:57.370: Find a better model.
2023-06-05 16:08:18.306: [iter 2 : loss : 1.1338 = 0.6930 + 0.4407 + 0.0000, time: 20.932103]
2023-06-05 16:08:18.598: epoch 2:	0.00168789  	0.00344275  	0.00292297  
2023-06-05 16:08:18.598: Find a better model.
2023-06-05 16:08:39.709: [iter 3 : loss : 1.1340 = 0.6929 + 0.4410 + 0.0000, time: 21.106882]
2023-06-05 16:08:40.001: epoch 3:	0.00179153  	0.00351075  	0.00301484  
2023-06-05 16:08:40.001: Find a better model.
2023-06-05 16:09:00.867: [iter 4 : loss : 1.1341 = 0.6928 + 0.4413 + 0.0000, time: 20.863117]
2023-06-05 16:09:01.160: epoch 4:	0.00236157  	0.00523656  	0.00398059  
2023-06-05 16:09:01.160: Find a better model.
2023-06-05 16:09:22.237: [iter 5 : loss : 1.1343 = 0.6927 + 0.4416 + 0.0000, time: 21.072856]
2023-06-05 16:09:22.528: epoch 5:	0.00297601  	0.00698261  	0.00514237  
2023-06-05 16:09:22.528: Find a better model.
2023-06-05 16:09:43.426: [iter 6 : loss : 1.1344 = 0.6925 + 0.4419 + 0.0000, time: 20.894423]
2023-06-05 16:09:43.715: epoch 6:	0.00333136  	0.00794641  	0.00605731  
2023-06-05 16:09:43.715: Find a better model.
2023-06-05 16:10:04.659: [iter 7 : loss : 1.1346 = 0.6923 + 0.4422 + 0.0000, time: 20.939954]
2023-06-05 16:10:04.948: epoch 7:	0.00398283  	0.00991560  	0.00786426  
2023-06-05 16:10:04.948: Find a better model.
2023-06-05 16:10:25.850: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 20.899261]
2023-06-05 16:10:26.142: epoch 8:	0.00440480  	0.01178193  	0.00904360  
2023-06-05 16:10:26.142: Find a better model.
2023-06-05 16:10:47.005: [iter 9 : loss : 1.1346 = 0.6916 + 0.4430 + 0.0000, time: 20.857672]
2023-06-05 16:10:47.292: epoch 9:	0.00523393  	0.01480255  	0.01100769  
2023-06-05 16:10:47.292: Find a better model.
2023-06-05 16:11:08.060: [iter 10 : loss : 1.1344 = 0.6910 + 0.4434 + 0.0000, time: 20.765100]
2023-06-05 16:11:08.345: epoch 10:	0.00587058  	0.01625881  	0.01293033  
2023-06-05 16:11:08.345: Find a better model.
2023-06-05 16:11:29.205: [iter 11 : loss : 1.1339 = 0.6900 + 0.4439 + 0.0000, time: 20.856714]
2023-06-05 16:11:29.486: epoch 11:	0.00671452  	0.01872197  	0.01570920  
2023-06-05 16:11:29.486: Find a better model.
2023-06-05 16:11:50.241: [iter 12 : loss : 1.1329 = 0.6882 + 0.4446 + 0.0000, time: 20.752043]
2023-06-05 16:11:50.520: epoch 12:	0.00837277  	0.02474654  	0.02029174  
2023-06-05 16:11:50.520: Find a better model.
2023-06-05 16:12:11.025: [iter 13 : loss : 1.1309 = 0.6857 + 0.4452 + 0.0000, time: 20.500341]
2023-06-05 16:12:11.303: epoch 13:	0.01066773  	0.03021967  	0.02556509  
2023-06-05 16:12:11.303: Find a better model.
2023-06-05 16:12:31.874: [iter 14 : loss : 1.1277 = 0.6816 + 0.4460 + 0.0001, time: 20.567305]
2023-06-05 16:12:32.151: epoch 14:	0.01343649  	0.03937749  	0.03275287  
2023-06-05 16:12:32.152: Find a better model.
2023-06-05 16:12:52.815: [iter 15 : loss : 1.1219 = 0.6748 + 0.4470 + 0.0001, time: 20.660122]
2023-06-05 16:12:53.102: epoch 15:	0.01660506  	0.04779766  	0.03947357  
2023-06-05 16:12:53.102: Find a better model.
2023-06-05 16:13:14.420: [iter 16 : loss : 1.1107 = 0.6629 + 0.4477 + 0.0002, time: 21.314346]
2023-06-05 16:13:14.698: epoch 16:	0.01968478  	0.05496409  	0.04652110  
2023-06-05 16:13:14.698: Find a better model.
2023-06-05 16:13:35.975: [iter 17 : loss : 1.0925 = 0.6434 + 0.4489 + 0.0002, time: 21.270135]
2023-06-05 16:13:36.256: epoch 17:	0.02311249  	0.06338922  	0.05343764  
2023-06-05 16:13:36.256: Find a better model.
2023-06-05 16:13:57.626: [iter 18 : loss : 1.0638 = 0.6127 + 0.4507 + 0.0004, time: 21.366255]
2023-06-05 16:13:57.902: epoch 18:	0.02556295  	0.06999685  	0.05851948  
2023-06-05 16:13:57.902: Find a better model.
2023-06-05 16:14:19.164: [iter 19 : loss : 1.0249 = 0.5714 + 0.4529 + 0.0006, time: 21.256154]
2023-06-05 16:14:19.442: epoch 19:	0.02748039  	0.07454127  	0.06215094  
2023-06-05 16:14:19.442: Find a better model.
2023-06-05 16:14:40.751: [iter 20 : loss : 0.9784 = 0.5217 + 0.4560 + 0.0008, time: 21.305171]
2023-06-05 16:14:41.038: epoch 20:	0.02834661  	0.07835735  	0.06403410  
2023-06-05 16:14:41.039: Find a better model.
2023-06-05 16:15:03.555: [iter 21 : loss : 0.9289 = 0.4687 + 0.4591 + 0.0011, time: 22.510396]
2023-06-05 16:15:03.815: epoch 21:	0.02897588  	0.08062768  	0.06540551  
2023-06-05 16:15:03.815: Find a better model.
2023-06-05 16:15:24.953: [iter 22 : loss : 0.8799 = 0.4163 + 0.4623 + 0.0014, time: 21.133331]
2023-06-05 16:15:25.222: epoch 22:	0.02906472  	0.08094192  	0.06578889  
2023-06-05 16:15:25.222: Find a better model.
2023-06-05 16:15:46.540: [iter 23 : loss : 0.8366 = 0.3702 + 0.4647 + 0.0017, time: 21.314382]
2023-06-05 16:15:46.801: epoch 23:	0.02917576  	0.08141972  	0.06599924  
2023-06-05 16:15:46.801: Find a better model.
2023-06-05 16:16:08.055: [iter 24 : loss : 0.7988 = 0.3305 + 0.4663 + 0.0020, time: 21.249264]
2023-06-05 16:16:08.312: epoch 24:	0.02941267  	0.08194774  	0.06609021  
2023-06-05 16:16:08.312: Find a better model.
2023-06-05 16:16:29.700: [iter 25 : loss : 0.7664 = 0.2969 + 0.4672 + 0.0023, time: 21.384082]
2023-06-05 16:16:29.960: epoch 25:	0.02969401  	0.08269694  	0.06663693  
2023-06-05 16:16:29.960: Find a better model.
2023-06-05 16:16:51.436: [iter 26 : loss : 0.7393 = 0.2692 + 0.4676 + 0.0025, time: 21.472412]
2023-06-05 16:16:51.699: epoch 26:	0.02978285  	0.08338636  	0.06681236  
2023-06-05 16:16:51.699: Find a better model.
2023-06-05 16:17:13.113: [iter 27 : loss : 0.7164 = 0.2459 + 0.4676 + 0.0028, time: 21.409894]
2023-06-05 16:17:13.373: epoch 27:	0.02982727  	0.08360669  	0.06700687  
2023-06-05 16:17:13.373: Find a better model.
2023-06-05 16:17:34.820: [iter 28 : loss : 0.6967 = 0.2264 + 0.4672 + 0.0031, time: 21.442096]
2023-06-05 16:17:35.092: epoch 28:	0.02978285  	0.08341358  	0.06711441  
2023-06-05 16:17:56.657: [iter 29 : loss : 0.6799 = 0.2100 + 0.4666 + 0.0033, time: 21.561054]
2023-06-05 16:17:56.916: epoch 29:	0.02991611  	0.08395747  	0.06768570  
2023-06-05 16:17:56.916: Find a better model.
2023-06-05 16:18:18.433: [iter 30 : loss : 0.6647 = 0.1952 + 0.4659 + 0.0035, time: 21.513463]
2023-06-05 16:18:18.689: epoch 30:	0.02982727  	0.08378203  	0.06773802  
2023-06-05 16:18:40.060: [iter 31 : loss : 0.6517 = 0.1825 + 0.4654 + 0.0038, time: 21.366330]
2023-06-05 16:18:40.320: epoch 31:	0.02975323  	0.08377314  	0.06767123  
2023-06-05 16:19:01.802: [iter 32 : loss : 0.6400 = 0.1714 + 0.4646 + 0.0040, time: 21.478258]
2023-06-05 16:19:02.075: epoch 32:	0.02979025  	0.08419915  	0.06784910  
2023-06-05 16:19:02.075: Find a better model.
2023-06-05 16:19:23.595: [iter 33 : loss : 0.6302 = 0.1620 + 0.4640 + 0.0042, time: 21.516022]
2023-06-05 16:19:23.852: epoch 33:	0.02994571  	0.08442952  	0.06801634  
2023-06-05 16:19:23.852: Find a better model.
2023-06-05 16:19:45.247: [iter 34 : loss : 0.6212 = 0.1533 + 0.4635 + 0.0044, time: 21.391358]
2023-06-05 16:19:45.506: epoch 34:	0.03003455  	0.08452236  	0.06812727  
2023-06-05 16:19:45.506: Find a better model.
2023-06-05 16:20:07.177: [iter 35 : loss : 0.6122 = 0.1449 + 0.4627 + 0.0046, time: 21.667246]
2023-06-05 16:20:07.435: epoch 35:	0.03002716  	0.08446844  	0.06805383  
2023-06-05 16:20:28.839: [iter 36 : loss : 0.6053 = 0.1384 + 0.4621 + 0.0048, time: 21.400643]
2023-06-05 16:20:29.110: epoch 36:	0.03007898  	0.08446968  	0.06800173  
2023-06-05 16:20:50.592: [iter 37 : loss : 0.5985 = 0.1320 + 0.4615 + 0.0050, time: 21.478369]
2023-06-05 16:20:50.852: epoch 37:	0.03004195  	0.08391949  	0.06802957  
2023-06-05 16:21:12.577: [iter 38 : loss : 0.5925 = 0.1264 + 0.4610 + 0.0052, time: 21.721130]
2023-06-05 16:21:12.838: epoch 38:	0.03004196  	0.08380085  	0.06806125  
2023-06-05 16:21:34.350: [iter 39 : loss : 0.5870 = 0.1212 + 0.4604 + 0.0053, time: 21.507999]
2023-06-05 16:21:34.607: epoch 39:	0.03001235  	0.08351222  	0.06795352  
2023-06-05 16:21:56.012: [iter 40 : loss : 0.5815 = 0.1160 + 0.4600 + 0.0055, time: 21.401579]
2023-06-05 16:21:56.268: epoch 40:	0.03001976  	0.08372312  	0.06808042  
2023-06-05 16:22:17.729: [iter 41 : loss : 0.5763 = 0.1111 + 0.4595 + 0.0057, time: 21.455714]
2023-06-05 16:22:17.988: epoch 41:	0.02999014  	0.08378278  	0.06804866  
2023-06-05 16:22:39.403: [iter 42 : loss : 0.5719 = 0.1070 + 0.4591 + 0.0058, time: 21.411134]
2023-06-05 16:22:39.665: epoch 42:	0.02998274  	0.08347094  	0.06805915  
2023-06-05 16:23:01.103: [iter 43 : loss : 0.5678 = 0.1030 + 0.4588 + 0.0060, time: 21.434155]
2023-06-05 16:23:01.358: epoch 43:	0.02990871  	0.08298651  	0.06805953  
2023-06-05 16:23:22.959: [iter 44 : loss : 0.5651 = 0.1006 + 0.4584 + 0.0061, time: 21.597009]
2023-06-05 16:23:23.218: epoch 44:	0.02991612  	0.08290879  	0.06809166  
2023-06-05 16:23:44.557: [iter 45 : loss : 0.5609 = 0.0967 + 0.4579 + 0.0063, time: 21.335042]
2023-06-05 16:23:44.814: epoch 45:	0.02977545  	0.08217964  	0.06781128  
2023-06-05 16:24:06.298: [iter 46 : loss : 0.5575 = 0.0935 + 0.4576 + 0.0065, time: 21.480571]
2023-06-05 16:24:06.551: epoch 46:	0.02976804  	0.08224822  	0.06779710  
2023-06-05 16:24:28.121: [iter 47 : loss : 0.5545 = 0.0907 + 0.4573 + 0.0066, time: 21.566002]
2023-06-05 16:24:28.377: epoch 47:	0.02968661  	0.08248791  	0.06782862  
2023-06-05 16:24:49.716: [iter 48 : loss : 0.5516 = 0.0879 + 0.4570 + 0.0067, time: 21.334063]
2023-06-05 16:24:49.972: epoch 48:	0.02954594  	0.08183583  	0.06755293  
2023-06-05 16:25:11.542: [iter 49 : loss : 0.5493 = 0.0858 + 0.4566 + 0.0069, time: 21.565016]
2023-06-05 16:25:11.799: epoch 49:	0.02942749  	0.08151217  	0.06741580  
2023-06-05 16:25:33.069: [iter 50 : loss : 0.5465 = 0.0831 + 0.4564 + 0.0070, time: 21.265082]
2023-06-05 16:25:33.324: epoch 50:	0.02937565  	0.08127301  	0.06719807  
2023-06-05 16:25:54.697: [iter 51 : loss : 0.5440 = 0.0808 + 0.4560 + 0.0072, time: 21.369019]
2023-06-05 16:25:54.952: epoch 51:	0.02942008  	0.08127897  	0.06727409  
2023-06-05 16:26:16.258: [iter 52 : loss : 0.5414 = 0.0783 + 0.4558 + 0.0073, time: 21.302502]
2023-06-05 16:26:16.517: epoch 52:	0.02932384  	0.08109975  	0.06712236  
2023-06-05 16:26:37.907: [iter 53 : loss : 0.5403 = 0.0772 + 0.4556 + 0.0074, time: 21.386360]
2023-06-05 16:26:38.173: epoch 53:	0.02924980  	0.08088473  	0.06691278  
2023-06-05 16:26:59.447: [iter 54 : loss : 0.5382 = 0.0753 + 0.4554 + 0.0075, time: 21.269993]
2023-06-05 16:26:59.703: epoch 54:	0.02917577  	0.08076831  	0.06681182  
2023-06-05 16:27:21.082: [iter 55 : loss : 0.5362 = 0.0734 + 0.4551 + 0.0077, time: 21.373198]
2023-06-05 16:27:21.342: epoch 55:	0.02913134  	0.08055482  	0.06661391  
2023-06-05 16:27:42.641: [iter 56 : loss : 0.5339 = 0.0712 + 0.4549 + 0.0078, time: 21.295687]
2023-06-05 16:27:42.899: epoch 56:	0.02907952  	0.08048066  	0.06653308  
2023-06-05 16:28:04.248: [iter 57 : loss : 0.5319 = 0.0693 + 0.4547 + 0.0079, time: 21.344380]
2023-06-05 16:28:04.503: epoch 57:	0.02916096  	0.08063101  	0.06656872  
2023-06-05 16:28:25.806: [iter 58 : loss : 0.5305 = 0.0680 + 0.4545 + 0.0080, time: 21.297988]
2023-06-05 16:28:26.078: epoch 58:	0.02915355  	0.08042508  	0.06629909  
2023-06-05 16:28:47.453: [iter 59 : loss : 0.5289 = 0.0664 + 0.4544 + 0.0081, time: 21.371659]
2023-06-05 16:28:47.710: epoch 59:	0.02911654  	0.08038013  	0.06626418  
2023-06-05 16:28:47.710: Early stopping is trigger at epoch: 59
2023-06-05 16:28:47.710: best_result@epoch 34:

2023-06-05 16:28:47.710: 		0.0300      	0.0845      	0.0681      
2023-06-05 16:36:03.281: my pid: 2320
2023-06-05 16:36:03.281: model: model.general_recommender.SGL
2023-06-05 16:36:03.281: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 16:36:03.281: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 16:36:07.372: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 16:36:28.682: [iter 1 : loss : 1.1342 = 0.6931 + 0.4412 + 0.0000, time: 21.309762]
2023-06-05 16:36:28.952: epoch 1:	0.00138437  	0.00273892  	0.00218523  
2023-06-05 16:36:28.952: Find a better model.
2023-06-05 16:36:50.097: [iter 2 : loss : 1.1337 = 0.6930 + 0.4407 + 0.0000, time: 21.142045]
2023-06-05 16:36:50.389: epoch 2:	0.00160646  	0.00285169  	0.00242541  
2023-06-05 16:36:50.389: Find a better model.
2023-06-05 16:37:11.652: [iter 3 : loss : 1.1339 = 0.6929 + 0.4409 + 0.0000, time: 21.259627]
2023-06-05 16:37:11.946: epoch 3:	0.00158425  	0.00360642  	0.00273285  
2023-06-05 16:37:11.946: Find a better model.
2023-06-05 16:37:33.089: [iter 4 : loss : 1.1341 = 0.6928 + 0.4412 + 0.0000, time: 21.138903]
2023-06-05 16:37:33.385: epoch 4:	0.00205804  	0.00415252  	0.00338396  
2023-06-05 16:37:33.385: Find a better model.
2023-06-05 16:37:54.670: [iter 5 : loss : 1.1342 = 0.6927 + 0.4415 + 0.0000, time: 21.282073]
2023-06-05 16:37:54.960: epoch 5:	0.00238377  	0.00520412  	0.00407882  
2023-06-05 16:37:54.960: Find a better model.
2023-06-05 16:38:16.066: [iter 6 : loss : 1.1344 = 0.6926 + 0.4418 + 0.0000, time: 21.101511]
2023-06-05 16:38:16.359: epoch 6:	0.00291679  	0.00714002  	0.00564088  
2023-06-05 16:38:16.359: Find a better model.
2023-06-05 16:38:37.483: [iter 7 : loss : 1.1346 = 0.6924 + 0.4422 + 0.0000, time: 21.120863]
2023-06-05 16:38:37.771: epoch 7:	0.00348682  	0.00887754  	0.00681287  
2023-06-05 16:38:37.771: Find a better model.
2023-06-05 16:38:59.023: [iter 8 : loss : 1.1346 = 0.6921 + 0.4425 + 0.0000, time: 21.248390]
2023-06-05 16:38:59.316: epoch 8:	0.00426414  	0.01053921  	0.00812660  
2023-06-05 16:38:59.316: Find a better model.
2023-06-05 16:39:20.507: [iter 9 : loss : 1.1345 = 0.6917 + 0.4428 + 0.0000, time: 21.187246]
2023-06-05 16:39:20.797: epoch 9:	0.00509327  	0.01305057  	0.01028915  
2023-06-05 16:39:20.797: Find a better model.
2023-06-05 16:39:41.886: [iter 10 : loss : 1.1344 = 0.6911 + 0.4433 + 0.0000, time: 21.085420]
2023-06-05 16:39:42.176: epoch 10:	0.00602604  	0.01581105  	0.01260961  
2023-06-05 16:39:42.176: Find a better model.
2023-06-05 16:40:03.254: [iter 11 : loss : 1.1340 = 0.6902 + 0.4438 + 0.0000, time: 21.073637]
2023-06-05 16:40:03.540: epoch 11:	0.00685517  	0.01837548  	0.01507492  
2023-06-05 16:40:03.540: Find a better model.
2023-06-05 16:40:26.035: [iter 12 : loss : 1.1330 = 0.6886 + 0.4444 + 0.0000, time: 22.491576]
2023-06-05 16:40:26.309: epoch 12:	0.00795821  	0.02188252  	0.01826498  
2023-06-05 16:40:26.309: Find a better model.
2023-06-05 16:40:47.421: [iter 13 : loss : 1.1312 = 0.6861 + 0.4451 + 0.0000, time: 21.108107]
2023-06-05 16:40:47.695: epoch 13:	0.01014210  	0.02923420  	0.02387409  
2023-06-05 16:40:47.695: Find a better model.
2023-06-05 16:41:08.776: [iter 14 : loss : 1.1280 = 0.6821 + 0.4458 + 0.0001, time: 21.077772]
2023-06-05 16:41:09.060: epoch 14:	0.01278500  	0.03674350  	0.03058917  
2023-06-05 16:41:09.060: Find a better model.
2023-06-05 16:41:30.048: [iter 15 : loss : 1.1224 = 0.6755 + 0.4468 + 0.0001, time: 20.983028]
2023-06-05 16:41:30.312: epoch 15:	0.01608683  	0.04433600  	0.03750426  
2023-06-05 16:41:30.312: Find a better model.
2023-06-05 16:41:51.982: [iter 16 : loss : 1.1117 = 0.6641 + 0.4475 + 0.0001, time: 21.665935]
2023-06-05 16:41:52.254: epoch 16:	0.01953670  	0.05231220  	0.04528034  
2023-06-05 16:41:52.254: Find a better model.
2023-06-05 16:42:13.797: [iter 17 : loss : 1.0942 = 0.6451 + 0.4488 + 0.0002, time: 21.537012]
2023-06-05 16:42:14.081: epoch 17:	0.02283116  	0.06056121  	0.05214778  
2023-06-05 16:42:14.081: Find a better model.
2023-06-05 16:42:35.766: [iter 18 : loss : 1.0660 = 0.6152 + 0.4505 + 0.0004, time: 21.681240]
2023-06-05 16:42:36.045: epoch 18:	0.02525945  	0.06727200  	0.05733769  
2023-06-05 16:42:36.045: Find a better model.
2023-06-05 16:42:57.730: [iter 19 : loss : 1.0278 = 0.5745 + 0.4528 + 0.0005, time: 21.675379]
2023-06-05 16:42:57.995: epoch 19:	0.02728054  	0.07271420  	0.06153686  
2023-06-05 16:42:57.995: Find a better model.
2023-06-05 16:43:19.545: [iter 20 : loss : 0.9815 = 0.5250 + 0.4558 + 0.0008, time: 21.546362]
2023-06-05 16:43:19.810: epoch 20:	0.02842063  	0.07623021  	0.06384960  
2023-06-05 16:43:19.810: Find a better model.
2023-06-05 16:43:41.532: [iter 21 : loss : 0.9317 = 0.4718 + 0.4589 + 0.0010, time: 21.718329]
2023-06-05 16:43:41.796: epoch 21:	0.02878339  	0.07748608  	0.06475564  
2023-06-05 16:43:41.796: Find a better model.
2023-06-05 16:44:03.373: [iter 22 : loss : 0.8826 = 0.4193 + 0.4619 + 0.0013, time: 21.572330]
2023-06-05 16:44:03.632: epoch 22:	0.02920537  	0.07812975  	0.06524469  
2023-06-05 16:44:03.632: Find a better model.
2023-06-05 16:44:25.088: [iter 23 : loss : 0.8386 = 0.3725 + 0.4645 + 0.0016, time: 21.452284]
2023-06-05 16:44:25.347: epoch 23:	0.02929421  	0.07880131  	0.06541535  
2023-06-05 16:44:25.347: Find a better model.
2023-06-05 16:44:46.909: [iter 24 : loss : 0.8005 = 0.3323 + 0.4662 + 0.0019, time: 21.558131]
2023-06-05 16:44:47.178: epoch 24:	0.02942747  	0.07901494  	0.06543791  
2023-06-05 16:44:47.178: Find a better model.
2023-06-05 16:45:08.867: [iter 25 : loss : 0.7678 = 0.2985 + 0.4670 + 0.0022, time: 21.683574]
2023-06-05 16:45:09.136: epoch 25:	0.02950151  	0.07959215  	0.06554104  
2023-06-05 16:45:09.136: Find a better model.
2023-06-05 16:45:30.734: [iter 26 : loss : 0.7405 = 0.2705 + 0.4675 + 0.0025, time: 21.594046]
2023-06-05 16:45:30.991: epoch 26:	0.02965696  	0.07999866  	0.06587644  
2023-06-05 16:45:30.991: Find a better model.
2023-06-05 16:45:52.657: [iter 27 : loss : 0.7171 = 0.2469 + 0.4675 + 0.0028, time: 21.662114]
2023-06-05 16:45:52.917: epoch 27:	0.02974581  	0.08039721  	0.06605775  
2023-06-05 16:45:52.917: Find a better model.
2023-06-05 16:46:14.694: [iter 28 : loss : 0.6975 = 0.2273 + 0.4671 + 0.0030, time: 21.773251]
2023-06-05 16:46:14.952: epoch 28:	0.02995311  	0.08148133  	0.06652098  
2023-06-05 16:46:14.952: Find a better model.
2023-06-05 16:46:36.845: [iter 29 : loss : 0.6804 = 0.2106 + 0.4665 + 0.0033, time: 21.888381]
2023-06-05 16:46:37.115: epoch 29:	0.02999754  	0.08154233  	0.06664281  
2023-06-05 16:46:37.115: Find a better model.
2023-06-05 16:46:58.898: [iter 30 : loss : 0.6650 = 0.1954 + 0.4660 + 0.0035, time: 21.778612]
2023-06-05 16:46:59.164: epoch 30:	0.03001975  	0.08171996  	0.06680426  
2023-06-05 16:46:59.164: Find a better model.
2023-06-05 16:47:20.878: [iter 31 : loss : 0.6519 = 0.1828 + 0.4654 + 0.0038, time: 21.710506]
2023-06-05 16:47:21.144: epoch 31:	0.02993832  	0.08162307  	0.06688451  
2023-06-05 16:47:42.815: [iter 32 : loss : 0.6406 = 0.1720 + 0.4647 + 0.0040, time: 21.666315]
2023-06-05 16:47:43.088: epoch 32:	0.02998273  	0.08166321  	0.06695870  
2023-06-05 16:48:05.067: [iter 33 : loss : 0.6309 = 0.1626 + 0.4641 + 0.0042, time: 21.974988]
2023-06-05 16:48:05.346: epoch 33:	0.02996792  	0.08129515  	0.06696630  
2023-06-05 16:48:27.262: [iter 34 : loss : 0.6215 = 0.1538 + 0.4633 + 0.0044, time: 21.910607]
2023-06-05 16:48:27.532: epoch 34:	0.02988648  	0.08114807  	0.06690091  
2023-06-05 16:48:49.255: [iter 35 : loss : 0.6125 = 0.1453 + 0.4627 + 0.0046, time: 21.718018]
2023-06-05 16:48:49.523: epoch 35:	0.02999014  	0.08153562  	0.06699383  
2023-06-05 16:49:11.456: [iter 36 : loss : 0.6052 = 0.1384 + 0.4620 + 0.0048, time: 21.929120]
2023-06-05 16:49:11.726: epoch 36:	0.03009379  	0.08177082  	0.06714753  
2023-06-05 16:49:11.726: Find a better model.
2023-06-05 16:49:33.810: [iter 37 : loss : 0.5988 = 0.1324 + 0.4615 + 0.0050, time: 22.080020]
2023-06-05 16:49:34.090: epoch 37:	0.03009380  	0.08170752  	0.06723031  
2023-06-05 16:49:55.827: [iter 38 : loss : 0.5921 = 0.1261 + 0.4609 + 0.0051, time: 21.732019]
2023-06-05 16:49:56.105: epoch 38:	0.02990132  	0.08109340  	0.06701499  
2023-06-05 16:50:17.869: [iter 39 : loss : 0.5872 = 0.1215 + 0.4604 + 0.0053, time: 21.761106]
2023-06-05 16:50:18.144: epoch 39:	0.02988650  	0.08093013  	0.06684038  
2023-06-05 16:50:39.795: [iter 40 : loss : 0.5814 = 0.1161 + 0.4599 + 0.0055, time: 21.648237]
2023-06-05 16:50:40.076: epoch 40:	0.02984949  	0.08063138  	0.06681293  
2023-06-05 16:51:01.631: [iter 41 : loss : 0.5763 = 0.1112 + 0.4595 + 0.0057, time: 21.550927]
2023-06-05 16:51:01.897: epoch 41:	0.02983469  	0.08063142  	0.06670497  
2023-06-05 16:51:23.791: [iter 42 : loss : 0.5721 = 0.1072 + 0.4590 + 0.0058, time: 21.890171]
2023-06-05 16:51:24.078: epoch 42:	0.02976066  	0.08030044  	0.06645917  
2023-06-05 16:51:45.848: [iter 43 : loss : 0.5681 = 0.1036 + 0.4586 + 0.0060, time: 21.765833]
2023-06-05 16:51:46.123: epoch 43:	0.02981248  	0.08030763  	0.06656170  
2023-06-05 16:52:09.571: [iter 44 : loss : 0.5651 = 0.1008 + 0.4582 + 0.0061, time: 23.444555]
2023-06-05 16:52:09.832: epoch 44:	0.02981248  	0.08004140  	0.06655002  
2023-06-05 16:52:31.528: [iter 45 : loss : 0.5610 = 0.0969 + 0.4578 + 0.0063, time: 21.691239]
2023-06-05 16:52:31.787: epoch 45:	0.02961999  	0.07932027  	0.06624202  
2023-06-05 16:52:53.599: [iter 46 : loss : 0.5578 = 0.0938 + 0.4575 + 0.0064, time: 21.807771]
2023-06-05 16:52:53.855: epoch 46:	0.02958298  	0.07918688  	0.06619756  
2023-06-05 16:53:15.546: [iter 47 : loss : 0.5543 = 0.0907 + 0.4571 + 0.0066, time: 21.686901]
2023-06-05 16:53:15.801: epoch 47:	0.02965701  	0.07943007  	0.06615607  
2023-06-05 16:53:37.362: [iter 48 : loss : 0.5518 = 0.0882 + 0.4569 + 0.0067, time: 21.557917]
2023-06-05 16:53:37.617: epoch 48:	0.02956076  	0.07910713  	0.06608827  
2023-06-05 16:53:59.513: [iter 49 : loss : 0.5489 = 0.0856 + 0.4565 + 0.0069, time: 21.891488]
2023-06-05 16:53:59.771: epoch 49:	0.02947192  	0.07868106  	0.06580215  
2023-06-05 16:54:21.333: [iter 50 : loss : 0.5468 = 0.0835 + 0.4563 + 0.0070, time: 21.558105]
2023-06-05 16:54:21.588: epoch 50:	0.02938308  	0.07830840  	0.06556311  
2023-06-05 16:54:43.100: [iter 51 : loss : 0.5440 = 0.0809 + 0.4560 + 0.0071, time: 21.508319]
2023-06-05 16:54:43.355: epoch 51:	0.02939047  	0.07800794  	0.06554829  
2023-06-05 16:55:04.930: [iter 52 : loss : 0.5413 = 0.0783 + 0.4557 + 0.0073, time: 21.570331]
2023-06-05 16:55:05.194: epoch 52:	0.02931644  	0.07805920  	0.06551018  
2023-06-05 16:55:26.703: [iter 53 : loss : 0.5405 = 0.0776 + 0.4555 + 0.0074, time: 21.505448]
2023-06-05 16:55:26.958: epoch 53:	0.02918318  	0.07768138  	0.06532959  
2023-06-05 16:55:48.684: [iter 54 : loss : 0.5383 = 0.0755 + 0.4552 + 0.0075, time: 21.722314]
2023-06-05 16:55:48.943: epoch 54:	0.02913136  	0.07737369  	0.06505771  
2023-06-05 16:56:10.653: [iter 55 : loss : 0.5359 = 0.0731 + 0.4551 + 0.0077, time: 21.705476]
2023-06-05 16:56:10.909: epoch 55:	0.02904991  	0.07696173  	0.06484292  
2023-06-05 16:56:32.486: [iter 56 : loss : 0.5339 = 0.0713 + 0.4548 + 0.0078, time: 21.573535]
2023-06-05 16:56:32.742: epoch 56:	0.02899070  	0.07659449  	0.06475311  
2023-06-05 16:56:54.307: [iter 57 : loss : 0.5322 = 0.0697 + 0.4546 + 0.0079, time: 21.560962]
2023-06-05 16:56:54.559: epoch 57:	0.02899070  	0.07635702  	0.06462375  
2023-06-05 16:57:16.032: [iter 58 : loss : 0.5312 = 0.0687 + 0.4545 + 0.0080, time: 21.468128]
2023-06-05 16:57:16.288: epoch 58:	0.02888706  	0.07630416  	0.06455771  
2023-06-05 16:57:37.857: [iter 59 : loss : 0.5290 = 0.0666 + 0.4543 + 0.0081, time: 21.564085]
2023-06-05 16:57:38.119: epoch 59:	0.02875380  	0.07620966  	0.06437717  
2023-06-05 16:57:59.614: [iter 60 : loss : 0.5278 = 0.0655 + 0.4541 + 0.0082, time: 21.490512]
2023-06-05 16:57:59.870: epoch 60:	0.02881302  	0.07612359  	0.06434007  
2023-06-05 16:58:21.672: [iter 61 : loss : 0.5267 = 0.0644 + 0.4539 + 0.0084, time: 21.799278]
2023-06-05 16:58:21.928: epoch 61:	0.02884264  	0.07602283  	0.06419277  
2023-06-05 16:58:21.929: Early stopping is trigger at epoch: 61
2023-06-05 16:58:21.929: best_result@epoch 36:

2023-06-05 16:58:21.929: 		0.0301      	0.0818      	0.0671      
2023-06-05 18:27:56.052: my pid: 9264
2023-06-05 18:27:56.052: model: model.general_recommender.SGL
2023-06-05 18:27:56.052: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 18:27:56.052: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 18:28:00.302: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 18:28:22.274: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.970363]
2023-06-05 18:28:22.547: epoch 1:	0.00126592  	0.00232918  	0.00208609  
2023-06-05 18:28:22.547: Find a better model.
2023-06-05 18:28:44.657: [iter 2 : loss : 1.1337 = 0.6930 + 0.4406 + 0.0000, time: 22.105803]
2023-06-05 18:28:44.951: epoch 2:	0.00133254  	0.00323557  	0.00231994  
2023-06-05 18:28:44.951: Find a better model.
2023-06-05 18:29:07.062: [iter 3 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 22.107522]
2023-06-05 18:29:07.360: epoch 3:	0.00158425  	0.00317601  	0.00272251  
2023-06-05 18:29:29.284: [iter 4 : loss : 1.1340 = 0.6928 + 0.4411 + 0.0000, time: 21.921149]
2023-06-05 18:29:29.592: epoch 4:	0.00205804  	0.00454658  	0.00338503  
2023-06-05 18:29:29.592: Find a better model.
2023-06-05 18:29:51.449: [iter 5 : loss : 1.1341 = 0.6927 + 0.4414 + 0.0000, time: 21.848836]
2023-06-05 18:29:51.741: epoch 5:	0.00239858  	0.00566576  	0.00438693  
2023-06-05 18:29:51.741: Find a better model.
2023-06-05 18:30:13.652: [iter 6 : loss : 1.1343 = 0.6926 + 0.4417 + 0.0000, time: 21.906413]
2023-06-05 18:30:13.950: epoch 6:	0.00310927  	0.00782910  	0.00586068  
2023-06-05 18:30:13.950: Find a better model.
2023-06-05 18:30:36.020: [iter 7 : loss : 1.1344 = 0.6924 + 0.4420 + 0.0000, time: 22.066263]
2023-06-05 18:30:36.320: epoch 7:	0.00359787  	0.00897087  	0.00704555  
2023-06-05 18:30:36.320: Find a better model.
2023-06-05 18:30:58.043: [iter 8 : loss : 1.1345 = 0.6921 + 0.4423 + 0.0000, time: 21.718602]
2023-06-05 18:30:58.336: epoch 8:	0.00430856  	0.01105026  	0.00862353  
2023-06-05 18:30:58.336: Find a better model.
2023-06-05 18:31:20.038: [iter 9 : loss : 1.1345 = 0.6917 + 0.4427 + 0.0000, time: 21.699063]
2023-06-05 18:31:20.331: epoch 9:	0.00524133  	0.01516220  	0.01163980  
2023-06-05 18:31:20.331: Find a better model.
2023-06-05 18:31:42.041: [iter 10 : loss : 1.1344 = 0.6912 + 0.4432 + 0.0000, time: 21.706478]
2023-06-05 18:31:42.331: epoch 10:	0.00627034  	0.01731399  	0.01389113  
2023-06-05 18:31:42.331: Find a better model.
2023-06-05 18:32:04.193: [iter 11 : loss : 1.1340 = 0.6904 + 0.4436 + 0.0000, time: 21.857033]
2023-06-05 18:32:04.482: epoch 11:	0.00773612  	0.02217599  	0.01766082  
2023-06-05 18:32:04.482: Find a better model.
2023-06-05 18:32:26.391: [iter 12 : loss : 1.1333 = 0.6891 + 0.4441 + 0.0000, time: 21.905072]
2023-06-05 18:32:26.691: epoch 12:	0.00908345  	0.02627953  	0.02091821  
2023-06-05 18:32:26.691: Find a better model.
2023-06-05 18:32:49.957: [iter 13 : loss : 1.1317 = 0.6868 + 0.4448 + 0.0000, time: 23.262768]
2023-06-05 18:32:50.234: epoch 13:	0.01088982  	0.03094137  	0.02570422  
2023-06-05 18:32:50.234: Find a better model.
2023-06-05 18:33:12.159: [iter 14 : loss : 1.1286 = 0.6830 + 0.4455 + 0.0001, time: 21.920704]
2023-06-05 18:33:12.439: epoch 14:	0.01322180  	0.03796752  	0.03160485  
2023-06-05 18:33:12.439: Find a better model.
2023-06-05 18:33:34.143: [iter 15 : loss : 1.1235 = 0.6769 + 0.4465 + 0.0001, time: 21.700995]
2023-06-05 18:33:34.416: epoch 15:	0.01610164  	0.04485224  	0.03804389  
2023-06-05 18:33:34.416: Find a better model.
2023-06-05 18:33:56.752: [iter 16 : loss : 1.1136 = 0.6662 + 0.4473 + 0.0001, time: 22.332899]
2023-06-05 18:33:57.020: epoch 16:	0.01973659  	0.05441309  	0.04590226  
2023-06-05 18:33:57.020: Find a better model.
2023-06-05 18:34:19.361: [iter 17 : loss : 1.0971 = 0.6484 + 0.4485 + 0.0002, time: 22.336980]
2023-06-05 18:34:19.655: epoch 17:	0.02293481  	0.06282368  	0.05320126  
2023-06-05 18:34:19.655: Find a better model.
2023-06-05 18:34:42.104: [iter 18 : loss : 1.0705 = 0.6200 + 0.4502 + 0.0003, time: 22.445537]
2023-06-05 18:34:42.381: epoch 18:	0.02517059  	0.06902061  	0.05809836  
2023-06-05 18:34:42.381: Find a better model.
2023-06-05 18:35:04.688: [iter 19 : loss : 1.0334 = 0.5805 + 0.4523 + 0.0005, time: 22.302722]
2023-06-05 18:35:04.960: epoch 19:	0.02720647  	0.07351490  	0.06225281  
2023-06-05 18:35:04.960: Find a better model.
2023-06-05 18:35:27.290: [iter 20 : loss : 0.9877 = 0.5317 + 0.4552 + 0.0007, time: 22.325125]
2023-06-05 18:35:27.557: epoch 20:	0.02829476  	0.07623417  	0.06429929  
2023-06-05 18:35:27.558: Find a better model.
2023-06-05 18:35:49.864: [iter 21 : loss : 0.9379 = 0.4783 + 0.4586 + 0.0010, time: 22.302749]
2023-06-05 18:35:50.130: epoch 21:	0.02885000  	0.07775013  	0.06529930  
2023-06-05 18:35:50.131: Find a better model.
2023-06-05 18:36:12.278: [iter 22 : loss : 0.8883 = 0.4254 + 0.4616 + 0.0013, time: 22.144224]
2023-06-05 18:36:12.544: epoch 22:	0.02920537  	0.07882571  	0.06590332  
2023-06-05 18:36:12.544: Find a better model.
2023-06-05 18:36:34.860: [iter 23 : loss : 0.8439 = 0.3779 + 0.4644 + 0.0016, time: 22.311485]
2023-06-05 18:36:35.121: epoch 23:	0.02940526  	0.08005721  	0.06656411  
2023-06-05 18:36:35.121: Find a better model.
2023-06-05 18:36:57.452: [iter 24 : loss : 0.8051 = 0.3371 + 0.4661 + 0.0019, time: 22.326128]
2023-06-05 18:36:57.719: epoch 24:	0.02953112  	0.08045205  	0.06678192  
2023-06-05 18:36:57.719: Find a better model.
2023-06-05 18:37:19.858: [iter 25 : loss : 0.7716 = 0.3024 + 0.4671 + 0.0022, time: 22.135904]
2023-06-05 18:37:20.119: epoch 25:	0.02960516  	0.08087969  	0.06702489  
2023-06-05 18:37:20.119: Find a better model.
2023-06-05 18:37:42.444: [iter 26 : loss : 0.7439 = 0.2740 + 0.4674 + 0.0025, time: 22.320178]
2023-06-05 18:37:42.711: epoch 26:	0.02987167  	0.08112230  	0.06710649  
2023-06-05 18:37:42.712: Find a better model.
2023-06-05 18:38:05.046: [iter 27 : loss : 0.7199 = 0.2497 + 0.4674 + 0.0028, time: 22.331675]
2023-06-05 18:38:05.306: epoch 27:	0.02999753  	0.08169033  	0.06717662  
2023-06-05 18:38:05.306: Find a better model.
2023-06-05 18:38:27.639: [iter 28 : loss : 0.6997 = 0.2297 + 0.4671 + 0.0030, time: 22.328979]
2023-06-05 18:38:27.899: epoch 28:	0.02998273  	0.08212443  	0.06716850  
2023-06-05 18:38:27.899: Find a better model.
2023-06-05 18:38:50.247: [iter 29 : loss : 0.6827 = 0.2128 + 0.4666 + 0.0033, time: 22.344254]
2023-06-05 18:38:50.508: epoch 29:	0.03002715  	0.08250870  	0.06747564  
2023-06-05 18:38:50.508: Find a better model.
2023-06-05 18:39:13.021: [iter 30 : loss : 0.6668 = 0.1973 + 0.4660 + 0.0035, time: 22.508652]
2023-06-05 18:39:13.281: epoch 30:	0.03013819  	0.08243763  	0.06755761  
2023-06-05 18:39:35.798: [iter 31 : loss : 0.6537 = 0.1848 + 0.4652 + 0.0037, time: 22.513313]
2023-06-05 18:39:36.058: epoch 31:	0.03021222  	0.08267752  	0.06773993  
2023-06-05 18:39:36.058: Find a better model.
2023-06-05 18:39:58.594: [iter 32 : loss : 0.6422 = 0.1737 + 0.4646 + 0.0040, time: 22.530687]
2023-06-05 18:39:58.853: epoch 32:	0.03023442  	0.08286566  	0.06797975  
2023-06-05 18:39:58.853: Find a better model.
2023-06-05 18:40:21.236: [iter 33 : loss : 0.6322 = 0.1642 + 0.4639 + 0.0042, time: 22.378502]
2023-06-05 18:40:21.495: epoch 33:	0.03022702  	0.08254454  	0.06809504  
2023-06-05 18:40:44.026: [iter 34 : loss : 0.6228 = 0.1552 + 0.4633 + 0.0044, time: 22.528300]
2023-06-05 18:40:44.287: epoch 34:	0.03020482  	0.08264381  	0.06817768  
2023-06-05 18:41:06.780: [iter 35 : loss : 0.6135 = 0.1463 + 0.4626 + 0.0046, time: 22.488372]
2023-06-05 18:41:07.037: epoch 35:	0.03024183  	0.08268720  	0.06824145  
2023-06-05 18:41:29.365: [iter 36 : loss : 0.6066 = 0.1398 + 0.4620 + 0.0048, time: 22.323573]
2023-06-05 18:41:29.639: epoch 36:	0.03022701  	0.08267055  	0.06817359  
2023-06-05 18:41:51.991: [iter 37 : loss : 0.5997 = 0.1334 + 0.4614 + 0.0049, time: 22.347650]
2023-06-05 18:41:52.249: epoch 37:	0.03013817  	0.08243122  	0.06807663  
2023-06-05 18:42:14.747: [iter 38 : loss : 0.5934 = 0.1274 + 0.4608 + 0.0051, time: 22.494309]
2023-06-05 18:42:15.006: epoch 38:	0.03017519  	0.08260492  	0.06808335  
2023-06-05 18:42:37.334: [iter 39 : loss : 0.5879 = 0.1224 + 0.4602 + 0.0053, time: 22.325635]
2023-06-05 18:42:37.612: epoch 39:	0.03007154  	0.08227077  	0.06794307  
2023-06-05 18:43:00.129: [iter 40 : loss : 0.5824 = 0.1171 + 0.4598 + 0.0055, time: 22.512482]
2023-06-05 18:43:00.384: epoch 40:	0.03010116  	0.08239707  	0.06808662  
2023-06-05 18:43:22.732: [iter 41 : loss : 0.5773 = 0.1123 + 0.4593 + 0.0056, time: 22.343980]
2023-06-05 18:43:22.990: epoch 41:	0.03002712  	0.08196533  	0.06788923  
2023-06-05 18:43:45.325: [iter 42 : loss : 0.5727 = 0.1081 + 0.4589 + 0.0058, time: 22.330146]
2023-06-05 18:43:45.589: epoch 42:	0.02989386  	0.08152378  	0.06751262  
2023-06-05 18:44:08.111: [iter 43 : loss : 0.5686 = 0.1041 + 0.4585 + 0.0060, time: 22.510527]
2023-06-05 18:44:08.372: epoch 43:	0.02999750  	0.08182795  	0.06766336  
2023-06-05 18:44:30.674: [iter 44 : loss : 0.5655 = 0.1013 + 0.4582 + 0.0061, time: 22.297150]
2023-06-05 18:44:30.959: epoch 44:	0.02998269  	0.08171128  	0.06762575  
2023-06-05 18:44:53.385: my pid: 11600
2023-06-05 18:44:53.385: model: model.general_recommender.SGL
2023-06-05 18:44:53.385: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 18:44:53.385: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 18:44:57.374: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 18:45:19.375: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 21.999464]
2023-06-05 18:45:19.654: epoch 1:	0.00114747  	0.00250094  	0.00194138  
2023-06-05 18:45:19.654: Find a better model.
2023-06-05 18:45:41.568: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 21.909914]
2023-06-05 18:45:41.859: epoch 2:	0.00151022  	0.00341402  	0.00239962  
2023-06-05 18:45:41.859: Find a better model.
2023-06-05 18:46:03.977: [iter 3 : loss : 1.1337 = 0.6929 + 0.4408 + 0.0000, time: 22.115105]
2023-06-05 18:46:04.268: epoch 3:	0.00160646  	0.00309484  	0.00270312  
2023-06-05 18:46:26.363: [iter 4 : loss : 1.1339 = 0.6929 + 0.4410 + 0.0000, time: 22.091541]
2023-06-05 18:46:26.667: epoch 4:	0.00224312  	0.00490755  	0.00383735  
2023-06-05 18:46:26.667: Find a better model.
2023-06-05 18:46:48.747: [iter 5 : loss : 1.1340 = 0.6927 + 0.4413 + 0.0000, time: 22.074235]
2023-06-05 18:46:49.040: epoch 5:	0.00248742  	0.00646294  	0.00492540  
2023-06-05 18:46:49.040: Find a better model.
2023-06-05 18:47:10.981: [iter 6 : loss : 1.1342 = 0.6926 + 0.4416 + 0.0000, time: 21.937589]
2023-06-05 18:47:11.275: epoch 6:	0.00290199  	0.00725112  	0.00600196  
2023-06-05 18:47:11.275: Find a better model.
2023-06-05 18:47:33.355: [iter 7 : loss : 1.1343 = 0.6924 + 0.4419 + 0.0000, time: 22.077188]
2023-06-05 18:47:33.659: epoch 7:	0.00333876  	0.00893669  	0.00689316  
2023-06-05 18:47:33.660: Find a better model.
2023-06-05 18:47:55.529: [iter 8 : loss : 1.1344 = 0.6922 + 0.4422 + 0.0000, time: 21.864118]
2023-06-05 18:47:55.825: epoch 8:	0.00381996  	0.01028028  	0.00825725  
2023-06-05 18:47:55.825: Find a better model.
2023-06-05 18:48:17.747: [iter 9 : loss : 1.1344 = 0.6918 + 0.4426 + 0.0000, time: 21.918044]
2023-06-05 18:48:18.037: epoch 9:	0.00498963  	0.01433831  	0.01115114  
2023-06-05 18:48:18.037: Find a better model.
2023-06-05 18:48:40.113: [iter 10 : loss : 1.1343 = 0.6913 + 0.4430 + 0.0000, time: 22.072435]
2023-06-05 18:48:40.402: epoch 10:	0.00614449  	0.01689515  	0.01348079  
2023-06-05 18:48:40.402: Find a better model.
2023-06-05 18:49:04.075: [iter 11 : loss : 1.1340 = 0.6906 + 0.4434 + 0.0000, time: 23.670292]
2023-06-05 18:49:04.352: epoch 11:	0.00761027  	0.02190638  	0.01690653  
2023-06-05 18:49:04.352: Find a better model.
2023-06-05 18:49:26.282: [iter 12 : loss : 1.1333 = 0.6894 + 0.4439 + 0.0000, time: 21.926724]
2023-06-05 18:49:26.556: epoch 12:	0.00902423  	0.02566818  	0.02090666  
2023-06-05 18:49:26.556: Find a better model.
2023-06-05 18:49:48.470: [iter 13 : loss : 1.1321 = 0.6875 + 0.4445 + 0.0000, time: 21.910404]
2023-06-05 18:49:48.748: epoch 13:	0.01130439  	0.03255829  	0.02578556  
2023-06-05 18:49:48.749: Find a better model.
2023-06-05 18:50:10.456: [iter 14 : loss : 1.1294 = 0.6841 + 0.4453 + 0.0001, time: 21.702637]
2023-06-05 18:50:10.735: epoch 14:	0.01343649  	0.03803832  	0.03104364  
2023-06-05 18:50:10.735: Find a better model.
2023-06-05 18:50:32.433: [iter 15 : loss : 1.1245 = 0.6783 + 0.4462 + 0.0001, time: 21.694279]
2023-06-05 18:50:32.711: epoch 15:	0.01647919  	0.04648795  	0.03814349  
2023-06-05 18:50:32.712: Find a better model.
2023-06-05 18:50:55.222: [iter 16 : loss : 1.1154 = 0.6684 + 0.4469 + 0.0001, time: 22.506015]
2023-06-05 18:50:55.493: epoch 16:	0.01992907  	0.05443492  	0.04577999  
2023-06-05 18:50:55.493: Find a better model.
2023-06-05 18:51:17.801: [iter 17 : loss : 1.1001 = 0.6518 + 0.4482 + 0.0002, time: 22.303626]
2023-06-05 18:51:18.069: epoch 17:	0.02284595  	0.06208800  	0.05249996  
2023-06-05 18:51:18.069: Find a better model.
2023-06-05 18:51:40.403: [iter 18 : loss : 1.0748 = 0.6247 + 0.4497 + 0.0003, time: 22.331578]
2023-06-05 18:51:40.683: epoch 18:	0.02515579  	0.06734748  	0.05731426  
2023-06-05 18:51:40.683: Find a better model.
2023-06-05 18:52:02.793: [iter 19 : loss : 1.0388 = 0.5864 + 0.4520 + 0.0005, time: 22.105566]
2023-06-05 18:52:03.057: epoch 19:	0.02689555  	0.07274316  	0.06107561  
2023-06-05 18:52:03.057: Find a better model.
2023-06-05 18:52:25.190: [iter 20 : loss : 0.9937 = 0.5382 + 0.4548 + 0.0007, time: 22.127998]
2023-06-05 18:52:25.448: epoch 20:	0.02796902  	0.07532682  	0.06309876  
2023-06-05 18:52:25.448: Find a better model.
2023-06-05 18:52:47.593: [iter 21 : loss : 0.9440 = 0.4848 + 0.4582 + 0.0010, time: 22.140767]
2023-06-05 18:52:47.850: epoch 21:	0.02859090  	0.07727398  	0.06436232  
2023-06-05 18:52:47.850: Find a better model.
2023-06-05 18:53:09.975: [iter 22 : loss : 0.8939 = 0.4313 + 0.4613 + 0.0013, time: 22.120165]
2023-06-05 18:53:10.235: epoch 22:	0.02880561  	0.07786857  	0.06490342  
2023-06-05 18:53:10.235: Find a better model.
2023-06-05 18:53:32.384: [iter 23 : loss : 0.8487 = 0.3832 + 0.4639 + 0.0016, time: 22.144644]
2023-06-05 18:53:32.657: epoch 23:	0.02900550  	0.07885258  	0.06533989  
2023-06-05 18:53:32.657: Find a better model.
2023-06-05 18:53:54.784: [iter 24 : loss : 0.8092 = 0.3414 + 0.4659 + 0.0019, time: 22.123011]
2023-06-05 18:53:55.040: epoch 24:	0.02903510  	0.07894634  	0.06542227  
2023-06-05 18:53:55.040: Find a better model.
2023-06-05 18:54:17.331: [iter 25 : loss : 0.7752 = 0.3063 + 0.4668 + 0.0022, time: 22.287410]
2023-06-05 18:54:17.589: epoch 25:	0.02916096  	0.07960143  	0.06565236  
2023-06-05 18:54:17.589: Find a better model.
2023-06-05 18:54:39.563: [iter 26 : loss : 0.7469 = 0.2772 + 0.4672 + 0.0025, time: 21.966051]
2023-06-05 18:54:39.825: epoch 26:	0.02929421  	0.08006931  	0.06587050  
2023-06-05 18:54:39.825: Find a better model.
2023-06-05 18:55:02.133: [iter 27 : loss : 0.7229 = 0.2529 + 0.4672 + 0.0027, time: 22.303415]
2023-06-05 18:55:02.393: epoch 27:	0.02947929  	0.08033694  	0.06599145  
2023-06-05 18:55:02.393: Find a better model.
2023-06-05 18:55:24.550: [iter 28 : loss : 0.7023 = 0.2325 + 0.4668 + 0.0030, time: 22.152991]
2023-06-05 18:55:24.810: epoch 28:	0.02963476  	0.08090775  	0.06644259  
2023-06-05 18:55:24.810: Find a better model.
2023-06-05 18:55:47.120: [iter 29 : loss : 0.6847 = 0.2152 + 0.4662 + 0.0032, time: 22.304921]
2023-06-05 18:55:47.376: epoch 29:	0.02978283  	0.08137964  	0.06692998  
2023-06-05 18:55:47.376: Find a better model.
2023-06-05 18:56:09.723: [iter 30 : loss : 0.6689 = 0.1998 + 0.4657 + 0.0035, time: 22.342383]
2023-06-05 18:56:09.980: epoch 30:	0.02987167  	0.08174077  	0.06708861  
2023-06-05 18:56:09.981: Find a better model.
2023-06-05 18:56:32.330: [iter 31 : loss : 0.6550 = 0.1862 + 0.4651 + 0.0037, time: 22.345430]
2023-06-05 18:56:32.588: epoch 31:	0.02996051  	0.08224423  	0.06722420  
2023-06-05 18:56:32.588: Find a better model.
2023-06-05 18:56:55.077: [iter 32 : loss : 0.6436 = 0.1752 + 0.4644 + 0.0039, time: 22.486376]
2023-06-05 18:56:55.335: epoch 32:	0.03005676  	0.08209937  	0.06733174  
2023-06-05 18:57:17.690: [iter 33 : loss : 0.6333 = 0.1655 + 0.4637 + 0.0041, time: 22.352398]
2023-06-05 18:57:17.945: epoch 33:	0.03024924  	0.08245314  	0.06749212  
2023-06-05 18:57:17.946: Find a better model.
2023-06-05 18:57:40.442: [iter 34 : loss : 0.6236 = 0.1563 + 0.4630 + 0.0043, time: 22.492371]
2023-06-05 18:57:40.706: epoch 34:	0.03025664  	0.08264716  	0.06768002  
2023-06-05 18:57:40.706: Find a better model.
2023-06-05 18:58:03.258: [iter 35 : loss : 0.6144 = 0.1476 + 0.4622 + 0.0045, time: 22.548065]
2023-06-05 18:58:03.513: epoch 35:	0.03024924  	0.08260194  	0.06762942  
2023-06-05 18:58:26.033: [iter 36 : loss : 0.6071 = 0.1407 + 0.4617 + 0.0047, time: 22.515000]
2023-06-05 18:58:26.291: epoch 36:	0.03023444  	0.08239299  	0.06769449  
2023-06-05 18:58:48.487: [iter 37 : loss : 0.6007 = 0.1345 + 0.4612 + 0.0049, time: 22.192032]
2023-06-05 18:58:48.749: epoch 37:	0.03035288  	0.08240560  	0.06772537  
2023-06-05 18:59:11.236: [iter 38 : loss : 0.5941 = 0.1284 + 0.4606 + 0.0051, time: 22.483063]
2023-06-05 18:59:11.494: epoch 38:	0.03025664  	0.08223760  	0.06754111  
2023-06-05 18:59:34.020: [iter 39 : loss : 0.5885 = 0.1232 + 0.4600 + 0.0053, time: 22.523032]
2023-06-05 18:59:34.275: epoch 39:	0.03036029  	0.08242129  	0.06766763  
2023-06-05 18:59:56.407: [iter 40 : loss : 0.5828 = 0.1178 + 0.4595 + 0.0055, time: 22.128038]
2023-06-05 18:59:56.675: epoch 40:	0.03046394  	0.08197235  	0.06759550  
2023-06-05 19:00:19.025: [iter 41 : loss : 0.5777 = 0.1131 + 0.4590 + 0.0056, time: 22.346002]
2023-06-05 19:00:19.281: epoch 41:	0.03043432  	0.08177736  	0.06763645  
2023-06-05 19:00:41.428: [iter 42 : loss : 0.5732 = 0.1088 + 0.4586 + 0.0058, time: 22.144206]
2023-06-05 19:00:41.696: epoch 42:	0.03041952  	0.08165743  	0.06771964  
2023-06-05 19:01:04.000: [iter 43 : loss : 0.5689 = 0.1048 + 0.4582 + 0.0059, time: 22.301202]
2023-06-05 19:01:04.257: epoch 43:	0.03037509  	0.08131839  	0.06745549  
2023-06-05 19:01:26.396: [iter 44 : loss : 0.5660 = 0.1021 + 0.4579 + 0.0061, time: 22.135992]
2023-06-05 19:01:26.665: epoch 44:	0.03027145  	0.08120867  	0.06747389  
2023-06-05 19:01:48.974: [iter 45 : loss : 0.5623 = 0.0985 + 0.4575 + 0.0063, time: 22.305002]
2023-06-05 19:01:49.229: epoch 45:	0.03027144  	0.08116384  	0.06735091  
2023-06-05 19:02:11.377: [iter 46 : loss : 0.5587 = 0.0952 + 0.4571 + 0.0064, time: 22.142411]
2023-06-05 19:02:11.647: epoch 46:	0.03019000  	0.08077274  	0.06730092  
2023-06-05 19:02:33.975: [iter 47 : loss : 0.5553 = 0.0921 + 0.4567 + 0.0065, time: 22.324121]
2023-06-05 19:02:34.233: epoch 47:	0.03021221  	0.08054836  	0.06734920  
2023-06-05 19:02:56.358: [iter 48 : loss : 0.5521 = 0.0892 + 0.4562 + 0.0067, time: 22.122303]
2023-06-05 19:02:56.628: epoch 48:	0.03023442  	0.08072328  	0.06746008  
2023-06-05 19:03:18.764: [iter 49 : loss : 0.5497 = 0.0869 + 0.4560 + 0.0068, time: 22.132041]
2023-06-05 19:03:19.020: epoch 49:	0.03016780  	0.08024352  	0.06722488  
2023-06-05 19:03:41.354: [iter 50 : loss : 0.5473 = 0.0846 + 0.4557 + 0.0070, time: 22.329312]
2023-06-05 19:03:41.628: epoch 50:	0.03010856  	0.07988366  	0.06700353  
2023-06-05 19:04:03.986: [iter 51 : loss : 0.5446 = 0.0819 + 0.4556 + 0.0071, time: 22.353114]
2023-06-05 19:04:04.242: epoch 51:	0.03008636  	0.07983916  	0.06711981  
2023-06-05 19:04:26.362: [iter 52 : loss : 0.5416 = 0.0792 + 0.4551 + 0.0072, time: 22.116799]
2023-06-05 19:04:26.633: epoch 52:	0.03015298  	0.07986386  	0.06713165  
2023-06-05 19:04:48.957: [iter 53 : loss : 0.5407 = 0.0784 + 0.4550 + 0.0074, time: 22.321046]
2023-06-05 19:04:49.217: epoch 53:	0.03010857  	0.07950112  	0.06697277  
2023-06-05 19:05:11.345: [iter 54 : loss : 0.5385 = 0.0762 + 0.4548 + 0.0075, time: 22.122399]
2023-06-05 19:05:11.615: epoch 54:	0.02997531  	0.07911371  	0.06682929  
2023-06-05 19:05:33.735: [iter 55 : loss : 0.5363 = 0.0741 + 0.4546 + 0.0076, time: 22.113501]
2023-06-05 19:05:33.995: epoch 55:	0.02994570  	0.07903383  	0.06673910  
2023-06-05 19:05:56.297: [iter 56 : loss : 0.5343 = 0.0722 + 0.4543 + 0.0077, time: 22.298020]
2023-06-05 19:05:56.552: epoch 56:	0.03005675  	0.07940416  	0.06689852  
2023-06-05 19:06:18.925: [iter 57 : loss : 0.5324 = 0.0704 + 0.4542 + 0.0079, time: 22.369767]
2023-06-05 19:06:19.179: epoch 57:	0.02991609  	0.07910073  	0.06668378  
2023-06-05 19:06:41.308: [iter 58 : loss : 0.5312 = 0.0692 + 0.4540 + 0.0080, time: 22.126006]
2023-06-05 19:06:41.563: epoch 58:	0.02987167  	0.07890977  	0.06672581  
2023-06-05 19:07:04.078: [iter 59 : loss : 0.5295 = 0.0676 + 0.4538 + 0.0081, time: 22.511438]
2023-06-05 19:07:04.337: epoch 59:	0.02977543  	0.07843188  	0.06642675  
2023-06-05 19:07:04.337: Early stopping is trigger at epoch: 59
2023-06-05 19:07:04.337: best_result@epoch 34:

2023-06-05 19:07:04.337: 		0.0303      	0.0826      	0.0677      
2023-06-05 20:00:41.374: my pid: 12796
2023-06-05 20:00:41.374: model: model.general_recommender.SGL
2023-06-05 20:00:41.374: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 20:00:41.374: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-05 20:00:45.461: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-05 20:01:07.664: [iter 1 : loss : 1.1342 = 0.6931 + 0.4411 + 0.0000, time: 22.203680]
2023-06-05 20:01:07.938: epoch 1:	0.00108084  	0.00203557  	0.00165003  
2023-06-05 20:01:07.938: Find a better model.
2023-06-05 20:01:30.412: [iter 2 : loss : 1.1335 = 0.6930 + 0.4405 + 0.0000, time: 22.470198]
2023-06-05 20:01:30.717: epoch 2:	0.00116968  	0.00282787  	0.00202557  
2023-06-05 20:01:30.717: Find a better model.
2023-06-05 20:01:52.993: [iter 3 : loss : 1.1336 = 0.6930 + 0.4406 + 0.0000, time: 22.272809]
2023-06-05 20:01:53.286: epoch 3:	0.00135475  	0.00243939  	0.00233181  
2023-06-05 20:02:15.603: [iter 4 : loss : 1.1338 = 0.6929 + 0.4409 + 0.0000, time: 22.313140]
2023-06-05 20:02:15.900: epoch 4:	0.00167308  	0.00318021  	0.00278244  
2023-06-05 20:02:15.900: Find a better model.
2023-06-05 20:02:38.376: [iter 5 : loss : 1.1339 = 0.6928 + 0.4412 + 0.0000, time: 22.473436]
2023-06-05 20:02:38.688: epoch 5:	0.00218389  	0.00474729  	0.00374445  
2023-06-05 20:02:38.688: Find a better model.
2023-06-05 20:03:00.990: [iter 6 : loss : 1.1340 = 0.6926 + 0.4414 + 0.0000, time: 22.297504]
2023-06-05 20:03:01.284: epoch 6:	0.00270210  	0.00599790  	0.00500590  
2023-06-05 20:03:01.284: Find a better model.
2023-06-05 20:03:23.589: [iter 7 : loss : 1.1342 = 0.6925 + 0.4417 + 0.0000, time: 22.301031]
2023-06-05 20:03:23.882: epoch 7:	0.00319070  	0.00748171  	0.00581905  
2023-06-05 20:03:23.882: Find a better model.
2023-06-05 20:03:46.179: [iter 8 : loss : 1.1342 = 0.6922 + 0.4419 + 0.0000, time: 22.294316]
2023-06-05 20:03:46.464: epoch 8:	0.00394581  	0.01041152  	0.00774509  
2023-06-05 20:03:46.464: Find a better model.
2023-06-05 20:04:08.590: [iter 9 : loss : 1.1342 = 0.6919 + 0.4423 + 0.0000, time: 22.123095]
2023-06-05 20:04:08.882: epoch 9:	0.00459727  	0.01167051  	0.00916491  
2023-06-05 20:04:08.882: Find a better model.
2023-06-05 20:04:30.952: [iter 10 : loss : 1.1342 = 0.6914 + 0.4427 + 0.0000, time: 22.065949]
2023-06-05 20:04:31.237: epoch 10:	0.00574473  	0.01554807  	0.01218637  
2023-06-05 20:04:31.237: Find a better model.
2023-06-05 20:04:55.153: [iter 11 : loss : 1.1339 = 0.6908 + 0.4431 + 0.0000, time: 23.912495]
2023-06-05 20:04:55.425: epoch 11:	0.00725492  	0.01999919  	0.01568055  
2023-06-05 20:04:55.425: Find a better model.
2023-06-05 20:05:17.505: [iter 12 : loss : 1.1333 = 0.6897 + 0.4436 + 0.0000, time: 22.074322]
2023-06-05 20:05:17.785: epoch 12:	0.00951282  	0.02732053  	0.02156871  
2023-06-05 20:05:17.785: Find a better model.
2023-06-05 20:05:39.683: [iter 13 : loss : 1.1322 = 0.6880 + 0.4441 + 0.0000, time: 21.894502]
2023-06-05 20:05:39.952: epoch 13:	0.01127477  	0.03093131  	0.02550806  
2023-06-05 20:05:39.952: Find a better model.
2023-06-05 20:06:01.895: [iter 14 : loss : 1.1299 = 0.6850 + 0.4449 + 0.0001, time: 21.938899]
2023-06-05 20:06:02.161: epoch 14:	0.01321440  	0.03715366  	0.03066585  
2023-06-05 20:06:02.161: Find a better model.
2023-06-05 20:06:24.085: [iter 15 : loss : 1.1255 = 0.6797 + 0.4458 + 0.0001, time: 21.919179]
2023-06-05 20:06:24.354: epoch 15:	0.01601280  	0.04501255  	0.03664486  
2023-06-05 20:06:24.354: Find a better model.
2023-06-05 20:06:46.905: [iter 16 : loss : 1.1171 = 0.6706 + 0.4464 + 0.0001, time: 22.547050]
2023-06-05 20:06:47.174: epoch 16:	0.01917397  	0.05302897  	0.04456962  
2023-06-05 20:06:47.174: Find a better model.
2023-06-05 20:07:09.855: [iter 17 : loss : 1.1030 = 0.6552 + 0.4476 + 0.0002, time: 22.676610]
2023-06-05 20:07:10.121: epoch 17:	0.02207605  	0.05934722  	0.05107526  
2023-06-05 20:07:10.121: Find a better model.
2023-06-05 20:07:32.647: [iter 18 : loss : 1.0792 = 0.6297 + 0.4492 + 0.0003, time: 22.521085]
2023-06-05 20:07:32.914: epoch 18:	0.02471902  	0.06619111  	0.05578142  
2023-06-05 20:07:32.914: Find a better model.
2023-06-05 20:07:55.457: [iter 19 : loss : 1.0447 = 0.5929 + 0.4513 + 0.0005, time: 22.539090]
2023-06-05 20:07:55.737: epoch 19:	0.02648100  	0.07056451  	0.05958867  
2023-06-05 20:07:55.737: Find a better model.
2023-06-05 20:08:18.435: [iter 20 : loss : 1.0003 = 0.5456 + 0.4540 + 0.0007, time: 22.695450]
2023-06-05 20:08:18.716: epoch 20:	0.02797645  	0.07557933  	0.06271800  
2023-06-05 20:08:18.716: Find a better model.
2023-06-05 20:08:41.446: [iter 21 : loss : 0.9507 = 0.4925 + 0.4573 + 0.0009, time: 22.726180]
2023-06-05 20:08:41.731: epoch 21:	0.02864273  	0.07751121  	0.06453449  
2023-06-05 20:08:41.731: Find a better model.
2023-06-05 20:09:04.396: [iter 22 : loss : 0.9001 = 0.4385 + 0.4604 + 0.0012, time: 22.660514]
2023-06-05 20:09:04.667: epoch 22:	0.02938306  	0.07997430  	0.06598578  
2023-06-05 20:09:04.667: Find a better model.
2023-06-05 20:09:27.198: [iter 23 : loss : 0.8539 = 0.3892 + 0.4631 + 0.0015, time: 22.522981]
2023-06-05 20:09:27.462: epoch 23:	0.02937566  	0.08030924  	0.06611353  
2023-06-05 20:09:27.462: Find a better model.
2023-06-05 20:09:49.990: [iter 24 : loss : 0.8132 = 0.3464 + 0.4650 + 0.0018, time: 22.525621]
2023-06-05 20:09:50.255: epoch 24:	0.02948672  	0.08105320  	0.06648745  
2023-06-05 20:09:50.255: Find a better model.
2023-06-05 20:10:12.830: [iter 25 : loss : 0.7787 = 0.3103 + 0.4662 + 0.0021, time: 22.570350]
2023-06-05 20:10:13.093: epoch 25:	0.02956816  	0.08168931  	0.06668162  
2023-06-05 20:10:13.093: Find a better model.
2023-06-05 20:10:35.628: [iter 26 : loss : 0.7494 = 0.2804 + 0.4665 + 0.0024, time: 22.531578]
2023-06-05 20:10:35.893: epoch 26:	0.02966439  	0.08200762  	0.06684306  
2023-06-05 20:10:35.893: Find a better model.
2023-06-05 20:10:58.664: [iter 27 : loss : 0.7248 = 0.2555 + 0.4667 + 0.0027, time: 22.766644]
2023-06-05 20:10:58.934: epoch 27:	0.02967920  	0.08218069  	0.06685318  
2023-06-05 20:10:58.934: Find a better model.
2023-06-05 20:11:21.429: [iter 28 : loss : 0.7037 = 0.2343 + 0.4664 + 0.0030, time: 22.492267]
2023-06-05 20:11:21.716: epoch 28:	0.02961997  	0.08181034  	0.06670710  
2023-06-05 20:11:44.406: [iter 29 : loss : 0.6859 = 0.2168 + 0.4658 + 0.0032, time: 22.686039]
2023-06-05 20:11:44.690: epoch 29:	0.02989388  	0.08260445  	0.06720676  
2023-06-05 20:11:44.691: Find a better model.
2023-06-05 20:12:07.419: [iter 30 : loss : 0.6698 = 0.2011 + 0.4653 + 0.0035, time: 22.725139]
2023-06-05 20:12:07.706: epoch 30:	0.02990129  	0.08251903  	0.06723960  
2023-06-05 20:12:30.555: [iter 31 : loss : 0.6559 = 0.1877 + 0.4646 + 0.0037, time: 22.843690]
2023-06-05 20:12:30.834: epoch 31:	0.02974582  	0.08237951  	0.06713435  
2023-06-05 20:12:53.376: [iter 32 : loss : 0.6440 = 0.1763 + 0.4639 + 0.0039, time: 22.538238]
2023-06-05 20:12:53.645: epoch 32:	0.02973842  	0.08186143  	0.06703971  
2023-06-05 20:13:16.311: [iter 33 : loss : 0.6338 = 0.1665 + 0.4632 + 0.0041, time: 22.662466]
2023-06-05 20:13:16.582: epoch 33:	0.02975322  	0.08187559  	0.06695121  
2023-06-05 20:13:39.382: [iter 34 : loss : 0.6242 = 0.1572 + 0.4626 + 0.0043, time: 22.794302]
2023-06-05 20:13:39.658: epoch 34:	0.02992350  	0.08255937  	0.06733058  
2023-06-05 20:14:02.374: [iter 35 : loss : 0.6149 = 0.1486 + 0.4619 + 0.0045, time: 22.707128]
2023-06-05 20:14:02.645: epoch 35:	0.02999013  	0.08257221  	0.06741638  
2023-06-05 20:14:25.483: [iter 36 : loss : 0.6076 = 0.1415 + 0.4614 + 0.0047, time: 22.835513]
2023-06-05 20:14:25.755: epoch 36:	0.02996051  	0.08271551  	0.06759221  
2023-06-05 20:14:25.756: Find a better model.
2023-06-05 20:14:48.339: [iter 37 : loss : 0.6007 = 0.1351 + 0.4606 + 0.0049, time: 22.579410]
2023-06-05 20:14:48.605: epoch 37:	0.03001233  	0.08277899  	0.06766160  
2023-06-05 20:14:48.605: Find a better model.
2023-06-05 20:15:13.101: [iter 38 : loss : 0.5939 = 0.1287 + 0.4601 + 0.0051, time: 24.493210]
2023-06-05 20:15:13.358: epoch 38:	0.03003453  	0.08282933  	0.06764398  
2023-06-05 20:15:13.358: Find a better model.
2023-06-05 20:15:36.095: [iter 39 : loss : 0.5886 = 0.1238 + 0.4595 + 0.0053, time: 22.734380]
2023-06-05 20:15:36.351: epoch 39:	0.03011598  	0.08294792  	0.06766022  
2023-06-05 20:15:36.351: Find a better model.
2023-06-05 20:15:59.122: [iter 40 : loss : 0.5828 = 0.1183 + 0.4591 + 0.0054, time: 22.766601]
2023-06-05 20:15:59.379: epoch 40:	0.03013819  	0.08304016  	0.06771834  
2023-06-05 20:15:59.379: Find a better model.
2023-06-05 20:16:21.876: [iter 41 : loss : 0.5778 = 0.1135 + 0.4586 + 0.0056, time: 22.492878]
2023-06-05 20:16:22.134: epoch 41:	0.03004195  	0.08248580  	0.06759063  
2023-06-05 20:16:44.848: [iter 42 : loss : 0.5733 = 0.1092 + 0.4583 + 0.0058, time: 22.711388]
2023-06-05 20:16:45.102: epoch 42:	0.02994571  	0.08200280  	0.06752505  
2023-06-05 20:17:07.843: [iter 43 : loss : 0.5688 = 0.1051 + 0.4577 + 0.0059, time: 22.736690]
2023-06-05 20:17:08.099: epoch 43:	0.03002714  	0.08224798  	0.06765603  
2023-06-05 20:17:30.830: [iter 44 : loss : 0.5660 = 0.1025 + 0.4574 + 0.0061, time: 22.728459]
2023-06-05 20:17:31.083: epoch 44:	0.02999753  	0.08216099  	0.06772467  
2023-06-05 20:17:53.838: [iter 45 : loss : 0.5617 = 0.0984 + 0.4570 + 0.0062, time: 22.749144]
2023-06-05 20:17:54.093: epoch 45:	0.03004194  	0.08218582  	0.06775656  
2023-06-05 20:18:16.838: [iter 46 : loss : 0.5586 = 0.0955 + 0.4567 + 0.0064, time: 22.741207]
2023-06-05 20:18:17.094: epoch 46:	0.02996051  	0.08235301  	0.06760590  
2023-06-05 20:18:39.825: [iter 47 : loss : 0.5553 = 0.0924 + 0.4564 + 0.0065, time: 22.727437]
2023-06-05 20:18:40.080: epoch 47:	0.02987167  	0.08181512  	0.06743071  
2023-06-05 20:19:02.837: [iter 48 : loss : 0.5524 = 0.0896 + 0.4561 + 0.0067, time: 22.754234]
2023-06-05 20:19:03.092: epoch 48:	0.02979764  	0.08104867  	0.06704543  
2023-06-05 20:19:25.816: [iter 49 : loss : 0.5495 = 0.0869 + 0.4558 + 0.0068, time: 22.718330]
2023-06-05 20:19:26.071: epoch 49:	0.02972361  	0.08071090  	0.06697211  
2023-06-05 20:19:48.800: [iter 50 : loss : 0.5471 = 0.0846 + 0.4555 + 0.0070, time: 22.726439]
2023-06-05 20:19:49.057: epoch 50:	0.02967919  	0.08060539  	0.06674393  
2023-06-05 20:20:11.797: [iter 51 : loss : 0.5447 = 0.0825 + 0.4551 + 0.0071, time: 22.737306]
2023-06-05 20:20:12.051: epoch 51:	0.02963476  	0.08028482  	0.06671409  
2023-06-05 20:20:34.815: [iter 52 : loss : 0.5416 = 0.0795 + 0.4549 + 0.0072, time: 22.759362]
2023-06-05 20:20:35.074: epoch 52:	0.02954593  	0.08022559  	0.06659172  
2023-06-05 20:20:57.767: [iter 53 : loss : 0.5405 = 0.0785 + 0.4546 + 0.0074, time: 22.690235]
2023-06-05 20:20:58.027: epoch 53:	0.02953113  	0.08006628  	0.06653168  
2023-06-05 20:21:20.759: [iter 54 : loss : 0.5384 = 0.0766 + 0.4543 + 0.0075, time: 22.728825]
2023-06-05 20:21:21.014: epoch 54:	0.02944229  	0.07961118  	0.06633407  
2023-06-05 20:21:43.749: [iter 55 : loss : 0.5362 = 0.0744 + 0.4542 + 0.0076, time: 22.731022]
2023-06-05 20:21:44.005: epoch 55:	0.02928682  	0.07926727  	0.06625006  
2023-06-05 20:22:06.549: [iter 56 : loss : 0.5338 = 0.0721 + 0.4540 + 0.0077, time: 22.540531]
2023-06-05 20:22:06.812: epoch 56:	0.02928682  	0.07955360  	0.06624416  
2023-06-05 20:22:29.531: [iter 57 : loss : 0.5321 = 0.0705 + 0.4538 + 0.0079, time: 22.715649]
2023-06-05 20:22:29.793: epoch 57:	0.02933123  	0.07931579  	0.06613651  
2023-06-05 20:22:52.527: [iter 58 : loss : 0.5307 = 0.0691 + 0.4537 + 0.0080, time: 22.729012]
2023-06-05 20:22:52.790: epoch 58:	0.02925720  	0.07889546  	0.06589396  
2023-06-05 20:23:15.336: [iter 59 : loss : 0.5290 = 0.0674 + 0.4535 + 0.0081, time: 22.541430]
2023-06-05 20:23:15.592: epoch 59:	0.02915355  	0.07854975  	0.06572182  
2023-06-05 20:23:38.311: [iter 60 : loss : 0.5281 = 0.0665 + 0.4533 + 0.0082, time: 22.715242]
2023-06-05 20:23:38.568: epoch 60:	0.02901289  	0.07794744  	0.06557672  
2023-06-05 20:24:01.508: [iter 61 : loss : 0.5262 = 0.0648 + 0.4532 + 0.0083, time: 22.936234]
2023-06-05 20:24:01.773: epoch 61:	0.02908692  	0.07771061  	0.06547257  
2023-06-05 20:24:24.333: [iter 62 : loss : 0.5253 = 0.0639 + 0.4530 + 0.0084, time: 22.557069]
2023-06-05 20:24:24.588: epoch 62:	0.02900548  	0.07771859  	0.06539042  
2023-06-05 20:24:47.492: [iter 63 : loss : 0.5243 = 0.0629 + 0.4528 + 0.0085, time: 22.900443]
2023-06-05 20:24:47.757: epoch 63:	0.02902769  	0.07771440  	0.06539646  
2023-06-05 20:25:10.504: [iter 64 : loss : 0.5226 = 0.0613 + 0.4527 + 0.0086, time: 22.743138]
2023-06-05 20:25:10.768: epoch 64:	0.02891664  	0.07697612  	0.06514859  
2023-06-05 20:25:33.466: [iter 65 : loss : 0.5214 = 0.0602 + 0.4525 + 0.0087, time: 22.695221]
2023-06-05 20:25:33.733: epoch 65:	0.02874636  	0.07657292  	0.06475880  
2023-06-05 20:25:33.733: Early stopping is trigger at epoch: 65
2023-06-05 20:25:33.733: best_result@epoch 40:

2023-06-05 20:25:33.733: 		0.0301      	0.0830      	0.0677      
2023-06-05 21:17:27.930: my pid: 9152
2023-06-05 21:17:27.930: model: model.general_recommender.SGL
2023-06-05 21:17:27.930: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-05 21:17:27.930: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.01
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 20:06:44.102: my pid: 6392
2023-06-08 20:06:44.102: model: model.general_recommender.SGL
2023-06-08 20:06:44.102: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-08 20:06:44.102: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=4
ssl_reg=0.05
ssl_ratio=0.3
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 20:06:48.769: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 20:07:12.467: [iter 1 : loss : 1.1359 = 0.6931 + 0.4428 + 0.0000, time: 23.698400]
2023-06-08 20:07:12.767: epoch 1:	0.00147320  	0.00274573  	0.00243243  
2023-06-08 20:07:12.767: Find a better model.
2023-06-08 20:07:36.511: [iter 2 : loss : 1.1346 = 0.6931 + 0.4415 + 0.0000, time: 23.741160]
2023-06-08 20:07:36.817: epoch 2:	0.00170270  	0.00237787  	0.00244210  
2023-06-08 20:07:59.813: [iter 3 : loss : 1.1347 = 0.6930 + 0.4417 + 0.0000, time: 22.992255]
2023-06-08 20:08:00.159: epoch 3:	0.00199882  	0.00317611  	0.00302814  
2023-06-08 20:08:00.159: Find a better model.
2023-06-08 20:08:23.067: [iter 4 : loss : 1.1349 = 0.6930 + 0.4419 + 0.0000, time: 22.904543]
2023-06-08 20:08:23.458: epoch 4:	0.00244300  	0.00445057  	0.00396876  
2023-06-08 20:08:23.458: Find a better model.
2023-06-08 20:08:47.251: [iter 5 : loss : 1.1351 = 0.6930 + 0.4422 + 0.0000, time: 23.789432]
2023-06-08 20:08:47.547: epoch 5:	0.00276873  	0.00454126  	0.00458598  
2023-06-08 20:08:47.547: Find a better model.
2023-06-08 20:09:11.131: [iter 6 : loss : 1.1353 = 0.6929 + 0.4424 + 0.0000, time: 23.577162]
2023-06-08 20:09:11.470: epoch 6:	0.00355345  	0.00654435  	0.00572349  
2023-06-08 20:09:11.470: Find a better model.
2023-06-08 20:09:35.188: [iter 7 : loss : 1.1354 = 0.6928 + 0.4426 + 0.0000, time: 23.713804]
2023-06-08 20:09:35.580: epoch 7:	0.00350163  	0.00571958  	0.00550782  
2023-06-08 20:09:57.947: [iter 8 : loss : 1.1356 = 0.6928 + 0.4428 + 0.0000, time: 22.363368]
2023-06-08 20:09:58.251: epoch 8:	0.00404945  	0.00667476  	0.00677063  
2023-06-08 20:09:58.252: Find a better model.
2023-06-08 20:10:19.737: [iter 9 : loss : 1.1357 = 0.6927 + 0.4430 + 0.0000, time: 21.481085]
2023-06-08 20:10:20.019: epoch 9:	0.00473793  	0.00849932  	0.00825804  
2023-06-08 20:10:20.019: Find a better model.
2023-06-08 20:10:41.304: [iter 10 : loss : 1.1358 = 0.6926 + 0.4432 + 0.0000, time: 21.281483]
2023-06-08 20:10:41.586: epoch 10:	0.00535237  	0.00944245  	0.00881021  
2023-06-08 20:10:41.586: Find a better model.
2023-06-08 20:11:02.932: [iter 11 : loss : 1.1359 = 0.6924 + 0.4435 + 0.0000, time: 21.342071]
2023-06-08 20:11:03.228: epoch 11:	0.00564108  	0.01079295  	0.01023683  
2023-06-08 20:11:03.228: Find a better model.
2023-06-08 20:11:24.486: [iter 12 : loss : 1.1360 = 0.6923 + 0.4437 + 0.0000, time: 21.251661]
2023-06-08 20:11:24.762: epoch 12:	0.00631475  	0.01177745  	0.01145857  
2023-06-08 20:11:24.762: Find a better model.
2023-06-08 20:11:45.658: [iter 13 : loss : 1.1360 = 0.6920 + 0.4440 + 0.0000, time: 20.890393]
2023-06-08 20:11:45.928: epoch 13:	0.00715869  	0.01490376  	0.01278308  
2023-06-08 20:11:45.928: Find a better model.
2023-06-08 20:12:07.260: [iter 14 : loss : 1.1360 = 0.6918 + 0.4442 + 0.0000, time: 21.326008]
2023-06-08 20:12:07.547: epoch 14:	0.00801003  	0.01615324  	0.01440613  
2023-06-08 20:12:07.547: Find a better model.
2023-06-08 20:12:29.035: [iter 15 : loss : 1.1359 = 0.6914 + 0.4445 + 0.0000, time: 21.481431]
2023-06-08 20:12:29.327: epoch 15:	0.00889098  	0.01872714  	0.01674509  
2023-06-08 20:12:29.328: Find a better model.
2023-06-08 20:12:50.851: [iter 16 : loss : 1.1357 = 0.6909 + 0.4448 + 0.0000, time: 21.518139]
2023-06-08 20:12:51.146: epoch 16:	0.00931295  	0.02013602  	0.01726928  
2023-06-08 20:12:51.147: Find a better model.
2023-06-08 20:13:12.631: [iter 17 : loss : 1.1353 = 0.6900 + 0.4452 + 0.0000, time: 21.480208]
2023-06-08 20:13:12.930: epoch 17:	0.00968311  	0.02189041  	0.01870943  
2023-06-08 20:13:12.930: Find a better model.
2023-06-08 20:13:34.412: [iter 18 : loss : 1.1344 = 0.6887 + 0.4457 + 0.0000, time: 21.476273]
2023-06-08 20:13:34.699: epoch 18:	0.01017913  	0.02312580  	0.02043869  
2023-06-08 20:13:34.699: Find a better model.
2023-06-08 20:13:56.202: [iter 19 : loss : 1.1333 = 0.6871 + 0.4462 + 0.0001, time: 21.497491]
2023-06-08 20:13:56.486: epoch 19:	0.01212613  	0.02859047  	0.02506051  
2023-06-08 20:13:56.487: Find a better model.
2023-06-08 20:14:17.847: [iter 20 : loss : 1.1316 = 0.6849 + 0.4467 + 0.0001, time: 21.357318]
2023-06-08 20:14:18.135: epoch 20:	0.01385848  	0.03405678  	0.02962025  
2023-06-08 20:14:18.135: Find a better model.
2023-06-08 20:14:40.431: [iter 21 : loss : 1.1289 = 0.6817 + 0.4471 + 0.0001, time: 22.293370]
2023-06-08 20:14:40.721: epoch 21:	0.01641256  	0.04062732  	0.03576284  
2023-06-08 20:14:40.721: Find a better model.
2023-06-08 20:15:02.983: [iter 22 : loss : 1.1247 = 0.6769 + 0.4476 + 0.0001, time: 22.257227]
2023-06-08 20:15:03.269: epoch 22:	0.01830038  	0.04563656  	0.04063628  
2023-06-08 20:15:03.269: Find a better model.
2023-06-08 20:15:25.403: [iter 23 : loss : 1.1179 = 0.6694 + 0.4483 + 0.0002, time: 22.130016]
2023-06-08 20:15:25.688: epoch 23:	0.02079526  	0.05242445  	0.04637749  
2023-06-08 20:15:25.689: Find a better model.
2023-06-08 20:15:47.756: [iter 24 : loss : 1.1073 = 0.6579 + 0.4491 + 0.0003, time: 22.064227]
2023-06-08 20:15:48.039: epoch 24:	0.02325314  	0.06002740  	0.05220602  
2023-06-08 20:15:48.039: Find a better model.
2023-06-08 20:16:10.180: [iter 25 : loss : 1.0911 = 0.6405 + 0.4502 + 0.0004, time: 22.136988]
2023-06-08 20:16:10.464: epoch 25:	0.02519281  	0.06585199  	0.05680503  
2023-06-08 20:16:10.464: Find a better model.
2023-06-08 20:16:32.518: [iter 26 : loss : 1.0688 = 0.6165 + 0.4518 + 0.0005, time: 22.050097]
2023-06-08 20:16:32.799: epoch 26:	0.02682890  	0.07055507  	0.06015607  
2023-06-08 20:16:32.799: Find a better model.
2023-06-08 20:16:54.922: [iter 27 : loss : 1.0392 = 0.5848 + 0.4536 + 0.0007, time: 22.118263]
2023-06-08 20:16:55.202: epoch 27:	0.02816148  	0.07429090  	0.06286155  
2023-06-08 20:16:55.202: Find a better model.
2023-06-08 20:17:17.515: [iter 28 : loss : 1.0037 = 0.5464 + 0.4562 + 0.0010, time: 22.308353]
2023-06-08 20:17:17.794: epoch 28:	0.02904247  	0.07674263  	0.06433505  
2023-06-08 20:17:17.794: Find a better model.
2023-06-08 20:17:39.945: [iter 29 : loss : 0.9644 = 0.5038 + 0.4593 + 0.0013, time: 22.147523]
2023-06-08 20:17:40.226: epoch 29:	0.02956069  	0.07912850  	0.06550143  
2023-06-08 20:17:40.226: Find a better model.
2023-06-08 20:18:02.520: [iter 30 : loss : 0.9238 = 0.4594 + 0.4627 + 0.0016, time: 22.290020]
2023-06-08 20:18:02.805: epoch 30:	0.02961253  	0.07981323  	0.06582768  
2023-06-08 20:18:02.805: Find a better model.
2023-06-08 20:18:25.076: [iter 31 : loss : 0.8841 = 0.4164 + 0.4658 + 0.0020, time: 22.266983]
2023-06-08 20:18:25.361: epoch 31:	0.02964956  	0.08061990  	0.06602104  
2023-06-08 20:18:25.361: Find a better model.
2023-06-08 20:18:47.506: [iter 32 : loss : 0.8485 = 0.3783 + 0.4679 + 0.0024, time: 22.141663]
2023-06-08 20:18:47.788: epoch 32:	0.02976061  	0.08099980  	0.06614228  
2023-06-08 20:18:47.788: Find a better model.
2023-06-08 20:19:10.073: [iter 33 : loss : 0.8166 = 0.3442 + 0.4697 + 0.0027, time: 22.279004]
2023-06-08 20:19:10.353: epoch 33:	0.02978282  	0.08116221  	0.06623807  
2023-06-08 20:19:10.353: Find a better model.
2023-06-08 20:19:32.674: [iter 34 : loss : 0.7882 = 0.3145 + 0.4706 + 0.0031, time: 22.316096]
2023-06-08 20:19:32.953: epoch 34:	0.02990128  	0.08168241  	0.06649528  
2023-06-08 20:19:32.953: Find a better model.
2023-06-08 20:19:55.203: [iter 35 : loss : 0.7630 = 0.2887 + 0.4709 + 0.0034, time: 22.246130]
2023-06-08 20:19:55.483: epoch 35:	0.02993830  	0.08221119  	0.06674361  
2023-06-08 20:19:55.483: Find a better model.
2023-06-08 20:20:19.416: [iter 36 : loss : 0.7417 = 0.2670 + 0.4710 + 0.0037, time: 23.928363]
2023-06-08 20:20:19.681: epoch 36:	0.03006415  	0.08236638  	0.06688243  
2023-06-08 20:20:19.681: Find a better model.
2023-06-08 20:20:41.616: [iter 37 : loss : 0.7229 = 0.2481 + 0.4707 + 0.0041, time: 21.929575]
2023-06-08 20:20:41.882: epoch 37:	0.03024923  	0.08373009  	0.06747339  
2023-06-08 20:20:41.882: Find a better model.
2023-06-08 20:21:03.695: [iter 38 : loss : 0.7059 = 0.2313 + 0.4702 + 0.0044, time: 21.806735]
2023-06-08 20:21:03.959: epoch 38:	0.03038990  	0.08428935  	0.06765264  
2023-06-08 20:21:03.959: Find a better model.
2023-06-08 20:21:25.826: [iter 39 : loss : 0.6920 = 0.2177 + 0.4697 + 0.0047, time: 21.863115]
2023-06-08 20:21:26.092: epoch 39:	0.03047875  	0.08438383  	0.06775583  
2023-06-08 20:21:26.092: Find a better model.
2023-06-08 20:21:48.057: [iter 40 : loss : 0.6785 = 0.2044 + 0.4691 + 0.0050, time: 21.961102]
2023-06-08 20:21:48.331: epoch 40:	0.03053056  	0.08478900  	0.06780544  
2023-06-08 20:21:48.331: Find a better model.
2023-06-08 20:22:09.810: [iter 41 : loss : 0.6666 = 0.1929 + 0.4685 + 0.0053, time: 21.476141]
2023-06-08 20:22:10.077: epoch 41:	0.03064161  	0.08503781  	0.06799483  
2023-06-08 20:22:10.077: Find a better model.
2023-06-08 20:22:31.836: [iter 42 : loss : 0.6561 = 0.1828 + 0.4677 + 0.0055, time: 21.754021]
2023-06-08 20:22:32.101: epoch 42:	0.03053057  	0.08490232  	0.06804759  
2023-06-08 20:22:53.835: [iter 43 : loss : 0.6464 = 0.1734 + 0.4672 + 0.0058, time: 21.729014]
2023-06-08 20:22:54.100: epoch 43:	0.03064902  	0.08526777  	0.06847063  
2023-06-08 20:22:54.100: Find a better model.
2023-06-08 20:23:15.796: [iter 44 : loss : 0.6386 = 0.1658 + 0.4667 + 0.0061, time: 21.692452]
2023-06-08 20:23:16.064: epoch 44:	0.03069345  	0.08540598  	0.06860232  
2023-06-08 20:23:16.064: Find a better model.
2023-06-08 20:23:37.782: [iter 45 : loss : 0.6303 = 0.1581 + 0.4659 + 0.0063, time: 21.713251]
2023-06-08 20:23:38.049: epoch 45:	0.03078228  	0.08561988  	0.06880805  
2023-06-08 20:23:38.049: Find a better model.
2023-06-08 20:23:59.981: [iter 46 : loss : 0.6233 = 0.1514 + 0.4654 + 0.0066, time: 21.927008]
2023-06-08 20:24:00.246: epoch 46:	0.03064903  	0.08527941  	0.06863193  
2023-06-08 20:24:21.997: [iter 47 : loss : 0.6164 = 0.1448 + 0.4648 + 0.0068, time: 21.747624]
2023-06-08 20:24:22.273: epoch 47:	0.03076748  	0.08574562  	0.06883317  
2023-06-08 20:24:22.274: Find a better model.
2023-06-08 20:24:44.382: [iter 48 : loss : 0.6105 = 0.1393 + 0.4642 + 0.0070, time: 22.105058]
2023-06-08 20:24:44.659: epoch 48:	0.03078229  	0.08562909  	0.06871390  
2023-06-08 20:25:06.966: [iter 49 : loss : 0.6049 = 0.1341 + 0.4635 + 0.0073, time: 22.303074]
2023-06-08 20:25:07.244: epoch 49:	0.03067123  	0.08516979  	0.06850892  
2023-06-08 20:25:29.358: [iter 50 : loss : 0.5999 = 0.1294 + 0.4631 + 0.0075, time: 22.109100]
2023-06-08 20:25:29.643: epoch 50:	0.03068603  	0.08514137  	0.06836836  
2023-06-08 20:25:51.915: [iter 51 : loss : 0.5945 = 0.1243 + 0.4625 + 0.0077, time: 22.268059]
2023-06-08 20:25:52.194: epoch 51:	0.03076747  	0.08520549  	0.06853442  
2023-06-08 20:26:14.540: [iter 52 : loss : 0.5898 = 0.1198 + 0.4621 + 0.0079, time: 22.341070]
2023-06-08 20:26:14.822: epoch 52:	0.03069343  	0.08483668  	0.06834639  
2023-06-08 20:26:36.948: [iter 53 : loss : 0.5866 = 0.1167 + 0.4617 + 0.0081, time: 22.123023]
2023-06-08 20:26:37.228: epoch 53:	0.03072304  	0.08487475  	0.06851639  
2023-06-08 20:26:59.338: [iter 54 : loss : 0.5824 = 0.1128 + 0.4613 + 0.0083, time: 22.106067]
2023-06-08 20:26:59.621: epoch 54:	0.03075264  	0.08465081  	0.06849854  
2023-06-08 20:27:21.945: [iter 55 : loss : 0.5788 = 0.1093 + 0.4609 + 0.0085, time: 22.320198]
2023-06-08 20:27:22.223: epoch 55:	0.03066382  	0.08449977  	0.06851882  
2023-06-08 20:27:44.326: [iter 56 : loss : 0.5747 = 0.1055 + 0.4604 + 0.0087, time: 22.099062]
2023-06-08 20:27:44.609: epoch 56:	0.03064160  	0.08416113  	0.06846334  
2023-06-08 20:28:06.896: [iter 57 : loss : 0.5713 = 0.1023 + 0.4601 + 0.0089, time: 22.283068]
2023-06-08 20:28:07.178: epoch 57:	0.03057498  	0.08393589  	0.06836207  
2023-06-08 20:28:29.497: [iter 58 : loss : 0.5689 = 0.0999 + 0.4599 + 0.0091, time: 22.315055]
2023-06-08 20:28:29.778: epoch 58:	0.03063420  	0.08424330  	0.06837123  
2023-06-08 20:28:51.908: [iter 59 : loss : 0.5661 = 0.0973 + 0.4595 + 0.0093, time: 22.126132]
2023-06-08 20:28:52.194: epoch 59:	0.03059718  	0.08400270  	0.06824903  
2023-06-08 20:29:14.480: [iter 60 : loss : 0.5635 = 0.0948 + 0.4592 + 0.0095, time: 22.282047]
2023-06-08 20:29:14.762: epoch 60:	0.03056016  	0.08402928  	0.06816730  
2023-06-08 20:29:37.073: [iter 61 : loss : 0.5605 = 0.0920 + 0.4588 + 0.0097, time: 22.307078]
2023-06-08 20:29:37.355: epoch 61:	0.03052315  	0.08386550  	0.06805509  
2023-06-08 20:29:59.472: [iter 62 : loss : 0.5588 = 0.0903 + 0.4586 + 0.0099, time: 22.113034]
2023-06-08 20:29:59.753: epoch 62:	0.03056016  	0.08383913  	0.06811802  
2023-06-08 20:30:22.027: [iter 63 : loss : 0.5561 = 0.0879 + 0.4583 + 0.0100, time: 22.268086]
2023-06-08 20:30:22.307: epoch 63:	0.03046392  	0.08375936  	0.06807404  
2023-06-08 20:30:44.643: [iter 64 : loss : 0.5541 = 0.0859 + 0.4580 + 0.0102, time: 22.331067]
2023-06-08 20:30:44.924: epoch 64:	0.03041210  	0.08358090  	0.06807938  
2023-06-08 20:31:07.058: [iter 65 : loss : 0.5518 = 0.0837 + 0.4578 + 0.0103, time: 22.131063]
2023-06-08 20:31:07.339: epoch 65:	0.03033066  	0.08354337  	0.06806019  
2023-06-08 20:31:29.637: [iter 66 : loss : 0.5499 = 0.0818 + 0.4576 + 0.0105, time: 22.293096]
2023-06-08 20:31:29.916: epoch 66:	0.03031586  	0.08359471  	0.06797910  
2023-06-08 20:31:52.204: [iter 67 : loss : 0.5478 = 0.0798 + 0.4573 + 0.0107, time: 22.284308]
2023-06-08 20:31:52.484: epoch 67:	0.03037508  	0.08368170  	0.06800007  
2023-06-08 20:32:14.635: [iter 68 : loss : 0.5467 = 0.0787 + 0.4571 + 0.0108, time: 22.147017]
2023-06-08 20:32:14.914: epoch 68:	0.03042691  	0.08374279  	0.06805526  
2023-06-08 20:32:37.175: [iter 69 : loss : 0.5445 = 0.0766 + 0.4569 + 0.0110, time: 22.257096]
2023-06-08 20:32:37.453: epoch 69:	0.03035288  	0.08327895  	0.06777615  
2023-06-08 20:32:59.826: [iter 70 : loss : 0.5430 = 0.0753 + 0.4566 + 0.0112, time: 22.368120]
2023-06-08 20:33:00.112: epoch 70:	0.03023443  	0.08291462  	0.06745853  
2023-06-08 20:33:22.365: [iter 71 : loss : 0.5414 = 0.0736 + 0.4565 + 0.0113, time: 22.248421]
2023-06-08 20:33:22.646: epoch 71:	0.03032327  	0.08288243  	0.06742181  
2023-06-08 20:33:44.791: [iter 72 : loss : 0.5403 = 0.0725 + 0.4564 + 0.0114, time: 22.141164]
2023-06-08 20:33:45.073: epoch 72:	0.03021222  	0.08240242  	0.06727559  
2023-06-08 20:33:45.073: Early stopping is trigger at epoch: 72
2023-06-08 20:33:45.073: best_result@epoch 47:

2023-06-08 20:33:45.073: 		0.0308      	0.0857      	0.0688      
2023-06-08 20:40:49.857: my pid: 15196
2023-06-08 20:40:49.857: model: model.general_recommender.SGL
2023-06-08 20:40:49.857: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-08 20:40:49.857: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.05
ssl_ratio=0.3
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 20:40:54.083: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 20:41:14.356: [iter 1 : loss : 1.1332 = 0.6930 + 0.4402 + 0.0000, time: 20.271869]
2023-06-08 20:41:14.634: epoch 1:	0.00151022  	0.00287624  	0.00249954  
2023-06-08 20:41:14.634: Find a better model.
2023-06-08 20:41:34.824: [iter 2 : loss : 1.1331 = 0.6929 + 0.4403 + 0.0000, time: 20.187832]
2023-06-08 20:41:35.115: epoch 2:	0.00230234  	0.00364689  	0.00323569  
2023-06-08 20:41:35.115: Find a better model.
2023-06-08 20:41:55.087: [iter 3 : loss : 1.1332 = 0.6926 + 0.4406 + 0.0000, time: 19.968872]
2023-06-08 20:41:55.394: epoch 3:	0.00330175  	0.00589503  	0.00515634  
2023-06-08 20:41:55.394: Find a better model.
2023-06-08 20:42:16.270: [iter 4 : loss : 1.1332 = 0.6922 + 0.4410 + 0.0000, time: 20.872061]
2023-06-08 20:42:16.564: epoch 4:	0.00424193  	0.00882731  	0.00771427  
2023-06-08 20:42:16.564: Find a better model.
2023-06-08 20:42:38.407: [iter 5 : loss : 1.1330 = 0.6916 + 0.4415 + 0.0000, time: 21.839685]
2023-06-08 20:42:38.714: epoch 5:	0.00612968  	0.01361458  	0.01207161  
2023-06-08 20:42:38.714: Find a better model.
2023-06-08 20:42:59.940: [iter 6 : loss : 1.1324 = 0.6903 + 0.4421 + 0.0000, time: 21.222359]
2023-06-08 20:43:00.348: epoch 6:	0.00809146  	0.01985290  	0.01714061  
2023-06-08 20:43:00.349: Find a better model.
2023-06-08 20:43:22.043: [iter 7 : loss : 1.1305 = 0.6875 + 0.4430 + 0.0000, time: 21.689618]
2023-06-08 20:43:22.437: epoch 7:	0.01058630  	0.02759563  	0.02390519  
2023-06-08 20:43:22.437: Find a better model.
2023-06-08 20:43:43.700: [iter 8 : loss : 1.1261 = 0.6820 + 0.4441 + 0.0000, time: 21.258771]
2023-06-08 20:43:44.000: epoch 8:	0.01449514  	0.03858378  	0.03360640  
2023-06-08 20:43:44.000: Find a better model.
2023-06-08 20:44:05.175: [iter 9 : loss : 1.1159 = 0.6706 + 0.4453 + 0.0001, time: 21.171280]
2023-06-08 20:44:05.510: epoch 9:	0.01959595  	0.05118858  	0.04484611  
2023-06-08 20:44:05.510: Find a better model.
2023-06-08 20:44:26.732: [iter 10 : loss : 1.0940 = 0.6470 + 0.4469 + 0.0001, time: 21.217817]
2023-06-08 20:44:27.025: epoch 10:	0.02343084  	0.06165438  	0.05355321  
2023-06-08 20:44:27.025: Find a better model.
2023-06-08 20:44:47.965: [iter 11 : loss : 1.0550 = 0.6057 + 0.4490 + 0.0002, time: 20.936676]
2023-06-08 20:44:48.281: epoch 11:	0.02647357  	0.06969579  	0.05940378  
2023-06-08 20:44:48.281: Find a better model.
2023-06-08 20:45:10.045: [iter 12 : loss : 0.9985 = 0.5461 + 0.4520 + 0.0004, time: 21.759387]
2023-06-08 20:45:10.391: epoch 12:	0.02823557  	0.07472900  	0.06281532  
2023-06-08 20:45:10.391: Find a better model.
2023-06-08 20:45:32.311: [iter 13 : loss : 0.9344 = 0.4783 + 0.4554 + 0.0006, time: 21.915349]
2023-06-08 20:45:32.612: epoch 13:	0.02903511  	0.07705599  	0.06471345  
2023-06-08 20:45:32.612: Find a better model.
2023-06-08 20:45:54.407: [iter 14 : loss : 0.8711 = 0.4117 + 0.4585 + 0.0009, time: 21.790985]
2023-06-08 20:45:54.791: epoch 14:	0.02932384  	0.07861906  	0.06556851  
2023-06-08 20:45:54.791: Find a better model.
2023-06-08 20:46:15.806: [iter 15 : loss : 0.8164 = 0.3545 + 0.4608 + 0.0011, time: 21.011076]
2023-06-08 20:46:16.157: epoch 15:	0.02941268  	0.07987201  	0.06594105  
2023-06-08 20:46:16.158: Find a better model.
2023-06-08 20:46:37.660: [iter 16 : loss : 0.7720 = 0.3085 + 0.4622 + 0.0013, time: 21.498460]
2023-06-08 20:46:37.969: epoch 16:	0.02945711  	0.08007271  	0.06585750  
2023-06-08 20:46:37.969: Find a better model.
2023-06-08 20:46:59.212: [iter 17 : loss : 0.7355 = 0.2712 + 0.4628 + 0.0016, time: 21.238114]
2023-06-08 20:46:59.548: epoch 17:	0.02964960  	0.08083515  	0.06613132  
2023-06-08 20:46:59.548: Find a better model.
2023-06-08 20:47:22.957: [iter 18 : loss : 0.7057 = 0.2412 + 0.4627 + 0.0018, time: 23.404631]
2023-06-08 20:47:23.294: epoch 18:	0.02964960  	0.08092399  	0.06633925  
2023-06-08 20:47:23.294: Find a better model.
2023-06-08 20:47:44.202: [iter 19 : loss : 0.6816 = 0.2172 + 0.4624 + 0.0020, time: 20.904262]
2023-06-08 20:47:44.496: epoch 19:	0.02973843  	0.08120039  	0.06648690  
2023-06-08 20:47:44.496: Find a better model.
2023-06-08 20:48:06.330: [iter 20 : loss : 0.6618 = 0.1977 + 0.4619 + 0.0022, time: 21.830707]
2023-06-08 20:48:06.631: epoch 20:	0.02981246  	0.08178969  	0.06673427  
2023-06-08 20:48:06.631: Find a better model.
2023-06-08 20:48:29.298: [iter 21 : loss : 0.6452 = 0.1817 + 0.4611 + 0.0023, time: 22.661299]
2023-06-08 20:48:29.573: epoch 21:	0.02984949  	0.08149865  	0.06685337  
2023-06-08 20:48:52.002: [iter 22 : loss : 0.6304 = 0.1674 + 0.4604 + 0.0025, time: 22.425625]
2023-06-08 20:48:52.324: epoch 22:	0.02986429  	0.08125875  	0.06690378  
2023-06-08 20:49:15.004: [iter 23 : loss : 0.6184 = 0.1559 + 0.4599 + 0.0027, time: 22.674443]
2023-06-08 20:49:15.324: epoch 23:	0.02988649  	0.08137851  	0.06698285  
2023-06-08 20:49:38.105: [iter 24 : loss : 0.6079 = 0.1458 + 0.4592 + 0.0028, time: 22.777128]
2023-06-08 20:49:38.428: epoch 24:	0.02986428  	0.08129112  	0.06695599  
2023-06-08 20:50:00.907: [iter 25 : loss : 0.5983 = 0.1368 + 0.4585 + 0.0030, time: 22.474057]
2023-06-08 20:50:01.336: epoch 25:	0.02980505  	0.08086289  	0.06699397  
2023-06-08 20:50:23.329: [iter 26 : loss : 0.5899 = 0.1287 + 0.4581 + 0.0031, time: 21.986741]
2023-06-08 20:50:23.692: epoch 26:	0.02979025  	0.08066891  	0.06701198  
2023-06-08 20:50:46.087: [iter 27 : loss : 0.5821 = 0.1215 + 0.4573 + 0.0032, time: 22.391064]
2023-06-08 20:50:46.463: epoch 27:	0.02998274  	0.08096162  	0.06724486  
2023-06-08 20:51:08.559: [iter 28 : loss : 0.5760 = 0.1157 + 0.4568 + 0.0034, time: 22.092113]
2023-06-08 20:51:08.927: epoch 28:	0.02992351  	0.08046558  	0.06710907  
2023-06-08 20:51:31.231: [iter 29 : loss : 0.5703 = 0.1104 + 0.4563 + 0.0035, time: 22.300892]
2023-06-08 20:51:31.590: epoch 29:	0.02984207  	0.08039204  	0.06678951  
2023-06-08 20:51:53.649: [iter 30 : loss : 0.5646 = 0.1051 + 0.4559 + 0.0036, time: 22.053747]
2023-06-08 20:51:54.016: epoch 30:	0.02981245  	0.07993728  	0.06681769  
2023-06-08 20:52:15.915: [iter 31 : loss : 0.5594 = 0.1002 + 0.4554 + 0.0038, time: 21.894267]
2023-06-08 20:52:16.250: epoch 31:	0.02969400  	0.07977273  	0.06663182  
2023-06-08 20:52:38.133: [iter 32 : loss : 0.5548 = 0.0960 + 0.4549 + 0.0039, time: 21.878935]
2023-06-08 20:52:38.470: epoch 32:	0.02958295  	0.07897508  	0.06627455  
2023-06-08 20:53:00.607: [iter 33 : loss : 0.5516 = 0.0929 + 0.4547 + 0.0040, time: 22.132481]
2023-06-08 20:53:00.981: epoch 33:	0.02956074  	0.07882599  	0.06625742  
2023-06-08 20:53:23.077: [iter 34 : loss : 0.5472 = 0.0888 + 0.4543 + 0.0041, time: 22.090478]
2023-06-08 20:53:23.450: epoch 34:	0.02950151  	0.07860851  	0.06618463  
2023-06-08 20:53:45.466: [iter 35 : loss : 0.5435 = 0.0855 + 0.4539 + 0.0042, time: 22.012491]
2023-06-08 20:53:45.830: epoch 35:	0.02944969  	0.07853570  	0.06591827  
2023-06-08 20:54:08.405: [iter 36 : loss : 0.5405 = 0.0827 + 0.4535 + 0.0043, time: 22.570948]
2023-06-08 20:54:08.767: epoch 36:	0.02959035  	0.07873603  	0.06603294  
2023-06-08 20:54:30.875: [iter 37 : loss : 0.5378 = 0.0802 + 0.4532 + 0.0044, time: 22.103175]
2023-06-08 20:54:31.257: epoch 37:	0.02946450  	0.07800659  	0.06564366  
2023-06-08 20:54:53.711: [iter 38 : loss : 0.5343 = 0.0768 + 0.4529 + 0.0045, time: 22.449314]
2023-06-08 20:54:54.068: epoch 38:	0.02947189  	0.07792614  	0.06566706  
2023-06-08 20:55:16.386: [iter 39 : loss : 0.5325 = 0.0753 + 0.4526 + 0.0046, time: 22.313169]
2023-06-08 20:55:16.738: epoch 39:	0.02935344  	0.07744094  	0.06543818  
2023-06-08 20:55:38.815: [iter 40 : loss : 0.5294 = 0.0723 + 0.4525 + 0.0047, time: 22.072119]
2023-06-08 20:55:39.178: epoch 40:	0.02919056  	0.07653731  	0.06508736  
2023-06-08 20:56:00.887: [iter 41 : loss : 0.5269 = 0.0699 + 0.4522 + 0.0048, time: 21.705131]
2023-06-08 20:56:01.145: epoch 41:	0.02904990  	0.07603775  	0.06468300  
2023-06-08 20:56:21.787: [iter 42 : loss : 0.5250 = 0.0681 + 0.4520 + 0.0049, time: 20.638082]
2023-06-08 20:56:22.048: epoch 42:	0.02901288  	0.07566591  	0.06447966  
2023-06-08 20:56:42.390: [iter 43 : loss : 0.5229 = 0.0662 + 0.4518 + 0.0050, time: 20.337982]
2023-06-08 20:56:42.648: epoch 43:	0.02897586  	0.07563499  	0.06455492  
2023-06-08 20:57:03.175: [iter 44 : loss : 0.5218 = 0.0651 + 0.4517 + 0.0050, time: 20.523096]
2023-06-08 20:57:03.440: epoch 44:	0.02893885  	0.07553192  	0.06434139  
2023-06-08 20:57:23.996: [iter 45 : loss : 0.5196 = 0.0630 + 0.4514 + 0.0051, time: 20.553001]
2023-06-08 20:57:24.273: epoch 45:	0.02885741  	0.07520433  	0.06419884  
2023-06-08 20:57:24.273: Early stopping is trigger at epoch: 45
2023-06-08 20:57:24.273: best_result@epoch 20:

2023-06-08 20:57:24.273: 		0.0298      	0.0818      	0.0667      
2023-06-08 21:11:05.515: my pid: 8228
2023-06-08 21:11:05.515: model: model.general_recommender.SGL
2023-06-08 21:11:05.515: Dataset statistics:
Name: book-crossing_polluted
The number of users: 6754
The number of items: 13670
The number of ratings: 377690
Average actions of users: 55.92
Average actions of items: 27.63
The sparsity of the dataset: 99.590922%

The number of training: 309044
The number of validation: 0
The number of testing: 68646
2023-06-08 21:11:05.515: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing_polluted
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=1
ssl_reg=0.05
ssl_ratio=0.3
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 21:11:10.254: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 21:11:31.052: [iter 1 : loss : 1.1324 = 0.6927 + 0.4397 + 0.0000, time: 20.796907]
2023-06-08 21:11:31.356: epoch 1:	0.00186556  	0.00392545  	0.00325924  
2023-06-08 21:11:31.356: Find a better model.
2023-06-08 21:11:51.769: [iter 2 : loss : 1.1314 = 0.6916 + 0.4398 + 0.0000, time: 20.410117]
2023-06-08 21:11:52.071: epoch 2:	0.00444181  	0.00982631  	0.00845647  
2023-06-08 21:11:52.071: Find a better model.
2023-06-08 21:12:11.909: [iter 3 : loss : 1.1291 = 0.6886 + 0.4405 + 0.0000, time: 19.834018]
2023-06-08 21:12:12.305: epoch 3:	0.00989779  	0.02676299  	0.02256514  
2023-06-08 21:12:12.305: Find a better model.
2023-06-08 21:12:31.965: [iter 4 : loss : 1.1211 = 0.6792 + 0.4419 + 0.0000, time: 19.651061]
2023-06-08 21:12:32.258: epoch 4:	0.01630894  	0.04530912  	0.03768295  
2023-06-08 21:12:32.258: Find a better model.
2023-06-08 21:12:51.788: [iter 5 : loss : 1.0959 = 0.6521 + 0.4438 + 0.0001, time: 19.527665]
2023-06-08 21:12:52.153: epoch 5:	0.02197238  	0.05917439  	0.05022264  
2023-06-08 21:12:52.154: Find a better model.
2023-06-08 21:13:11.535: [iter 6 : loss : 1.0419 = 0.5957 + 0.4460 + 0.0002, time: 19.378215]
2023-06-08 21:13:11.830: epoch 6:	0.02533349  	0.06776990  	0.05684100  
2023-06-08 21:13:11.830: Find a better model.
2023-06-08 21:13:32.141: [iter 7 : loss : 0.9650 = 0.5159 + 0.4488 + 0.0003, time: 20.307086]
2023-06-08 21:13:32.468: epoch 7:	0.02676232  	0.07239179  	0.05994376  
2023-06-08 21:13:32.468: Find a better model.
2023-06-08 21:13:52.366: [iter 8 : loss : 0.8862 = 0.4343 + 0.4514 + 0.0004, time: 19.893312]
2023-06-08 21:13:52.745: epoch 8:	0.02739900  	0.07380451  	0.06115846  
2023-06-08 21:13:52.745: Find a better model.
2023-06-08 21:14:12.703: [iter 9 : loss : 0.8164 = 0.3626 + 0.4532 + 0.0006, time: 19.955545]
2023-06-08 21:14:12.994: epoch 9:	0.02778397  	0.07475199  	0.06154302  
2023-06-08 21:14:12.994: Find a better model.
2023-06-08 21:14:32.576: [iter 10 : loss : 0.7616 = 0.3065 + 0.4543 + 0.0008, time: 19.578470]
2023-06-08 21:14:32.855: epoch 10:	0.02785800  	0.07530411  	0.06181273  
2023-06-08 21:14:32.855: Find a better model.
2023-06-08 21:14:52.448: [iter 11 : loss : 0.7194 = 0.2636 + 0.4548 + 0.0009, time: 19.589506]
2023-06-08 21:14:52.729: epoch 11:	0.02796165  	0.07618108  	0.06200727  
2023-06-08 21:14:52.730: Find a better model.
2023-06-08 21:15:13.338: [iter 12 : loss : 0.6856 = 0.2296 + 0.4549 + 0.0011, time: 20.604711]
2023-06-08 21:15:13.609: epoch 12:	0.02822077  	0.07640078  	0.06216219  
2023-06-08 21:15:13.610: Find a better model.
2023-06-08 21:15:33.339: [iter 13 : loss : 0.6597 = 0.2039 + 0.4547 + 0.0012, time: 19.725029]
2023-06-08 21:15:33.703: epoch 13:	0.02823557  	0.07636407  	0.06236777  
2023-06-08 21:15:53.849: [iter 14 : loss : 0.6376 = 0.1820 + 0.4543 + 0.0013, time: 20.142207]
2023-06-08 21:15:54.159: epoch 14:	0.02825037  	0.07626507  	0.06226442  
2023-06-08 21:16:15.990: [iter 15 : loss : 0.6205 = 0.1652 + 0.4539 + 0.0015, time: 21.825615]
2023-06-08 21:16:16.366: epoch 15:	0.02811712  	0.07590318  	0.06231054  
2023-06-08 21:16:35.794: [iter 16 : loss : 0.6065 = 0.1515 + 0.4534 + 0.0016, time: 19.425066]
2023-06-08 21:16:36.075: epoch 16:	0.02811711  	0.07572668  	0.06245995  
2023-06-08 21:16:56.435: [iter 17 : loss : 0.5937 = 0.1391 + 0.4529 + 0.0017, time: 20.356575]
2023-06-08 21:16:56.706: epoch 17:	0.02798385  	0.07466281  	0.06207281  
2023-06-08 21:17:16.425: [iter 18 : loss : 0.5829 = 0.1287 + 0.4524 + 0.0018, time: 19.714005]
2023-06-08 21:17:16.779: epoch 18:	0.02800607  	0.07455509  	0.06208516  
2023-06-08 21:17:36.457: [iter 19 : loss : 0.5738 = 0.1200 + 0.4520 + 0.0019, time: 19.672992]
2023-06-08 21:17:36.735: epoch 19:	0.02786539  	0.07437555  	0.06188133  
2023-06-08 21:17:56.264: [iter 20 : loss : 0.5656 = 0.1121 + 0.4515 + 0.0020, time: 19.525060]
2023-06-08 21:17:56.545: epoch 20:	0.02788020  	0.07434606  	0.06179414  
2023-06-08 21:18:17.213: [iter 21 : loss : 0.5591 = 0.1062 + 0.4509 + 0.0020, time: 20.665305]
2023-06-08 21:18:17.606: epoch 21:	0.02762849  	0.07390726  	0.06139352  
2023-06-08 21:18:38.545: [iter 22 : loss : 0.5525 = 0.0999 + 0.4505 + 0.0021, time: 20.933531]
2023-06-08 21:18:38.861: epoch 22:	0.02748043  	0.07374737  	0.06110930  
2023-06-08 21:18:59.176: [iter 23 : loss : 0.5472 = 0.0948 + 0.4502 + 0.0022, time: 20.311620]
2023-06-08 21:18:59.472: epoch 23:	0.02731756  	0.07337084  	0.06107226  
2023-06-08 21:19:19.733: [iter 24 : loss : 0.5424 = 0.0903 + 0.4498 + 0.0023, time: 20.257601]
2023-06-08 21:19:20.018: epoch 24:	0.02733237  	0.07306047  	0.06101224  
2023-06-08 21:19:40.104: [iter 25 : loss : 0.5378 = 0.0861 + 0.4494 + 0.0024, time: 20.083035]
2023-06-08 21:19:40.403: epoch 25:	0.02733236  	0.07271747  	0.06086968  
2023-06-08 21:20:01.354: [iter 26 : loss : 0.5336 = 0.0820 + 0.4492 + 0.0024, time: 20.948373]
2023-06-08 21:20:01.616: epoch 26:	0.02730276  	0.07254895  	0.06065278  
2023-06-08 21:20:22.354: [iter 27 : loss : 0.5298 = 0.0784 + 0.4489 + 0.0025, time: 20.732535]
2023-06-08 21:20:22.687: epoch 27:	0.02719172  	0.07199586  	0.06031609  
2023-06-08 21:20:42.950: [iter 28 : loss : 0.5268 = 0.0756 + 0.4486 + 0.0026, time: 20.257585]
2023-06-08 21:20:43.232: epoch 28:	0.02730277  	0.07179802  	0.06040231  
2023-06-08 21:21:03.365: [iter 29 : loss : 0.5241 = 0.0731 + 0.4484 + 0.0026, time: 20.128981]
2023-06-08 21:21:03.630: epoch 29:	0.02730276  	0.07204251  	0.06048960  
2023-06-08 21:21:24.091: [iter 30 : loss : 0.5207 = 0.0698 + 0.4482 + 0.0027, time: 20.457990]
2023-06-08 21:21:24.380: epoch 30:	0.02716951  	0.07131149  	0.05998913  
2023-06-08 21:21:45.212: [iter 31 : loss : 0.5177 = 0.0670 + 0.4479 + 0.0028, time: 20.828004]
2023-06-08 21:21:45.587: epoch 31:	0.02710287  	0.07068050  	0.05976229  
2023-06-08 21:22:05.628: [iter 32 : loss : 0.5151 = 0.0646 + 0.4477 + 0.0028, time: 20.036256]
2023-06-08 21:22:05.883: epoch 32:	0.02693260  	0.07047563  	0.05959610  
2023-06-08 21:22:26.383: [iter 33 : loss : 0.5136 = 0.0631 + 0.4476 + 0.0029, time: 20.496212]
2023-06-08 21:22:26.644: epoch 33:	0.02682896  	0.06995680  	0.05941030  
2023-06-08 21:22:46.734: [iter 34 : loss : 0.5114 = 0.0610 + 0.4474 + 0.0029, time: 20.086004]
2023-06-08 21:22:46.995: epoch 34:	0.02666609  	0.06929275  	0.05894554  
2023-06-08 21:23:07.898: [iter 35 : loss : 0.5091 = 0.0588 + 0.4473 + 0.0030, time: 20.898132]
2023-06-08 21:23:08.158: epoch 35:	0.02654023  	0.06897464  	0.05876359  
2023-06-08 21:23:29.024: [iter 36 : loss : 0.5075 = 0.0573 + 0.4471 + 0.0030, time: 20.861069]
2023-06-08 21:23:29.380: epoch 36:	0.02634774  	0.06823989  	0.05826331  
2023-06-08 21:23:49.663: [iter 37 : loss : 0.5061 = 0.0560 + 0.4469 + 0.0031, time: 20.279070]
2023-06-08 21:23:49.930: epoch 37:	0.02633293  	0.06807414  	0.05811841  
2023-06-08 21:23:49.930: Early stopping is trigger at epoch: 37
2023-06-08 21:23:49.930: best_result@epoch 12:

2023-06-08 21:23:49.930: 		0.0282      	0.0764      	0.0622      
