2023-06-06 10:44:28.181: my pid: 15480
2023-06-06 10:44:28.181: model: model.general_recommender.SGL
2023-06-06 10:44:28.181: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 10:44:28.181: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 10:44:33.038: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 10:45:13.106: [iter 1 : loss : 1.6029 = 0.6931 + 0.9098 + 0.0000, time: 40.067916]
2023-06-06 10:45:13.515: epoch 1:	0.00424363  	0.01040566  	0.00850284  
2023-06-06 10:45:13.515: Find a better model.
2023-06-06 10:45:53.303: [iter 2 : loss : 1.6002 = 0.6931 + 0.9072 + 0.0000, time: 39.781935]
2023-06-06 10:45:53.725: epoch 2:	0.00511922  	0.01190594  	0.00970527  
2023-06-06 10:45:53.725: Find a better model.
2023-06-06 10:46:33.683: [iter 3 : loss : 1.5998 = 0.6930 + 0.9068 + 0.0000, time: 39.950704]
2023-06-06 10:46:34.086: epoch 3:	0.00659105  	0.01435280  	0.01220121  
2023-06-06 10:46:34.087: Find a better model.
2023-06-06 10:47:13.958: [iter 4 : loss : 1.5996 = 0.6929 + 0.9066 + 0.0000, time: 39.865055]
2023-06-06 10:47:14.363: epoch 4:	0.00787493  	0.01663115  	0.01465562  
2023-06-06 10:47:14.364: Find a better model.
2023-06-06 10:47:54.357: [iter 5 : loss : 1.5995 = 0.6929 + 0.9066 + 0.0000, time: 39.986150]
2023-06-06 10:47:54.768: epoch 5:	0.00930923  	0.01892601  	0.01651079  
2023-06-06 10:47:54.768: Find a better model.
2023-06-06 10:48:34.937: [iter 6 : loss : 1.5992 = 0.6928 + 0.9065 + 0.0000, time: 40.162215]
2023-06-06 10:48:35.343: epoch 6:	0.01085633  	0.02269096  	0.01996795  
2023-06-06 10:48:35.343: Find a better model.
2023-06-06 10:49:15.326: [iter 7 : loss : 1.5991 = 0.6926 + 0.9064 + 0.0000, time: 39.977505]
2023-06-06 10:49:15.741: epoch 7:	0.01271500  	0.02695461  	0.02392431  
2023-06-06 10:49:15.741: Find a better model.
2023-06-06 10:49:57.497: [iter 8 : loss : 1.5989 = 0.6924 + 0.9064 + 0.0000, time: 41.749305]
2023-06-06 10:49:57.895: epoch 8:	0.01470261  	0.03139130  	0.02770577  
2023-06-06 10:49:57.895: Find a better model.
2023-06-06 10:50:38.515: [iter 9 : loss : 1.5984 = 0.6921 + 0.9063 + 0.0000, time: 40.614474]
2023-06-06 10:50:38.907: epoch 9:	0.01700718  	0.03785904  	0.03327918  
2023-06-06 10:50:38.907: Find a better model.
2023-06-06 10:51:19.447: [iter 10 : loss : 1.5982 = 0.6917 + 0.9064 + 0.0000, time: 40.533202]
2023-06-06 10:51:19.846: epoch 10:	0.02053650  	0.04595568  	0.04107528  
2023-06-06 10:51:19.846: Find a better model.
2023-06-06 10:52:01.181: [iter 11 : loss : 1.5974 = 0.6910 + 0.9064 + 0.0000, time: 41.328661]
2023-06-06 10:52:01.566: epoch 11:	0.02414641  	0.05523411  	0.04908301  
2023-06-06 10:52:01.566: Find a better model.
2023-06-06 10:52:42.816: [iter 12 : loss : 1.5964 = 0.6899 + 0.9065 + 0.0000, time: 41.243213]
2023-06-06 10:52:43.207: epoch 12:	0.02875533  	0.06517629  	0.05967969  
2023-06-06 10:52:43.207: Find a better model.
2023-06-06 10:53:24.608: [iter 13 : loss : 1.5944 = 0.6876 + 0.9068 + 0.0000, time: 41.394202]
2023-06-06 10:53:24.999: epoch 13:	0.03570041  	0.07966981  	0.07393591  
2023-06-06 10:53:24.999: Find a better model.
2023-06-06 10:54:06.438: [iter 14 : loss : 1.5903 = 0.6828 + 0.9074 + 0.0001, time: 41.432492]
2023-06-06 10:54:06.836: epoch 14:	0.04379520  	0.09543241  	0.09084731  
2023-06-06 10:54:06.836: Find a better model.
2023-06-06 10:54:48.373: [iter 15 : loss : 1.5805 = 0.6717 + 0.9086 + 0.0001, time: 41.530477]
2023-06-06 10:54:48.778: epoch 15:	0.05424285  	0.11349603  	0.11093443  
2023-06-06 10:54:48.778: Find a better model.
2023-06-06 10:55:30.189: [iter 16 : loss : 1.5594 = 0.6477 + 0.9114 + 0.0003, time: 41.405793]
2023-06-06 10:55:30.583: epoch 16:	0.06495341  	0.13033183  	0.12931652  
2023-06-06 10:55:30.584: Find a better model.
2023-06-06 10:56:12.165: [iter 17 : loss : 1.5213 = 0.6049 + 0.9159 + 0.0005, time: 41.565164]
2023-06-06 10:56:12.554: epoch 17:	0.07362850  	0.14298820  	0.14328642  
2023-06-06 10:56:12.554: Find a better model.
2023-06-06 10:56:54.148: [iter 18 : loss : 1.4682 = 0.5453 + 0.9220 + 0.0009, time: 41.587938]
2023-06-06 10:56:54.535: epoch 18:	0.07891405  	0.15109390  	0.15208487  
2023-06-06 10:56:54.535: Find a better model.
2023-06-06 10:57:35.901: [iter 19 : loss : 1.4097 = 0.4798 + 0.9286 + 0.0014, time: 41.357837]
2023-06-06 10:57:36.289: epoch 19:	0.08266868  	0.15748072  	0.15808810  
2023-06-06 10:57:36.289: Find a better model.
2023-06-06 10:58:17.605: [iter 20 : loss : 1.3539 = 0.4182 + 0.9339 + 0.0019, time: 41.308742]
2023-06-06 10:58:17.993: epoch 20:	0.08482810  	0.16157003  	0.16171229  
2023-06-06 10:58:17.993: Find a better model.
2023-06-06 10:58:59.454: [iter 21 : loss : 1.3060 = 0.3663 + 0.9374 + 0.0024, time: 41.455428]
2023-06-06 10:58:59.852: epoch 21:	0.08665442  	0.16486162  	0.16452461  
2023-06-06 10:58:59.853: Find a better model.
2023-06-06 10:59:41.086: [iter 22 : loss : 1.2654 = 0.3232 + 0.9393 + 0.0029, time: 41.225860]
2023-06-06 10:59:41.488: epoch 22:	0.08762132  	0.16677484  	0.16639310  
2023-06-06 10:59:41.488: Find a better model.
2023-06-06 11:00:22.452: [iter 23 : loss : 1.2316 = 0.2884 + 0.9399 + 0.0033, time: 40.957158]
2023-06-06 11:00:22.861: epoch 23:	0.08842169  	0.16805355  	0.16762170  
2023-06-06 11:00:22.861: Find a better model.
2023-06-06 11:01:04.046: [iter 24 : loss : 1.2034 = 0.2597 + 0.9399 + 0.0038, time: 41.178518]
2023-06-06 11:01:04.450: epoch 24:	0.08914138  	0.16900323  	0.16873191  
2023-06-06 11:01:04.450: Find a better model.
2023-06-06 11:01:45.472: [iter 25 : loss : 1.1794 = 0.2359 + 0.9393 + 0.0042, time: 41.016491]
2023-06-06 11:01:45.870: epoch 25:	0.08985048  	0.16996428  	0.16962868  
2023-06-06 11:01:45.870: Find a better model.
2023-06-06 11:02:26.836: [iter 26 : loss : 1.1588 = 0.2156 + 0.9385 + 0.0047, time: 40.960033]
2023-06-06 11:02:27.238: epoch 26:	0.09019428  	0.17151476  	0.17060579  
2023-06-06 11:02:27.238: Find a better model.
2023-06-06 11:03:08.458: [iter 27 : loss : 1.1425 = 0.1999 + 0.9375 + 0.0051, time: 41.213013]
2023-06-06 11:03:08.859: epoch 27:	0.09050587  	0.17134839  	0.17096171  
2023-06-06 11:03:50.008: [iter 28 : loss : 1.1263 = 0.1844 + 0.9364 + 0.0055, time: 41.141697]
2023-06-06 11:03:50.411: epoch 28:	0.09040381  	0.17064676  	0.17062935  
2023-06-06 11:04:31.553: [iter 29 : loss : 1.1135 = 0.1723 + 0.9354 + 0.0058, time: 41.134715]
2023-06-06 11:04:31.957: epoch 29:	0.09060799  	0.17059143  	0.17065100  
2023-06-06 11:05:13.444: [iter 30 : loss : 1.1021 = 0.1616 + 0.9343 + 0.0062, time: 41.480439]
2023-06-06 11:05:13.854: epoch 30:	0.09052742  	0.16992007  	0.17037652  
2023-06-06 11:05:55.003: [iter 31 : loss : 1.0912 = 0.1515 + 0.9332 + 0.0065, time: 41.141369]
2023-06-06 11:05:55.405: epoch 31:	0.09059726  	0.17023706  	0.17032130  
2023-06-06 11:06:36.745: [iter 32 : loss : 1.0822 = 0.1432 + 0.9321 + 0.0069, time: 41.332963]
2023-06-06 11:06:37.147: epoch 32:	0.09037168  	0.16954607  	0.16977638  
2023-06-06 11:07:18.555: [iter 33 : loss : 1.0737 = 0.1356 + 0.9310 + 0.0072, time: 41.401361]
2023-06-06 11:07:18.959: epoch 33:	0.09017827  	0.16878068  	0.16919668  
2023-06-06 11:08:01.904: [iter 34 : loss : 1.0665 = 0.1288 + 0.9302 + 0.0075, time: 42.938622]
2023-06-06 11:08:02.294: epoch 34:	0.09011381  	0.16844779  	0.16891587  
2023-06-06 11:08:43.723: [iter 35 : loss : 1.0601 = 0.1231 + 0.9293 + 0.0078, time: 41.423723]
2023-06-06 11:08:44.113: epoch 35:	0.08984520  	0.16734619  	0.16810898  
2023-06-06 11:09:25.629: [iter 36 : loss : 1.0540 = 0.1175 + 0.9285 + 0.0081, time: 41.509095]
2023-06-06 11:09:26.018: epoch 36:	0.08983441  	0.16701506  	0.16793288  
2023-06-06 11:10:07.639: [iter 37 : loss : 1.0480 = 0.1120 + 0.9277 + 0.0083, time: 41.614706]
2023-06-06 11:10:08.027: epoch 37:	0.08972692  	0.16656926  	0.16736527  
2023-06-06 11:10:49.583: [iter 38 : loss : 1.0433 = 0.1077 + 0.9270 + 0.0086, time: 41.549712]
2023-06-06 11:10:49.973: epoch 38:	0.08957122  	0.16632168  	0.16672388  
2023-06-06 11:11:31.665: [iter 39 : loss : 1.0385 = 0.1033 + 0.9263 + 0.0089, time: 41.684448]
2023-06-06 11:11:32.072: epoch 39:	0.08928113  	0.16538587  	0.16604553  
2023-06-06 11:12:13.585: [iter 40 : loss : 1.0343 = 0.0996 + 0.9256 + 0.0091, time: 41.506691]
2023-06-06 11:12:13.993: epoch 40:	0.08916300  	0.16479640  	0.16558112  
2023-06-06 11:12:55.583: [iter 41 : loss : 1.0302 = 0.0959 + 0.9249 + 0.0094, time: 41.583515]
2023-06-06 11:12:55.986: epoch 41:	0.08893738  	0.16436458  	0.16507939  
2023-06-06 11:13:37.144: [iter 42 : loss : 1.0267 = 0.0927 + 0.9244 + 0.0096, time: 41.152446]
2023-06-06 11:13:37.551: epoch 42:	0.08868489  	0.16315752  	0.16445430  
2023-06-06 11:14:18.785: [iter 43 : loss : 1.0232 = 0.0896 + 0.9238 + 0.0098, time: 41.226314]
2023-06-06 11:14:19.189: epoch 43:	0.08842709  	0.16247870  	0.16383071  
2023-06-06 11:15:00.545: [iter 44 : loss : 1.0200 = 0.0867 + 0.9233 + 0.0100, time: 41.350087]
2023-06-06 11:15:00.953: epoch 44:	0.08816926  	0.16191378  	0.16318673  
2023-06-06 11:15:44.762: [iter 45 : loss : 1.0173 = 0.0843 + 0.9228 + 0.0103, time: 43.803447]
2023-06-06 11:15:45.147: epoch 45:	0.08789531  	0.16116363  	0.16230759  
2023-06-06 11:16:26.677: [iter 46 : loss : 1.0145 = 0.0817 + 0.9224 + 0.0105, time: 41.522787]
2023-06-06 11:16:27.067: epoch 46:	0.08768578  	0.16044128  	0.16182281  
2023-06-06 11:17:08.395: [iter 47 : loss : 1.0123 = 0.0797 + 0.9220 + 0.0107, time: 41.321549]
2023-06-06 11:17:08.795: epoch 47:	0.08734741  	0.15960415  	0.16123892  
2023-06-06 11:17:50.444: [iter 48 : loss : 1.0096 = 0.0772 + 0.9215 + 0.0109, time: 41.642504]
2023-06-06 11:17:50.843: epoch 48:	0.08711643  	0.15895025  	0.16040799  
2023-06-06 11:18:32.302: [iter 49 : loss : 1.0074 = 0.0752 + 0.9211 + 0.0111, time: 41.453187]
2023-06-06 11:18:32.710: epoch 49:	0.08700901  	0.15830447  	0.15994373  
2023-06-06 11:19:14.243: [iter 50 : loss : 1.0052 = 0.0732 + 0.9208 + 0.0112, time: 41.525503]
2023-06-06 11:19:14.656: epoch 50:	0.08661154  	0.15738878  	0.15924001  
2023-06-06 11:19:57.500: [iter 51 : loss : 1.0031 = 0.0712 + 0.9204 + 0.0114, time: 42.837614]
2023-06-06 11:19:57.935: epoch 51:	0.08636975  	0.15679631  	0.15865688  
2023-06-06 11:19:57.936: Early stopping is trigger at epoch: 51
2023-06-06 11:19:57.936: best_result@epoch 26:

2023-06-06 11:19:57.936: 		0.0902      	0.1715      	0.1706      
2023-06-06 11:20:16.850: my pid: 14104
2023-06-06 11:20:16.850: model: model.general_recommender.SGL
2023-06-06 11:20:16.850: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 11:20:16.850: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 11:20:22.097: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 11:21:02.510: [iter 1 : loss : 1.6035 = 0.6931 + 0.9104 + 0.0000, time: 40.412631]
2023-06-06 11:21:02.910: epoch 1:	0.00396968  	0.00993247  	0.00801219  
2023-06-06 11:21:02.910: Find a better model.
2023-06-06 11:21:41.140: [iter 2 : loss : 1.6008 = 0.6931 + 0.9077 + 0.0000, time: 38.223814]
2023-06-06 11:21:41.528: epoch 2:	0.00532334  	0.01258268  	0.01020344  
2023-06-06 11:21:41.528: Find a better model.
2023-06-06 11:22:19.924: [iter 3 : loss : 1.6004 = 0.6930 + 0.9074 + 0.0000, time: 38.389722]
2023-06-06 11:22:20.325: epoch 3:	0.00695096  	0.01507573  	0.01288794  
2023-06-06 11:22:20.325: Find a better model.
2023-06-06 11:22:58.275: [iter 4 : loss : 1.6003 = 0.6930 + 0.9073 + 0.0000, time: 37.943307]
2023-06-06 11:22:58.692: epoch 4:	0.00806832  	0.01648690  	0.01447706  
2023-06-06 11:22:58.692: Find a better model.
2023-06-06 11:23:37.125: [iter 5 : loss : 1.6002 = 0.6929 + 0.9073 + 0.0000, time: 38.427689]
2023-06-06 11:23:37.526: epoch 5:	0.00907824  	0.01817262  	0.01590827  
2023-06-06 11:23:37.526: Find a better model.
2023-06-06 11:24:16.087: [iter 6 : loss : 1.6000 = 0.6928 + 0.9072 + 0.0000, time: 38.553591]
2023-06-06 11:24:16.471: epoch 6:	0.01063071  	0.02207072  	0.01908993  
2023-06-06 11:24:16.472: Find a better model.
2023-06-06 11:24:54.810: [iter 7 : loss : 1.5999 = 0.6926 + 0.9073 + 0.0000, time: 38.330843]
2023-06-06 11:24:55.201: epoch 7:	0.01250012  	0.02691162  	0.02354814  
2023-06-06 11:24:55.201: Find a better model.
2023-06-06 11:25:35.432: [iter 8 : loss : 1.5998 = 0.6925 + 0.9074 + 0.0000, time: 40.224983]
2023-06-06 11:25:35.829: epoch 8:	0.01392904  	0.02938860  	0.02602258  
2023-06-06 11:25:35.829: Find a better model.
2023-06-06 11:26:14.601: [iter 9 : loss : 1.5995 = 0.6922 + 0.9072 + 0.0000, time: 38.765647]
2023-06-06 11:26:14.988: epoch 9:	0.01615305  	0.03507172  	0.03131563  
2023-06-06 11:26:14.988: Find a better model.
2023-06-06 11:26:53.501: [iter 10 : loss : 1.5993 = 0.6919 + 0.9075 + 0.0000, time: 38.506001]
2023-06-06 11:26:53.892: epoch 10:	0.01949435  	0.04239894  	0.03841798  
2023-06-06 11:26:53.892: Find a better model.
2023-06-06 11:27:33.558: [iter 11 : loss : 1.5987 = 0.6913 + 0.9074 + 0.0000, time: 39.660118]
2023-06-06 11:27:33.948: epoch 11:	0.02386705  	0.05230045  	0.04752328  
2023-06-06 11:27:33.948: Find a better model.
2023-06-06 11:28:13.744: [iter 12 : loss : 1.5980 = 0.6903 + 0.9076 + 0.0000, time: 39.788871]
2023-06-06 11:28:14.129: epoch 12:	0.02905075  	0.06412628  	0.05914294  
2023-06-06 11:28:14.129: Find a better model.
2023-06-06 11:28:53.733: [iter 13 : loss : 1.5965 = 0.6886 + 0.9079 + 0.0000, time: 39.598134]
2023-06-06 11:28:54.119: epoch 13:	0.03615164  	0.07855807  	0.07389154  
2023-06-06 11:28:54.120: Find a better model.
2023-06-06 11:29:33.579: [iter 14 : loss : 1.5936 = 0.6849 + 0.9086 + 0.0001, time: 39.452740]
2023-06-06 11:29:33.967: epoch 14:	0.04490709  	0.09606165  	0.09108289  
2023-06-06 11:29:33.967: Find a better model.
2023-06-06 11:30:13.538: [iter 15 : loss : 1.5863 = 0.6764 + 0.9098 + 0.0001, time: 39.565200]
2023-06-06 11:30:13.928: epoch 15:	0.05560182  	0.11361268  	0.11030559  
2023-06-06 11:30:13.928: Find a better model.
2023-06-06 11:30:53.664: [iter 16 : loss : 1.5700 = 0.6573 + 0.9125 + 0.0002, time: 39.729660]
2023-06-06 11:30:54.050: epoch 16:	0.06608151  	0.12991185  	0.12863868  
2023-06-06 11:30:54.051: Find a better model.
2023-06-06 11:31:33.527: [iter 17 : loss : 1.5378 = 0.6205 + 0.9168 + 0.0004, time: 39.470219]
2023-06-06 11:31:33.914: epoch 17:	0.07398312  	0.14194997  	0.14235422  
2023-06-06 11:31:33.915: Find a better model.
2023-06-06 11:32:13.686: [iter 18 : loss : 1.4887 = 0.5649 + 0.9230 + 0.0008, time: 39.764849]
2023-06-06 11:32:14.068: epoch 18:	0.07918287  	0.15096651  	0.15155473  
2023-06-06 11:32:14.068: Find a better model.
2023-06-06 11:32:53.710: [iter 19 : loss : 1.4307 = 0.4995 + 0.9300 + 0.0012, time: 39.636364]
2023-06-06 11:32:54.095: epoch 19:	0.08264195  	0.15664274  	0.15763684  
2023-06-06 11:32:54.095: Find a better model.
2023-06-06 11:33:33.722: [iter 20 : loss : 1.3728 = 0.4353 + 0.9357 + 0.0017, time: 39.621015]
2023-06-06 11:33:34.099: epoch 20:	0.08520421  	0.16100372  	0.16214411  
2023-06-06 11:33:34.100: Find a better model.
2023-06-06 11:34:13.859: [iter 21 : loss : 1.3219 = 0.3800 + 0.9397 + 0.0022, time: 39.752256]
2023-06-06 11:34:14.245: epoch 21:	0.08663835  	0.16383831  	0.16486804  
2023-06-06 11:34:14.246: Find a better model.
2023-06-06 11:34:53.820: [iter 22 : loss : 1.2785 = 0.3338 + 0.9419 + 0.0027, time: 39.568591]
2023-06-06 11:34:54.203: epoch 22:	0.08807259  	0.16713335  	0.16750148  
2023-06-06 11:34:54.204: Find a better model.
2023-06-06 11:35:33.769: [iter 23 : loss : 1.2422 = 0.2964 + 0.9426 + 0.0032, time: 39.558368]
2023-06-06 11:35:34.152: epoch 23:	0.08879770  	0.16892052  	0.16884844  
2023-06-06 11:35:34.152: Find a better model.
2023-06-06 11:36:13.844: [iter 24 : loss : 1.2122 = 0.2659 + 0.9426 + 0.0037, time: 39.685891]
2023-06-06 11:36:14.226: epoch 24:	0.08959267  	0.17009979  	0.17018986  
2023-06-06 11:36:14.227: Find a better model.
2023-06-06 11:36:53.982: [iter 25 : loss : 1.1867 = 0.2405 + 0.9420 + 0.0042, time: 39.749122]
2023-06-06 11:36:54.362: epoch 25:	0.09004399  	0.17110077  	0.17106894  
2023-06-06 11:36:54.363: Find a better model.
2023-06-06 11:37:34.041: [iter 26 : loss : 1.1649 = 0.2191 + 0.9412 + 0.0046, time: 39.672517]
2023-06-06 11:37:34.427: epoch 26:	0.09033408  	0.17145857  	0.17131729  
2023-06-06 11:37:34.427: Find a better model.
2023-06-06 11:38:14.216: [iter 27 : loss : 1.1477 = 0.2025 + 0.9402 + 0.0050, time: 39.781981]
2023-06-06 11:38:14.626: epoch 27:	0.09072628  	0.17215113  	0.17166625  
2023-06-06 11:38:14.626: Find a better model.
2023-06-06 11:38:54.398: [iter 28 : loss : 1.1307 = 0.1864 + 0.9389 + 0.0054, time: 39.766134]
2023-06-06 11:38:54.792: epoch 28:	0.09072077  	0.17184551  	0.17165595  
2023-06-06 11:39:34.338: [iter 29 : loss : 1.1174 = 0.1738 + 0.9378 + 0.0058, time: 39.539984]
2023-06-06 11:39:34.735: epoch 29:	0.09068318  	0.17130344  	0.17105295  
2023-06-06 11:40:14.711: [iter 30 : loss : 1.1054 = 0.1627 + 0.9366 + 0.0061, time: 39.969344]
2023-06-06 11:40:15.097: epoch 30:	0.09086041  	0.17133185  	0.17115475  
2023-06-06 11:40:54.858: [iter 31 : loss : 1.0942 = 0.1522 + 0.9355 + 0.0065, time: 39.753855]
2023-06-06 11:40:55.241: epoch 31:	0.09094633  	0.17151746  	0.17106070  
2023-06-06 11:41:34.945: [iter 32 : loss : 1.0849 = 0.1438 + 0.9344 + 0.0068, time: 39.698058]
2023-06-06 11:41:35.328: epoch 32:	0.09089264  	0.17097105  	0.17067339  
2023-06-06 11:42:15.041: [iter 33 : loss : 1.0763 = 0.1360 + 0.9332 + 0.0071, time: 39.707158]
2023-06-06 11:42:15.422: epoch 33:	0.09081203  	0.17070983  	0.17031634  
2023-06-06 11:42:55.221: [iter 34 : loss : 1.0687 = 0.1290 + 0.9323 + 0.0074, time: 39.792050]
2023-06-06 11:42:55.627: epoch 34:	0.09061865  	0.17016838  	0.16997722  
2023-06-06 11:43:35.468: [iter 35 : loss : 1.0621 = 0.1230 + 0.9314 + 0.0077, time: 39.835415]
2023-06-06 11:43:35.858: epoch 35:	0.09041458  	0.16938446  	0.16943915  
2023-06-06 11:44:15.661: [iter 36 : loss : 1.0559 = 0.1174 + 0.9305 + 0.0080, time: 39.795781]
2023-06-06 11:44:16.040: epoch 36:	0.09040381  	0.16881824  	0.16903622  
2023-06-06 11:44:55.984: [iter 37 : loss : 1.0499 = 0.1119 + 0.9297 + 0.0083, time: 39.935425]
2023-06-06 11:44:56.367: epoch 37:	0.09016744  	0.16824204  	0.16846912  
2023-06-06 11:45:36.257: [iter 38 : loss : 1.0451 = 0.1076 + 0.9289 + 0.0086, time: 39.884401]
2023-06-06 11:45:36.661: epoch 38:	0.09001168  	0.16752680  	0.16802809  
2023-06-06 11:46:16.552: [iter 39 : loss : 1.0402 = 0.1031 + 0.9283 + 0.0088, time: 39.884828]
2023-06-06 11:46:16.938: epoch 39:	0.08982372  	0.16676056  	0.16741218  
2023-06-06 11:46:56.921: [iter 40 : loss : 1.0360 = 0.0994 + 0.9275 + 0.0091, time: 39.976151]
2023-06-06 11:46:57.310: epoch 40:	0.08970554  	0.16672359  	0.16714144  
2023-06-06 11:47:37.222: [iter 41 : loss : 1.0318 = 0.0957 + 0.9268 + 0.0093, time: 39.905920]
2023-06-06 11:47:37.628: epoch 41:	0.08954973  	0.16640942  	0.16661693  
2023-06-06 11:48:17.756: [iter 42 : loss : 1.0283 = 0.0924 + 0.9263 + 0.0095, time: 40.122397]
2023-06-06 11:48:18.143: epoch 42:	0.08921673  	0.16552314  	0.16599801  
2023-06-06 11:48:58.128: [iter 43 : loss : 1.0248 = 0.0893 + 0.9257 + 0.0098, time: 39.978289]
2023-06-06 11:48:58.511: epoch 43:	0.08893205  	0.16483088  	0.16547833  
2023-06-06 11:49:38.351: [iter 44 : loss : 1.0214 = 0.0862 + 0.9251 + 0.0100, time: 39.833614]
2023-06-06 11:49:38.752: epoch 44:	0.08885148  	0.16444966  	0.16514762  
2023-06-06 11:50:18.565: [iter 45 : loss : 1.0185 = 0.0837 + 0.9246 + 0.0102, time: 39.806585]
2023-06-06 11:50:18.950: epoch 45:	0.08844326  	0.16354528  	0.16433619  
2023-06-06 11:50:58.889: [iter 46 : loss : 1.0161 = 0.0815 + 0.9242 + 0.0104, time: 39.933526]
2023-06-06 11:50:59.272: epoch 46:	0.08835196  	0.16305228  	0.16389738  
2023-06-06 11:51:39.092: [iter 47 : loss : 1.0139 = 0.0795 + 0.9238 + 0.0106, time: 39.812520]
2023-06-06 11:51:39.477: epoch 47:	0.08814240  	0.16239628  	0.16350643  
2023-06-06 11:52:19.459: [iter 48 : loss : 1.0110 = 0.0768 + 0.9234 + 0.0108, time: 39.975400]
2023-06-06 11:52:19.852: epoch 48:	0.08795973  	0.16188356  	0.16298099  
2023-06-06 11:52:59.789: [iter 49 : loss : 1.0090 = 0.0750 + 0.9230 + 0.0110, time: 39.928735]
2023-06-06 11:53:00.178: epoch 49:	0.08779321  	0.16138162  	0.16248450  
2023-06-06 11:53:40.127: [iter 50 : loss : 1.0066 = 0.0728 + 0.9225 + 0.0112, time: 39.942981]
2023-06-06 11:53:40.510: epoch 50:	0.08764820  	0.16100922  	0.16198578  
2023-06-06 11:54:20.615: [iter 51 : loss : 1.0045 = 0.0709 + 0.9222 + 0.0114, time: 40.098194]
2023-06-06 11:54:20.997: epoch 51:	0.08728293  	0.16041262  	0.16143905  
2023-06-06 11:55:00.923: [iter 52 : loss : 1.0032 = 0.0697 + 0.9219 + 0.0116, time: 39.919314]
2023-06-06 11:55:01.309: epoch 52:	0.08710030  	0.15971637  	0.16094527  
2023-06-06 11:55:01.310: Early stopping is trigger at epoch: 52
2023-06-06 11:55:01.310: best_result@epoch 27:

2023-06-06 11:55:01.310: 		0.0907      	0.1722      	0.1717      
2023-06-06 14:36:31.012: my pid: 10228
2023-06-06 14:36:31.012: model: model.general_recommender.SGL
2023-06-06 14:36:31.012: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 14:36:31.012: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 14:36:35.800: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 14:37:13.107: [iter 1 : loss : 1.6041 = 0.6931 + 0.9110 + 0.0000, time: 37.305765]
2023-06-06 14:37:13.514: epoch 1:	0.00405563  	0.00990312  	0.00765507  
2023-06-06 14:37:13.515: Find a better model.
2023-06-06 14:37:50.674: [iter 2 : loss : 1.6014 = 0.6931 + 0.9084 + 0.0000, time: 37.153245]
2023-06-06 14:37:51.097: epoch 2:	0.00547912  	0.01241467  	0.01050391  
2023-06-06 14:37:51.097: Find a better model.
2023-06-06 14:38:28.531: [iter 3 : loss : 1.6011 = 0.6930 + 0.9081 + 0.0000, time: 37.427389]
2023-06-06 14:38:28.941: epoch 3:	0.00728939  	0.01605183  	0.01356616  
2023-06-06 14:38:28.941: Find a better model.
2023-06-06 14:39:06.127: [iter 4 : loss : 1.6011 = 0.6929 + 0.9082 + 0.0000, time: 37.178845]
2023-06-06 14:39:06.532: epoch 4:	0.00824559  	0.01720677  	0.01506977  
2023-06-06 14:39:06.532: Find a better model.
2023-06-06 14:39:43.738: [iter 5 : loss : 1.6011 = 0.6929 + 0.9082 + 0.0000, time: 37.199219]
2023-06-06 14:39:44.156: epoch 5:	0.00936294  	0.01892512  	0.01688066  
2023-06-06 14:39:44.156: Find a better model.
2023-06-06 14:40:21.461: [iter 6 : loss : 1.6010 = 0.6928 + 0.9082 + 0.0000, time: 37.299508]
2023-06-06 14:40:21.870: epoch 6:	0.01084022  	0.02312494  	0.02010254  
2023-06-06 14:40:21.870: Find a better model.
2023-06-06 14:41:01.410: [iter 7 : loss : 1.6010 = 0.6926 + 0.9083 + 0.0000, time: 39.533251]
2023-06-06 14:41:01.803: epoch 7:	0.01234434  	0.02555721  	0.02266251  
2023-06-06 14:41:01.803: Find a better model.
2023-06-06 14:41:39.342: [iter 8 : loss : 1.6009 = 0.6924 + 0.9085 + 0.0000, time: 37.533052]
2023-06-06 14:41:39.734: epoch 8:	0.01461127  	0.02994422  	0.02745589  
2023-06-06 14:41:39.734: Find a better model.
2023-06-06 14:42:17.179: [iter 9 : loss : 1.6006 = 0.6921 + 0.9084 + 0.0000, time: 37.437263]
2023-06-06 14:42:17.569: epoch 9:	0.01707699  	0.03561197  	0.03258101  
2023-06-06 14:42:17.570: Find a better model.
2023-06-06 14:42:54.955: [iter 10 : loss : 1.6005 = 0.6917 + 0.9087 + 0.0000, time: 37.378989]
2023-06-06 14:42:55.347: epoch 10:	0.02095550  	0.04376814  	0.04022243  
2023-06-06 14:42:55.347: Find a better model.
2023-06-06 14:43:33.573: [iter 11 : loss : 1.5999 = 0.6911 + 0.9088 + 0.0000, time: 38.219023]
2023-06-06 14:43:33.963: epoch 11:	0.02524227  	0.05442414  	0.04924923  
2023-06-06 14:43:33.963: Find a better model.
2023-06-06 14:44:12.183: [iter 12 : loss : 1.5992 = 0.6901 + 0.9091 + 0.0000, time: 38.214810]
2023-06-06 14:44:12.571: epoch 12:	0.03056544  	0.06668796  	0.06182776  
2023-06-06 14:44:12.571: Find a better model.
2023-06-06 14:44:50.973: [iter 13 : loss : 1.5977 = 0.6881 + 0.9095 + 0.0000, time: 38.394985]
2023-06-06 14:44:51.364: epoch 13:	0.03942280  	0.08416241  	0.07892966  
2023-06-06 14:44:51.364: Find a better model.
2023-06-06 14:45:29.732: [iter 14 : loss : 1.5946 = 0.6840 + 0.9105 + 0.0001, time: 38.360581]
2023-06-06 14:45:30.140: epoch 14:	0.04959653  	0.10305051  	0.09847800  
2023-06-06 14:45:30.140: Find a better model.
2023-06-06 14:46:08.524: [iter 15 : loss : 1.5868 = 0.6747 + 0.9119 + 0.0001, time: 38.378307]
2023-06-06 14:46:08.913: epoch 15:	0.05989897  	0.11937659  	0.11864170  
2023-06-06 14:46:08.913: Find a better model.
2023-06-06 14:46:47.347: [iter 16 : loss : 1.5693 = 0.6540 + 0.9150 + 0.0003, time: 38.426139]
2023-06-06 14:46:47.731: epoch 16:	0.06999736  	0.13539164  	0.13570626  
2023-06-06 14:46:47.731: Find a better model.
2023-06-06 14:47:25.865: [iter 17 : loss : 1.5356 = 0.6152 + 0.9199 + 0.0005, time: 38.126491]
2023-06-06 14:47:26.260: epoch 17:	0.07665269  	0.14510316  	0.14736019  
2023-06-06 14:47:26.260: Find a better model.
2023-06-06 14:48:04.712: [iter 18 : loss : 1.4855 = 0.5582 + 0.9265 + 0.0008, time: 38.444995]
2023-06-06 14:48:05.117: epoch 18:	0.08093383  	0.15217288  	0.15455192  
2023-06-06 14:48:05.117: Find a better model.
2023-06-06 14:48:43.594: [iter 19 : loss : 1.4273 = 0.4923 + 0.9337 + 0.0013, time: 38.469042]
2023-06-06 14:48:43.981: epoch 19:	0.08382364  	0.15743791  	0.15933651  
2023-06-06 14:48:43.982: Find a better model.
2023-06-06 14:49:22.378: [iter 20 : loss : 1.3699 = 0.4288 + 0.9393 + 0.0018, time: 38.389023]
2023-06-06 14:49:22.764: epoch 20:	0.08611723  	0.16211607  	0.16341643  
2023-06-06 14:49:22.764: Find a better model.
2023-06-06 14:50:01.368: [iter 21 : loss : 1.3198 = 0.3744 + 0.9431 + 0.0023, time: 38.597000]
2023-06-06 14:50:01.751: epoch 21:	0.08716472  	0.16444482  	0.16582561  
2023-06-06 14:50:01.751: Find a better model.
2023-06-06 14:50:40.248: [iter 22 : loss : 1.2766 = 0.3288 + 0.9450 + 0.0028, time: 38.491044]
2023-06-06 14:50:40.635: epoch 22:	0.08863654  	0.16711406  	0.16813484  
2023-06-06 14:50:40.635: Find a better model.
2023-06-06 14:51:19.179: [iter 23 : loss : 1.2407 = 0.2919 + 0.9455 + 0.0033, time: 38.537068]
2023-06-06 14:51:19.563: epoch 23:	0.08929731  	0.16880061  	0.16970244  
2023-06-06 14:51:19.563: Find a better model.
2023-06-06 14:51:58.030: [iter 24 : loss : 1.2111 = 0.2618 + 0.9455 + 0.0038, time: 38.460328]
2023-06-06 14:51:58.417: epoch 24:	0.09008152  	0.17091361  	0.17101681  
2023-06-06 14:51:58.417: Find a better model.
2023-06-06 14:52:36.837: [iter 25 : loss : 1.1858 = 0.2368 + 0.9447 + 0.0042, time: 38.413085]
2023-06-06 14:52:37.236: epoch 25:	0.09021582  	0.17108841  	0.17127183  
2023-06-06 14:52:37.236: Find a better model.
2023-06-06 14:53:15.745: [iter 26 : loss : 1.1642 = 0.2156 + 0.9438 + 0.0047, time: 38.502692]
2023-06-06 14:53:16.145: epoch 26:	0.09050040  	0.17183001  	0.17176814  
2023-06-06 14:53:16.145: Find a better model.
2023-06-06 14:53:54.745: [iter 27 : loss : 1.1470 = 0.1992 + 0.9428 + 0.0051, time: 38.593038]
2023-06-06 14:53:55.157: epoch 27:	0.09057034  	0.17237066  	0.17194200  
2023-06-06 14:53:55.157: Find a better model.
2023-06-06 14:54:33.542: [iter 28 : loss : 1.1303 = 0.1834 + 0.9414 + 0.0055, time: 38.379416]
2023-06-06 14:54:33.932: epoch 28:	0.09047370  	0.17177819  	0.17165650  
2023-06-06 14:55:12.364: [iter 29 : loss : 1.1171 = 0.1710 + 0.9403 + 0.0058, time: 38.424459]
2023-06-06 14:55:12.753: epoch 29:	0.09045754  	0.17114401  	0.17135288  
2023-06-06 14:55:51.147: [iter 30 : loss : 1.1054 = 0.1602 + 0.9390 + 0.0062, time: 38.388087]
2023-06-06 14:55:51.536: epoch 30:	0.09043072  	0.17059617  	0.17117748  
2023-06-06 14:56:29.913: [iter 31 : loss : 1.0942 = 0.1498 + 0.9378 + 0.0065, time: 38.370002]
2023-06-06 14:56:30.311: epoch 31:	0.09053280  	0.17066769  	0.17130254  
2023-06-06 14:57:08.873: [iter 32 : loss : 1.0852 = 0.1416 + 0.9368 + 0.0069, time: 38.556131]
2023-06-06 14:57:09.270: epoch 32:	0.09058645  	0.17059340  	0.17130266  
2023-06-06 14:57:47.872: [iter 33 : loss : 1.0767 = 0.1339 + 0.9356 + 0.0072, time: 38.594002]
2023-06-06 14:57:48.273: epoch 33:	0.09050588  	0.17041394  	0.17068505  
2023-06-06 14:58:26.702: [iter 34 : loss : 1.0689 = 0.1268 + 0.9346 + 0.0075, time: 38.421636]
2023-06-06 14:58:27.111: epoch 34:	0.09033400  	0.16933890  	0.17010190  
2023-06-06 14:59:05.619: [iter 35 : loss : 1.0627 = 0.1211 + 0.9338 + 0.0078, time: 38.502163]
2023-06-06 14:59:06.027: epoch 35:	0.08977001  	0.16791958  	0.16917714  
2023-06-06 14:59:44.284: [iter 36 : loss : 1.0565 = 0.1156 + 0.9329 + 0.0081, time: 38.241524]
2023-06-06 14:59:44.673: epoch 36:	0.08973783  	0.16756465  	0.16861671  
2023-06-06 15:00:23.183: [iter 37 : loss : 1.0507 = 0.1102 + 0.9321 + 0.0083, time: 38.501144]
2023-06-06 15:00:23.572: epoch 37:	0.08968417  	0.16734253  	0.16842513  
2023-06-06 15:01:02.157: [iter 38 : loss : 1.0459 = 0.1059 + 0.9313 + 0.0086, time: 38.578032]
2023-06-06 15:01:02.544: epoch 38:	0.08946928  	0.16644236  	0.16781427  
2023-06-06 15:01:41.193: [iter 39 : loss : 1.0411 = 0.1016 + 0.9307 + 0.0089, time: 38.642989]
2023-06-06 15:01:41.584: epoch 39:	0.08931886  	0.16619588  	0.16729467  
2023-06-06 15:02:19.992: [iter 40 : loss : 1.0368 = 0.0978 + 0.9299 + 0.0091, time: 38.401011]
2023-06-06 15:02:20.381: epoch 40:	0.08919533  	0.16537836  	0.16667596  
2023-06-06 15:02:58.764: [iter 41 : loss : 1.0328 = 0.0943 + 0.9292 + 0.0094, time: 38.377512]
2023-06-06 15:02:59.169: epoch 41:	0.08895363  	0.16497640  	0.16617243  
2023-06-06 15:03:37.747: [iter 42 : loss : 1.0297 = 0.0914 + 0.9287 + 0.0096, time: 38.570495]
2023-06-06 15:03:38.148: epoch 42:	0.08883008  	0.16445425  	0.16575733  
2023-06-06 15:04:16.730: [iter 43 : loss : 1.0263 = 0.0883 + 0.9281 + 0.0098, time: 38.576087]
2023-06-06 15:04:17.135: epoch 43:	0.08873339  	0.16385055  	0.16525923  
2023-06-06 15:04:55.688: [iter 44 : loss : 1.0226 = 0.0850 + 0.9276 + 0.0100, time: 38.546371]
2023-06-06 15:04:56.096: epoch 44:	0.08843257  	0.16278815  	0.16440149  
2023-06-06 15:05:34.730: [iter 45 : loss : 1.0200 = 0.0827 + 0.9270 + 0.0103, time: 38.626971]
2023-06-06 15:05:35.137: epoch 45:	0.08831446  	0.16244864  	0.16398971  
2023-06-06 15:06:13.285: [iter 46 : loss : 1.0177 = 0.0806 + 0.9267 + 0.0105, time: 38.142187]
2023-06-06 15:06:13.673: epoch 46:	0.08815330  	0.16200770  	0.16347545  
2023-06-06 15:06:52.300: [iter 47 : loss : 1.0152 = 0.0783 + 0.9262 + 0.0107, time: 38.619524]
2023-06-06 15:06:52.691: epoch 47:	0.08802435  	0.16158858  	0.16299750  
2023-06-06 15:07:31.297: [iter 48 : loss : 1.0126 = 0.0759 + 0.9258 + 0.0109, time: 38.599014]
2023-06-06 15:07:31.688: epoch 48:	0.08754096  	0.16042793  	0.16214092  
2023-06-06 15:08:10.439: [iter 49 : loss : 1.0106 = 0.0741 + 0.9254 + 0.0111, time: 38.745288]
2023-06-06 15:08:10.828: epoch 49:	0.08747648  	0.15991652  	0.16173397  
2023-06-06 15:08:50.151: [iter 50 : loss : 1.0084 = 0.0722 + 0.9250 + 0.0112, time: 39.315094]
2023-06-06 15:08:50.563: epoch 50:	0.08717570  	0.15928693  	0.16115247  
2023-06-06 15:09:29.210: [iter 51 : loss : 1.0062 = 0.0701 + 0.9247 + 0.0114, time: 38.640973]
2023-06-06 15:09:29.619: epoch 51:	0.08712195  	0.15900236  	0.16067253  
2023-06-06 15:10:08.032: [iter 52 : loss : 1.0048 = 0.0689 + 0.9244 + 0.0116, time: 38.406046]
2023-06-06 15:10:08.434: epoch 52:	0.08676753  	0.15815042  	0.15996316  
2023-06-06 15:10:08.435: Early stopping is trigger at epoch: 52
2023-06-06 15:10:08.435: best_result@epoch 27:

2023-06-06 15:10:08.435: 		0.0906      	0.1724      	0.1719      
2023-06-06 15:22:23.620: my pid: 8104
2023-06-06 15:22:23.620: model: model.general_recommender.SGL
2023-06-06 15:22:23.620: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 15:22:23.620: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 15:22:28.461: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 15:23:04.607: [iter 1 : loss : 1.6045 = 0.6931 + 0.9114 + 0.0000, time: 36.144849]
2023-06-06 15:23:05.004: epoch 1:	0.00431347  	0.00997754  	0.00837702  
2023-06-06 15:23:05.004: Find a better model.
2023-06-06 15:23:40.938: [iter 2 : loss : 1.6020 = 0.6931 + 0.9090 + 0.0000, time: 35.927236]
2023-06-06 15:23:41.360: epoch 2:	0.00557581  	0.01294561  	0.01065421  
2023-06-06 15:23:41.360: Find a better model.
2023-06-06 15:24:17.582: [iter 3 : loss : 1.6019 = 0.6930 + 0.9089 + 0.0000, time: 36.215382]
2023-06-06 15:24:17.985: epoch 3:	0.00703691  	0.01535348  	0.01327527  
2023-06-06 15:24:17.985: Find a better model.
2023-06-06 15:24:53.997: [iter 4 : loss : 1.6020 = 0.6929 + 0.9090 + 0.0000, time: 36.005193]
2023-06-06 15:24:54.410: epoch 4:	0.00789641  	0.01689967  	0.01458906  
2023-06-06 15:24:54.410: Find a better model.
2023-06-06 15:25:30.424: [iter 5 : loss : 1.6020 = 0.6929 + 0.9092 + 0.0000, time: 36.008357]
2023-06-06 15:25:30.828: epoch 5:	0.00872368  	0.01886868  	0.01648490  
2023-06-06 15:25:30.828: Find a better model.
2023-06-06 15:26:06.928: [iter 6 : loss : 1.6020 = 0.6928 + 0.9092 + 0.0000, time: 36.094023]
2023-06-06 15:26:07.347: epoch 6:	0.01024394  	0.02204592  	0.01898233  
2023-06-06 15:26:07.347: Find a better model.
2023-06-06 15:26:44.510: [iter 7 : loss : 1.6021 = 0.6926 + 0.9094 + 0.0000, time: 37.156918]
2023-06-06 15:26:44.898: epoch 7:	0.01200590  	0.02508672  	0.02231236  
2023-06-06 15:26:44.898: Find a better model.
2023-06-06 15:27:20.976: [iter 8 : loss : 1.6021 = 0.6924 + 0.9096 + 0.0000, time: 36.070996]
2023-06-06 15:27:21.411: epoch 8:	0.01393440  	0.02999090  	0.02682702  
2023-06-06 15:27:21.411: Find a better model.
2023-06-06 15:27:57.651: [iter 9 : loss : 1.6018 = 0.6922 + 0.9097 + 0.0000, time: 36.233975]
2023-06-06 15:27:58.038: epoch 9:	0.01616376  	0.03459131  	0.03106223  
2023-06-06 15:27:58.038: Find a better model.
2023-06-06 15:28:34.263: [iter 10 : loss : 1.6019 = 0.6918 + 0.9101 + 0.0000, time: 36.218042]
2023-06-06 15:28:34.655: epoch 10:	0.01938689  	0.04167817  	0.03740519  
2023-06-06 15:28:34.656: Find a better model.
2023-06-06 15:29:11.598: [iter 11 : loss : 1.6014 = 0.6912 + 0.9102 + 0.0000, time: 36.935237]
2023-06-06 15:29:11.986: epoch 11:	0.02351250  	0.05010713  	0.04666736  
2023-06-06 15:29:11.987: Find a better model.
2023-06-06 15:29:49.142: [iter 12 : loss : 1.6009 = 0.6904 + 0.9105 + 0.0000, time: 37.149602]
2023-06-06 15:29:49.538: epoch 12:	0.02901849  	0.06252331  	0.05771712  
2023-06-06 15:29:49.538: Find a better model.
2023-06-06 15:30:26.594: [iter 13 : loss : 1.5999 = 0.6887 + 0.9111 + 0.0000, time: 37.050197]
2023-06-06 15:30:26.983: epoch 13:	0.03722591  	0.07818943  	0.07392733  
2023-06-06 15:30:26.983: Find a better model.
2023-06-06 15:31:04.030: [iter 14 : loss : 1.5977 = 0.6855 + 0.9121 + 0.0001, time: 37.039983]
2023-06-06 15:31:04.430: epoch 14:	0.04652401  	0.09600879  	0.09248669  
2023-06-06 15:31:04.431: Find a better model.
2023-06-06 15:31:41.735: [iter 15 : loss : 1.5920 = 0.6782 + 0.9136 + 0.0001, time: 37.298102]
2023-06-06 15:31:42.125: epoch 15:	0.05806199  	0.11535457  	0.11360833  
2023-06-06 15:31:42.125: Find a better model.
2023-06-06 15:32:19.188: [iter 16 : loss : 1.5788 = 0.6619 + 0.9166 + 0.0002, time: 37.056578]
2023-06-06 15:32:19.575: epoch 16:	0.06832684  	0.13162722  	0.13223620  
2023-06-06 15:32:19.575: Find a better model.
2023-06-06 15:32:56.761: [iter 17 : loss : 1.5510 = 0.6292 + 0.9214 + 0.0004, time: 37.179974]
2023-06-06 15:32:57.145: epoch 17:	0.07584702  	0.14322387  	0.14542428  
2023-06-06 15:32:57.145: Find a better model.
2023-06-06 15:33:34.367: [iter 18 : loss : 1.5058 = 0.5770 + 0.9281 + 0.0007, time: 37.214202]
2023-06-06 15:33:34.752: epoch 18:	0.08056318  	0.15049259  	0.15321998  
2023-06-06 15:33:34.752: Find a better model.
2023-06-06 15:34:11.965: [iter 19 : loss : 1.4495 = 0.5125 + 0.9358 + 0.0012, time: 37.206497]
2023-06-06 15:34:12.378: epoch 19:	0.08358186  	0.15634622  	0.15852843  
2023-06-06 15:34:12.378: Find a better model.
2023-06-06 15:34:49.306: [iter 20 : loss : 1.3911 = 0.4473 + 0.9421 + 0.0017, time: 36.922408]
2023-06-06 15:34:49.713: epoch 20:	0.08575738  	0.16129516  	0.16215275  
2023-06-06 15:34:49.713: Find a better model.
2023-06-06 15:35:26.734: [iter 21 : loss : 1.3387 = 0.3900 + 0.9466 + 0.0022, time: 37.015027]
2023-06-06 15:35:27.129: epoch 21:	0.08763736  	0.16542666  	0.16501406  
2023-06-06 15:35:27.130: Find a better model.
2023-06-06 15:36:04.442: [iter 22 : loss : 1.2932 = 0.3416 + 0.9488 + 0.0027, time: 37.304245]
2023-06-06 15:36:04.840: epoch 22:	0.08858278  	0.16820283  	0.16740818  
2023-06-06 15:36:04.841: Find a better model.
2023-06-06 15:36:42.145: [iter 23 : loss : 1.2551 = 0.3022 + 0.9497 + 0.0032, time: 37.298468]
2023-06-06 15:36:42.551: epoch 23:	0.08927583  	0.16894852  	0.16871364  
2023-06-06 15:36:42.552: Find a better model.
2023-06-06 15:37:19.674: [iter 24 : loss : 1.2234 = 0.2701 + 0.9496 + 0.0037, time: 37.115451]
2023-06-06 15:37:20.073: epoch 24:	0.09017828  	0.17079632  	0.17017028  
2023-06-06 15:37:20.073: Find a better model.
2023-06-06 15:37:57.351: [iter 25 : loss : 1.1964 = 0.2434 + 0.9489 + 0.0041, time: 37.270675]
2023-06-06 15:37:57.749: epoch 25:	0.09045237  	0.17162961  	0.17065294  
2023-06-06 15:37:57.749: Find a better model.
2023-06-06 15:38:34.732: [iter 26 : loss : 1.1737 = 0.2212 + 0.9480 + 0.0046, time: 36.975055]
2023-06-06 15:38:35.132: epoch 26:	0.09093042  	0.17266932  	0.17157120  
2023-06-06 15:38:35.132: Find a better model.
2023-06-06 15:39:12.492: [iter 27 : loss : 1.1554 = 0.2036 + 0.9468 + 0.0050, time: 37.352840]
2023-06-06 15:39:12.895: epoch 27:	0.09125266  	0.17315495  	0.17171422  
2023-06-06 15:39:12.896: Find a better model.
2023-06-06 15:39:50.012: [iter 28 : loss : 1.1380 = 0.1871 + 0.9455 + 0.0054, time: 37.109516]
2023-06-06 15:39:50.418: epoch 28:	0.09116672  	0.17323731  	0.17159598  
2023-06-06 15:39:50.418: Find a better model.
2023-06-06 15:40:29.766: [iter 29 : loss : 1.1240 = 0.1740 + 0.9443 + 0.0058, time: 39.341418]
2023-06-06 15:40:30.151: epoch 29:	0.09127953  	0.17324428  	0.17158781  
2023-06-06 15:40:30.151: Find a better model.
2023-06-06 15:41:07.387: [iter 30 : loss : 1.1118 = 0.1626 + 0.9430 + 0.0061, time: 37.228343]
2023-06-06 15:41:07.773: epoch 30:	0.09116136  	0.17323971  	0.17157221  
2023-06-06 15:41:45.142: [iter 31 : loss : 1.1003 = 0.1521 + 0.9418 + 0.0065, time: 37.361631]
2023-06-06 15:41:45.531: epoch 31:	0.09125807  	0.17267869  	0.17109856  
2023-06-06 15:42:22.723: [iter 32 : loss : 1.0908 = 0.1434 + 0.9406 + 0.0068, time: 37.184355]
2023-06-06 15:42:23.111: epoch 32:	0.09120974  	0.17211311  	0.17088006  
2023-06-06 15:43:00.323: [iter 33 : loss : 1.0820 = 0.1355 + 0.9394 + 0.0071, time: 37.206283]
2023-06-06 15:43:00.707: epoch 33:	0.09097878  	0.17121659  	0.17037134  
2023-06-06 15:43:37.971: [iter 34 : loss : 1.0742 = 0.1284 + 0.9384 + 0.0074, time: 37.256466]
2023-06-06 15:43:38.371: epoch 34:	0.09079611  	0.17061207  	0.16985677  
2023-06-06 15:44:15.673: [iter 35 : loss : 1.0678 = 0.1226 + 0.9375 + 0.0077, time: 37.294371]
2023-06-06 15:44:16.059: epoch 35:	0.09064572  	0.17051767  	0.16939862  
2023-06-06 15:44:53.063: [iter 36 : loss : 1.0612 = 0.1167 + 0.9365 + 0.0080, time: 36.998011]
2023-06-06 15:44:53.458: epoch 36:	0.09060813  	0.17010467  	0.16922900  
2023-06-06 15:45:30.659: [iter 37 : loss : 1.0553 = 0.1113 + 0.9357 + 0.0083, time: 37.194016]
2023-06-06 15:45:31.045: epoch 37:	0.09047382  	0.16941942  	0.16889286  
2023-06-06 15:46:08.319: [iter 38 : loss : 1.0505 = 0.1071 + 0.9348 + 0.0086, time: 37.268157]
2023-06-06 15:46:08.714: epoch 38:	0.09019988  	0.16873734  	0.16793720  
2023-06-06 15:46:46.882: my pid: 7400
2023-06-06 15:46:46.882: model: model.general_recommender.SGL
2023-06-06 15:46:46.882: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 15:46:46.882: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 15:46:52.023: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 15:47:27.253: [iter 1 : loss : 1.6047 = 0.6931 + 0.9116 + 0.0000, time: 35.230092]
2023-06-06 15:47:27.664: epoch 1:	0.00393745  	0.00958805  	0.00756430  
2023-06-06 15:47:27.664: Find a better model.
2023-06-06 15:48:02.377: [iter 2 : loss : 1.6029 = 0.6931 + 0.9098 + 0.0000, time: 34.705660]
2023-06-06 15:48:02.788: epoch 2:	0.00546300  	0.01318208  	0.01050910  
2023-06-06 15:48:02.788: Find a better model.
2023-06-06 15:48:37.639: [iter 3 : loss : 1.6029 = 0.6930 + 0.9099 + 0.0000, time: 34.845090]
2023-06-06 15:48:38.045: epoch 3:	0.00636007  	0.01470357  	0.01192338  
2023-06-06 15:48:38.046: Find a better model.
2023-06-06 15:49:12.771: [iter 4 : loss : 1.6031 = 0.6929 + 0.9101 + 0.0000, time: 34.719161]
2023-06-06 15:49:13.166: epoch 4:	0.00738608  	0.01707724  	0.01400859  
2023-06-06 15:49:13.166: Find a better model.
2023-06-06 15:49:47.901: [iter 5 : loss : 1.6033 = 0.6928 + 0.9105 + 0.0000, time: 34.728269]
2023-06-06 15:49:48.327: epoch 5:	0.00817575  	0.01865625  	0.01518863  
2023-06-06 15:49:48.327: Find a better model.
2023-06-06 15:50:23.115: [iter 6 : loss : 1.6033 = 0.6927 + 0.9106 + 0.0000, time: 34.780660]
2023-06-06 15:50:23.528: epoch 6:	0.00934147  	0.02131523  	0.01775030  
2023-06-06 15:50:23.528: Find a better model.
2023-06-06 15:50:58.130: [iter 7 : loss : 1.6035 = 0.6926 + 0.9109 + 0.0000, time: 34.595779]
2023-06-06 15:50:58.546: epoch 7:	0.01121626  	0.02431241  	0.02086580  
2023-06-06 15:50:58.546: Find a better model.
2023-06-06 15:51:35.532: [iter 8 : loss : 1.6036 = 0.6924 + 0.9112 + 0.0000, time: 36.978836]
2023-06-06 15:51:35.949: epoch 8:	0.01280094  	0.02797689  	0.02442851  
2023-06-06 15:51:35.949: Find a better model.
2023-06-06 15:52:10.541: [iter 9 : loss : 1.6035 = 0.6921 + 0.9113 + 0.0000, time: 34.582683]
2023-06-06 15:52:10.955: epoch 9:	0.01426748  	0.03135915  	0.02847538  
2023-06-06 15:52:10.955: Find a better model.
2023-06-06 15:52:45.946: [iter 10 : loss : 1.6036 = 0.6918 + 0.9118 + 0.0000, time: 34.984510]
2023-06-06 15:52:46.367: epoch 10:	0.01760880  	0.03946184  	0.03494227  
2023-06-06 15:52:46.367: Find a better model.
2023-06-06 15:53:21.914: [iter 11 : loss : 1.6032 = 0.6912 + 0.9120 + 0.0000, time: 35.539702]
2023-06-06 15:53:22.336: epoch 11:	0.02199231  	0.04850375  	0.04387490  
2023-06-06 15:53:22.336: Find a better model.
2023-06-06 15:53:58.026: [iter 12 : loss : 1.6029 = 0.6903 + 0.9125 + 0.0000, time: 35.683647]
2023-06-06 15:53:58.436: epoch 12:	0.02635423  	0.05801116  	0.05394389  
2023-06-06 15:53:58.436: Find a better model.
2023-06-06 15:54:34.049: [iter 13 : loss : 1.6021 = 0.6888 + 0.9132 + 0.0001, time: 35.606427]
2023-06-06 15:54:34.456: epoch 13:	0.03455628  	0.07340650  	0.06975827  
2023-06-06 15:54:34.456: Find a better model.
2023-06-06 15:55:10.072: [iter 14 : loss : 1.6003 = 0.6859 + 0.9144 + 0.0001, time: 35.608746]
2023-06-06 15:55:10.502: epoch 14:	0.04411757  	0.09207889  	0.08860370  
2023-06-06 15:55:10.502: Find a better model.
2023-06-06 15:55:46.210: [iter 15 : loss : 1.5957 = 0.6795 + 0.9161 + 0.0001, time: 35.702229]
2023-06-06 15:55:46.613: epoch 15:	0.05569848  	0.11147793  	0.10945565  
2023-06-06 15:55:46.613: Find a better model.
2023-06-06 15:56:23.809: [iter 16 : loss : 1.5849 = 0.6655 + 0.9192 + 0.0002, time: 37.190388]
2023-06-06 15:56:24.198: epoch 16:	0.06667776  	0.12892738  	0.12954892  
2023-06-06 15:56:24.198: Find a better model.
2023-06-06 15:57:00.208: [iter 17 : loss : 1.5613 = 0.6370 + 0.9240 + 0.0004, time: 36.003631]
2023-06-06 15:57:00.593: epoch 17:	0.07488547  	0.14164178  	0.14408241  
2023-06-06 15:57:00.593: Find a better model.
2023-06-06 15:57:36.599: [iter 18 : loss : 1.5205 = 0.5892 + 0.9307 + 0.0007, time: 35.999138]
2023-06-06 15:57:36.988: epoch 18:	0.08017633  	0.15105821  	0.15360340  
2023-06-06 15:57:36.988: Find a better model.
2023-06-06 15:58:12.768: [iter 19 : loss : 1.4667 = 0.5269 + 0.9387 + 0.0011, time: 35.773233]
2023-06-06 15:58:13.154: epoch 19:	0.08330797  	0.15658198  	0.15878074  
2023-06-06 15:58:13.154: Find a better model.
2023-06-06 15:58:48.963: [iter 20 : loss : 1.4083 = 0.4611 + 0.9456 + 0.0016, time: 35.801080]
2023-06-06 15:58:49.375: epoch 20:	0.08590781  	0.16172902  	0.16277327  
2023-06-06 15:58:49.375: Find a better model.
2023-06-06 15:59:24.979: [iter 21 : loss : 1.3541 = 0.4014 + 0.9507 + 0.0021, time: 35.596973]
2023-06-06 15:59:25.390: epoch 21:	0.08757832  	0.16517515  	0.16558461  
2023-06-06 15:59:25.390: Find a better model.
2023-06-06 16:00:00.990: [iter 22 : loss : 1.3067 = 0.3506 + 0.9535 + 0.0026, time: 35.593397]
2023-06-06 16:00:01.404: epoch 22:	0.08857200  	0.16787679  	0.16766557  
2023-06-06 16:00:01.404: Find a better model.
2023-06-06 16:00:36.973: [iter 23 : loss : 1.2665 = 0.3089 + 0.9545 + 0.0031, time: 35.563075]
2023-06-06 16:00:37.370: epoch 23:	0.08919507  	0.16865934  	0.16881768  
2023-06-06 16:00:37.370: Find a better model.
2023-06-06 16:01:13.131: [iter 24 : loss : 1.2332 = 0.2751 + 0.9546 + 0.0036, time: 35.754432]
2023-06-06 16:01:13.524: epoch 24:	0.08960331  	0.16927996  	0.16981852  
2023-06-06 16:01:13.524: Find a better model.
2023-06-06 16:01:49.372: [iter 25 : loss : 1.2052 = 0.2472 + 0.9540 + 0.0041, time: 35.841087]
2023-06-06 16:01:49.769: epoch 25:	0.09018894  	0.17105530  	0.17096776  
2023-06-06 16:01:49.770: Find a better model.
2023-06-06 16:02:25.601: [iter 26 : loss : 1.1816 = 0.2241 + 0.9531 + 0.0045, time: 35.825059]
2023-06-06 16:02:26.000: epoch 26:	0.09046288  	0.17144464  	0.17136121  
2023-06-06 16:02:26.000: Find a better model.
2023-06-06 16:03:04.070: [iter 27 : loss : 1.1627 = 0.2059 + 0.9518 + 0.0049, time: 38.063521]
2023-06-06 16:03:04.470: epoch 27:	0.09072075  	0.17194833  	0.17146337  
2023-06-06 16:03:04.470: Find a better model.
2023-06-06 16:03:40.334: [iter 28 : loss : 1.1445 = 0.1887 + 0.9504 + 0.0053, time: 35.856745]
2023-06-06 16:03:40.717: epoch 28:	0.09088724  	0.17272539  	0.17173152  
2023-06-06 16:03:40.718: Find a better model.
2023-06-06 16:04:16.617: [iter 29 : loss : 1.1303 = 0.1755 + 0.9492 + 0.0057, time: 35.893124]
2023-06-06 16:04:17.005: epoch 29:	0.09105384  	0.17290191  	0.17173760  
2023-06-06 16:04:17.005: Find a better model.
2023-06-06 16:04:53.060: [iter 30 : loss : 1.1177 = 0.1638 + 0.9479 + 0.0061, time: 36.049379]
2023-06-06 16:04:53.484: epoch 30:	0.09122033  	0.17320257  	0.17189112  
2023-06-06 16:04:53.484: Find a better model.
2023-06-06 16:05:29.450: [iter 31 : loss : 1.1057 = 0.1527 + 0.9465 + 0.0064, time: 35.960072]
2023-06-06 16:05:29.835: epoch 31:	0.09091955  	0.17237763  	0.17159070  
2023-06-06 16:06:05.599: [iter 32 : loss : 1.0961 = 0.1440 + 0.9453 + 0.0068, time: 35.756634]
2023-06-06 16:06:05.985: epoch 32:	0.09090881  	0.17232431  	0.17118824  
2023-06-06 16:06:41.772: [iter 33 : loss : 1.0871 = 0.1361 + 0.9440 + 0.0071, time: 35.779688]
2023-06-06 16:06:42.161: epoch 33:	0.09091417  	0.17198221  	0.17073043  
2023-06-06 16:07:17.962: [iter 34 : loss : 1.0792 = 0.1287 + 0.9431 + 0.0074, time: 35.793993]
2023-06-06 16:07:18.363: epoch 34:	0.09062401  	0.17084277  	0.17009456  
2023-06-06 16:07:54.776: [iter 35 : loss : 1.0726 = 0.1228 + 0.9421 + 0.0077, time: 36.406376]
2023-06-06 16:07:55.252: epoch 35:	0.09039837  	0.17053455  	0.16959071  
2023-06-06 16:08:21.132: my pid: 13896
2023-06-06 16:08:21.132: model: model.general_recommender.SGL
2023-06-06 16:08:21.132: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 16:08:21.132: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 16:08:25.965: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 16:09:07.270: [iter 1 : loss : 1.6029 = 0.6931 + 0.9098 + 0.0000, time: 41.303824]
2023-06-06 16:09:07.675: epoch 1:	0.00424363  	0.01040566  	0.00850284  
2023-06-06 16:09:07.676: Find a better model.
2023-06-06 16:09:47.356: [iter 2 : loss : 1.6002 = 0.6931 + 0.9072 + 0.0000, time: 39.675493]
2023-06-06 16:09:47.762: epoch 2:	0.00511922  	0.01190594  	0.00970527  
2023-06-06 16:09:47.762: Find a better model.
2023-06-06 16:10:27.759: [iter 3 : loss : 1.5998 = 0.6930 + 0.9068 + 0.0000, time: 39.991378]
2023-06-06 16:10:28.155: epoch 3:	0.00659105  	0.01435280  	0.01220121  
2023-06-06 16:10:28.155: Find a better model.
2023-06-06 16:11:07.975: [iter 4 : loss : 1.5996 = 0.6929 + 0.9066 + 0.0000, time: 39.813184]
2023-06-06 16:11:08.419: epoch 4:	0.00787493  	0.01663115  	0.01465562  
2023-06-06 16:11:08.420: Find a better model.
2023-06-06 16:11:48.534: [iter 5 : loss : 1.5995 = 0.6929 + 0.9066 + 0.0000, time: 40.108370]
2023-06-06 16:11:48.940: epoch 5:	0.00930923  	0.01892601  	0.01651079  
2023-06-06 16:11:48.940: Find a better model.
2023-06-06 16:12:28.890: [iter 6 : loss : 1.5992 = 0.6928 + 0.9065 + 0.0000, time: 39.944205]
2023-06-06 16:12:29.314: epoch 6:	0.01085633  	0.02269096  	0.01996795  
2023-06-06 16:12:29.314: Find a better model.
2023-06-06 16:13:09.246: [iter 7 : loss : 1.5991 = 0.6926 + 0.9064 + 0.0000, time: 39.924928]
2023-06-06 16:13:09.652: epoch 7:	0.01271500  	0.02695461  	0.02392431  
2023-06-06 16:13:09.652: Find a better model.
2023-06-06 16:13:49.473: [iter 8 : loss : 1.5989 = 0.6924 + 0.9064 + 0.0000, time: 39.815190]
2023-06-06 16:13:49.876: epoch 8:	0.01470261  	0.03139130  	0.02770577  
2023-06-06 16:13:49.876: Find a better model.
2023-06-06 16:14:32.191: [iter 9 : loss : 1.5984 = 0.6921 + 0.9063 + 0.0000, time: 42.309015]
2023-06-06 16:14:32.613: epoch 9:	0.01700718  	0.03785904  	0.03327918  
2023-06-06 16:14:32.613: Find a better model.
2023-06-06 16:15:12.991: [iter 10 : loss : 1.5982 = 0.6917 + 0.9064 + 0.0000, time: 40.369879]
2023-06-06 16:15:13.394: epoch 10:	0.02053650  	0.04595568  	0.04107528  
2023-06-06 16:15:13.394: Find a better model.
2023-06-06 16:15:53.171: [iter 11 : loss : 1.5974 = 0.6910 + 0.9064 + 0.0000, time: 39.770773]
2023-06-06 16:15:53.564: epoch 11:	0.02404436  	0.05489384  	0.04883448  
2023-06-06 16:15:53.565: Find a better model.
2023-06-06 16:16:33.932: [iter 12 : loss : 1.5965 = 0.6898 + 0.9066 + 0.0000, time: 40.360950]
2023-06-06 16:16:34.342: epoch 12:	0.02861568  	0.06506307  	0.05887633  
2023-06-06 16:16:34.342: Find a better model.
2023-06-06 16:17:14.654: [iter 13 : loss : 1.5945 = 0.6876 + 0.9069 + 0.0000, time: 40.305120]
2023-06-06 16:17:15.061: epoch 13:	0.03525996  	0.07941832  	0.07311839  
2023-06-06 16:17:15.061: Find a better model.
2023-06-06 16:17:55.176: [iter 14 : loss : 1.5902 = 0.6826 + 0.9075 + 0.0001, time: 40.109081]
2023-06-06 16:17:55.584: epoch 14:	0.04332256  	0.09576885  	0.09036555  
2023-06-06 16:17:55.584: Find a better model.
2023-06-06 16:18:35.737: [iter 15 : loss : 1.5802 = 0.6713 + 0.9088 + 0.0001, time: 40.145900]
2023-06-06 16:18:36.142: epoch 15:	0.05372189  	0.11334838  	0.11025878  
2023-06-06 16:18:36.142: Find a better model.
2023-06-06 16:19:16.173: [iter 16 : loss : 1.5589 = 0.6471 + 0.9115 + 0.0003, time: 40.025019]
2023-06-06 16:19:16.584: epoch 16:	0.06458288  	0.12988588  	0.12836578  
2023-06-06 16:19:16.584: Find a better model.
2023-06-06 16:19:56.533: [iter 17 : loss : 1.5207 = 0.6043 + 0.9159 + 0.0005, time: 39.941864]
2023-06-06 16:19:56.935: epoch 17:	0.07316656  	0.14234917  	0.14258219  
2023-06-06 16:19:56.935: Find a better model.
2023-06-06 16:20:37.018: [iter 18 : loss : 1.4678 = 0.5450 + 0.9220 + 0.0009, time: 40.076838]
2023-06-06 16:20:37.423: epoch 18:	0.07868306  	0.15087667  	0.15161586  
2023-06-06 16:20:37.423: Find a better model.
2023-06-06 16:21:17.456: [iter 19 : loss : 1.4096 = 0.4799 + 0.9284 + 0.0014, time: 40.025968]
2023-06-06 16:21:17.841: epoch 19:	0.08211005  	0.15619949  	0.15724799  
2023-06-06 16:21:17.841: Find a better model.
2023-06-06 16:21:59.667: [iter 20 : loss : 1.3541 = 0.4186 + 0.9336 + 0.0019, time: 41.820040]
2023-06-06 16:22:00.056: epoch 20:	0.08472066  	0.16135491  	0.16141886  
2023-06-06 16:22:00.056: Find a better model.
2023-06-06 16:22:41.003: [iter 21 : loss : 1.3065 = 0.3667 + 0.9374 + 0.0024, time: 40.940480]
2023-06-06 16:22:41.410: epoch 21:	0.08643955  	0.16431986  	0.16417858  
2023-06-06 16:22:41.410: Find a better model.
2023-06-06 16:23:22.575: [iter 22 : loss : 1.2659 = 0.3237 + 0.9394 + 0.0029, time: 41.158492]
2023-06-06 16:23:22.962: epoch 22:	0.08770721  	0.16658334  	0.16601054  
2023-06-06 16:23:22.963: Find a better model.
2023-06-06 16:24:04.011: [iter 23 : loss : 1.2321 = 0.2888 + 0.9399 + 0.0033, time: 41.042717]
2023-06-06 16:24:04.413: epoch 23:	0.08831957  	0.16813101  	0.16747473  
2023-06-06 16:24:04.413: Find a better model.
2023-06-06 16:24:45.489: [iter 24 : loss : 1.2039 = 0.2601 + 0.9399 + 0.0038, time: 41.069912]
2023-06-06 16:24:45.874: epoch 24:	0.08916820  	0.16896051  	0.16873012  
2023-06-06 16:24:45.874: Find a better model.
2023-06-06 16:25:27.018: [iter 25 : loss : 1.1799 = 0.2363 + 0.9393 + 0.0042, time: 41.137095]
2023-06-06 16:25:27.420: epoch 25:	0.08966246  	0.16960520  	0.16960394  
2023-06-06 16:25:27.420: Find a better model.
2023-06-06 16:26:08.600: [iter 26 : loss : 1.1592 = 0.2159 + 0.9386 + 0.0047, time: 41.172046]
2023-06-06 16:26:08.982: epoch 26:	0.09010299  	0.17058571  	0.17026459  
2023-06-06 16:26:08.982: Find a better model.
2023-06-06 16:26:50.194: [iter 27 : loss : 1.1428 = 0.2002 + 0.9376 + 0.0051, time: 41.205767]
2023-06-06 16:26:50.584: epoch 27:	0.09063482  	0.17148279  	0.17088373  
2023-06-06 16:26:50.584: Find a better model.
2023-06-06 16:27:32.007: [iter 28 : loss : 1.1266 = 0.1847 + 0.9364 + 0.0055, time: 41.417118]
2023-06-06 16:27:32.407: epoch 28:	0.09040925  	0.17085840  	0.17083408  
2023-06-06 16:28:13.756: [iter 29 : loss : 1.1138 = 0.1725 + 0.9354 + 0.0058, time: 41.342248]
2023-06-06 16:28:14.142: epoch 29:	0.09043076  	0.17037641  	0.17049459  
2023-06-06 16:28:55.579: [iter 30 : loss : 1.1024 = 0.1618 + 0.9344 + 0.0062, time: 41.431334]
2023-06-06 16:28:55.966: epoch 30:	0.09042532  	0.17010607  	0.17029370  
2023-06-06 16:29:37.139: [iter 31 : loss : 1.0915 = 0.1517 + 0.9332 + 0.0065, time: 41.167321]
2023-06-06 16:29:37.530: epoch 31:	0.09053813  	0.16995418  	0.17028441  
2023-06-06 16:30:19.056: [iter 32 : loss : 1.0824 = 0.1434 + 0.9322 + 0.0069, time: 41.520087]
2023-06-06 16:30:19.451: epoch 32:	0.09051124  	0.16993834  	0.17003945  
2023-06-06 16:31:00.904: [iter 33 : loss : 1.0739 = 0.1357 + 0.9311 + 0.0072, time: 41.446701]
2023-06-06 16:31:01.313: epoch 33:	0.09035017  	0.16896330  	0.16942695  
2023-06-06 16:31:42.603: [iter 34 : loss : 1.0667 = 0.1290 + 0.9302 + 0.0075, time: 41.283037]
2023-06-06 16:31:42.990: epoch 34:	0.09008692  	0.16831206  	0.16891782  
2023-06-06 16:32:24.413: [iter 35 : loss : 1.0603 = 0.1232 + 0.9294 + 0.0078, time: 41.415586]
2023-06-06 16:32:24.799: epoch 35:	0.08980757  	0.16746171  	0.16811399  
2023-06-06 16:33:06.053: [iter 36 : loss : 1.0542 = 0.1176 + 0.9285 + 0.0080, time: 41.247993]
2023-06-06 16:33:06.450: epoch 36:	0.08974313  	0.16700706  	0.16767940  
2023-06-06 16:33:48.020: [iter 37 : loss : 1.0482 = 0.1121 + 0.9277 + 0.0083, time: 41.562859]
2023-06-06 16:33:48.424: epoch 37:	0.08957125  	0.16636463  	0.16690344  
2023-06-06 16:34:30.030: [iter 38 : loss : 1.0434 = 0.1079 + 0.9270 + 0.0086, time: 41.599992]
2023-06-06 16:34:30.425: epoch 38:	0.08947994  	0.16598795  	0.16629808  
2023-06-06 16:35:12.018: [iter 39 : loss : 1.0386 = 0.1034 + 0.9263 + 0.0088, time: 41.584456]
2023-06-06 16:35:12.416: epoch 39:	0.08913614  	0.16504097  	0.16547370  
2023-06-06 16:35:53.955: [iter 40 : loss : 1.0344 = 0.0997 + 0.9256 + 0.0091, time: 41.531994]
2023-06-06 16:35:54.365: epoch 40:	0.08899112  	0.16453086  	0.16527626  
2023-06-06 16:36:35.586: [iter 41 : loss : 1.0303 = 0.0960 + 0.9249 + 0.0094, time: 41.213539]
2023-06-06 16:36:35.969: epoch 41:	0.08872251  	0.16399115  	0.16470495  
2023-06-06 16:37:17.584: [iter 42 : loss : 1.0268 = 0.0928 + 0.9244 + 0.0096, time: 41.608193]
2023-06-06 16:37:17.972: epoch 42:	0.08850767  	0.16340929  	0.16422877  
2023-06-06 16:37:59.923: [iter 43 : loss : 1.0233 = 0.0896 + 0.9239 + 0.0098, time: 41.944966]
2023-06-06 16:38:00.382: epoch 43:	0.08847545  	0.16288812  	0.16372462  
2023-06-06 16:38:25.443: my pid: 14556
2023-06-06 16:38:25.443: model: model.general_recommender.SGL
2023-06-06 16:38:25.443: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 16:38:25.443: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 16:38:30.579: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 16:39:10.164: [iter 1 : loss : 1.6035 = 0.6931 + 0.9104 + 0.0000, time: 39.585195]
2023-06-06 16:39:10.560: epoch 1:	0.00396968  	0.00993247  	0.00801219  
2023-06-06 16:39:10.561: Find a better model.
2023-06-06 16:39:49.052: [iter 2 : loss : 1.6008 = 0.6931 + 0.9077 + 0.0000, time: 38.486249]
2023-06-06 16:39:49.461: epoch 2:	0.00532334  	0.01258268  	0.01020344  
2023-06-06 16:39:49.461: Find a better model.
2023-06-06 16:40:28.280: [iter 3 : loss : 1.6004 = 0.6930 + 0.9074 + 0.0000, time: 38.812238]
2023-06-06 16:40:28.689: epoch 3:	0.00695096  	0.01507573  	0.01288794  
2023-06-06 16:40:28.689: Find a better model.
2023-06-06 16:41:07.153: [iter 4 : loss : 1.6003 = 0.6930 + 0.9073 + 0.0000, time: 38.456949]
2023-06-06 16:41:07.560: epoch 4:	0.00806832  	0.01648690  	0.01447706  
2023-06-06 16:41:07.560: Find a better model.
2023-06-06 16:41:46.061: [iter 5 : loss : 1.6002 = 0.6929 + 0.9073 + 0.0000, time: 38.493459]
2023-06-06 16:41:46.466: epoch 5:	0.00907824  	0.01817262  	0.01590827  
2023-06-06 16:41:46.467: Find a better model.
2023-06-06 16:42:25.193: [iter 6 : loss : 1.6000 = 0.6928 + 0.9072 + 0.0000, time: 38.720361]
2023-06-06 16:42:25.601: epoch 6:	0.01063071  	0.02207072  	0.01908993  
2023-06-06 16:42:25.601: Find a better model.
2023-06-06 16:43:04.104: [iter 7 : loss : 1.5999 = 0.6926 + 0.9073 + 0.0000, time: 38.495544]
2023-06-06 16:43:04.489: epoch 7:	0.01250012  	0.02691162  	0.02354814  
2023-06-06 16:43:04.489: Find a better model.
2023-06-06 16:43:44.748: [iter 8 : loss : 1.5998 = 0.6925 + 0.9074 + 0.0000, time: 40.252298]
2023-06-06 16:43:45.139: epoch 8:	0.01392904  	0.02938860  	0.02602258  
2023-06-06 16:43:45.139: Find a better model.
2023-06-06 16:44:23.929: [iter 9 : loss : 1.5995 = 0.6922 + 0.9072 + 0.0000, time: 38.784204]
2023-06-06 16:44:24.341: epoch 9:	0.01615305  	0.03507172  	0.03131563  
2023-06-06 16:44:24.341: Find a better model.
2023-06-06 16:45:03.224: [iter 10 : loss : 1.5993 = 0.6919 + 0.9075 + 0.0000, time: 38.877345]
2023-06-06 16:45:03.613: epoch 10:	0.01949435  	0.04239894  	0.03841798  
2023-06-06 16:45:03.613: Find a better model.
2023-06-06 16:45:42.254: [iter 11 : loss : 1.5988 = 0.6913 + 0.9075 + 0.0000, time: 38.633618]
2023-06-06 16:45:42.642: epoch 11:	0.02323320  	0.05131017  	0.04634450  
2023-06-06 16:45:42.642: Find a better model.
2023-06-06 16:46:21.419: [iter 12 : loss : 1.5981 = 0.6903 + 0.9077 + 0.0000, time: 38.771473]
2023-06-06 16:46:21.806: epoch 12:	0.02854584  	0.06349265  	0.05811292  
2023-06-06 16:46:21.806: Find a better model.
2023-06-06 16:47:00.454: [iter 13 : loss : 1.5966 = 0.6885 + 0.9080 + 0.0000, time: 38.641247]
2023-06-06 16:47:00.837: epoch 13:	0.03559301  	0.07861494  	0.07295826  
2023-06-06 16:47:00.837: Find a better model.
2023-06-06 16:47:39.451: [iter 14 : loss : 1.5936 = 0.6847 + 0.9088 + 0.0001, time: 38.607179]
2023-06-06 16:47:39.834: epoch 14:	0.04445047  	0.09531113  	0.09004739  
2023-06-06 16:47:39.835: Find a better model.
2023-06-06 16:48:18.646: [iter 15 : loss : 1.5861 = 0.6760 + 0.9100 + 0.0001, time: 38.805455]
2023-06-06 16:48:19.033: epoch 15:	0.05490885  	0.11245072  	0.10914613  
2023-06-06 16:48:19.033: Find a better model.
2023-06-06 16:48:57.801: [iter 16 : loss : 1.5693 = 0.6564 + 0.9127 + 0.0002, time: 38.762064]
2023-06-06 16:48:58.186: epoch 16:	0.06522212  	0.12893145  	0.12733187  
2023-06-06 16:48:58.186: Find a better model.
2023-06-06 16:49:37.041: [iter 17 : loss : 1.5368 = 0.6193 + 0.9170 + 0.0004, time: 38.847491]
2023-06-06 16:49:37.437: epoch 17:	0.07334927  	0.14105219  	0.14138007  
2023-06-06 16:49:37.438: Find a better model.
2023-06-06 16:50:16.590: [iter 18 : loss : 1.4876 = 0.5638 + 0.9230 + 0.0008, time: 39.145851]
2023-06-06 16:50:16.980: epoch 18:	0.07867249  	0.14985740  	0.15035167  
2023-06-06 16:50:16.980: Find a better model.
2023-06-06 16:50:55.975: [iter 19 : loss : 1.4299 = 0.4988 + 0.9298 + 0.0012, time: 38.988467]
2023-06-06 16:50:56.379: epoch 19:	0.08217467  	0.15555844  	0.15663394  
2023-06-06 16:50:56.379: Find a better model.
2023-06-06 16:51:35.195: [iter 20 : loss : 1.3724 = 0.4352 + 0.9355 + 0.0017, time: 38.810169]
2023-06-06 16:51:35.578: epoch 20:	0.08465633  	0.16078642  	0.16103256  
2023-06-06 16:51:35.578: Find a better model.
2023-06-06 16:52:15.393: [iter 21 : loss : 1.3221 = 0.3800 + 0.9398 + 0.0022, time: 39.808668]
2023-06-06 16:52:15.777: epoch 21:	0.08669212  	0.16393851  	0.16432209  
2023-06-06 16:52:15.777: Find a better model.
2023-06-06 16:52:55.300: [iter 22 : loss : 1.2788 = 0.3341 + 0.9420 + 0.0027, time: 39.516516]
2023-06-06 16:52:55.681: epoch 22:	0.08787927  	0.16644838  	0.16653293  
2023-06-06 16:52:55.681: Find a better model.
2023-06-06 16:53:35.254: [iter 23 : loss : 1.2425 = 0.2966 + 0.9426 + 0.0032, time: 39.566485]
2023-06-06 16:53:35.639: epoch 23:	0.08867954  	0.16838118  	0.16822824  
2023-06-06 16:53:35.639: Find a better model.
2023-06-06 16:54:15.393: [iter 24 : loss : 1.2126 = 0.2662 + 0.9427 + 0.0037, time: 39.747807]
2023-06-06 16:54:15.777: epoch 24:	0.08949058  	0.17015167  	0.16989975  
2023-06-06 16:54:15.778: Find a better model.
2023-06-06 16:54:55.562: [iter 25 : loss : 1.1871 = 0.2408 + 0.9421 + 0.0041, time: 39.777236]
2023-06-06 16:54:55.947: epoch 25:	0.09022653  	0.17130145  	0.17096356  
2023-06-06 16:54:55.947: Find a better model.
2023-06-06 16:55:35.738: [iter 26 : loss : 1.1652 = 0.2194 + 0.9413 + 0.0046, time: 39.783235]
2023-06-06 16:55:36.124: epoch 26:	0.09052204  	0.17222814  	0.17143917  
2023-06-06 16:55:36.124: Find a better model.
2023-06-06 16:56:16.093: [iter 27 : loss : 1.1479 = 0.2027 + 0.9402 + 0.0050, time: 39.962702]
2023-06-06 16:56:16.487: epoch 27:	0.09073693  	0.17239410  	0.17134507  
2023-06-06 16:56:16.487: Find a better model.
2023-06-06 16:56:56.427: [iter 28 : loss : 1.1309 = 0.1866 + 0.9390 + 0.0054, time: 39.934618]
2023-06-06 16:56:56.813: epoch 28:	0.09091955  	0.17251386  	0.17149448  
2023-06-06 16:56:56.813: Find a better model.
2023-06-06 16:57:36.619: [iter 29 : loss : 1.1176 = 0.1740 + 0.9379 + 0.0058, time: 39.799990]
2023-06-06 16:57:37.003: epoch 29:	0.09086048  	0.17208694  	0.17117627  
2023-06-06 16:58:17.060: [iter 30 : loss : 1.1056 = 0.1629 + 0.9367 + 0.0061, time: 40.048371]
2023-06-06 16:58:17.456: epoch 30:	0.09109686  	0.17243390  	0.17138043  
2023-06-06 16:58:57.349: [iter 31 : loss : 1.0944 = 0.1523 + 0.9356 + 0.0065, time: 39.887049]
2023-06-06 16:58:57.735: epoch 31:	0.09095713  	0.17156547  	0.17106164  
2023-06-06 16:59:37.264: [iter 32 : loss : 1.0851 = 0.1439 + 0.9344 + 0.0068, time: 39.522017]
2023-06-06 16:59:37.652: epoch 32:	0.09082288  	0.17143193  	0.17070413  
2023-06-06 17:00:42.169: my pid: 11736
2023-06-06 17:00:42.169: model: model.general_recommender.SGL
2023-06-06 17:00:42.169: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 17:00:42.169: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 17:00:47.407: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 17:01:25.452: [iter 1 : loss : 1.6041 = 0.6931 + 0.9110 + 0.0000, time: 38.043967]
2023-06-06 17:01:25.856: epoch 1:	0.00405563  	0.00990312  	0.00765507  
2023-06-06 17:01:25.856: Find a better model.
2023-06-06 17:02:03.140: [iter 2 : loss : 1.6014 = 0.6931 + 0.9084 + 0.0000, time: 37.276295]
2023-06-06 17:02:03.554: epoch 2:	0.00547912  	0.01241467  	0.01050391  
2023-06-06 17:02:03.554: Find a better model.
2023-06-06 17:02:40.988: [iter 3 : loss : 1.6011 = 0.6930 + 0.9081 + 0.0000, time: 37.428566]
2023-06-06 17:02:41.403: epoch 3:	0.00728939  	0.01605183  	0.01356616  
2023-06-06 17:02:41.403: Find a better model.
2023-06-06 17:03:19.106: [iter 4 : loss : 1.6011 = 0.6929 + 0.9082 + 0.0000, time: 37.696208]
2023-06-06 17:03:19.509: epoch 4:	0.00824559  	0.01720677  	0.01506977  
2023-06-06 17:03:19.509: Find a better model.
2023-06-06 17:03:56.846: [iter 5 : loss : 1.6011 = 0.6929 + 0.9082 + 0.0000, time: 37.330397]
2023-06-06 17:03:57.272: epoch 5:	0.00936294  	0.01892512  	0.01688066  
2023-06-06 17:03:57.272: Find a better model.
2023-06-06 17:04:34.571: [iter 6 : loss : 1.6010 = 0.6928 + 0.9082 + 0.0000, time: 37.292498]
2023-06-06 17:04:34.977: epoch 6:	0.01084022  	0.02312494  	0.02010254  
2023-06-06 17:04:34.977: Find a better model.
2023-06-06 17:05:12.322: [iter 7 : loss : 1.6010 = 0.6926 + 0.9083 + 0.0000, time: 37.337550]
2023-06-06 17:05:12.726: epoch 7:	0.01234434  	0.02555721  	0.02266251  
2023-06-06 17:05:12.726: Find a better model.
2023-06-06 17:05:50.262: [iter 8 : loss : 1.6009 = 0.6924 + 0.9085 + 0.0000, time: 37.528846]
2023-06-06 17:05:50.668: epoch 8:	0.01461127  	0.02994422  	0.02745589  
2023-06-06 17:05:50.668: Find a better model.
2023-06-06 17:06:28.141: [iter 9 : loss : 1.6006 = 0.6921 + 0.9084 + 0.0000, time: 37.466190]
2023-06-06 17:06:28.554: epoch 9:	0.01707699  	0.03561197  	0.03258101  
2023-06-06 17:06:28.554: Find a better model.
2023-06-06 17:07:05.868: [iter 10 : loss : 1.6005 = 0.6917 + 0.9087 + 0.0000, time: 37.307841]
2023-06-06 17:07:06.291: epoch 10:	0.02095550  	0.04376814  	0.04022243  
2023-06-06 17:07:06.291: Find a better model.
2023-06-06 17:07:43.672: [iter 11 : loss : 1.5999 = 0.6911 + 0.9088 + 0.0000, time: 37.374626]
2023-06-06 17:07:44.060: epoch 11:	0.02471583  	0.05338322  	0.04875554  
2023-06-06 17:07:44.060: Find a better model.
2023-06-06 17:08:23.235: [iter 12 : loss : 1.5993 = 0.6900 + 0.9092 + 0.0000, time: 39.168228]
2023-06-06 17:08:23.623: epoch 12:	0.03017331  	0.06609386  	0.06089753  
2023-06-06 17:08:23.624: Find a better model.
2023-06-06 17:09:01.181: [iter 13 : loss : 1.5978 = 0.6880 + 0.9097 + 0.0001, time: 37.551249]
2023-06-06 17:09:01.572: epoch 13:	0.03890175  	0.08277422  	0.07757673  
2023-06-06 17:09:01.572: Find a better model.
2023-06-06 17:09:39.144: [iter 14 : loss : 1.5946 = 0.6838 + 0.9107 + 0.0001, time: 37.564977]
2023-06-06 17:09:39.541: epoch 14:	0.04889278  	0.10111286  	0.09708432  
2023-06-06 17:09:39.541: Find a better model.
2023-06-06 17:10:17.153: [iter 15 : loss : 1.5866 = 0.6742 + 0.9122 + 0.0001, time: 37.604411]
2023-06-06 17:10:17.538: epoch 15:	0.05971631  	0.11865426  	0.11741032  
2023-06-06 17:10:17.538: Find a better model.
2023-06-06 17:10:55.390: [iter 16 : loss : 1.5688 = 0.6531 + 0.9154 + 0.0003, time: 37.845392]
2023-06-06 17:10:55.804: epoch 16:	0.06932586  	0.13346316  	0.13427316  
2023-06-06 17:10:55.804: Find a better model.
2023-06-06 17:11:33.174: [iter 17 : loss : 1.5347 = 0.6141 + 0.9202 + 0.0005, time: 37.362037]
2023-06-06 17:11:33.579: epoch 17:	0.07563752  	0.14343795  	0.14545180  
2023-06-06 17:11:33.579: Find a better model.
2023-06-06 17:12:10.963: [iter 18 : loss : 1.4847 = 0.5572 + 0.9266 + 0.0008, time: 37.377824]
2023-06-06 17:12:11.372: epoch 18:	0.08017110  	0.15131758  	0.15340406  
2023-06-06 17:12:11.372: Find a better model.
2023-06-06 17:12:48.891: [iter 19 : loss : 1.4267 = 0.4919 + 0.9335 + 0.0013, time: 37.512328]
2023-06-06 17:12:49.312: epoch 19:	0.08329185  	0.15705323  	0.15854828  
2023-06-06 17:12:49.312: Find a better model.
2023-06-06 17:13:26.663: [iter 20 : loss : 1.3698 = 0.4290 + 0.9390 + 0.0018, time: 37.344053]
2023-06-06 17:13:27.064: epoch 20:	0.08576279  	0.16135094  	0.16277629  
2023-06-06 17:13:27.065: Find a better model.
2023-06-06 17:14:05.351: [iter 21 : loss : 1.3201 = 0.3746 + 0.9432 + 0.0023, time: 38.280006]
2023-06-06 17:14:05.752: epoch 21:	0.08729897  	0.16443019  	0.16561337  
2023-06-06 17:14:05.752: Find a better model.
2023-06-06 17:14:43.904: [iter 22 : loss : 1.2770 = 0.3291 + 0.9451 + 0.0028, time: 38.146006]
2023-06-06 17:14:44.321: epoch 22:	0.08852379  	0.16699310  	0.16781200  
2023-06-06 17:14:44.321: Find a better model.
2023-06-06 17:15:22.312: [iter 23 : loss : 1.2411 = 0.2922 + 0.9456 + 0.0033, time: 37.984935]
2023-06-06 17:15:22.696: epoch 23:	0.08951745  	0.16963212  	0.16976251  
2023-06-06 17:15:22.696: Find a better model.
2023-06-06 17:16:03.315: [iter 24 : loss : 1.2115 = 0.2622 + 0.9455 + 0.0038, time: 40.611890]
2023-06-06 17:16:03.702: epoch 24:	0.08985052  	0.17071776  	0.17068194  
2023-06-06 17:16:03.702: Find a better model.
2023-06-06 17:16:42.281: [iter 25 : loss : 1.1861 = 0.2371 + 0.9448 + 0.0042, time: 38.572061]
2023-06-06 17:16:42.669: epoch 25:	0.09025335  	0.17151722  	0.17153242  
2023-06-06 17:16:42.669: Find a better model.
2023-06-06 17:17:21.165: [iter 26 : loss : 1.1644 = 0.2159 + 0.9439 + 0.0047, time: 38.489460]
2023-06-06 17:17:21.556: epoch 26:	0.09035001  	0.17133856  	0.17164180  
2023-06-06 17:18:00.018: [iter 27 : loss : 1.1473 = 0.1994 + 0.9428 + 0.0051, time: 38.455018]
2023-06-06 17:18:00.416: epoch 27:	0.09074753  	0.17238308  	0.17225935  
2023-06-06 17:18:00.416: Find a better model.
2023-06-06 17:18:38.607: [iter 28 : loss : 1.1305 = 0.1836 + 0.9415 + 0.0055, time: 38.183511]
2023-06-06 17:18:38.992: epoch 28:	0.09063477  	0.17212342  	0.17203805  
2023-06-06 17:19:17.431: [iter 29 : loss : 1.1174 = 0.1712 + 0.9403 + 0.0058, time: 38.432169]
2023-06-06 17:19:17.815: epoch 29:	0.09073679  	0.17185478  	0.17177045  
2023-06-06 17:19:56.184: [iter 30 : loss : 1.1056 = 0.1603 + 0.9391 + 0.0062, time: 38.362641]
2023-06-06 17:19:56.576: epoch 30:	0.09060788  	0.17122592  	0.17137045  
2023-06-06 17:20:35.125: [iter 31 : loss : 1.0944 = 0.1499 + 0.9379 + 0.0065, time: 38.542290]
2023-06-06 17:20:35.515: epoch 31:	0.09064550  	0.17077979  	0.17113993  
2023-06-06 17:21:14.123: [iter 32 : loss : 1.0853 = 0.1417 + 0.9368 + 0.0069, time: 38.600046]
2023-06-06 17:21:14.514: epoch 32:	0.09052199  	0.17026438  	0.17094800  
2023-06-06 17:21:53.109: [iter 33 : loss : 1.0768 = 0.1340 + 0.9357 + 0.0072, time: 38.587726]
2023-06-06 17:21:53.502: epoch 33:	0.09042528  	0.16931750  	0.17056181  
2023-06-06 17:22:31.931: [iter 34 : loss : 1.0691 = 0.1269 + 0.9347 + 0.0075, time: 38.422590]
2023-06-06 17:22:32.336: epoch 34:	0.09037160  	0.16924739  	0.17024077  
2023-06-06 17:23:10.878: [iter 35 : loss : 1.0628 = 0.1212 + 0.9338 + 0.0078, time: 38.533183]
2023-06-06 17:23:11.291: epoch 35:	0.09000634  	0.16849300  	0.16955577  
2023-06-06 17:23:49.921: [iter 36 : loss : 1.0566 = 0.1156 + 0.9329 + 0.0081, time: 38.623018]
2023-06-06 17:23:50.332: epoch 36:	0.08994190  	0.16835354  	0.16911760  
2023-06-06 17:24:28.867: [iter 37 : loss : 1.0507 = 0.1103 + 0.9321 + 0.0083, time: 38.529449]
2023-06-06 17:24:29.279: epoch 37:	0.08990966  	0.16789353  	0.16897893  
2023-06-06 17:25:07.848: [iter 38 : loss : 1.0459 = 0.1060 + 0.9313 + 0.0086, time: 38.562018]
2023-06-06 17:25:08.251: epoch 38:	0.08972701  	0.16748120  	0.16849522  
2023-06-06 17:25:46.624: [iter 39 : loss : 1.0412 = 0.1016 + 0.9307 + 0.0089, time: 38.361027]
2023-06-06 17:25:47.010: epoch 39:	0.08965185  	0.16704543  	0.16806199  
2023-06-06 17:26:25.631: [iter 40 : loss : 1.0369 = 0.0979 + 0.9299 + 0.0091, time: 38.615104]
2023-06-06 17:26:26.017: epoch 40:	0.08932421  	0.16594659  	0.16727057  
2023-06-06 17:27:04.675: [iter 41 : loss : 1.0328 = 0.0942 + 0.9292 + 0.0094, time: 38.651980]
2023-06-06 17:27:05.062: epoch 41:	0.08915231  	0.16536447  	0.16665797  
2023-06-06 17:27:43.618: [iter 42 : loss : 1.0297 = 0.0914 + 0.9287 + 0.0096, time: 38.549498]
2023-06-06 17:27:44.011: epoch 42:	0.08910398  	0.16516496  	0.16625242  
2023-06-06 17:28:22.638: [iter 43 : loss : 1.0263 = 0.0883 + 0.9282 + 0.0098, time: 38.620001]
2023-06-06 17:28:23.029: epoch 43:	0.08893748  	0.16437674  	0.16559416  
2023-06-06 17:29:01.545: [iter 44 : loss : 1.0227 = 0.0851 + 0.9276 + 0.0100, time: 38.508976]
2023-06-06 17:29:01.933: epoch 44:	0.08873872  	0.16398583  	0.16499074  
2023-06-06 17:29:40.410: [iter 45 : loss : 1.0201 = 0.0828 + 0.9270 + 0.0103, time: 38.469966]
2023-06-06 17:29:40.802: epoch 45:	0.08852926  	0.16312703  	0.16420208  
2023-06-06 17:30:19.317: [iter 46 : loss : 1.0177 = 0.0806 + 0.9267 + 0.0105, time: 38.508066]
2023-06-06 17:30:19.703: epoch 46:	0.08827138  	0.16257821  	0.16371553  
2023-06-06 17:30:58.471: [iter 47 : loss : 1.0152 = 0.0783 + 0.9262 + 0.0107, time: 38.761993]
2023-06-06 17:30:58.859: epoch 47:	0.08811025  	0.16187328  	0.16314916  
2023-06-06 17:31:37.485: [iter 48 : loss : 1.0126 = 0.0759 + 0.9258 + 0.0109, time: 38.619426]
2023-06-06 17:31:37.873: epoch 48:	0.08782020  	0.16122420  	0.16243905  
2023-06-06 17:32:16.512: [iter 49 : loss : 1.0106 = 0.0742 + 0.9254 + 0.0111, time: 38.631988]
2023-06-06 17:32:16.904: epoch 49:	0.08765367  	0.16043681  	0.16178390  
2023-06-06 17:32:55.456: [iter 50 : loss : 1.0085 = 0.0723 + 0.9250 + 0.0112, time: 38.546441]
2023-06-06 17:32:55.845: epoch 50:	0.08763748  	0.16038281  	0.16169202  
2023-06-06 17:33:34.509: [iter 51 : loss : 1.0062 = 0.0701 + 0.9247 + 0.0114, time: 38.657139]
2023-06-06 17:33:34.893: epoch 51:	0.08736891  	0.15968093  	0.16107526  
2023-06-06 17:34:13.639: [iter 52 : loss : 1.0049 = 0.0689 + 0.9244 + 0.0116, time: 38.740232]
2023-06-06 17:34:14.025: epoch 52:	0.08719706  	0.15928768  	0.16070503  
2023-06-06 17:34:14.025: Early stopping is trigger at epoch: 52
2023-06-06 17:34:14.025: best_result@epoch 27:

2023-06-06 17:34:14.025: 		0.0907      	0.1724      	0.1723      
2023-06-06 18:08:38.681: my pid: 428
2023-06-06 18:08:38.681: model: model.general_recommender.SGL
2023-06-06 18:08:38.681: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-06 18:08:38.682: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-06 18:08:43.492: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-06 18:09:19.777: [iter 1 : loss : 1.6045 = 0.6931 + 0.9114 + 0.0000, time: 36.285529]
2023-06-06 18:09:20.187: epoch 1:	0.00431347  	0.00997754  	0.00837702  
2023-06-06 18:09:20.187: Find a better model.
2023-06-06 18:09:56.461: [iter 2 : loss : 1.6020 = 0.6931 + 0.9090 + 0.0000, time: 36.267498]
2023-06-06 18:09:56.869: epoch 2:	0.00557581  	0.01294561  	0.01065421  
2023-06-06 18:09:56.869: Find a better model.
2023-06-06 18:10:33.171: [iter 3 : loss : 1.6019 = 0.6930 + 0.9089 + 0.0000, time: 36.294483]
2023-06-06 18:10:33.590: epoch 3:	0.00703691  	0.01535348  	0.01327527  
2023-06-06 18:10:33.591: Find a better model.
2023-06-06 18:11:09.978: [iter 4 : loss : 1.6020 = 0.6929 + 0.9090 + 0.0000, time: 36.381608]
2023-06-06 18:11:10.397: epoch 4:	0.00789641  	0.01689967  	0.01458906  
2023-06-06 18:11:10.397: Find a better model.
2023-06-06 18:11:46.563: [iter 5 : loss : 1.6020 = 0.6929 + 0.9092 + 0.0000, time: 36.158496]
2023-06-06 18:11:46.971: epoch 5:	0.00872368  	0.01886868  	0.01648490  
2023-06-06 18:11:46.971: Find a better model.
2023-06-06 18:12:23.112: [iter 6 : loss : 1.6020 = 0.6928 + 0.9092 + 0.0000, time: 36.134147]
2023-06-06 18:12:23.517: epoch 6:	0.01024394  	0.02204592  	0.01898233  
2023-06-06 18:12:23.517: Find a better model.
2023-06-06 18:13:01.946: [iter 7 : loss : 1.6021 = 0.6926 + 0.9094 + 0.0000, time: 38.423340]
2023-06-06 18:13:02.358: epoch 7:	0.01200590  	0.02508672  	0.02231236  
2023-06-06 18:13:02.358: Find a better model.
2023-06-06 18:13:38.793: [iter 8 : loss : 1.6021 = 0.6924 + 0.9096 + 0.0000, time: 36.427258]
2023-06-06 18:13:39.189: epoch 8:	0.01393440  	0.02999090  	0.02682702  
2023-06-06 18:13:39.189: Find a better model.
2023-06-06 18:14:15.633: [iter 9 : loss : 1.6018 = 0.6922 + 0.9097 + 0.0000, time: 36.437548]
2023-06-06 18:14:16.026: epoch 9:	0.01616376  	0.03459131  	0.03106223  
2023-06-06 18:14:16.027: Find a better model.
2023-06-06 18:14:52.265: [iter 10 : loss : 1.6019 = 0.6918 + 0.9101 + 0.0000, time: 36.232213]
2023-06-06 18:14:52.655: epoch 10:	0.01938689  	0.04167817  	0.03740519  
2023-06-06 18:14:52.655: Find a better model.
2023-06-06 18:15:28.970: [iter 11 : loss : 1.6015 = 0.6912 + 0.9102 + 0.0000, time: 36.307914]
2023-06-06 18:15:29.378: epoch 11:	0.02326003  	0.04957352  	0.04601697  
2023-06-06 18:15:29.378: Find a better model.
2023-06-06 18:16:05.785: [iter 12 : loss : 1.6010 = 0.6903 + 0.9106 + 0.0000, time: 36.400012]
2023-06-06 18:16:06.177: epoch 12:	0.02827190  	0.06117254  	0.05645916  
2023-06-06 18:16:06.177: Find a better model.
2023-06-06 18:16:42.567: [iter 13 : loss : 1.6000 = 0.6887 + 0.9113 + 0.0000, time: 36.383173]
2023-06-06 18:16:42.956: epoch 13:	0.03665115  	0.07657602  	0.07304930  
2023-06-06 18:16:42.956: Find a better model.
2023-06-06 18:17:19.224: [iter 14 : loss : 1.5977 = 0.6853 + 0.9123 + 0.0001, time: 36.260316]
2023-06-06 18:17:19.613: epoch 14:	0.04654008  	0.09584475  	0.09249357  
2023-06-06 18:17:19.613: Find a better model.
2023-06-06 18:17:55.995: [iter 15 : loss : 1.5919 = 0.6779 + 0.9139 + 0.0001, time: 36.375100]
2023-06-06 18:17:56.404: epoch 15:	0.05733145  	0.11351014  	0.11257669  
2023-06-06 18:17:56.404: Find a better model.
2023-06-06 18:18:32.744: [iter 16 : loss : 1.5785 = 0.6612 + 0.9170 + 0.0002, time: 36.333513]
2023-06-06 18:18:33.131: epoch 16:	0.06777357  	0.13030620  	0.13121189  
2023-06-06 18:18:33.132: Find a better model.
2023-06-06 18:19:09.572: [iter 17 : loss : 1.5505 = 0.6283 + 0.9217 + 0.0004, time: 36.433518]
2023-06-06 18:19:09.960: epoch 17:	0.07537965  	0.14212063  	0.14458807  
2023-06-06 18:19:09.960: Find a better model.
2023-06-06 18:19:46.360: [iter 18 : loss : 1.5054 = 0.5764 + 0.9282 + 0.0007, time: 36.394295]
2023-06-06 18:19:46.744: epoch 18:	0.08013883  	0.15016611  	0.15226971  
2023-06-06 18:19:46.744: Find a better model.
2023-06-06 18:20:23.301: [iter 19 : loss : 1.4495 = 0.5126 + 0.9357 + 0.0012, time: 36.549915]
2023-06-06 18:20:23.688: epoch 19:	0.08287829  	0.15511675  	0.15725167  
2023-06-06 18:20:23.688: Find a better model.
2023-06-06 18:21:00.011: [iter 20 : loss : 1.3916 = 0.4481 + 0.9419 + 0.0017, time: 36.317176]
2023-06-06 18:21:00.416: epoch 20:	0.08522023  	0.15995605  	0.16136767  
2023-06-06 18:21:00.416: Find a better model.
2023-06-06 18:21:37.528: [iter 21 : loss : 1.3397 = 0.3909 + 0.9467 + 0.0022, time: 37.105369]
2023-06-06 18:21:37.916: epoch 21:	0.08709490  	0.16363761  	0.16440226  
2023-06-06 18:21:37.916: Find a better model.
2023-06-06 18:22:15.338: [iter 22 : loss : 1.2942 = 0.3425 + 0.9490 + 0.0027, time: 37.416095]
2023-06-06 18:22:15.725: epoch 22:	0.08842171  	0.16700532  	0.16665310  
2023-06-06 18:22:15.725: Find a better model.
2023-06-06 18:22:53.240: [iter 23 : loss : 1.2560 = 0.3031 + 0.9497 + 0.0032, time: 37.509316]
2023-06-06 18:22:53.627: epoch 23:	0.08910932  	0.16813421  	0.16849692  
2023-06-06 18:22:53.627: Find a better model.
2023-06-06 18:23:31.048: [iter 24 : loss : 1.2243 = 0.2709 + 0.9497 + 0.0037, time: 37.414990]
2023-06-06 18:23:31.445: epoch 24:	0.08985603  	0.17002945  	0.16987096  
2023-06-06 18:23:31.445: Find a better model.
2023-06-06 18:24:08.882: [iter 25 : loss : 1.1972 = 0.2441 + 0.9490 + 0.0041, time: 37.429038]
2023-06-06 18:24:09.295: epoch 25:	0.09032347  	0.17058970  	0.17081812  
2023-06-06 18:24:09.295: Find a better model.
2023-06-06 18:24:46.490: [iter 26 : loss : 1.1745 = 0.2218 + 0.9481 + 0.0045, time: 37.188498]
2023-06-06 18:24:46.877: epoch 26:	0.09078001  	0.17174721  	0.17154373  
2023-06-06 18:24:46.878: Find a better model.
2023-06-06 18:25:24.292: [iter 27 : loss : 1.1561 = 0.2042 + 0.9469 + 0.0050, time: 37.407007]
2023-06-06 18:25:24.682: epoch 27:	0.09095728  	0.17248993  	0.17188272  
2023-06-06 18:25:24.682: Find a better model.
2023-06-06 18:26:02.033: [iter 28 : loss : 1.1385 = 0.1876 + 0.9456 + 0.0054, time: 37.345007]
2023-06-06 18:26:02.435: epoch 28:	0.09107006  	0.17298411  	0.17202629  
2023-06-06 18:26:02.435: Find a better model.
2023-06-06 18:26:39.846: [iter 29 : loss : 1.1245 = 0.1744 + 0.9444 + 0.0057, time: 37.404979]
2023-06-06 18:26:40.232: epoch 29:	0.09117761  	0.17306875  	0.17198759  
2023-06-06 18:26:40.232: Find a better model.
2023-06-06 18:27:17.670: [iter 30 : loss : 1.1122 = 0.1630 + 0.9431 + 0.0061, time: 37.423072]
2023-06-06 18:27:18.056: epoch 30:	0.09126355  	0.17280968  	0.17196216  
2023-06-06 18:27:55.545: [iter 31 : loss : 1.1007 = 0.1524 + 0.9419 + 0.0064, time: 37.481814]
2023-06-06 18:27:55.931: epoch 31:	0.09139782  	0.17320539  	0.17190912  
2023-06-06 18:27:55.931: Find a better model.
2023-06-06 18:28:33.323: [iter 32 : loss : 1.0912 = 0.1437 + 0.9407 + 0.0068, time: 37.384688]
2023-06-06 18:28:33.710: epoch 32:	0.09124206  	0.17258734  	0.17145942  
2023-06-06 18:29:11.177: [iter 33 : loss : 1.0823 = 0.1357 + 0.9395 + 0.0071, time: 37.460333]
2023-06-06 18:29:11.572: epoch 33:	0.09111849  	0.17192151  	0.17089064  
2023-06-06 18:29:49.182: [iter 34 : loss : 1.0746 = 0.1287 + 0.9385 + 0.0074, time: 37.602378]
2023-06-06 18:29:49.574: epoch 34:	0.09097889  	0.17147884  	0.17059572  
2023-06-06 18:30:27.137: [iter 35 : loss : 1.0681 = 0.1228 + 0.9376 + 0.0077, time: 37.556496]
2023-06-06 18:30:27.533: epoch 35:	0.09078011  	0.17080186  	0.16986685  
2023-06-06 18:31:05.119: [iter 36 : loss : 1.0614 = 0.1168 + 0.9366 + 0.0080, time: 37.578485]
2023-06-06 18:31:05.513: epoch 36:	0.09059208  	0.16969775  	0.16940945  
2023-06-06 18:31:43.047: [iter 37 : loss : 1.0555 = 0.1114 + 0.9357 + 0.0083, time: 37.527314]
2023-06-06 18:31:43.447: epoch 37:	0.09054379  	0.16946076  	0.16920075  
2023-06-06 18:32:21.301: [iter 38 : loss : 1.0506 = 0.1072 + 0.9349 + 0.0086, time: 37.847321]
2023-06-06 18:32:21.689: epoch 38:	0.09036107  	0.16917923  	0.16853632  
2023-06-06 18:32:59.472: [iter 39 : loss : 1.0457 = 0.1027 + 0.9342 + 0.0088, time: 37.775984]
2023-06-06 18:32:59.863: epoch 39:	0.09017312  	0.16837461  	0.16762102  
2023-06-06 18:33:37.433: [iter 40 : loss : 1.0414 = 0.0989 + 0.9335 + 0.0091, time: 37.563071]
2023-06-06 18:33:37.825: epoch 40:	0.08992062  	0.16753000  	0.16697374  
2023-06-06 18:34:15.617: [iter 41 : loss : 1.0372 = 0.0952 + 0.9327 + 0.0093, time: 37.784389]
2023-06-06 18:34:16.007: epoch 41:	0.08961983  	0.16652726  	0.16655050  
2023-06-06 18:34:53.689: [iter 42 : loss : 1.0338 = 0.0922 + 0.9321 + 0.0095, time: 37.675511]
2023-06-06 18:34:54.078: epoch 42:	0.08956616  	0.16644512  	0.16625409  
2023-06-06 18:35:32.019: [iter 43 : loss : 1.0301 = 0.0887 + 0.9316 + 0.0098, time: 37.934126]
2023-06-06 18:35:32.423: epoch 43:	0.08928683  	0.16575666  	0.16590224  
2023-06-06 18:36:10.207: [iter 44 : loss : 1.0267 = 0.0857 + 0.9309 + 0.0100, time: 37.777292]
2023-06-06 18:36:10.598: epoch 44:	0.08913103  	0.16537754  	0.16547808  
2023-06-06 18:36:48.584: [iter 45 : loss : 1.0239 = 0.0832 + 0.9304 + 0.0102, time: 37.978977]
2023-06-06 18:36:48.973: epoch 45:	0.08862071  	0.16434902  	0.16462058  
2023-06-06 18:37:27.040: [iter 46 : loss : 1.0214 = 0.0810 + 0.9300 + 0.0104, time: 38.059763]
2023-06-06 18:37:27.452: epoch 46:	0.08857772  	0.16381553  	0.16430876  
2023-06-06 18:38:05.550: [iter 47 : loss : 1.0191 = 0.0789 + 0.9296 + 0.0106, time: 38.092041]
2023-06-06 18:38:05.941: epoch 47:	0.08845423  	0.16321020  	0.16380981  
2023-06-06 18:38:43.888: [iter 48 : loss : 1.0163 = 0.0764 + 0.9291 + 0.0108, time: 37.941074]
2023-06-06 18:38:44.302: epoch 48:	0.08818024  	0.16254719  	0.16324230  
2023-06-06 18:39:22.527: [iter 49 : loss : 1.0143 = 0.0746 + 0.9287 + 0.0110, time: 38.218146]
2023-06-06 18:39:22.918: epoch 49:	0.08790628  	0.16177855  	0.16257584  
2023-06-06 18:40:01.003: [iter 50 : loss : 1.0122 = 0.0727 + 0.9282 + 0.0112, time: 38.078862]
2023-06-06 18:40:01.410: epoch 50:	0.08779348  	0.16162905  	0.16221881  
2023-06-06 18:40:39.729: [iter 51 : loss : 1.0098 = 0.0705 + 0.9279 + 0.0114, time: 38.312741]
2023-06-06 18:40:40.112: epoch 51:	0.08764312  	0.16123100  	0.16180541  
2023-06-06 18:41:18.476: [iter 52 : loss : 1.0085 = 0.0693 + 0.9276 + 0.0116, time: 38.357966]
2023-06-06 18:41:18.865: epoch 52:	0.08753033  	0.16073932  	0.16129282  
2023-06-06 18:41:56.871: [iter 53 : loss : 1.0070 = 0.0680 + 0.9272 + 0.0117, time: 37.999037]
2023-06-06 18:41:57.284: epoch 53:	0.08711134  	0.15987523  	0.16067314  
2023-06-06 18:42:35.709: [iter 54 : loss : 1.0050 = 0.0661 + 0.9270 + 0.0119, time: 38.417462]
2023-06-06 18:42:36.102: epoch 54:	0.08686417  	0.15932247  	0.16003630  
2023-06-06 18:43:14.810: [iter 55 : loss : 1.0035 = 0.0648 + 0.9266 + 0.0121, time: 38.701108]
2023-06-06 18:43:15.196: epoch 55:	0.08661708  	0.15888247  	0.15960865  
2023-06-06 18:43:53.802: [iter 56 : loss : 1.0024 = 0.0637 + 0.9265 + 0.0122, time: 38.597973]
2023-06-06 18:43:54.192: epoch 56:	0.08637536  	0.15838708  	0.15902923  
2023-06-06 18:43:54.192: Early stopping is trigger at epoch: 56
2023-06-06 18:43:54.192: best_result@epoch 31:

2023-06-06 18:43:54.192: 		0.0914      	0.1732      	0.1719      
2023-06-07 09:21:44.319: my pid: 6640
2023-06-07 09:21:44.319: model: model.general_recommender.SGL
2023-06-07 09:21:44.319: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-07 09:21:44.319: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 09:21:49.138: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 09:22:23.980: [iter 1 : loss : 1.6047 = 0.6931 + 0.9116 + 0.0000, time: 34.842563]
2023-06-07 09:22:24.391: epoch 1:	0.00393745  	0.00958805  	0.00756430  
2023-06-07 09:22:24.391: Find a better model.
2023-06-07 09:22:58.898: [iter 2 : loss : 1.6029 = 0.6931 + 0.9098 + 0.0000, time: 34.501338]
2023-06-07 09:22:59.304: epoch 2:	0.00546300  	0.01318208  	0.01050910  
2023-06-07 09:22:59.304: Find a better model.
2023-06-07 09:23:33.917: [iter 3 : loss : 1.6029 = 0.6930 + 0.9099 + 0.0000, time: 34.606577]
2023-06-07 09:23:34.324: epoch 3:	0.00636007  	0.01470357  	0.01192338  
2023-06-07 09:23:34.325: Find a better model.
2023-06-07 09:24:09.129: [iter 4 : loss : 1.6031 = 0.6929 + 0.9101 + 0.0000, time: 34.796573]
2023-06-07 09:24:09.535: epoch 4:	0.00738608  	0.01707724  	0.01400859  
2023-06-07 09:24:09.535: Find a better model.
2023-06-07 09:24:44.334: [iter 5 : loss : 1.6033 = 0.6928 + 0.9105 + 0.0000, time: 34.792115]
2023-06-07 09:24:44.739: epoch 5:	0.00817575  	0.01865625  	0.01518863  
2023-06-07 09:24:44.740: Find a better model.
2023-06-07 09:25:20.349: [iter 6 : loss : 1.6033 = 0.6927 + 0.9106 + 0.0000, time: 35.603529]
2023-06-07 09:25:20.779: epoch 6:	0.00934147  	0.02131523  	0.01775030  
2023-06-07 09:25:20.779: Find a better model.
2023-06-07 09:25:55.974: [iter 7 : loss : 1.6035 = 0.6926 + 0.9109 + 0.0000, time: 35.187429]
2023-06-07 09:25:56.397: epoch 7:	0.01121626  	0.02431241  	0.02086580  
2023-06-07 09:25:56.397: Find a better model.
2023-06-07 09:26:32.569: [iter 8 : loss : 1.6036 = 0.6924 + 0.9112 + 0.0000, time: 36.165936]
2023-06-07 09:26:33.041: epoch 8:	0.01280094  	0.02797689  	0.02442851  
2023-06-07 09:26:33.041: Find a better model.
2023-06-07 09:27:10.532: [iter 9 : loss : 1.6035 = 0.6921 + 0.9113 + 0.0000, time: 37.485282]
2023-06-07 09:27:11.012: epoch 9:	0.01426748  	0.03135915  	0.02847538  
2023-06-07 09:27:11.012: Find a better model.
2023-06-07 09:27:45.537: [iter 10 : loss : 1.6036 = 0.6918 + 0.9118 + 0.0000, time: 34.518494]
2023-06-07 09:27:45.956: epoch 10:	0.01760880  	0.03946184  	0.03494227  
2023-06-07 09:27:45.956: Find a better model.
2023-06-07 09:28:21.144: [iter 11 : loss : 1.6033 = 0.6912 + 0.9120 + 0.0000, time: 35.182080]
2023-06-07 09:28:21.548: epoch 11:	0.02141750  	0.04815641  	0.04277661  
2023-06-07 09:28:21.548: Find a better model.
2023-06-07 09:28:57.279: [iter 12 : loss : 1.6029 = 0.6903 + 0.9126 + 0.0000, time: 35.723274]
2023-06-07 09:28:57.682: epoch 12:	0.02614472  	0.05741391  	0.05347310  
2023-06-07 09:28:57.682: Find a better model.
2023-06-07 09:29:32.642: [iter 13 : loss : 1.6021 = 0.6888 + 0.9133 + 0.0001, time: 34.953159]
2023-06-07 09:29:33.045: epoch 13:	0.03359481  	0.07190999  	0.06763364  
2023-06-07 09:29:33.045: Find a better model.
2023-06-07 09:30:09.429: [iter 14 : loss : 1.6003 = 0.6858 + 0.9145 + 0.0001, time: 36.377586]
2023-06-07 09:30:09.880: epoch 14:	0.04293582  	0.08970717  	0.08671996  
2023-06-07 09:30:09.880: Find a better model.
2023-06-07 09:30:45.449: [iter 15 : loss : 1.5957 = 0.6793 + 0.9163 + 0.0001, time: 35.561615]
2023-06-07 09:30:45.876: epoch 15:	0.05487129  	0.10970088  	0.10807329  
2023-06-07 09:30:45.877: Find a better model.
2023-06-07 09:31:23.087: [iter 16 : loss : 1.5848 = 0.6652 + 0.9195 + 0.0002, time: 37.202836]
2023-06-07 09:31:23.500: epoch 16:	0.06558200  	0.12636225  	0.12811841  
2023-06-07 09:31:23.500: Find a better model.
2023-06-07 09:32:00.045: [iter 17 : loss : 1.5613 = 0.6367 + 0.9243 + 0.0004, time: 36.538465]
2023-06-07 09:32:00.446: epoch 17:	0.07435372  	0.14063454  	0.14293690  
2023-06-07 09:32:00.446: Find a better model.
2023-06-07 09:32:36.454: [iter 18 : loss : 1.5209 = 0.5894 + 0.9308 + 0.0007, time: 35.999385]
2023-06-07 09:32:36.880: epoch 18:	0.07947266  	0.14968187  	0.15230757  
2023-06-07 09:32:36.880: Find a better model.
2023-06-07 09:33:13.457: [iter 19 : loss : 1.4677 = 0.5281 + 0.9385 + 0.0011, time: 36.566279]
2023-06-07 09:33:13.969: epoch 19:	0.08302861  	0.15606061  	0.15815482  
2023-06-07 09:33:13.969: Find a better model.
2023-06-07 09:33:51.690: [iter 20 : loss : 1.4097 = 0.4630 + 0.9452 + 0.0016, time: 37.714666]
2023-06-07 09:33:52.206: epoch 20:	0.08547803  	0.16096601  	0.16224957  
2023-06-07 09:33:52.206: Find a better model.
2023-06-07 09:34:31.308: [iter 21 : loss : 1.3561 = 0.4033 + 0.9507 + 0.0021, time: 39.096178]
2023-06-07 09:34:31.734: epoch 21:	0.08691220  	0.16386104  	0.16495253  
2023-06-07 09:34:31.734: Find a better model.
2023-06-07 09:35:10.671: [iter 22 : loss : 1.3085 = 0.3523 + 0.9535 + 0.0026, time: 38.928830]
2023-06-07 09:35:11.120: epoch 22:	0.08831955  	0.16741391  	0.16754512  
2023-06-07 09:35:11.120: Find a better model.
2023-06-07 09:35:49.918: [iter 23 : loss : 1.2682 = 0.3105 + 0.9546 + 0.0031, time: 38.791614]
2023-06-07 09:35:50.316: epoch 23:	0.08924344  	0.16906492  	0.16934785  
2023-06-07 09:35:50.317: Find a better model.
2023-06-07 09:36:29.529: [iter 24 : loss : 1.2348 = 0.2766 + 0.9547 + 0.0036, time: 39.205949]
2023-06-07 09:36:30.077: epoch 24:	0.08988270  	0.17005427  	0.17019539  
2023-06-07 09:36:30.077: Find a better model.
2023-06-07 09:37:09.556: [iter 25 : loss : 1.2065 = 0.2484 + 0.9541 + 0.0040, time: 39.471955]
2023-06-07 09:37:10.613: epoch 25:	0.08993642  	0.17037816  	0.17060915  
2023-06-07 09:37:10.614: Find a better model.
2023-06-07 09:37:49.790: [iter 26 : loss : 1.1828 = 0.2251 + 0.9532 + 0.0045, time: 39.160905]
2023-06-07 09:37:50.222: epoch 26:	0.09026411  	0.17103042  	0.17103139  
2023-06-07 09:37:50.222: Find a better model.
2023-06-07 09:38:29.241: [iter 27 : loss : 1.1636 = 0.2068 + 0.9519 + 0.0049, time: 39.012522]
2023-06-07 09:38:29.638: epoch 27:	0.09046288  	0.17173351  	0.17130670  
2023-06-07 09:38:29.639: Find a better model.
2023-06-07 09:39:08.581: [iter 28 : loss : 1.1453 = 0.1895 + 0.9505 + 0.0053, time: 38.934690]
2023-06-07 09:39:09.022: epoch 28:	0.09068858  	0.17214870  	0.17147200  
2023-06-07 09:39:09.022: Find a better model.
2023-06-07 09:39:48.209: [iter 29 : loss : 1.1311 = 0.1761 + 0.9493 + 0.0057, time: 39.181401]
2023-06-07 09:39:48.603: epoch 29:	0.09072613  	0.17248274  	0.17147176  
2023-06-07 09:39:48.603: Find a better model.
2023-06-07 09:40:27.640: [iter 30 : loss : 1.1184 = 0.1644 + 0.9480 + 0.0060, time: 39.029498]
2023-06-07 09:40:28.080: epoch 30:	0.09078524  	0.17195606  	0.17123535  
2023-06-07 09:41:07.428: [iter 31 : loss : 1.1064 = 0.1533 + 0.9466 + 0.0064, time: 39.342365]
2023-06-07 09:41:07.818: epoch 31:	0.09072616  	0.17189886  	0.17113914  
2023-06-07 09:41:46.663: [iter 32 : loss : 1.0966 = 0.1444 + 0.9454 + 0.0067, time: 38.838881]
2023-06-07 09:41:47.196: epoch 32:	0.09077449  	0.17154923  	0.17078765  
2023-06-07 09:42:25.996: [iter 33 : loss : 1.0877 = 0.1365 + 0.9441 + 0.0071, time: 38.792615]
2023-06-07 09:42:26.412: epoch 33:	0.09079064  	0.17147091  	0.17072366  
2023-06-07 09:43:05.647: [iter 34 : loss : 1.0797 = 0.1291 + 0.9432 + 0.0074, time: 39.229226]
2023-06-07 09:43:06.106: epoch 34:	0.09068320  	0.17126215  	0.17040102  
2023-06-08 11:16:51.253: my pid: 8296
2023-06-08 11:16:51.254: model: model.general_recommender.SGL
2023-06-08 11:16:51.254: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-08 11:16:51.254: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=4
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 11:16:57.597: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 11:17:55.125: [iter 1 : loss : 1.6058 = 0.6931 + 0.9127 + 0.0000, time: 57.524658]
2023-06-08 11:17:55.600: epoch 1:	0.00186936  	0.00303500  	0.00280087  
2023-06-08 11:17:55.600: Find a better model.
2023-06-08 11:18:53.085: [iter 2 : loss : 1.6022 = 0.6931 + 0.9090 + 0.0000, time: 57.475753]
2023-06-08 11:18:53.859: epoch 2:	0.00208960  	0.00314544  	0.00291763  
2023-06-08 11:18:53.860: Find a better model.
2023-06-08 11:19:51.071: [iter 3 : loss : 1.6019 = 0.6931 + 0.9088 + 0.0000, time: 57.192448]
2023-06-08 11:19:51.831: epoch 3:	0.00278792  	0.00416859  	0.00409841  
2023-06-08 11:19:51.832: Find a better model.
2023-06-08 11:20:47.926: [iter 4 : loss : 1.6020 = 0.6931 + 0.9089 + 0.0000, time: 56.074949]
2023-06-08 11:20:48.662: epoch 4:	0.00330897  	0.00441155  	0.00449356  
2023-06-08 11:20:48.662: Find a better model.
2023-06-08 11:21:46.188: [iter 5 : loss : 1.6020 = 0.6930 + 0.9090 + 0.0000, time: 57.517828]
2023-06-08 11:21:46.942: epoch 5:	0.00345401  	0.00451646  	0.00459624  
2023-06-08 11:21:46.943: Find a better model.
2023-06-08 11:22:44.720: [iter 6 : loss : 1.6020 = 0.6930 + 0.9090 + 0.0000, time: 57.764915]
2023-06-08 11:22:45.484: epoch 6:	0.00350772  	0.00514450  	0.00511846  
2023-06-08 11:22:45.484: Find a better model.
2023-06-08 11:23:42.176: [iter 7 : loss : 1.6021 = 0.6930 + 0.9091 + 0.0000, time: 56.683136]
2023-06-08 11:23:43.005: epoch 7:	0.00402877  	0.00560832  	0.00577747  
2023-06-08 11:23:43.005: Find a better model.
2023-06-08 11:24:39.737: [iter 8 : loss : 1.6022 = 0.6929 + 0.9092 + 0.0000, time: 56.724634]
2023-06-08 11:24:40.234: epoch 8:	0.00513534  	0.00731331  	0.00758399  
2023-06-08 11:24:40.234: Find a better model.
2023-06-08 11:25:24.510: [iter 9 : loss : 1.6021 = 0.6929 + 0.9092 + 0.0000, time: 44.269791]
2023-06-08 11:25:25.002: epoch 9:	0.00533409  	0.00712884  	0.00783330  
2023-06-08 11:26:08.659: [iter 10 : loss : 1.6023 = 0.6928 + 0.9094 + 0.0000, time: 43.650495]
2023-06-08 11:26:09.126: epoch 10:	0.00583903  	0.00795402  	0.00885346  
2023-06-08 11:26:09.126: Find a better model.
2023-06-08 11:27:08.356: [iter 11 : loss : 1.6022 = 0.6928 + 0.9094 + 0.0000, time: 59.220993]
2023-06-08 11:27:09.093: epoch 11:	0.00646214  	0.00900155  	0.00975666  
2023-06-08 11:27:09.093: Find a better model.
2023-06-08 11:28:06.777: [iter 12 : loss : 1.6022 = 0.6927 + 0.9095 + 0.0000, time: 57.674240]
2023-06-08 11:28:07.536: epoch 12:	0.00711212  	0.00995882  	0.01073436  
2023-06-08 11:28:07.536: Find a better model.
2023-06-08 11:29:05.356: [iter 13 : loss : 1.6022 = 0.6926 + 0.9096 + 0.0000, time: 57.811427]
2023-06-08 11:29:06.147: epoch 13:	0.00796088  	0.01186380  	0.01278306  
2023-06-08 11:29:06.148: Find a better model.
2023-06-08 11:30:00.483: [iter 14 : loss : 1.6023 = 0.6925 + 0.9097 + 0.0000, time: 54.327609]
2023-06-08 11:30:01.205: epoch 14:	0.00887410  	0.01325637  	0.01356056  
2023-06-08 11:30:01.205: Find a better model.
2023-06-08 11:30:54.582: [iter 15 : loss : 1.6022 = 0.6924 + 0.9098 + 0.0000, time: 53.363133]
2023-06-08 11:30:55.277: epoch 15:	0.01001293  	0.01513691  	0.01598471  
2023-06-08 11:30:55.277: Find a better model.
2023-06-08 11:31:48.714: [iter 16 : loss : 1.6023 = 0.6922 + 0.9100 + 0.0000, time: 53.428637]
2023-06-08 11:31:49.597: epoch 16:	0.01085094  	0.01711613  	0.01794377  
2023-06-08 11:31:49.597: Find a better model.
2023-06-08 11:32:44.999: [iter 17 : loss : 1.6021 = 0.6920 + 0.9102 + 0.0000, time: 55.388364]
2023-06-08 11:32:45.732: epoch 17:	0.01274183  	0.01983375  	0.02079613  
2023-06-08 11:32:45.732: Find a better model.
2023-06-08 11:33:43.364: [iter 18 : loss : 1.6019 = 0.6917 + 0.9102 + 0.0000, time: 57.623974]
2023-06-08 11:33:44.123: epoch 18:	0.01511622  	0.02483114  	0.02490142  
2023-06-08 11:33:44.123: Find a better model.
2023-06-08 11:34:42.077: [iter 19 : loss : 1.6018 = 0.6912 + 0.9106 + 0.0000, time: 57.942175]
2023-06-08 11:34:42.794: epoch 19:	0.01785051  	0.03076359  	0.03095614  
2023-06-08 11:34:42.794: Find a better model.
2023-06-08 11:35:40.310: [iter 20 : loss : 1.6014 = 0.6906 + 0.9108 + 0.0001, time: 57.507756]
2023-06-08 11:35:41.043: epoch 20:	0.02154099  	0.03943103  	0.03950310  
2023-06-08 11:35:41.043: Find a better model.
2023-06-08 11:36:39.725: [iter 21 : loss : 1.6008 = 0.6896 + 0.9111 + 0.0001, time: 58.673322]
2023-06-08 11:36:40.410: epoch 21:	0.02715996  	0.05185952  	0.05125325  
2023-06-08 11:36:40.410: Find a better model.
2023-06-08 11:37:38.872: [iter 22 : loss : 1.5997 = 0.6881 + 0.9116 + 0.0001, time: 58.452486]
2023-06-08 11:37:39.599: epoch 22:	0.03455631  	0.06822213  	0.06803713  
2023-06-08 11:37:39.599: Find a better model.
2023-06-08 11:38:38.339: [iter 23 : loss : 1.5977 = 0.6852 + 0.9124 + 0.0001, time: 58.731726]
2023-06-08 11:38:39.191: epoch 23:	0.04426259  	0.08770043  	0.08758154  
2023-06-08 11:38:39.191: Find a better model.
2023-06-08 11:39:38.373: [iter 24 : loss : 1.5934 = 0.6797 + 0.9136 + 0.0002, time: 59.174213]
2023-06-08 11:39:39.073: epoch 24:	0.05503795  	0.10839573  	0.10794403  
2023-06-08 11:39:39.074: Find a better model.
2023-06-08 11:40:37.441: [iter 25 : loss : 1.5844 = 0.6687 + 0.9154 + 0.0003, time: 58.360394]
2023-06-08 11:40:38.152: epoch 25:	0.06560878  	0.12747243  	0.12766513  
2023-06-08 11:40:38.152: Find a better model.
2023-06-08 11:41:37.272: [iter 26 : loss : 1.5666 = 0.6475 + 0.9186 + 0.0005, time: 59.110909]
2023-06-08 11:41:37.979: epoch 26:	0.07393472  	0.14018567  	0.14203456  
2023-06-08 11:41:37.979: Find a better model.
2023-06-08 11:42:36.073: [iter 27 : loss : 1.5364 = 0.6117 + 0.9239 + 0.0008, time: 58.079726]
2023-06-08 11:42:36.785: epoch 27:	0.07931694  	0.14912282  	0.15150511  
2023-06-08 11:42:36.785: Find a better model.
2023-06-08 11:43:35.574: [iter 28 : loss : 1.4925 = 0.5603 + 0.9310 + 0.0012, time: 58.781344]
2023-06-08 11:43:36.312: epoch 28:	0.08251306  	0.15408763  	0.15673873  
2023-06-08 11:43:36.312: Find a better model.
2023-06-08 11:44:35.584: [iter 29 : loss : 1.4423 = 0.5016 + 0.9390 + 0.0018, time: 59.260047]
2023-06-08 11:44:36.305: epoch 29:	0.08421046  	0.15817499  	0.15991244  
2023-06-08 11:44:36.305: Find a better model.
2023-06-08 11:45:35.653: [iter 30 : loss : 1.3918 = 0.4436 + 0.9459 + 0.0024, time: 59.341878]
2023-06-08 11:45:36.458: epoch 30:	0.08576282  	0.16101027  	0.16231835  
2023-06-08 11:45:36.458: Find a better model.
2023-06-08 11:46:34.912: [iter 31 : loss : 1.3456 = 0.3919 + 0.9507 + 0.0030, time: 58.446226]
2023-06-08 11:46:35.724: epoch 31:	0.08710577  	0.16385809  	0.16511637  
2023-06-08 11:46:35.725: Find a better model.
2023-06-08 11:47:34.060: [iter 32 : loss : 1.3049 = 0.3479 + 0.9533 + 0.0037, time: 58.324355]
2023-06-08 11:47:34.765: epoch 32:	0.08823384  	0.16641678  	0.16707648  
2023-06-08 11:47:34.765: Find a better model.
2023-06-08 11:48:34.065: [iter 33 : loss : 1.2706 = 0.3119 + 0.9544 + 0.0043, time: 59.293592]
2023-06-08 11:48:34.767: epoch 33:	0.08915229  	0.16868563  	0.16876926  
2023-06-08 11:48:34.767: Find a better model.
2023-06-08 11:49:34.521: [iter 34 : loss : 1.2413 = 0.2818 + 0.9546 + 0.0049, time: 59.745880]
2023-06-08 11:49:35.337: epoch 34:	0.08992045  	0.17086782  	0.17020905  
2023-06-08 11:49:35.338: Find a better model.
2023-06-08 11:50:33.791: [iter 35 : loss : 1.2167 = 0.2571 + 0.9541 + 0.0055, time: 58.446204]
2023-06-08 11:50:34.544: epoch 35:	0.09039851  	0.17211054  	0.17111948  
2023-06-08 11:50:34.544: Find a better model.
2023-06-08 11:51:33.084: [iter 36 : loss : 1.1947 = 0.2354 + 0.9532 + 0.0060, time: 58.531571]
2023-06-08 11:51:33.512: epoch 36:	0.09088731  	0.17294942  	0.17197505  
2023-06-08 11:51:33.512: Find a better model.
2023-06-08 11:52:16.955: [iter 37 : loss : 1.1757 = 0.2169 + 0.9522 + 0.0065, time: 43.436251]
2023-06-08 11:52:17.511: epoch 37:	0.09132788  	0.17346001  	0.17259870  
2023-06-08 11:52:17.511: Find a better model.
2023-06-08 11:53:01.686: [iter 38 : loss : 1.1596 = 0.2017 + 0.9509 + 0.0071, time: 44.168021]
2023-06-08 11:53:02.115: epoch 38:	0.09158578  	0.17351638  	0.17281674  
2023-06-08 11:53:02.116: Find a better model.
2023-06-08 11:53:49.402: [iter 39 : loss : 1.1458 = 0.1885 + 0.9498 + 0.0075, time: 47.278763]
2023-06-08 11:53:49.989: epoch 39:	0.09185436  	0.17431284  	0.17309894  
2023-06-08 11:53:49.989: Find a better model.
2023-06-08 11:54:33.837: [iter 40 : loss : 1.1329 = 0.1764 + 0.9485 + 0.0080, time: 43.839044]
2023-06-08 11:54:34.257: epoch 40:	0.09195107  	0.17459324  	0.17326565  
2023-06-08 11:54:34.257: Find a better model.
2023-06-08 11:55:17.997: [iter 41 : loss : 1.1215 = 0.1659 + 0.9472 + 0.0085, time: 43.732546]
2023-06-08 11:55:18.492: epoch 41:	0.09196717  	0.17440824  	0.17322756  
2023-06-08 11:56:03.022: [iter 42 : loss : 1.1121 = 0.1572 + 0.9460 + 0.0089, time: 44.522575]
2023-06-08 11:56:03.522: epoch 42:	0.09189197  	0.17416555  	0.17315213  
2023-06-08 11:56:48.077: [iter 43 : loss : 1.1028 = 0.1487 + 0.9449 + 0.0093, time: 44.548015]
2023-06-08 11:56:48.498: epoch 43:	0.09170391  	0.17393957  	0.17289458  
2023-06-08 11:57:32.899: [iter 44 : loss : 1.0948 = 0.1414 + 0.9437 + 0.0097, time: 44.392722]
2023-06-08 11:57:33.309: epoch 44:	0.09162330  	0.17319167  	0.17257628  
2023-06-08 11:58:15.378: [iter 45 : loss : 1.0875 = 0.1348 + 0.9426 + 0.0101, time: 42.062575]
2023-06-08 11:58:15.799: epoch 45:	0.09136003  	0.17288411  	0.17224275  
2023-06-08 11:58:56.617: [iter 46 : loss : 1.0812 = 0.1291 + 0.9416 + 0.0105, time: 40.810672]
2023-06-08 11:58:57.022: epoch 46:	0.09137615  	0.17268568  	0.17214148  
2023-06-08 11:59:37.953: [iter 47 : loss : 1.0753 = 0.1237 + 0.9407 + 0.0108, time: 40.924191]
2023-06-08 11:59:38.353: epoch 47:	0.09116128  	0.17225023  	0.17168638  
2023-06-08 12:00:19.320: [iter 48 : loss : 1.0694 = 0.1184 + 0.9398 + 0.0112, time: 40.959796]
2023-06-08 12:00:19.748: epoch 48:	0.09104848  	0.17140336  	0.17117074  
2023-06-08 12:01:00.538: [iter 49 : loss : 1.0645 = 0.1141 + 0.9389 + 0.0115, time: 40.784349]
2023-06-08 12:01:00.949: epoch 49:	0.09098395  	0.17089279  	0.17061134  
2023-06-08 12:01:42.174: [iter 50 : loss : 1.0598 = 0.1099 + 0.9380 + 0.0119, time: 41.217359]
2023-06-08 12:01:42.573: epoch 50:	0.09107532  	0.17071250  	0.17052338  
2023-06-08 12:02:23.742: [iter 51 : loss : 1.0549 = 0.1055 + 0.9373 + 0.0122, time: 41.162975]
2023-06-08 12:02:24.140: epoch 51:	0.09077451  	0.16976100  	0.16989532  
2023-06-08 12:03:05.268: [iter 52 : loss : 1.0519 = 0.1028 + 0.9366 + 0.0125, time: 41.120516]
2023-06-08 12:03:05.672: epoch 52:	0.09057577  	0.16901080  	0.16933706  
2023-06-08 12:03:46.884: [iter 53 : loss : 1.0480 = 0.0994 + 0.9358 + 0.0128, time: 41.194225]
2023-06-08 12:03:47.289: epoch 53:	0.09048982  	0.16865312  	0.16900218  
2023-06-08 12:04:28.499: [iter 54 : loss : 1.0444 = 0.0961 + 0.9352 + 0.0131, time: 41.201527]
2023-06-08 12:04:28.914: epoch 54:	0.09025350  	0.16814117  	0.16850093  
2023-06-08 12:05:09.838: [iter 55 : loss : 1.0412 = 0.0934 + 0.9345 + 0.0134, time: 40.917956]
2023-06-08 12:05:10.239: epoch 55:	0.09017293  	0.16758984  	0.16808291  
2023-06-08 12:05:51.360: [iter 56 : loss : 1.0386 = 0.0909 + 0.9341 + 0.0136, time: 41.114159]
2023-06-08 12:05:51.784: epoch 56:	0.09002253  	0.16729087  	0.16779688  
2023-06-08 12:06:33.000: [iter 57 : loss : 1.0354 = 0.0880 + 0.9336 + 0.0139, time: 41.209643]
2023-06-08 12:06:33.400: epoch 57:	0.08994193  	0.16691640  	0.16751128  
2023-06-08 12:07:14.821: [iter 58 : loss : 1.0329 = 0.0858 + 0.9330 + 0.0142, time: 41.415192]
2023-06-08 12:07:15.221: epoch 58:	0.08980768  	0.16650237  	0.16724384  
2023-06-08 12:07:56.398: [iter 59 : loss : 1.0302 = 0.0833 + 0.9325 + 0.0144, time: 41.169959]
2023-06-08 12:07:56.818: epoch 59:	0.08967336  	0.16632903  	0.16697083  
2023-06-08 12:08:37.978: [iter 60 : loss : 1.0284 = 0.0817 + 0.9320 + 0.0146, time: 41.152003]
2023-06-08 12:08:38.374: epoch 60:	0.08952837  	0.16580680  	0.16649051  
2023-06-08 12:09:19.564: [iter 61 : loss : 1.0262 = 0.0797 + 0.9316 + 0.0149, time: 41.181841]
2023-06-08 12:09:19.971: epoch 61:	0.08944239  	0.16599239  	0.16625072  
2023-06-08 12:10:01.408: [iter 62 : loss : 1.0239 = 0.0775 + 0.9312 + 0.0151, time: 41.430460]
2023-06-08 12:10:01.829: epoch 62:	0.08926513  	0.16507512  	0.16566384  
2023-06-08 12:10:43.355: [iter 63 : loss : 1.0223 = 0.0760 + 0.9310 + 0.0153, time: 41.520108]
2023-06-08 12:10:43.779: epoch 63:	0.08911471  	0.16466890  	0.16549508  
2023-06-08 12:11:25.146: [iter 64 : loss : 1.0205 = 0.0745 + 0.9305 + 0.0155, time: 41.359115]
2023-06-08 12:11:25.543: epoch 64:	0.08891601  	0.16453628  	0.16531608  
2023-06-08 12:12:07.129: [iter 65 : loss : 1.0191 = 0.0731 + 0.9302 + 0.0157, time: 41.579801]
2023-06-08 12:12:07.528: epoch 65:	0.08879782  	0.16439575  	0.16499902  
2023-06-08 12:12:07.528: Early stopping is trigger at epoch: 65
2023-06-08 12:12:07.528: best_result@epoch 40:

2023-06-08 12:12:07.528: 		0.0920      	0.1746      	0.1733      
2023-06-08 14:36:08.713: my pid: 13456
2023-06-08 14:36:08.714: model: model.general_recommender.SGL
2023-06-08 14:36:08.714: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-08 14:36:08.714: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 14:36:13.807: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 14:36:47.810: [iter 1 : loss : 1.6030 = 0.6931 + 0.9100 + 0.0000, time: 34.002340]
2023-06-08 14:36:48.219: epoch 1:	0.00183176  	0.00289542  	0.00277803  
2023-06-08 14:36:48.220: Find a better model.
2023-06-08 14:37:21.608: [iter 2 : loss : 1.6014 = 0.6930 + 0.9084 + 0.0000, time: 33.380707]
2023-06-08 14:37:22.065: epoch 2:	0.00247636  	0.00409116  	0.00387573  
2023-06-08 14:37:22.065: Find a better model.
2023-06-08 14:38:04.213: [iter 3 : loss : 1.6013 = 0.6928 + 0.9085 + 0.0000, time: 42.137605]
2023-06-08 14:38:04.968: epoch 3:	0.00387299  	0.00667211  	0.00622040  
2023-06-08 14:38:04.968: Find a better model.
2023-06-08 14:38:47.768: [iter 4 : loss : 1.6013 = 0.6926 + 0.9087 + 0.0000, time: 42.794271]
2023-06-08 14:38:48.274: epoch 4:	0.00529649  	0.00859046  	0.00856878  
2023-06-08 14:38:48.274: Find a better model.
2023-06-08 14:39:31.874: [iter 5 : loss : 1.6012 = 0.6923 + 0.9089 + 0.0000, time: 43.575301]
2023-06-08 14:39:32.623: epoch 5:	0.00741295  	0.01294860  	0.01212681  
2023-06-08 14:39:32.623: Find a better model.
2023-06-08 14:40:17.056: [iter 6 : loss : 1.6009 = 0.6918 + 0.9091 + 0.0000, time: 44.420194]
2023-06-08 14:40:17.820: epoch 6:	0.01112491  	0.02021646  	0.01979924  
2023-06-08 14:40:17.820: Find a better model.
2023-06-08 14:41:02.605: [iter 7 : loss : 1.6005 = 0.6909 + 0.9096 + 0.0000, time: 44.777090]
2023-06-08 14:41:03.304: epoch 7:	0.01699637  	0.03296234  	0.03165171  
2023-06-08 14:41:03.304: Find a better model.
2023-06-08 14:41:47.004: [iter 8 : loss : 1.5995 = 0.6892 + 0.9102 + 0.0000, time: 43.680517]
2023-06-08 14:41:47.477: epoch 8:	0.02563434  	0.05237719  	0.05129353  
2023-06-08 14:41:47.477: Find a better model.
2023-06-08 14:42:31.605: [iter 9 : loss : 1.5965 = 0.6853 + 0.9111 + 0.0000, time: 44.110687]
2023-06-08 14:42:32.356: epoch 9:	0.03961090  	0.08129226  	0.07954027  
2023-06-08 14:42:32.356: Find a better model.
2023-06-08 14:43:16.843: [iter 10 : loss : 1.5880 = 0.6746 + 0.9133 + 0.0001, time: 44.479579]
2023-06-08 14:43:17.626: epoch 10:	0.05713273  	0.11236282  	0.11191183  
2023-06-08 14:43:17.626: Find a better model.
2023-06-08 14:43:55.019: [iter 11 : loss : 1.5616 = 0.6438 + 0.9176 + 0.0002, time: 37.385523]
2023-06-08 14:43:55.512: epoch 11:	0.07201167  	0.13686778  	0.13831437  
2023-06-08 14:43:55.512: Find a better model.
2023-06-08 14:44:31.252: [iter 12 : loss : 1.5055 = 0.5793 + 0.9258 + 0.0004, time: 35.734286]
2023-06-08 14:44:31.703: epoch 12:	0.07995610  	0.14965534  	0.15149836  
2023-06-08 14:44:31.703: Find a better model.
2023-06-08 14:45:08.091: [iter 13 : loss : 1.4283 = 0.4918 + 0.9358 + 0.0007, time: 36.380981]
2023-06-08 14:45:08.571: epoch 13:	0.08447360  	0.15800622  	0.15934999  
2023-06-08 14:45:08.572: Find a better model.
2023-06-08 14:45:45.085: [iter 14 : loss : 1.3520 = 0.4071 + 0.9437 + 0.0011, time: 36.506024]
2023-06-08 14:45:45.545: epoch 14:	0.08682632  	0.16329844  	0.16371498  
2023-06-08 14:45:45.545: Find a better model.
2023-06-08 14:46:21.020: [iter 15 : loss : 1.2890 = 0.3398 + 0.9477 + 0.0015, time: 35.468957]
2023-06-08 14:46:21.504: epoch 15:	0.08857746  	0.16729833  	0.16721340  
2023-06-08 14:46:21.504: Find a better model.
2023-06-08 14:46:57.740: [iter 16 : loss : 1.2400 = 0.2891 + 0.9490 + 0.0019, time: 36.228151]
2023-06-08 14:46:58.238: epoch 16:	0.08966261  	0.16969950  	0.16909790  
2023-06-08 14:46:58.238: Find a better model.
2023-06-08 14:47:33.867: [iter 17 : loss : 1.2011 = 0.2503 + 0.9486 + 0.0022, time: 35.622348]
2023-06-08 14:47:34.363: epoch 17:	0.09017296  	0.17087615  	0.16997500  
2023-06-08 14:47:34.363: Find a better model.
2023-06-08 14:48:11.234: [iter 18 : loss : 1.1701 = 0.2202 + 0.9473 + 0.0026, time: 36.864307]
2023-06-08 14:48:11.680: epoch 18:	0.09042542  	0.17124024  	0.17050031  
2023-06-08 14:48:11.680: Find a better model.
2023-06-08 14:48:47.447: [iter 19 : loss : 1.1458 = 0.1970 + 0.9459 + 0.0029, time: 35.760336]
2023-06-08 14:48:47.853: epoch 19:	0.09091956  	0.17236532  	0.17120147  
2023-06-08 14:48:47.853: Find a better model.
2023-06-08 14:49:23.637: [iter 20 : loss : 1.1251 = 0.1777 + 0.9442 + 0.0032, time: 35.778282]
2023-06-08 14:49:24.049: epoch 20:	0.09091954  	0.17259604  	0.17109537  
2023-06-08 14:49:24.049: Find a better model.
2023-06-08 14:50:00.891: [iter 21 : loss : 1.1083 = 0.1623 + 0.9425 + 0.0035, time: 36.835442]
2023-06-08 14:50:01.384: epoch 21:	0.09054887  	0.17161788  	0.17067295  
2023-06-08 14:50:38.368: [iter 22 : loss : 1.0940 = 0.1494 + 0.9409 + 0.0037, time: 36.976471]
2023-06-08 14:50:38.776: epoch 22:	0.09030721  	0.17112480  	0.17004149  
2023-06-08 14:51:16.188: [iter 23 : loss : 1.0816 = 0.1383 + 0.9393 + 0.0040, time: 37.404525]
2023-06-08 14:51:16.729: epoch 23:	0.09029651  	0.17067896  	0.16965128  
2023-06-08 14:51:53.575: [iter 24 : loss : 1.0709 = 0.1288 + 0.9379 + 0.0042, time: 36.839672]
2023-06-08 14:51:53.997: epoch 24:	0.08988832  	0.16957468  	0.16918449  
2023-06-08 14:52:31.010: [iter 25 : loss : 1.0615 = 0.1206 + 0.9365 + 0.0044, time: 37.005780]
2023-06-08 14:52:31.533: epoch 25:	0.08960363  	0.16884725  	0.16831909  
2023-06-08 14:53:09.246: [iter 26 : loss : 1.0531 = 0.1132 + 0.9353 + 0.0047, time: 37.705737]
2023-06-08 14:53:09.717: epoch 26:	0.08929205  	0.16795555  	0.16750948  
2023-06-08 14:53:47.103: [iter 27 : loss : 1.0469 = 0.1078 + 0.9342 + 0.0049, time: 37.377885]
2023-06-08 14:53:47.563: epoch 27:	0.08915774  	0.16727456  	0.16673151  
2023-06-08 14:54:25.162: [iter 28 : loss : 1.0395 = 0.1014 + 0.9330 + 0.0051, time: 37.591025]
2023-06-08 14:54:25.722: epoch 28:	0.08880318  	0.16650216  	0.16589569  
2023-06-08 14:55:03.532: [iter 29 : loss : 1.0340 = 0.0967 + 0.9321 + 0.0053, time: 37.803168]
2023-06-08 14:55:03.965: epoch 29:	0.08857223  	0.16562118  	0.16507863  
2023-06-08 14:55:41.814: [iter 30 : loss : 1.0289 = 0.0922 + 0.9312 + 0.0055, time: 37.842060]
2023-06-08 14:55:42.288: epoch 30:	0.08807269  	0.16437313  	0.16415223  
2023-06-08 14:56:26.437: [iter 31 : loss : 1.0237 = 0.0876 + 0.9304 + 0.0057, time: 44.129950]
2023-06-08 14:56:27.117: epoch 31:	0.08790617  	0.16407217  	0.16378519  
2023-06-08 14:57:11.428: [iter 32 : loss : 1.0197 = 0.0843 + 0.9296 + 0.0058, time: 44.301440]
2023-06-08 14:57:12.078: epoch 32:	0.08775578  	0.16337496  	0.16324212  
2023-06-08 14:57:53.726: [iter 33 : loss : 1.0155 = 0.0807 + 0.9288 + 0.0060, time: 41.641519]
2023-06-08 14:57:54.365: epoch 33:	0.08744955  	0.16260830  	0.16252430  
2023-06-08 14:58:36.110: [iter 34 : loss : 1.0121 = 0.0777 + 0.9282 + 0.0062, time: 41.737405]
2023-06-08 14:58:36.744: epoch 34:	0.08729915  	0.16198787  	0.16190816  
2023-06-08 14:59:14.925: [iter 35 : loss : 1.0094 = 0.0753 + 0.9277 + 0.0063, time: 38.175540]
2023-06-08 14:59:15.350: epoch 35:	0.08686946  	0.16110095  	0.16125922  
2023-06-08 14:59:50.131: [iter 36 : loss : 1.0056 = 0.0720 + 0.9271 + 0.0065, time: 34.774064]
2023-06-08 14:59:50.541: epoch 36:	0.08662773  	0.16033369  	0.16054855  
2023-06-08 15:00:25.479: [iter 37 : loss : 1.0029 = 0.0696 + 0.9266 + 0.0066, time: 34.932062]
2023-06-08 15:00:25.877: epoch 37:	0.08668676  	0.16023432  	0.16023135  
2023-06-08 15:01:00.710: [iter 38 : loss : 1.0008 = 0.0680 + 0.9261 + 0.0068, time: 34.826883]
2023-06-08 15:01:01.119: epoch 38:	0.08639128  	0.15933248  	0.15958357  
2023-06-08 15:01:36.105: [iter 39 : loss : 0.9984 = 0.0657 + 0.9258 + 0.0069, time: 34.978509]
2023-06-08 15:01:36.516: epoch 39:	0.08606365  	0.15877405  	0.15893808  
2023-06-08 15:02:12.994: [iter 40 : loss : 0.9963 = 0.0639 + 0.9253 + 0.0071, time: 36.470820]
2023-06-08 15:02:13.413: epoch 40:	0.08585413  	0.15795644  	0.15810397  
2023-06-08 15:02:48.639: [iter 41 : loss : 0.9938 = 0.0618 + 0.9248 + 0.0072, time: 35.220845]
2023-06-08 15:02:49.034: epoch 41:	0.08557476  	0.15743828  	0.15743534  
2023-06-08 15:03:24.418: [iter 42 : loss : 0.9925 = 0.0607 + 0.9246 + 0.0073, time: 35.376442]
2023-06-08 15:03:24.814: epoch 42:	0.08529009  	0.15634941  	0.15667009  
2023-06-08 15:04:00.052: [iter 43 : loss : 0.9903 = 0.0587 + 0.9242 + 0.0074, time: 35.231204]
2023-06-08 15:04:00.468: epoch 43:	0.08515583  	0.15598547  	0.15636700  
2023-06-08 15:04:35.594: [iter 44 : loss : 0.9884 = 0.0571 + 0.9238 + 0.0076, time: 35.119576]
2023-06-08 15:04:35.991: epoch 44:	0.08484431  	0.15528248  	0.15559866  
2023-06-08 15:05:11.406: [iter 45 : loss : 0.9874 = 0.0562 + 0.9235 + 0.0077, time: 35.409043]
2023-06-08 15:05:11.807: epoch 45:	0.08453277  	0.15439942  	0.15497573  
2023-06-08 15:05:11.808: Early stopping is trigger at epoch: 45
2023-06-08 15:05:11.808: best_result@epoch 20:

2023-06-08 15:05:11.808: 		0.0909      	0.1726      	0.1711      
2023-06-08 15:10:50.584: my pid: 2088
2023-06-08 15:10:50.584: model: model.general_recommender.SGL
2023-06-08 15:10:50.584: Dataset statistics:
Name: Douban
The number of users: 9308
The number of items: 15471
The number of ratings: 723484
Average actions of users: 77.73
Average actions of items: 46.76
The sparsity of the dataset: 99.497595%

The number of training: 590728
The number of validation: 0
The number of testing: 132756
2023-06-08 15:10:50.584: NeuRec:[NeuRec]:
recommender=SGL
dataset=Douban
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=1
ssl_reg=0.1
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 15:10:55.576: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 15:11:26.925: [iter 1 : loss : 1.6022 = 0.6928 + 0.9094 + 0.0000, time: 31.349596]
2023-06-08 15:11:27.382: epoch 1:	0.00706914  	0.01828696  	0.01507437  
2023-06-08 15:11:27.383: Find a better model.
2023-06-08 15:11:59.100: [iter 2 : loss : 1.6006 = 0.6919 + 0.9087 + 0.0000, time: 31.711023]
2023-06-08 15:11:59.520: epoch 2:	0.01397200  	0.03490141  	0.02953982  
2023-06-08 15:11:59.521: Find a better model.
2023-06-08 15:12:30.682: [iter 3 : loss : 1.5985 = 0.6893 + 0.9092 + 0.0000, time: 31.156129]
2023-06-08 15:12:31.083: epoch 3:	0.03008734  	0.06846400  	0.06173833  
2023-06-08 15:12:31.083: Find a better model.
2023-06-08 15:13:02.259: [iter 4 : loss : 1.5893 = 0.6777 + 0.9115 + 0.0000, time: 31.169977]
2023-06-08 15:13:02.659: epoch 4:	0.05088559  	0.10423125  	0.10150335  
2023-06-08 15:13:02.659: Find a better model.
2023-06-08 15:13:36.118: [iter 5 : loss : 1.5527 = 0.6349 + 0.9177 + 0.0001, time: 33.447955]
2023-06-08 15:13:36.695: epoch 5:	0.06670454  	0.12914900  	0.13021547  
2023-06-08 15:13:36.695: Find a better model.
2023-06-08 15:14:12.466: [iter 6 : loss : 1.4784 = 0.5522 + 0.9259 + 0.0003, time: 35.763816]
2023-06-08 15:14:13.090: epoch 6:	0.07567505  	0.14417021  	0.14638507  
2023-06-08 15:14:13.090: Find a better model.
2023-06-08 15:14:48.500: [iter 7 : loss : 1.3905 = 0.4574 + 0.9326 + 0.0006, time: 35.401066]
2023-06-08 15:14:49.138: epoch 7:	0.08041272  	0.15296979  	0.15448116  
2023-06-08 15:14:49.138: Find a better model.
2023-06-08 15:15:24.126: [iter 8 : loss : 1.3132 = 0.3762 + 0.9362 + 0.0008, time: 34.982007]
2023-06-08 15:15:24.796: epoch 8:	0.08311459  	0.15829633  	0.15911348  
2023-06-08 15:15:24.797: Find a better model.
2023-06-08 15:16:03.021: [iter 9 : loss : 1.2513 = 0.3127 + 0.9375 + 0.0011, time: 38.216199]
2023-06-08 15:16:03.703: epoch 9:	0.08473687  	0.16122074  	0.16151926  
2023-06-08 15:16:03.703: Find a better model.
2023-06-08 15:16:37.921: [iter 10 : loss : 1.2043 = 0.2654 + 0.9375 + 0.0013, time: 34.210949]
2023-06-08 15:16:38.395: epoch 10:	0.08572520  	0.16374014  	0.16296318  
2023-06-08 15:16:38.395: Find a better model.
2023-06-08 15:17:11.992: [iter 11 : loss : 1.1669 = 0.2288 + 0.9366 + 0.0016, time: 33.589040]
2023-06-08 15:17:12.504: epoch 11:	0.08598851  	0.16411476  	0.16318914  
2023-06-08 15:17:12.504: Find a better model.
2023-06-08 15:17:46.083: [iter 12 : loss : 1.1376 = 0.2004 + 0.9354 + 0.0018, time: 33.572272]
2023-06-08 15:17:46.597: epoch 12:	0.08611204  	0.16434279  	0.16277087  
2023-06-08 15:17:46.597: Find a better model.
2023-06-08 15:18:20.280: [iter 13 : loss : 1.1145 = 0.1784 + 0.9341 + 0.0020, time: 33.676917]
2023-06-08 15:18:20.722: epoch 13:	0.08596696  	0.16367324  	0.16247550  
2023-06-08 15:18:52.683: [iter 14 : loss : 1.0954 = 0.1604 + 0.9329 + 0.0022, time: 31.954694]
2023-06-08 15:18:53.094: epoch 14:	0.08588638  	0.16311826  	0.16157603  
2023-06-08 15:19:24.764: [iter 15 : loss : 1.0791 = 0.1453 + 0.9315 + 0.0023, time: 31.662732]
2023-06-08 15:19:25.176: epoch 15:	0.08573052  	0.16207479  	0.16076785  
2023-06-08 15:19:56.583: [iter 16 : loss : 1.0660 = 0.1330 + 0.9304 + 0.0025, time: 31.399479]
2023-06-08 15:19:56.991: epoch 16:	0.08549416  	0.16123895  	0.16015354  
2023-06-08 15:20:30.225: [iter 17 : loss : 1.0547 = 0.1228 + 0.9293 + 0.0027, time: 33.227597]
2023-06-08 15:20:30.676: epoch 17:	0.08505906  	0.16017531  	0.15925598  
2023-06-08 15:21:04.118: [iter 18 : loss : 1.0447 = 0.1137 + 0.9282 + 0.0028, time: 33.435754]
2023-06-08 15:21:04.582: epoch 18:	0.08464542  	0.15926880  	0.15837203  
2023-06-08 15:21:37.477: [iter 19 : loss : 1.0367 = 0.1064 + 0.9273 + 0.0030, time: 32.888765]
2023-06-08 15:21:37.920: epoch 19:	0.08450041  	0.15867575  	0.15772529  
2023-06-08 15:22:15.584: [iter 20 : loss : 1.0289 = 0.0994 + 0.9265 + 0.0031, time: 37.657034]
2023-06-08 15:22:16.218: epoch 20:	0.08417277  	0.15756282  	0.15665008  
2023-06-08 15:22:54.765: [iter 21 : loss : 1.0224 = 0.0936 + 0.9256 + 0.0032, time: 38.541619]
2023-06-08 15:22:55.479: epoch 21:	0.08375918  	0.15642551  	0.15558966  
2023-06-08 15:23:33.895: [iter 22 : loss : 1.0167 = 0.0885 + 0.9248 + 0.0034, time: 38.406599]
2023-06-08 15:23:34.570: epoch 22:	0.08339925  	0.15539081  	0.15473537  
2023-06-08 15:24:13.164: [iter 23 : loss : 1.0116 = 0.0840 + 0.9242 + 0.0035, time: 38.585782]
2023-06-08 15:24:13.842: epoch 23:	0.08280302  	0.15378459  	0.15330105  
2023-06-08 15:24:52.471: [iter 24 : loss : 1.0071 = 0.0799 + 0.9236 + 0.0036, time: 38.620281]
2023-06-08 15:24:53.128: epoch 24:	0.08252365  	0.15263894  	0.15240648  
2023-06-08 15:25:31.974: [iter 25 : loss : 1.0031 = 0.0764 + 0.9230 + 0.0037, time: 38.835262]
2023-06-08 15:25:32.680: epoch 25:	0.08207784  	0.15134466  	0.15138939  
2023-06-08 15:26:11.622: [iter 26 : loss : 0.9991 = 0.0727 + 0.9226 + 0.0038, time: 38.932033]
2023-06-08 15:26:12.317: epoch 26:	0.08185225  	0.15077451  	0.15052670  
2023-06-08 15:26:50.926: [iter 27 : loss : 0.9966 = 0.0705 + 0.9222 + 0.0039, time: 38.594640]
2023-06-08 15:26:51.540: epoch 27:	0.08161047  	0.15019794  	0.14964820  
2023-06-08 15:27:25.163: [iter 28 : loss : 0.9929 = 0.0672 + 0.9217 + 0.0040, time: 33.615739]
2023-06-08 15:27:25.635: epoch 28:	0.08115395  	0.14894637  	0.14834286  
2023-06-08 15:27:59.767: [iter 29 : loss : 0.9901 = 0.0647 + 0.9213 + 0.0041, time: 34.127043]
2023-06-08 15:28:00.213: epoch 29:	0.08076185  	0.14799041  	0.14737447  
2023-06-08 15:28:34.456: [iter 30 : loss : 0.9878 = 0.0627 + 0.9209 + 0.0042, time: 34.236328]
2023-06-08 15:28:34.897: epoch 30:	0.08043956  	0.14696728  	0.14651203  
2023-06-08 15:29:08.894: [iter 31 : loss : 0.9853 = 0.0603 + 0.9207 + 0.0043, time: 33.989244]
2023-06-08 15:29:09.476: epoch 31:	0.08013880  	0.14644442  	0.14570954  
2023-06-08 15:29:43.444: [iter 32 : loss : 0.9832 = 0.0585 + 0.9203 + 0.0044, time: 33.962588]
2023-06-08 15:29:43.877: epoch 32:	0.07963924  	0.14538810  	0.14500895  
2023-06-08 15:30:18.039: [iter 33 : loss : 0.9812 = 0.0567 + 0.9200 + 0.0045, time: 34.154934]
2023-06-08 15:30:18.611: epoch 33:	0.07926860  	0.14468411  	0.14400753  
2023-06-08 15:30:52.598: [iter 34 : loss : 0.9793 = 0.0549 + 0.9198 + 0.0045, time: 33.979339]
2023-06-08 15:30:53.064: epoch 34:	0.07897861  	0.14409392  	0.14331645  
2023-06-08 15:31:27.205: [iter 35 : loss : 0.9778 = 0.0536 + 0.9196 + 0.0046, time: 34.134338]
2023-06-08 15:31:27.670: epoch 35:	0.07869392  	0.14333397  	0.14250661  
2023-06-08 15:32:01.070: [iter 36 : loss : 0.9758 = 0.0518 + 0.9193 + 0.0047, time: 33.391836]
2023-06-08 15:32:01.598: epoch 36:	0.07821044  	0.14241147  	0.14187999  
2023-06-08 15:32:35.630: [iter 37 : loss : 0.9744 = 0.0506 + 0.9191 + 0.0048, time: 34.025034]
2023-06-08 15:32:36.059: epoch 37:	0.07822658  	0.14212158  	0.14132372  
2023-06-08 15:32:36.059: Early stopping is trigger at epoch: 37
2023-06-08 15:32:36.059: best_result@epoch 12:

2023-06-08 15:32:36.059: 		0.0861      	0.1643      	0.1628      
