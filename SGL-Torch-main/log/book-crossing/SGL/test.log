2023-06-07 09:49:06.898: my pid: 8116
2023-06-07 09:49:06.898: model: model.general_recommender.SGL
2023-06-07 09:49:06.898: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 09:49:06.898: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 09:49:55.964: my pid: 14996
2023-06-07 09:49:55.965: model: model.general_recommender.SGL
2023-06-07 09:49:55.965: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 09:49:55.965: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 09:50:00.490: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 09:50:25.701: [iter 1 : loss : 1.1312 = 0.6931 + 0.4381 + 0.0000, time: 25.210288]
2023-06-07 09:50:26.207: epoch 1:	0.00228754  	0.00467963  	0.00366414  
2023-06-07 09:50:26.208: Find a better model.
2023-06-07 09:50:52.990: [iter 2 : loss : 1.1298 = 0.6930 + 0.4367 + 0.0000, time: 26.778086]
2023-06-07 09:50:53.310: epoch 2:	0.00288718  	0.00646205  	0.00503523  
2023-06-07 09:50:53.310: Find a better model.
2023-06-07 09:51:22.704: [iter 3 : loss : 1.1295 = 0.6930 + 0.4366 + 0.0000, time: 29.388536]
2023-06-07 09:51:23.303: epoch 3:	0.00384217  	0.00862259  	0.00711940  
2023-06-07 09:51:23.303: Find a better model.
2023-06-07 09:51:53.620: [iter 4 : loss : 1.1293 = 0.6929 + 0.4365 + 0.0000, time: 30.314522]
2023-06-07 09:51:54.021: epoch 4:	0.00456026  	0.01077871  	0.00860023  
2023-06-07 09:51:54.021: Find a better model.
2023-06-07 09:52:24.606: [iter 5 : loss : 1.1292 = 0.6927 + 0.4364 + 0.0000, time: 30.581192]
2023-06-07 09:52:25.146: epoch 5:	0.00544861  	0.01356987  	0.01121735  
2023-06-07 09:52:25.146: Find a better model.
2023-06-07 09:52:54.486: [iter 6 : loss : 1.1289 = 0.6925 + 0.4364 + 0.0000, time: 29.327580]
2023-06-07 09:52:55.008: epoch 6:	0.00630735  	0.01626251  	0.01310698  
2023-06-07 09:52:55.008: Find a better model.
2023-06-07 09:53:22.453: [iter 7 : loss : 1.1287 = 0.6923 + 0.4364 + 0.0000, time: 27.437318]
2023-06-07 09:53:22.998: epoch 7:	0.00724752  	0.01950730  	0.01528528  
2023-06-07 09:53:22.998: Find a better model.
2023-06-07 09:53:51.776: [iter 8 : loss : 1.1282 = 0.6918 + 0.4364 + 0.0000, time: 28.774310]
2023-06-07 09:53:52.249: epoch 8:	0.00721051  	0.01946709  	0.01533636  
2023-06-07 09:54:22.663: [iter 9 : loss : 1.1273 = 0.6908 + 0.4365 + 0.0000, time: 30.404531]
2023-06-07 09:54:23.199: epoch 9:	0.00692920  	0.01809226  	0.01476765  
2023-06-07 09:54:53.198: [iter 10 : loss : 1.1257 = 0.6891 + 0.4366 + 0.0000, time: 29.994541]
2023-06-07 09:54:53.720: epoch 10:	0.00782496  	0.02150385  	0.01688979  
2023-06-07 09:54:53.720: Find a better model.
2023-06-07 09:55:22.666: [iter 11 : loss : 1.1238 = 0.6870 + 0.4367 + 0.0000, time: 28.936285]
2023-06-07 09:55:23.229: epoch 11:	0.00915008  	0.02598063  	0.02057294  
2023-06-07 09:55:23.229: Find a better model.
2023-06-07 09:55:52.752: [iter 12 : loss : 1.1202 = 0.6832 + 0.4370 + 0.0000, time: 29.519113]
2023-06-07 09:55:53.267: epoch 12:	0.01072695  	0.03095596  	0.02526327  
2023-06-07 09:55:53.267: Find a better model.
2023-06-07 09:56:24.386: [iter 13 : loss : 1.1131 = 0.6756 + 0.4374 + 0.0001, time: 31.112994]
2023-06-07 09:56:24.940: epoch 13:	0.01299229  	0.03798797  	0.03139868  
2023-06-07 09:56:24.940: Find a better model.
2023-06-07 09:56:56.367: [iter 14 : loss : 1.0994 = 0.6613 + 0.4380 + 0.0001, time: 31.421215]
2023-06-07 09:56:56.896: epoch 14:	0.01578330  	0.04678475  	0.03845461  
2023-06-07 09:56:56.896: Find a better model.
2023-06-07 09:57:26.738: [iter 15 : loss : 1.0762 = 0.6369 + 0.4391 + 0.0002, time: 29.838214]
2023-06-07 09:57:27.279: epoch 15:	0.01927761  	0.05705223  	0.04650150  
2023-06-07 09:57:27.279: Find a better model.
2023-06-07 09:57:58.980: [iter 16 : loss : 1.0407 = 0.5995 + 0.4408 + 0.0004, time: 31.698636]
2023-06-07 09:57:59.544: epoch 16:	0.02266832  	0.06532717  	0.05364343  
2023-06-07 09:57:59.545: Find a better model.
2023-06-07 09:58:30.783: [iter 17 : loss : 0.9942 = 0.5503 + 0.4434 + 0.0006, time: 31.223078]
2023-06-07 09:58:31.332: epoch 17:	0.02571109  	0.07289089  	0.06013628  
2023-06-07 09:58:31.332: Find a better model.
2023-06-07 09:59:00.992: [iter 18 : loss : 0.9411 = 0.4936 + 0.4467 + 0.0008, time: 29.656088]
2023-06-07 09:59:01.509: epoch 18:	0.02762113  	0.07769122  	0.06395180  
2023-06-07 09:59:01.509: Find a better model.
2023-06-07 09:59:32.417: [iter 19 : loss : 0.8875 = 0.4359 + 0.4504 + 0.0011, time: 30.904635]
2023-06-07 09:59:32.793: epoch 19:	0.02888709  	0.08041823  	0.06599820  
2023-06-07 09:59:32.793: Find a better model.
2023-06-07 10:00:04.079: [iter 20 : loss : 0.8379 = 0.3824 + 0.4540 + 0.0014, time: 31.280575]
2023-06-07 10:00:04.590: epoch 20:	0.02913138  	0.08068391  	0.06658486  
2023-06-07 10:00:04.590: Find a better model.
2023-06-07 10:00:35.010: [iter 21 : loss : 0.7951 = 0.3367 + 0.4567 + 0.0018, time: 30.409245]
2023-06-07 10:00:35.501: epoch 21:	0.02948672  	0.08165109  	0.06720711  
2023-06-07 10:00:35.501: Find a better model.
2023-06-07 10:01:05.586: [iter 22 : loss : 0.7593 = 0.2986 + 0.4587 + 0.0021, time: 30.082006]
2023-06-07 10:01:05.954: epoch 22:	0.02971623  	0.08216812  	0.06754071  
2023-06-07 10:01:05.954: Find a better model.
2023-06-07 10:01:38.327: [iter 23 : loss : 0.7288 = 0.2668 + 0.4596 + 0.0024, time: 32.360957]
2023-06-07 10:01:38.846: epoch 23:	0.02976805  	0.08249197  	0.06788029  
2023-06-07 10:01:38.847: Find a better model.
2023-06-07 10:02:11.446: [iter 24 : loss : 0.7030 = 0.2405 + 0.4599 + 0.0026, time: 32.591278]
2023-06-07 10:02:12.012: epoch 24:	0.03010118  	0.08299171  	0.06845879  
2023-06-07 10:02:12.013: Find a better model.
2023-06-07 10:02:42.671: [iter 25 : loss : 0.6830 = 0.2202 + 0.4598 + 0.0029, time: 30.654506]
2023-06-07 10:02:43.180: epoch 25:	0.03014561  	0.08319417  	0.06864812  
2023-06-07 10:02:43.180: Find a better model.
2023-06-07 10:03:15.344: [iter 26 : loss : 0.6644 = 0.2018 + 0.4595 + 0.0031, time: 32.160197]
2023-06-07 10:03:15.779: epoch 26:	0.03024927  	0.08360779  	0.06876332  
2023-06-07 10:03:15.779: Find a better model.
2023-06-07 10:03:42.626: [iter 27 : loss : 0.6496 = 0.1873 + 0.4589 + 0.0034, time: 26.842169]
2023-06-07 10:03:42.930: epoch 27:	0.03036772  	0.08332465  	0.06919947  
2023-06-07 10:04:08.467: [iter 28 : loss : 0.6366 = 0.1747 + 0.4583 + 0.0036, time: 25.531520]
2023-06-07 10:04:08.888: epoch 28:	0.03041212  	0.08372491  	0.06957506  
2023-06-07 10:04:08.888: Find a better model.
2023-06-07 10:04:38.701: [iter 29 : loss : 0.6254 = 0.1640 + 0.4576 + 0.0038, time: 29.796850]
2023-06-07 10:04:39.226: epoch 29:	0.03045653  	0.08399191  	0.06966054  
2023-06-07 10:04:39.226: Find a better model.
2023-06-07 10:05:10.174: [iter 30 : loss : 0.6148 = 0.1539 + 0.4569 + 0.0040, time: 30.943538]
2023-06-07 10:05:10.566: epoch 30:	0.03068603  	0.08491484  	0.07029124  
2023-06-07 10:05:10.566: Find a better model.
2023-06-07 10:05:42.925: [iter 31 : loss : 0.6068 = 0.1463 + 0.4562 + 0.0043, time: 32.354142]
2023-06-07 10:05:43.434: epoch 31:	0.03078968  	0.08565982  	0.07076333  
2023-06-07 10:05:43.434: Find a better model.
2023-06-07 10:06:15.734: [iter 32 : loss : 0.5982 = 0.1383 + 0.4554 + 0.0044, time: 32.287840]
2023-06-07 10:06:16.221: epoch 32:	0.03076747  	0.08542813  	0.07081151  
2023-06-07 10:06:48.373: [iter 33 : loss : 0.5903 = 0.1310 + 0.4547 + 0.0046, time: 32.138576]
2023-06-07 10:06:48.953: epoch 33:	0.03079708  	0.08541343  	0.07096730  
2023-06-07 10:07:20.689: [iter 34 : loss : 0.5844 = 0.1256 + 0.4540 + 0.0048, time: 31.731255]
2023-06-07 10:07:21.264: epoch 34:	0.03085631  	0.08589689  	0.07107193  
2023-06-07 10:07:21.264: Find a better model.
2023-06-07 10:07:53.025: [iter 35 : loss : 0.5781 = 0.1197 + 0.4534 + 0.0050, time: 31.756862]
2023-06-07 10:07:53.563: epoch 35:	0.03093034  	0.08560580  	0.07111253  
2023-06-07 10:08:24.879: [iter 36 : loss : 0.5725 = 0.1146 + 0.4527 + 0.0052, time: 31.311950]
2023-06-07 10:08:25.392: epoch 36:	0.03104138  	0.08593231  	0.07136682  
2023-06-07 10:08:25.392: Find a better model.
2023-06-07 10:08:56.883: [iter 37 : loss : 0.5674 = 0.1100 + 0.4521 + 0.0053, time: 31.485048]
2023-06-07 10:08:57.388: epoch 37:	0.03115984  	0.08646521  	0.07159043  
2023-06-07 10:08:57.388: Find a better model.
2023-06-07 10:09:28.667: [iter 38 : loss : 0.5627 = 0.1056 + 0.4516 + 0.0055, time: 31.275111]
2023-06-07 10:09:29.177: epoch 38:	0.03113023  	0.08630082  	0.07158408  
2023-06-07 10:10:01.924: [iter 39 : loss : 0.5594 = 0.1027 + 0.4510 + 0.0057, time: 32.739577]
2023-06-07 10:10:02.355: epoch 39:	0.03108581  	0.08622786  	0.07171870  
2023-06-07 10:10:35.451: [iter 40 : loss : 0.5545 = 0.0982 + 0.4505 + 0.0058, time: 33.088336]
2023-06-07 10:10:35.970: epoch 40:	0.03105620  	0.08591608  	0.07170087  
2023-06-07 10:11:09.060: [iter 41 : loss : 0.5512 = 0.0952 + 0.4500 + 0.0060, time: 33.084603]
2023-06-07 10:11:09.565: epoch 41:	0.03118945  	0.08616360  	0.07191969  
2023-06-07 10:11:43.249: [iter 42 : loss : 0.5474 = 0.0917 + 0.4496 + 0.0061, time: 33.678701]
2023-06-07 10:11:43.918: epoch 42:	0.03112283  	0.08601788  	0.07182035  
2023-06-07 10:12:16.297: [iter 43 : loss : 0.5448 = 0.0894 + 0.4492 + 0.0063, time: 32.374360]
2023-06-07 10:12:16.798: epoch 43:	0.03112283  	0.08586625  	0.07186092  
2023-06-07 10:12:49.151: [iter 44 : loss : 0.5412 = 0.0860 + 0.4487 + 0.0064, time: 32.347294]
2023-06-07 10:12:49.627: epoch 44:	0.03125608  	0.08636039  	0.07203985  
2023-06-07 10:13:21.559: [iter 45 : loss : 0.5390 = 0.0841 + 0.4483 + 0.0066, time: 31.926506]
2023-06-07 10:13:22.070: epoch 45:	0.03120426  	0.08617566  	0.07187789  
2023-06-07 10:13:53.416: [iter 46 : loss : 0.5363 = 0.0816 + 0.4480 + 0.0067, time: 31.332815]
2023-06-07 10:13:53.942: epoch 46:	0.03118945  	0.08566272  	0.07184885  
2023-06-07 10:14:25.646: [iter 47 : loss : 0.5341 = 0.0797 + 0.4476 + 0.0068, time: 31.696549]
2023-06-07 10:14:26.169: epoch 47:	0.03127828  	0.08574612  	0.07198765  
2023-06-07 10:14:59.004: [iter 48 : loss : 0.5313 = 0.0771 + 0.4473 + 0.0070, time: 32.831287]
2023-06-07 10:14:59.328: epoch 48:	0.03131529  	0.08571925  	0.07192500  
2023-06-07 10:15:33.394: [iter 49 : loss : 0.5289 = 0.0749 + 0.4469 + 0.0071, time: 34.057913]
2023-06-07 10:15:33.915: epoch 49:	0.03127828  	0.08572890  	0.07196946  
2023-06-07 10:16:07.873: [iter 50 : loss : 0.5270 = 0.0732 + 0.4466 + 0.0072, time: 33.953147]
2023-06-07 10:16:08.374: epoch 50:	0.03134491  	0.08575669  	0.07191716  
2023-06-07 10:16:42.674: [iter 51 : loss : 0.5250 = 0.0713 + 0.4463 + 0.0073, time: 34.293286]
2023-06-07 10:16:43.189: epoch 51:	0.03136712  	0.08573683  	0.07183962  
2023-06-07 10:17:17.256: [iter 52 : loss : 0.5231 = 0.0696 + 0.4460 + 0.0075, time: 34.062063]
2023-06-07 10:17:17.730: epoch 52:	0.03123386  	0.08599043  	0.07174397  
2023-06-07 10:17:51.974: [iter 53 : loss : 0.5212 = 0.0678 + 0.4458 + 0.0076, time: 34.238293]
2023-06-07 10:17:52.455: epoch 53:	0.03127087  	0.08549181  	0.07167546  
2023-06-07 10:18:26.382: [iter 54 : loss : 0.5197 = 0.0665 + 0.4455 + 0.0077, time: 33.919795]
2023-06-07 10:18:26.887: epoch 54:	0.03132270  	0.08573058  	0.07173155  
2023-06-07 10:18:58.205: [iter 55 : loss : 0.5185 = 0.0654 + 0.4453 + 0.0078, time: 31.307643]
2023-06-07 10:18:58.668: epoch 55:	0.03115242  	0.08532359  	0.07156369  
2023-06-07 10:19:33.237: [iter 56 : loss : 0.5169 = 0.0639 + 0.4451 + 0.0079, time: 34.557560]
2023-06-07 10:19:33.735: epoch 56:	0.03106358  	0.08493255  	0.07134520  
2023-06-07 10:19:59.385: [iter 57 : loss : 0.5154 = 0.0625 + 0.4448 + 0.0080, time: 25.645914]
2023-06-07 10:19:59.664: epoch 57:	0.03112281  	0.08454232  	0.07113151  
2023-06-07 10:20:25.307: [iter 58 : loss : 0.5140 = 0.0612 + 0.4446 + 0.0082, time: 25.640328]
2023-06-07 10:20:25.604: epoch 58:	0.03103396  	0.08432159  	0.07107365  
2023-06-07 10:20:50.584: [iter 59 : loss : 0.5132 = 0.0605 + 0.4445 + 0.0083, time: 24.975158]
2023-06-07 10:20:50.907: epoch 59:	0.03106358  	0.08424262  	0.07115501  
2023-06-07 10:21:17.682: [iter 60 : loss : 0.5110 = 0.0583 + 0.4443 + 0.0084, time: 26.771080]
2023-06-07 10:21:17.971: epoch 60:	0.03098215  	0.08406043  	0.07103775  
2023-06-07 10:21:43.665: [iter 61 : loss : 0.5101 = 0.0576 + 0.4441 + 0.0085, time: 25.689981]
2023-06-07 10:21:43.981: epoch 61:	0.03082668  	0.08353548  	0.07081688  
2023-06-07 10:22:12.262: [iter 62 : loss : 0.5087 = 0.0562 + 0.4439 + 0.0086, time: 28.276401]
2023-06-07 10:22:12.572: epoch 62:	0.03084148  	0.08324074  	0.07065926  
2023-06-07 10:22:12.572: Early stopping is trigger at epoch: 62
2023-06-07 10:22:12.572: best_result@epoch 37:

2023-06-07 10:22:12.572: 		0.0312      	0.0865      	0.0716      
2023-06-07 10:23:17.356: my pid: 472
2023-06-07 10:23:17.357: model: model.general_recommender.SGL
2023-06-07 10:23:17.357: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 10:23:17.357: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 10:23:21.860: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 10:23:47.147: [iter 1 : loss : 1.1320 = 0.6931 + 0.4390 + 0.0000, time: 25.285985]
2023-06-07 10:23:47.406: epoch 1:	0.00244300  	0.00498475  	0.00386034  
2023-06-07 10:23:47.407: Find a better model.
2023-06-07 10:24:12.694: [iter 2 : loss : 1.1306 = 0.6930 + 0.4376 + 0.0000, time: 25.284048]
2023-06-07 10:24:13.054: epoch 2:	0.00301303  	0.00694166  	0.00516603  
2023-06-07 10:24:13.054: Find a better model.
2023-06-07 10:24:43.838: [iter 3 : loss : 1.1304 = 0.6929 + 0.4374 + 0.0000, time: 30.781019]
2023-06-07 10:24:44.236: epoch 3:	0.00382736  	0.00821996  	0.00673627  
2023-06-07 10:24:44.236: Find a better model.
2023-06-07 10:25:13.025: [iter 4 : loss : 1.1302 = 0.6928 + 0.4374 + 0.0000, time: 28.782449]
2023-06-07 10:25:13.489: epoch 4:	0.00456766  	0.01097819  	0.00849841  
2023-06-07 10:25:13.489: Find a better model.
2023-06-07 10:25:42.551: [iter 5 : loss : 1.1301 = 0.6927 + 0.4374 + 0.0000, time: 29.056684]
2023-06-07 10:25:43.089: epoch 5:	0.00555965  	0.01389612  	0.01124416  
2023-06-07 10:25:43.089: Find a better model.
2023-06-07 10:26:11.529: [iter 6 : loss : 1.1299 = 0.6925 + 0.4374 + 0.0000, time: 28.434826]
2023-06-07 10:26:11.982: epoch 6:	0.00630735  	0.01635650  	0.01269865  
2023-06-07 10:26:11.982: Find a better model.
2023-06-07 10:26:41.294: [iter 7 : loss : 1.1297 = 0.6922 + 0.4375 + 0.0000, time: 29.306704]
2023-06-07 10:26:41.749: epoch 7:	0.00691439  	0.01761876  	0.01392479  
2023-06-07 10:26:41.749: Find a better model.
2023-06-07 10:27:10.226: [iter 8 : loss : 1.1291 = 0.6916 + 0.4375 + 0.0000, time: 28.471655]
2023-06-07 10:27:10.726: epoch 8:	0.00665529  	0.01689851  	0.01334719  
2023-06-07 10:27:39.451: [iter 9 : loss : 1.1280 = 0.6904 + 0.4377 + 0.0000, time: 28.720219]
2023-06-07 10:27:40.000: epoch 9:	0.00715869  	0.01816529  	0.01515975  
2023-06-07 10:27:40.000: Find a better model.
2023-06-07 10:28:08.457: [iter 10 : loss : 1.1269 = 0.6890 + 0.4378 + 0.0000, time: 28.453497]
2023-06-07 10:28:08.937: epoch 10:	0.00860966  	0.02317016  	0.01945005  
2023-06-07 10:28:08.937: Find a better model.
2023-06-07 10:28:37.375: [iter 11 : loss : 1.1253 = 0.6872 + 0.4380 + 0.0000, time: 28.429283]
2023-06-07 10:28:37.868: epoch 11:	0.01043082  	0.02846835  	0.02368970  
2023-06-07 10:28:37.868: Find a better model.
2023-06-07 10:29:07.607: [iter 12 : loss : 1.1220 = 0.6837 + 0.4383 + 0.0000, time: 29.733763]
2023-06-07 10:29:08.243: epoch 12:	0.01238523  	0.03510473  	0.02932018  
2023-06-07 10:29:08.243: Find a better model.
2023-06-07 10:29:37.191: [iter 13 : loss : 1.1155 = 0.6766 + 0.4388 + 0.0001, time: 28.943797]
2023-06-07 10:29:37.491: epoch 13:	0.01509479  	0.04292152  	0.03606012  
2023-06-07 10:29:37.492: Find a better model.
2023-06-07 10:30:05.816: [iter 14 : loss : 1.1027 = 0.6629 + 0.4396 + 0.0001, time: 28.319059]
2023-06-07 10:30:06.367: epoch 14:	0.01827076  	0.05232107  	0.04405745  
2023-06-07 10:30:06.367: Find a better model.
2023-06-07 10:30:34.537: [iter 15 : loss : 1.0799 = 0.6390 + 0.4407 + 0.0002, time: 28.163481]
2023-06-07 10:30:35.036: epoch 15:	0.02154301  	0.06104634  	0.05243157  
2023-06-07 10:30:35.036: Find a better model.
2023-06-07 10:31:03.888: [iter 16 : loss : 1.0440 = 0.6011 + 0.4425 + 0.0004, time: 28.847571]
2023-06-07 10:31:04.339: epoch 16:	0.02464500  	0.06843165  	0.05896013  
2023-06-07 10:31:04.339: Find a better model.
2023-06-07 10:31:32.567: [iter 17 : loss : 0.9961 = 0.5502 + 0.4453 + 0.0006, time: 28.223560]
2023-06-07 10:31:33.063: epoch 17:	0.02712507  	0.07494370  	0.06416884  
2023-06-07 10:31:33.063: Find a better model.
2023-06-07 10:32:01.928: [iter 18 : loss : 0.9408 = 0.4911 + 0.4489 + 0.0008, time: 28.859407]
2023-06-07 10:32:02.392: epoch 18:	0.02865013  	0.07857870  	0.06658312  
2023-06-07 10:32:02.392: Find a better model.
2023-06-07 10:32:30.786: [iter 19 : loss : 0.8852 = 0.4312 + 0.4528 + 0.0012, time: 28.389638]
2023-06-07 10:32:31.252: epoch 19:	0.02930902  	0.08137145  	0.06792647  
2023-06-07 10:32:31.252: Find a better model.
2023-06-07 10:32:55.335: [iter 20 : loss : 0.8343 = 0.3764 + 0.4564 + 0.0015, time: 24.078943]
2023-06-07 10:32:55.617: epoch 20:	0.02966441  	0.08249212  	0.06829768  
2023-06-07 10:32:55.618: Find a better model.
2023-06-07 10:33:19.292: [iter 21 : loss : 0.7908 = 0.3300 + 0.4590 + 0.0018, time: 23.670815]
2023-06-07 10:33:19.587: epoch 21:	0.03005678  	0.08299169  	0.06869921  
2023-06-07 10:33:19.587: Find a better model.
2023-06-07 10:33:44.124: [iter 22 : loss : 0.7552 = 0.2922 + 0.4609 + 0.0021, time: 24.532393]
2023-06-07 10:33:44.470: epoch 22:	0.03018263  	0.08326720  	0.06895111  
2023-06-07 10:33:44.470: Find a better model.
2023-06-07 10:34:09.136: [iter 23 : loss : 0.7249 = 0.2609 + 0.4617 + 0.0024, time: 24.660406]
2023-06-07 10:34:09.406: epoch 23:	0.03060461  	0.08468921  	0.06973187  
2023-06-07 10:34:09.406: Find a better model.
2023-06-07 10:34:34.155: [iter 24 : loss : 0.6996 = 0.2352 + 0.4618 + 0.0027, time: 24.745531]
2023-06-07 10:34:34.452: epoch 24:	0.03076750  	0.08533724  	0.06998871  
2023-06-07 10:34:34.452: Find a better model.
2023-06-07 10:34:59.565: [iter 25 : loss : 0.6798 = 0.2154 + 0.4616 + 0.0029, time: 25.108539]
2023-06-07 10:34:59.883: epoch 25:	0.03090815  	0.08578113  	0.07015730  
2023-06-07 10:34:59.883: Find a better model.
2023-06-07 10:35:23.582: [iter 26 : loss : 0.6616 = 0.1974 + 0.4611 + 0.0032, time: 23.695062]
2023-06-07 10:35:23.907: epoch 26:	0.03101920  	0.08645543  	0.07061096  
2023-06-07 10:35:23.907: Find a better model.
2023-06-07 10:35:47.870: [iter 27 : loss : 0.6471 = 0.1833 + 0.4604 + 0.0034, time: 23.957764]
2023-06-07 10:35:48.243: epoch 27:	0.03122649  	0.08697426  	0.07078173  
2023-06-07 10:35:48.243: Find a better model.
2023-06-07 10:36:13.251: [iter 28 : loss : 0.6341 = 0.1708 + 0.4597 + 0.0036, time: 25.004019]
2023-06-07 10:36:13.520: epoch 28:	0.03130053  	0.08742709  	0.07111029  
2023-06-07 10:36:13.520: Find a better model.
2023-06-07 10:36:38.735: [iter 29 : loss : 0.6233 = 0.1605 + 0.4590 + 0.0039, time: 25.211298]
2023-06-07 10:36:39.039: epoch 29:	0.03135233  	0.08776703  	0.07131807  
2023-06-07 10:36:39.039: Find a better model.
2023-06-07 10:37:03.056: [iter 30 : loss : 0.6130 = 0.1507 + 0.4582 + 0.0041, time: 24.011309]
2023-06-07 10:37:03.350: epoch 30:	0.03138936  	0.08811844  	0.07140355  
2023-06-07 10:37:03.350: Find a better model.
2023-06-07 10:37:28.443: [iter 31 : loss : 0.6047 = 0.1429 + 0.4575 + 0.0043, time: 25.089175]
2023-06-07 10:37:28.707: epoch 31:	0.03151521  	0.08864387  	0.07173911  
2023-06-07 10:37:28.707: Find a better model.
2023-06-07 10:37:53.510: [iter 32 : loss : 0.5965 = 0.1353 + 0.4567 + 0.0045, time: 24.797735]
2023-06-07 10:37:53.786: epoch 32:	0.03145599  	0.08835460  	0.07161212  
2023-06-07 10:38:18.254: [iter 33 : loss : 0.5891 = 0.1285 + 0.4559 + 0.0047, time: 24.464028]
2023-06-07 10:38:18.607: epoch 33:	0.03150779  	0.08861466  	0.07176901  
2023-06-07 10:38:43.220: [iter 34 : loss : 0.5829 = 0.1228 + 0.4552 + 0.0048, time: 24.610450]
2023-06-07 10:38:43.584: epoch 34:	0.03162624  	0.08854216  	0.07193702  
2023-06-07 10:39:09.002: [iter 35 : loss : 0.5769 = 0.1172 + 0.4547 + 0.0050, time: 25.413738]
2023-06-07 10:39:09.295: epoch 35:	0.03159662  	0.08877872  	0.07213996  
2023-06-07 10:39:09.296: Find a better model.
2023-06-07 10:39:34.128: [iter 36 : loss : 0.5716 = 0.1124 + 0.4540 + 0.0052, time: 24.829190]
2023-06-07 10:39:34.398: epoch 36:	0.03159660  	0.08832393  	0.07215465  
2023-06-07 10:39:59.877: [iter 37 : loss : 0.5665 = 0.1078 + 0.4533 + 0.0054, time: 25.474944]
2023-06-07 10:40:00.261: epoch 37:	0.03147817  	0.08803059  	0.07212466  
2023-06-07 10:40:26.043: [iter 38 : loss : 0.5621 = 0.1038 + 0.4528 + 0.0055, time: 25.777909]
2023-06-07 10:40:26.320: epoch 38:	0.03155960  	0.08776697  	0.07227375  
2023-06-07 10:40:51.140: [iter 39 : loss : 0.5589 = 0.1010 + 0.4522 + 0.0057, time: 24.814305]
2023-06-07 10:40:51.458: epoch 39:	0.03156701  	0.08766245  	0.07233366  
2023-06-07 10:41:17.496: [iter 40 : loss : 0.5541 = 0.0966 + 0.4517 + 0.0058, time: 26.034131]
2023-06-07 10:41:17.768: epoch 40:	0.03160401  	0.08783285  	0.07234190  
2023-06-07 10:41:42.817: [iter 41 : loss : 0.5506 = 0.0934 + 0.4512 + 0.0060, time: 25.045573]
2023-06-07 10:41:43.107: epoch 41:	0.03174467  	0.08826459  	0.07244807  
2023-06-07 10:42:09.344: [iter 42 : loss : 0.5470 = 0.0900 + 0.4508 + 0.0061, time: 26.232042]
2023-06-07 10:42:09.632: epoch 42:	0.03167063  	0.08792742  	0.07234514  
2023-06-07 10:42:35.101: [iter 43 : loss : 0.5445 = 0.0878 + 0.4504 + 0.0063, time: 25.465820]
2023-06-07 10:42:35.384: epoch 43:	0.03158919  	0.08760264  	0.07203845  
2023-06-07 10:43:02.004: [iter 44 : loss : 0.5411 = 0.0847 + 0.4500 + 0.0064, time: 26.615665]
2023-06-07 10:43:02.280: epoch 44:	0.03155217  	0.08775507  	0.07207725  
2023-06-07 10:43:27.707: [iter 45 : loss : 0.5386 = 0.0824 + 0.4496 + 0.0066, time: 25.423980]
2023-06-07 10:43:28.045: epoch 45:	0.03148555  	0.08757442  	0.07193167  
2023-06-07 10:43:54.178: [iter 46 : loss : 0.5361 = 0.0801 + 0.4493 + 0.0067, time: 26.128586]
2023-06-07 10:43:54.438: epoch 46:	0.03158180  	0.08779167  	0.07220192  
2023-06-07 10:44:18.534: [iter 47 : loss : 0.5338 = 0.0781 + 0.4489 + 0.0068, time: 24.092431]
2023-06-07 10:44:18.794: epoch 47:	0.03161883  	0.08778752  	0.07214749  
2023-06-07 10:44:43.140: [iter 48 : loss : 0.5314 = 0.0759 + 0.4486 + 0.0070, time: 24.342141]
2023-06-07 10:44:43.397: epoch 48:	0.03157440  	0.08756076  	0.07210227  
2023-06-07 10:45:07.747: [iter 49 : loss : 0.5286 = 0.0733 + 0.4482 + 0.0071, time: 24.345810]
2023-06-07 10:45:08.013: epoch 49:	0.03152258  	0.08742721  	0.07209759  
2023-06-07 10:45:32.310: [iter 50 : loss : 0.5270 = 0.0718 + 0.4479 + 0.0072, time: 24.292908]
2023-06-07 10:45:32.571: epoch 50:	0.03146335  	0.08740536  	0.07194477  
2023-06-07 10:45:57.127: [iter 51 : loss : 0.5253 = 0.0703 + 0.4476 + 0.0074, time: 24.553009]
2023-06-07 10:45:57.387: epoch 51:	0.03142632  	0.08716366  	0.07177600  
2023-06-07 10:46:21.838: [iter 52 : loss : 0.5231 = 0.0683 + 0.4473 + 0.0075, time: 24.447549]
2023-06-07 10:46:22.106: epoch 52:	0.03138191  	0.08651991  	0.07146693  
2023-06-07 10:46:47.743: [iter 53 : loss : 0.5214 = 0.0667 + 0.4471 + 0.0076, time: 25.632273]
2023-06-07 10:46:48.052: epoch 53:	0.03141152  	0.08667562  	0.07156795  
2023-06-07 10:47:02.461: my pid: 8764
2023-06-07 10:47:02.461: model: model.general_recommender.SGL
2023-06-07 10:47:02.461: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 10:47:02.461: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 10:47:06.590: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 10:47:28.985: [iter 1 : loss : 1.1327 = 0.6931 + 0.4396 + 0.0000, time: 22.395512]
2023-06-07 10:47:29.281: epoch 1:	0.00238377  	0.00422574  	0.00345626  
2023-06-07 10:47:29.281: Find a better model.
2023-06-07 10:47:51.502: [iter 2 : loss : 1.1314 = 0.6930 + 0.4384 + 0.0000, time: 22.215935]
2023-06-07 10:47:51.794: epoch 2:	0.00289458  	0.00533446  	0.00441650  
2023-06-07 10:47:51.795: Find a better model.
2023-06-07 10:48:13.237: [iter 3 : loss : 1.1313 = 0.6929 + 0.4383 + 0.0000, time: 21.439204]
2023-06-07 10:48:13.562: epoch 3:	0.00353864  	0.00774300  	0.00622886  
2023-06-07 10:48:13.562: Find a better model.
2023-06-07 10:48:36.120: [iter 4 : loss : 1.1311 = 0.6928 + 0.4383 + 0.0000, time: 22.554905]
2023-06-07 10:48:36.462: epoch 4:	0.00447882  	0.01107299  	0.00884502  
2023-06-07 10:48:36.462: Find a better model.
2023-06-07 10:48:58.752: [iter 5 : loss : 1.1311 = 0.6926 + 0.4384 + 0.0000, time: 22.286125]
2023-06-07 10:48:59.096: epoch 5:	0.00510067  	0.01356418  	0.01097396  
2023-06-07 10:48:59.096: Find a better model.
2023-06-07 10:49:21.333: [iter 6 : loss : 1.1309 = 0.6924 + 0.4385 + 0.0000, time: 22.234668]
2023-06-07 10:49:21.655: epoch 6:	0.00542640  	0.01453861  	0.01178682  
2023-06-07 10:49:21.655: Find a better model.
2023-06-07 10:49:46.381: [iter 7 : loss : 1.1307 = 0.6919 + 0.4387 + 0.0000, time: 24.722360]
2023-06-07 10:49:46.675: epoch 7:	0.00598902  	0.01584233  	0.01267924  
2023-06-07 10:49:46.675: Find a better model.
2023-06-07 10:50:14.294: [iter 8 : loss : 1.1298 = 0.6909 + 0.4389 + 0.0000, time: 27.615595]
2023-06-07 10:50:14.775: epoch 8:	0.00632956  	0.01581967  	0.01354411  
2023-06-07 10:50:42.527: [iter 9 : loss : 1.1290 = 0.6899 + 0.4391 + 0.0000, time: 27.743578]
2023-06-07 10:50:43.046: epoch 9:	0.00734376  	0.01832172  	0.01579158  
2023-06-07 10:50:43.046: Find a better model.
2023-06-07 10:51:10.560: [iter 10 : loss : 1.1280 = 0.6887 + 0.4393 + 0.0000, time: 27.508693]
2023-06-07 10:51:11.075: epoch 10:	0.00889838  	0.02273848  	0.01959274  
2023-06-07 10:51:11.075: Find a better model.
2023-06-07 10:51:39.675: [iter 11 : loss : 1.1265 = 0.6868 + 0.4397 + 0.0000, time: 28.596696]
2023-06-07 10:51:40.165: epoch 11:	0.01125257  	0.02828586  	0.02489962  
2023-06-07 10:51:40.165: Find a better model.
2023-06-07 10:52:07.887: [iter 12 : loss : 1.1233 = 0.6831 + 0.4401 + 0.0000, time: 27.718130]
2023-06-07 10:52:08.380: epoch 12:	0.01387327  	0.03641987  	0.03223229  
2023-06-07 10:52:08.381: Find a better model.
2023-06-07 10:52:36.129: [iter 13 : loss : 1.1167 = 0.6759 + 0.4407 + 0.0001, time: 27.738060]
2023-06-07 10:52:36.591: epoch 13:	0.01700482  	0.04660305  	0.04003732  
2023-06-07 10:52:36.591: Find a better model.
2023-06-07 10:53:03.998: [iter 14 : loss : 1.1038 = 0.6619 + 0.4417 + 0.0001, time: 27.402021]
2023-06-07 10:53:04.461: epoch 14:	0.02055096  	0.05711233  	0.04866281  
2023-06-07 10:53:04.462: Find a better model.
2023-06-07 10:53:32.409: [iter 15 : loss : 1.0807 = 0.6374 + 0.4430 + 0.0002, time: 27.942564]
2023-06-07 10:53:32.799: epoch 15:	0.02392685  	0.06603213  	0.05666982  
2023-06-07 10:53:32.799: Find a better model.
2023-06-07 10:54:00.152: [iter 16 : loss : 1.0441 = 0.5985 + 0.4452 + 0.0004, time: 27.349162]
2023-06-07 10:54:00.598: epoch 16:	0.02659202  	0.07401229  	0.06264948  
2023-06-07 10:54:00.598: Find a better model.
2023-06-07 10:54:28.775: [iter 17 : loss : 0.9951 = 0.5462 + 0.4483 + 0.0006, time: 28.172440]
2023-06-07 10:54:29.277: epoch 17:	0.02833181  	0.07829496  	0.06589629  
2023-06-07 10:54:29.277: Find a better model.
2023-06-07 10:54:57.984: [iter 18 : loss : 0.9388 = 0.4857 + 0.4522 + 0.0009, time: 28.701372]
2023-06-07 10:54:58.362: epoch 18:	0.02968658  	0.08185696  	0.06802766  
2023-06-07 10:54:58.362: Find a better model.
2023-06-07 10:55:25.590: [iter 19 : loss : 0.8827 = 0.4253 + 0.4563 + 0.0012, time: 27.221817]
2023-06-07 10:55:26.090: epoch 19:	0.03016780  	0.08352444  	0.06883024  
2023-06-07 10:55:26.090: Find a better model.
2023-06-07 10:55:53.533: [iter 20 : loss : 0.8316 = 0.3703 + 0.4598 + 0.0015, time: 27.435612]
2023-06-07 10:55:54.022: epoch 20:	0.03046390  	0.08428650  	0.06920867  
2023-06-07 10:55:54.023: Find a better model.
2023-06-07 10:56:22.089: [iter 21 : loss : 0.7886 = 0.3246 + 0.4622 + 0.0018, time: 28.060349]
2023-06-07 10:56:22.561: epoch 21:	0.03076006  	0.08551463  	0.06967582  
2023-06-07 10:56:22.561: Find a better model.
2023-06-07 10:56:51.530: [iter 22 : loss : 0.7536 = 0.2876 + 0.4639 + 0.0021, time: 28.964367]
2023-06-07 10:56:52.004: epoch 22:	0.03090812  	0.08602104  	0.07012438  
2023-06-07 10:56:52.004: Find a better model.
2023-06-07 10:57:20.669: [iter 23 : loss : 0.7236 = 0.2567 + 0.4645 + 0.0024, time: 28.654982]
2023-06-07 10:57:21.136: epoch 23:	0.03101176  	0.08701333  	0.07054655  
2023-06-07 10:57:21.136: Find a better model.
2023-06-07 10:57:49.750: [iter 24 : loss : 0.6987 = 0.2316 + 0.4644 + 0.0027, time: 28.606253]
2023-06-07 10:57:50.208: epoch 24:	0.03105617  	0.08729640  	0.07066971  
2023-06-07 10:57:50.208: Find a better model.
2023-06-07 10:58:17.202: [iter 25 : loss : 0.6791 = 0.2121 + 0.4640 + 0.0029, time: 26.990832]
2023-06-07 10:58:17.502: epoch 25:	0.03106358  	0.08749038  	0.07098164  
2023-06-07 10:58:17.502: Find a better model.
2023-06-07 10:58:40.404: [iter 26 : loss : 0.6613 = 0.1947 + 0.4634 + 0.0032, time: 22.899231]
2023-06-07 10:58:40.682: epoch 26:	0.03111540  	0.08748172  	0.07101878  
2023-06-07 10:59:04.227: [iter 27 : loss : 0.6469 = 0.1809 + 0.4626 + 0.0034, time: 23.541514]
2023-06-07 10:59:04.604: epoch 27:	0.03128567  	0.08780123  	0.07146333  
2023-06-07 10:59:04.605: Find a better model.
2023-06-07 10:59:28.665: [iter 28 : loss : 0.6343 = 0.1688 + 0.4619 + 0.0037, time: 24.056006]
2023-06-07 10:59:29.068: epoch 28:	0.03145595  	0.08833914  	0.07177293  
2023-06-07 10:59:29.068: Find a better model.
2023-06-07 10:59:52.776: [iter 29 : loss : 0.6233 = 0.1583 + 0.4611 + 0.0039, time: 23.705579]
2023-06-07 10:59:53.080: epoch 29:	0.03140413  	0.08842072  	0.07207898  
2023-06-07 10:59:53.080: Find a better model.
2023-06-07 11:00:17.132: [iter 30 : loss : 0.6133 = 0.1489 + 0.4603 + 0.0041, time: 24.048409]
2023-06-07 11:00:17.473: epoch 30:	0.03161141  	0.08934590  	0.07268383  
2023-06-07 11:00:17.473: Find a better model.
2023-06-07 11:00:41.586: [iter 31 : loss : 0.6052 = 0.1413 + 0.4596 + 0.0043, time: 24.109117]
2023-06-07 11:00:41.908: epoch 31:	0.03157439  	0.08936664  	0.07282232  
2023-06-07 11:00:41.908: Find a better model.
2023-06-07 11:01:05.776: [iter 32 : loss : 0.5968 = 0.1337 + 0.4586 + 0.0045, time: 23.862687]
2023-06-07 11:01:06.111: epoch 32:	0.03178168  	0.09011368  	0.07306179  
2023-06-07 11:01:06.111: Find a better model.
2023-06-07 11:01:29.390: [iter 33 : loss : 0.5895 = 0.1269 + 0.4579 + 0.0047, time: 23.274314]
2023-06-07 11:01:29.688: epoch 33:	0.03197419  	0.09077574  	0.07352414  
2023-06-07 11:01:29.688: Find a better model.
2023-06-07 11:01:53.524: [iter 34 : loss : 0.5835 = 0.1214 + 0.4572 + 0.0049, time: 23.832399]
2023-06-07 11:01:53.900: epoch 34:	0.03188534  	0.09070677  	0.07379156  
2023-06-07 11:02:18.392: [iter 35 : loss : 0.5776 = 0.1159 + 0.4566 + 0.0050, time: 24.487695]
2023-06-07 11:02:18.657: epoch 35:	0.03201860  	0.09062143  	0.07392189  
2023-06-07 11:02:43.161: [iter 36 : loss : 0.5721 = 0.1110 + 0.4559 + 0.0052, time: 24.501116]
2023-06-07 11:02:43.456: epoch 36:	0.03201119  	0.09085327  	0.07401671  
2023-06-07 11:02:43.456: Find a better model.
2023-06-07 11:03:07.950: [iter 37 : loss : 0.5672 = 0.1066 + 0.4553 + 0.0054, time: 24.490375]
2023-06-07 11:03:08.222: epoch 37:	0.03203341  	0.09111046  	0.07406695  
2023-06-07 11:03:08.222: Find a better model.
2023-06-07 11:03:31.710: [iter 38 : loss : 0.5626 = 0.1023 + 0.4548 + 0.0055, time: 23.482160]
2023-06-07 11:03:32.006: epoch 38:	0.03201119  	0.09100742  	0.07410856  
2023-06-07 11:03:56.798: [iter 39 : loss : 0.5596 = 0.0998 + 0.4542 + 0.0057, time: 24.787901]
2023-06-07 11:03:57.101: epoch 39:	0.03201860  	0.09070647  	0.07393228  
2023-06-07 11:04:22.097: [iter 40 : loss : 0.5548 = 0.0953 + 0.4536 + 0.0058, time: 24.992018]
2023-06-07 11:04:22.374: epoch 40:	0.03202600  	0.09092825  	0.07401630  
2023-06-07 11:04:46.282: [iter 41 : loss : 0.5515 = 0.0924 + 0.4532 + 0.0060, time: 23.904050]
2023-06-07 11:04:46.637: epoch 41:	0.03199639  	0.09077069  	0.07402496  
2023-06-07 11:05:10.669: [iter 42 : loss : 0.5481 = 0.0892 + 0.4528 + 0.0061, time: 24.028887]
2023-06-07 11:05:11.048: epoch 42:	0.03200378  	0.09046184  	0.07400157  
2023-06-07 11:05:36.037: [iter 43 : loss : 0.5456 = 0.0870 + 0.4524 + 0.0063, time: 24.985004]
2023-06-07 11:05:36.416: epoch 43:	0.03187052  	0.09006467  	0.07396948  
2023-06-07 11:06:01.502: [iter 44 : loss : 0.5422 = 0.0839 + 0.4519 + 0.0064, time: 25.081014]
2023-06-07 11:06:01.783: epoch 44:	0.03185571  	0.08983628  	0.07390531  
2023-06-07 11:06:25.816: [iter 45 : loss : 0.5397 = 0.0816 + 0.4515 + 0.0066, time: 24.029561]
2023-06-07 11:06:26.103: epoch 45:	0.03188531  	0.08989552  	0.07385673  
2023-06-07 11:06:51.608: [iter 46 : loss : 0.5371 = 0.0792 + 0.4512 + 0.0067, time: 25.500555]
2023-06-07 11:06:51.908: epoch 46:	0.03183350  	0.08943396  	0.07365026  
2023-06-07 11:07:17.250: [iter 47 : loss : 0.5349 = 0.0772 + 0.4509 + 0.0068, time: 25.337675]
2023-06-07 11:07:17.532: epoch 47:	0.03168543  	0.08951275  	0.07356122  
2023-06-07 11:07:42.243: [iter 48 : loss : 0.5324 = 0.0749 + 0.4505 + 0.0070, time: 24.706100]
2023-06-07 11:07:42.618: epoch 48:	0.03156698  	0.08914288  	0.07332097  
2023-06-07 11:08:08.342: [iter 49 : loss : 0.5299 = 0.0727 + 0.4501 + 0.0071, time: 25.719656]
2023-06-07 11:08:08.633: epoch 49:	0.03158919  	0.08901431  	0.07338948  
2023-06-07 11:08:33.293: [iter 50 : loss : 0.5286 = 0.0715 + 0.4499 + 0.0072, time: 24.656200]
2023-06-07 11:08:33.572: epoch 50:	0.03147814  	0.08873930  	0.07344277  
2023-06-07 11:08:59.574: [iter 51 : loss : 0.5265 = 0.0696 + 0.4495 + 0.0073, time: 25.997952]
2023-06-07 11:08:59.915: epoch 51:	0.03141890  	0.08814912  	0.07319467  
2023-06-07 11:09:24.749: [iter 52 : loss : 0.5247 = 0.0679 + 0.4493 + 0.0075, time: 24.829369]
2023-06-07 11:09:25.053: epoch 52:	0.03138189  	0.08770182  	0.07299466  
2023-06-07 11:09:50.620: [iter 53 : loss : 0.5229 = 0.0662 + 0.4491 + 0.0076, time: 25.563360]
2023-06-07 11:09:50.931: epoch 53:	0.03135968  	0.08746373  	0.07287797  
2023-06-07 11:10:15.560: [iter 54 : loss : 0.5213 = 0.0648 + 0.4488 + 0.0077, time: 24.625782]
2023-06-07 11:10:15.852: epoch 54:	0.03127084  	0.08726870  	0.07284985  
2023-06-07 11:10:39.295: [iter 55 : loss : 0.5204 = 0.0640 + 0.4486 + 0.0078, time: 23.437962]
2023-06-07 11:10:39.563: epoch 55:	0.03118941  	0.08656783  	0.07245327  
2023-06-07 11:11:03.194: [iter 56 : loss : 0.5188 = 0.0625 + 0.4484 + 0.0079, time: 23.627172]
2023-06-07 11:11:03.478: epoch 56:	0.03117461  	0.08649067  	0.07236309  
2023-06-07 11:11:27.352: [iter 57 : loss : 0.5174 = 0.0612 + 0.4482 + 0.0080, time: 23.869993]
2023-06-07 11:11:27.639: epoch 57:	0.03115980  	0.08629330  	0.07225483  
2023-06-07 11:11:51.748: [iter 58 : loss : 0.5160 = 0.0599 + 0.4479 + 0.0081, time: 24.105416]
2023-06-07 11:11:52.034: epoch 58:	0.03125603  	0.08627386  	0.07203203  
2023-06-07 11:12:15.962: [iter 59 : loss : 0.5151 = 0.0591 + 0.4478 + 0.0082, time: 23.922986]
2023-06-07 11:12:16.244: epoch 59:	0.03121902  	0.08617607  	0.07202213  
2023-06-07 11:12:40.339: [iter 60 : loss : 0.5130 = 0.0571 + 0.4476 + 0.0084, time: 24.090771]
2023-06-07 11:12:40.625: epoch 60:	0.03104135  	0.08563407  	0.07187036  
2023-06-07 11:13:04.694: [iter 61 : loss : 0.5123 = 0.0565 + 0.4474 + 0.0085, time: 24.064026]
2023-06-07 11:13:04.982: epoch 61:	0.03105616  	0.08536391  	0.07176302  
2023-06-07 11:13:28.932: [iter 62 : loss : 0.5109 = 0.0551 + 0.4473 + 0.0086, time: 23.944995]
2023-06-07 11:13:29.219: epoch 62:	0.03093030  	0.08495024  	0.07151910  
2023-06-07 11:13:29.219: Early stopping is trigger at epoch: 62
2023-06-07 11:13:29.219: best_result@epoch 37:

2023-06-07 11:13:29.219: 		0.0320      	0.0911      	0.0741      
2023-06-07 11:19:39.871: my pid: 8344
2023-06-07 11:19:39.871: model: model.general_recommender.SGL
2023-06-07 11:19:39.871: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 11:19:39.871: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 11:19:43.871: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 11:20:05.385: [iter 1 : loss : 1.1332 = 0.6931 + 0.4402 + 0.0000, time: 21.513652]
2023-06-07 11:20:05.684: epoch 1:	0.00251703  	0.00493327  	0.00391188  
2023-06-07 11:20:05.684: Find a better model.
2023-06-07 11:20:27.583: [iter 2 : loss : 1.1323 = 0.6930 + 0.4393 + 0.0000, time: 21.894823]
2023-06-07 11:20:28.003: epoch 2:	0.00303524  	0.00552657  	0.00456976  
2023-06-07 11:20:28.003: Find a better model.
2023-06-07 11:20:53.786: [iter 3 : loss : 1.1322 = 0.6929 + 0.4393 + 0.0000, time: 25.777720]
2023-06-07 11:20:54.251: epoch 3:	0.00355345  	0.00726750  	0.00654065  
2023-06-07 11:20:54.251: Find a better model.
2023-06-07 11:21:19.614: [iter 4 : loss : 1.1322 = 0.6927 + 0.4395 + 0.0000, time: 25.357833]
2023-06-07 11:21:20.102: epoch 4:	0.00400504  	0.00920907  	0.00778415  
2023-06-07 11:21:20.102: Find a better model.
2023-06-07 11:21:45.888: [iter 5 : loss : 1.1322 = 0.6926 + 0.4397 + 0.0000, time: 25.781064]
2023-06-07 11:21:46.363: epoch 5:	0.00481196  	0.01138086  	0.00955329  
2023-06-07 11:21:46.363: Find a better model.
2023-06-07 11:22:12.649: [iter 6 : loss : 1.1320 = 0.6922 + 0.4399 + 0.0000, time: 26.280863]
2023-06-07 11:22:13.136: epoch 6:	0.00475274  	0.01094209  	0.00901765  
2023-06-07 11:22:39.923: [iter 7 : loss : 1.1314 = 0.6913 + 0.4402 + 0.0000, time: 26.782543]
2023-06-07 11:22:40.366: epoch 7:	0.00527094  	0.01249322  	0.01032866  
2023-06-07 11:22:40.367: Find a better model.
2023-06-07 11:23:08.185: [iter 8 : loss : 1.1309 = 0.6904 + 0.4404 + 0.0000, time: 27.815230]
2023-06-07 11:23:08.497: epoch 8:	0.00630735  	0.01588371  	0.01307189  
2023-06-07 11:23:08.497: Find a better model.
2023-06-07 11:23:35.653: [iter 9 : loss : 1.1304 = 0.6896 + 0.4407 + 0.0000, time: 27.150566]
2023-06-07 11:23:36.161: epoch 9:	0.00775093  	0.01925957  	0.01581231  
2023-06-07 11:23:36.162: Find a better model.
2023-06-07 11:24:03.019: [iter 10 : loss : 1.1295 = 0.6883 + 0.4411 + 0.0000, time: 26.849576]
2023-06-07 11:24:03.596: epoch 10:	0.00945361  	0.02349334  	0.02045653  
2023-06-07 11:24:03.597: Find a better model.
2023-06-07 11:24:30.823: [iter 11 : loss : 1.1280 = 0.6863 + 0.4416 + 0.0000, time: 27.222420]
2023-06-07 11:24:31.314: epoch 11:	0.01207430  	0.03154745  	0.02701835  
2023-06-07 11:24:31.314: Find a better model.
2023-06-07 11:24:58.565: [iter 12 : loss : 1.1248 = 0.6825 + 0.4422 + 0.0001, time: 27.246088]
2023-06-07 11:24:59.040: epoch 12:	0.01505038  	0.03949299  	0.03448088  
2023-06-07 11:24:59.040: Find a better model.
2023-06-07 11:25:26.192: [iter 13 : loss : 1.1183 = 0.6753 + 0.4430 + 0.0001, time: 27.148726]
2023-06-07 11:25:26.647: epoch 13:	0.01846323  	0.04927996  	0.04316678  
2023-06-07 11:25:26.647: Find a better model.
2023-06-07 11:25:53.849: [iter 14 : loss : 1.1057 = 0.6615 + 0.4441 + 0.0001, time: 27.199866]
2023-06-07 11:25:54.167: epoch 14:	0.02188353  	0.05927068  	0.05125272  
2023-06-07 11:25:54.168: Find a better model.
2023-06-07 11:26:23.249: [iter 15 : loss : 1.0834 = 0.6375 + 0.4456 + 0.0002, time: 29.075831]
2023-06-07 11:26:23.538: epoch 15:	0.02494108  	0.06791793  	0.05798265  
2023-06-07 11:26:23.538: Find a better model.
2023-06-07 11:26:50.318: [iter 16 : loss : 1.0477 = 0.5994 + 0.4479 + 0.0004, time: 26.775056]
2023-06-07 11:26:50.768: epoch 16:	0.02733977  	0.07537236  	0.06335776  
2023-06-07 11:26:50.768: Find a better model.
2023-06-07 11:27:17.491: [iter 17 : loss : 0.9995 = 0.5476 + 0.4513 + 0.0006, time: 26.716554]
2023-06-07 11:27:17.950: epoch 17:	0.02901289  	0.07915592  	0.06632696  
2023-06-07 11:27:17.950: Find a better model.
2023-06-07 11:27:39.938: [iter 18 : loss : 0.9435 = 0.4872 + 0.4554 + 0.0009, time: 21.983599]
2023-06-07 11:27:40.208: epoch 18:	0.02961995  	0.08089814  	0.06779032  
2023-06-07 11:27:40.208: Find a better model.
2023-06-07 11:28:02.147: [iter 19 : loss : 0.8871 = 0.4263 + 0.4596 + 0.0012, time: 21.935865]
2023-06-07 11:28:02.432: epoch 19:	0.02995310  	0.08195314  	0.06855302  
2023-06-07 11:28:02.432: Find a better model.
2023-06-07 11:28:24.399: [iter 20 : loss : 0.8357 = 0.3711 + 0.4631 + 0.0015, time: 21.961571]
2023-06-07 11:28:24.737: epoch 20:	0.03012338  	0.08254967  	0.06862659  
2023-06-07 11:28:24.737: Find a better model.
2023-06-07 11:28:49.096: [iter 21 : loss : 0.7923 = 0.3250 + 0.4656 + 0.0018, time: 24.354715]
2023-06-07 11:28:49.451: epoch 21:	0.03044171  	0.08363128  	0.06931452  
2023-06-07 11:28:49.451: Find a better model.
2023-06-07 11:29:14.194: [iter 22 : loss : 0.7569 = 0.2876 + 0.4672 + 0.0021, time: 24.738344]
2023-06-07 11:29:14.558: epoch 22:	0.03052315  	0.08413210  	0.06955820  
2023-06-07 11:29:14.558: Find a better model.
2023-06-07 11:29:38.774: [iter 23 : loss : 0.7268 = 0.2567 + 0.4677 + 0.0024, time: 24.211517]
2023-06-07 11:29:39.091: epoch 23:	0.03071564  	0.08462232  	0.06994078  
2023-06-07 11:29:39.091: Find a better model.
2023-06-07 11:30:02.752: [iter 24 : loss : 0.7019 = 0.2317 + 0.4676 + 0.0027, time: 23.656818]
2023-06-07 11:30:03.203: epoch 24:	0.03088591  	0.08482613  	0.07011552  
2023-06-07 11:30:03.203: Find a better model.
2023-06-07 11:30:29.577: [iter 25 : loss : 0.6820 = 0.2120 + 0.4671 + 0.0029, time: 26.367970]
2023-06-07 11:30:29.866: epoch 25:	0.03101918  	0.08569197  	0.07054386  
2023-06-07 11:30:29.866: Find a better model.
2023-06-07 11:30:55.730: [iter 26 : loss : 0.6640 = 0.1945 + 0.4664 + 0.0032, time: 25.858921]
2023-06-07 11:30:56.121: epoch 26:	0.03124867  	0.08663071  	0.07107852  
2023-06-07 11:30:56.121: Find a better model.
2023-06-07 11:31:22.221: [iter 27 : loss : 0.6495 = 0.1805 + 0.4655 + 0.0034, time: 26.093790]
2023-06-07 11:31:22.578: epoch 27:	0.03136713  	0.08687740  	0.07125942  
2023-06-07 11:31:22.578: Find a better model.
2023-06-07 11:31:48.791: [iter 28 : loss : 0.6370 = 0.1686 + 0.4647 + 0.0037, time: 26.210080]
2023-06-07 11:31:49.194: epoch 28:	0.03144855  	0.08746343  	0.07169124  
2023-06-07 11:31:49.195: Find a better model.
2023-06-07 11:32:15.385: [iter 29 : loss : 0.6260 = 0.1582 + 0.4639 + 0.0039, time: 26.185538]
2023-06-07 11:32:15.681: epoch 29:	0.03154480  	0.08775648  	0.07183110  
2023-06-07 11:32:15.681: Find a better model.
2023-06-07 11:32:42.029: [iter 30 : loss : 0.6158 = 0.1486 + 0.4631 + 0.0041, time: 26.345318]
2023-06-07 11:32:42.316: epoch 30:	0.03152260  	0.08774661  	0.07191723  
2023-06-07 11:33:08.389: [iter 31 : loss : 0.6076 = 0.1410 + 0.4623 + 0.0043, time: 26.068897]
2023-06-07 11:33:08.764: epoch 31:	0.03155221  	0.08758766  	0.07200813  
2023-06-07 11:33:35.079: [iter 32 : loss : 0.5993 = 0.1334 + 0.4614 + 0.0045, time: 26.309687]
2023-06-07 11:33:35.480: epoch 32:	0.03153739  	0.08752596  	0.07199000  
2023-06-07 11:34:02.526: [iter 33 : loss : 0.5919 = 0.1266 + 0.4606 + 0.0047, time: 27.041452]
2023-06-07 11:34:02.926: epoch 33:	0.03162622  	0.08795913  	0.07224921  
2023-06-07 11:34:02.927: Find a better model.
2023-06-07 11:34:29.764: [iter 34 : loss : 0.5857 = 0.1209 + 0.4600 + 0.0048, time: 26.797388]
2023-06-07 11:34:30.160: epoch 34:	0.03170767  	0.08824431  	0.07240859  
2023-06-07 11:34:30.160: Find a better model.
2023-06-07 11:34:56.886: [iter 35 : loss : 0.5798 = 0.1155 + 0.4593 + 0.0050, time: 26.722198]
2023-06-07 11:34:57.203: epoch 35:	0.03162624  	0.08804426  	0.07235163  
2023-06-07 11:35:20.997: [iter 36 : loss : 0.5746 = 0.1108 + 0.4586 + 0.0052, time: 23.789405]
2023-06-07 11:35:21.269: epoch 36:	0.03168546  	0.08810925  	0.07245974  
2023-06-07 11:35:45.286: [iter 37 : loss : 0.5695 = 0.1062 + 0.4579 + 0.0054, time: 24.010921]
2023-06-07 11:35:45.621: epoch 37:	0.03172988  	0.08835865  	0.07271539  
2023-06-07 11:35:45.621: Find a better model.
2023-06-07 11:36:09.532: [iter 38 : loss : 0.5651 = 0.1022 + 0.4574 + 0.0055, time: 23.907146]
2023-06-07 11:36:09.800: epoch 38:	0.03177430  	0.08844500  	0.07285948  
2023-06-07 11:36:09.800: Find a better model.
2023-06-07 11:36:33.827: [iter 39 : loss : 0.5618 = 0.0993 + 0.4568 + 0.0057, time: 24.022223]
2023-06-07 11:36:34.224: epoch 39:	0.03174470  	0.08831712  	0.07303698  
2023-06-07 11:36:58.635: [iter 40 : loss : 0.5573 = 0.0952 + 0.4562 + 0.0058, time: 24.405868]
2023-06-07 11:36:58.965: epoch 40:	0.03167065  	0.08820841  	0.07302833  
2023-06-07 11:37:22.824: [iter 41 : loss : 0.5537 = 0.0920 + 0.4558 + 0.0060, time: 23.854095]
2023-06-07 11:37:23.123: epoch 41:	0.03161143  	0.08804282  	0.07295633  
2023-06-07 11:37:46.517: [iter 42 : loss : 0.5503 = 0.0888 + 0.4553 + 0.0061, time: 23.387308]
2023-06-07 11:37:46.807: epoch 42:	0.03164104  	0.08810006  	0.07298374  
2023-06-07 11:38:11.374: [iter 43 : loss : 0.5479 = 0.0867 + 0.4549 + 0.0063, time: 24.562235]
2023-06-07 11:38:11.681: epoch 43:	0.03173728  	0.08806270  	0.07296839  
2023-06-07 11:38:36.366: [iter 44 : loss : 0.5445 = 0.0835 + 0.4546 + 0.0064, time: 24.681363]
2023-06-07 11:38:36.661: epoch 44:	0.03169285  	0.08778166  	0.07281016  
2023-06-07 11:39:00.739: [iter 45 : loss : 0.5422 = 0.0815 + 0.4542 + 0.0066, time: 24.074011]
2023-06-07 11:39:01.028: epoch 45:	0.03165584  	0.08804712  	0.07285137  
2023-06-07 11:39:24.834: [iter 46 : loss : 0.5395 = 0.0790 + 0.4538 + 0.0067, time: 23.802978]
2023-06-07 11:39:25.196: epoch 46:	0.03166324  	0.08809375  	0.07283760  
2023-06-07 11:39:48.152: [iter 47 : loss : 0.5373 = 0.0770 + 0.4535 + 0.0068, time: 22.951890]
2023-06-07 11:39:48.421: epoch 47:	0.03167064  	0.08814608  	0.07277281  
2023-06-07 11:40:11.371: [iter 48 : loss : 0.5349 = 0.0749 + 0.4530 + 0.0070, time: 22.946510]
2023-06-07 11:40:11.633: epoch 48:	0.03164102  	0.08810858  	0.07280779  
2023-06-07 11:40:36.546: [iter 49 : loss : 0.5325 = 0.0727 + 0.4527 + 0.0071, time: 24.909806]
2023-06-07 11:40:36.823: epoch 49:	0.03153738  	0.08761030  	0.07252517  
2023-06-07 11:41:00.122: [iter 50 : loss : 0.5310 = 0.0713 + 0.4525 + 0.0072, time: 23.293164]
2023-06-07 11:41:00.399: epoch 50:	0.03141893  	0.08749051  	0.07237353  
2023-06-07 11:41:23.853: [iter 51 : loss : 0.5289 = 0.0695 + 0.4522 + 0.0073, time: 23.450681]
2023-06-07 11:41:24.126: epoch 51:	0.03133009  	0.08731070  	0.07227709  
2023-06-07 11:41:47.664: [iter 52 : loss : 0.5272 = 0.0678 + 0.4519 + 0.0074, time: 23.534076]
2023-06-07 11:41:47.935: epoch 52:	0.03136710  	0.08710229  	0.07226701  
2023-06-07 11:42:11.485: [iter 53 : loss : 0.5253 = 0.0661 + 0.4516 + 0.0076, time: 23.545532]
2023-06-07 11:42:11.761: epoch 53:	0.03138931  	0.08693612  	0.07208422  
2023-06-07 11:42:35.226: [iter 54 : loss : 0.5237 = 0.0646 + 0.4514 + 0.0077, time: 23.461222]
2023-06-07 11:42:35.505: epoch 54:	0.03139671  	0.08679273  	0.07204551  
2023-06-07 11:42:58.846: [iter 55 : loss : 0.5228 = 0.0638 + 0.4512 + 0.0078, time: 23.337057]
2023-06-07 11:42:59.125: epoch 55:	0.03118202  	0.08643144  	0.07178517  
2023-06-07 11:43:22.644: [iter 56 : loss : 0.5213 = 0.0624 + 0.4510 + 0.0079, time: 23.515140]
2023-06-07 11:43:22.921: epoch 56:	0.03121903  	0.08637123  	0.07179397  
2023-06-07 11:43:46.230: [iter 57 : loss : 0.5197 = 0.0609 + 0.4508 + 0.0080, time: 23.303467]
2023-06-07 11:43:46.507: epoch 57:	0.03125605  	0.08634332  	0.07175528  
2023-06-07 11:44:09.837: [iter 58 : loss : 0.5186 = 0.0600 + 0.4505 + 0.0081, time: 23.326082]
2023-06-07 11:44:10.109: epoch 58:	0.03124124  	0.08620653  	0.07163471  
2023-06-07 11:44:33.591: [iter 59 : loss : 0.5176 = 0.0589 + 0.4504 + 0.0082, time: 23.478054]
2023-06-07 11:44:33.866: epoch 59:	0.03121903  	0.08595540  	0.07159761  
2023-06-07 11:44:57.389: [iter 60 : loss : 0.5156 = 0.0570 + 0.4502 + 0.0083, time: 23.519114]
2023-06-07 11:44:57.661: epoch 60:	0.03115240  	0.08571031  	0.07126214  
2023-06-07 11:45:20.956: [iter 61 : loss : 0.5148 = 0.0563 + 0.4500 + 0.0084, time: 23.291011]
2023-06-07 11:45:21.234: epoch 61:	0.03113760  	0.08566636  	0.07126086  
2023-06-07 11:45:44.583: [iter 62 : loss : 0.5135 = 0.0551 + 0.4499 + 0.0085, time: 23.346056]
2023-06-07 11:45:44.854: epoch 62:	0.03104876  	0.08534148  	0.07110951  
2023-06-07 11:46:08.155: [iter 63 : loss : 0.5123 = 0.0540 + 0.4496 + 0.0086, time: 23.297154]
2023-06-07 11:46:08.433: epoch 63:	0.03107837  	0.08522235  	0.07111058  
2023-06-07 11:46:08.434: Early stopping is trigger at epoch: 63
2023-06-07 11:46:08.434: best_result@epoch 38:

2023-06-07 11:46:08.434: 		0.0318      	0.0884      	0.0729      
2023-06-07 14:51:06.038: my pid: 11400
2023-06-07 14:51:06.039: model: model.general_recommender.SGL
2023-06-07 14:51:06.039: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 14:51:06.039: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=3
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 14:51:10.540: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 14:51:34.231: [iter 1 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 23.690192]
2023-06-07 14:51:34.585: epoch 1:	0.00236897  	0.00444165  	0.00380806  
2023-06-07 14:51:34.585: Find a better model.
2023-06-07 14:51:58.012: [iter 2 : loss : 1.1334 = 0.6930 + 0.4405 + 0.0000, time: 23.421186]
2023-06-07 14:51:58.399: epoch 2:	0.00283536  	0.00534255  	0.00473027  
2023-06-07 14:51:58.399: Find a better model.
2023-06-07 14:52:22.014: [iter 3 : loss : 1.1335 = 0.6928 + 0.4407 + 0.0000, time: 23.605732]
2023-06-07 14:52:22.356: epoch 3:	0.00333136  	0.00684919  	0.00557830  
2023-06-07 14:52:22.356: Find a better model.
2023-06-07 14:52:46.559: [iter 4 : loss : 1.1336 = 0.6927 + 0.4409 + 0.0000, time: 24.198957]
2023-06-07 14:52:46.996: epoch 4:	0.00367190  	0.00848562  	0.00701315  
2023-06-07 14:52:46.996: Find a better model.
2023-06-07 14:53:11.502: [iter 5 : loss : 1.1336 = 0.6923 + 0.4412 + 0.0000, time: 24.503091]
2023-06-07 14:53:11.934: epoch 5:	0.00391620  	0.00914130  	0.00770860  
2023-06-07 14:53:11.934: Find a better model.
2023-06-07 14:53:36.750: [iter 6 : loss : 1.1331 = 0.6916 + 0.4416 + 0.0000, time: 24.813228]
2023-06-07 14:53:37.118: epoch 6:	0.00450103  	0.01111332  	0.00929768  
2023-06-07 14:53:37.119: Find a better model.
2023-06-07 14:53:58.976: [iter 7 : loss : 1.1328 = 0.6908 + 0.4420 + 0.0000, time: 21.853429]
2023-06-07 14:53:59.274: epoch 7:	0.00533016  	0.01469932  	0.01200852  
2023-06-07 14:53:59.275: Find a better model.
2023-06-07 14:54:21.833: [iter 8 : loss : 1.1325 = 0.6902 + 0.4423 + 0.0000, time: 22.555127]
2023-06-07 14:54:22.122: epoch 8:	0.00643320  	0.01746362  	0.01421537  
2023-06-07 14:54:22.122: Find a better model.
2023-06-07 14:54:45.009: [iter 9 : loss : 1.1322 = 0.6893 + 0.4428 + 0.0000, time: 22.883128]
2023-06-07 14:54:45.308: epoch 9:	0.00785457  	0.02119182  	0.01796669  
2023-06-07 14:54:45.308: Find a better model.
2023-06-07 14:55:09.154: [iter 10 : loss : 1.1314 = 0.6880 + 0.4433 + 0.0000, time: 23.843864]
2023-06-07 14:55:09.469: epoch 10:	0.00948322  	0.02605536  	0.02236656  
2023-06-07 14:55:09.469: Find a better model.
2023-06-07 14:55:32.554: [iter 11 : loss : 1.1301 = 0.6860 + 0.4440 + 0.0000, time: 23.079206]
2023-06-07 14:55:32.934: epoch 11:	0.01208170  	0.03216914  	0.02781069  
2023-06-07 14:55:32.934: Find a better model.
2023-06-07 14:55:57.902: [iter 12 : loss : 1.1273 = 0.6825 + 0.4447 + 0.0001, time: 24.963220]
2023-06-07 14:55:58.232: epoch 12:	0.01488751  	0.04082578  	0.03524652  
2023-06-07 14:55:58.232: Find a better model.
2023-06-07 14:56:23.130: [iter 13 : loss : 1.1217 = 0.6759 + 0.4456 + 0.0001, time: 24.893076]
2023-06-07 14:56:23.528: epoch 13:	0.01815972  	0.04964609  	0.04370684  
2023-06-07 14:56:23.528: Find a better model.
2023-06-07 14:56:48.500: [iter 14 : loss : 1.1108 = 0.6639 + 0.4468 + 0.0001, time: 24.968173]
2023-06-07 14:56:48.935: epoch 14:	0.02149117  	0.06072337  	0.05152716  
2023-06-07 14:56:48.935: Find a better model.
2023-06-07 14:57:13.853: [iter 15 : loss : 1.0918 = 0.6432 + 0.4484 + 0.0002, time: 24.913064]
2023-06-07 14:57:14.202: epoch 15:	0.02505215  	0.06946426  	0.05841673  
2023-06-07 14:57:14.202: Find a better model.
2023-06-07 14:57:39.301: [iter 16 : loss : 1.0606 = 0.6096 + 0.4506 + 0.0004, time: 25.094181]
2023-06-07 14:57:39.719: epoch 16:	0.02761368  	0.07634039  	0.06385600  
2023-06-07 14:57:39.719: Find a better model.
2023-06-07 14:58:04.816: [iter 17 : loss : 1.0166 = 0.5621 + 0.4539 + 0.0005, time: 25.091613]
2023-06-07 14:58:05.144: epoch 17:	0.02932381  	0.08069876  	0.06715386  
2023-06-07 14:58:05.145: Find a better model.
2023-06-07 14:58:30.016: [iter 18 : loss : 0.9630 = 0.5041 + 0.4581 + 0.0008, time: 24.866695]
2023-06-07 14:58:30.376: epoch 18:	0.03017519  	0.08202493  	0.06857636  
2023-06-07 14:58:30.377: Find a better model.
2023-06-07 14:58:54.838: [iter 19 : loss : 0.9069 = 0.4432 + 0.4626 + 0.0011, time: 24.454732]
2023-06-07 14:58:55.216: epoch 19:	0.03084149  	0.08516707  	0.06960420  
2023-06-07 14:58:55.216: Find a better model.
2023-06-07 14:59:19.053: [iter 20 : loss : 0.8541 = 0.3863 + 0.4664 + 0.0014, time: 23.831121]
2023-06-07 14:59:19.437: epoch 20:	0.03097477  	0.08568160  	0.06966346  
2023-06-07 14:59:19.437: Find a better model.
2023-06-07 14:59:44.912: [iter 21 : loss : 0.8084 = 0.3376 + 0.4691 + 0.0017, time: 25.470136]
2023-06-07 14:59:45.302: epoch 21:	0.03102659  	0.08616436  	0.06963949  
2023-06-07 14:59:45.302: Find a better model.
2023-06-07 15:00:11.211: [iter 22 : loss : 0.7709 = 0.2979 + 0.4710 + 0.0020, time: 25.906146]
2023-06-07 15:00:11.505: epoch 22:	0.03101919  	0.08629850  	0.06984350  
2023-06-07 15:00:11.505: Find a better model.
2023-06-07 15:00:34.964: [iter 23 : loss : 0.7393 = 0.2654 + 0.4716 + 0.0023, time: 23.454223]
2023-06-07 15:00:35.288: epoch 23:	0.03122649  	0.08706321  	0.07023898  
2023-06-07 15:00:35.288: Find a better model.
2023-06-07 15:00:57.771: [iter 24 : loss : 0.7128 = 0.2387 + 0.4715 + 0.0026, time: 22.477119]
2023-06-07 15:00:58.101: epoch 24:	0.03131532  	0.08739951  	0.07057514  
2023-06-07 15:00:58.101: Find a better model.
2023-06-07 15:01:20.621: [iter 25 : loss : 0.6914 = 0.2177 + 0.4709 + 0.0029, time: 22.497061]
2023-06-07 15:01:21.016: epoch 25:	0.03144116  	0.08808206  	0.07105064  
2023-06-07 15:01:21.016: Find a better model.
2023-06-07 15:01:43.364: [iter 26 : loss : 0.6727 = 0.1993 + 0.4703 + 0.0031, time: 22.344072]
2023-06-07 15:01:43.689: epoch 26:	0.03139674  	0.08785462  	0.07110418  
2023-06-07 15:02:06.498: [iter 27 : loss : 0.6575 = 0.1847 + 0.4694 + 0.0033, time: 22.805078]
2023-06-07 15:02:06.907: epoch 27:	0.03156700  	0.08878507  	0.07171260  
2023-06-07 15:02:06.907: Find a better model.
2023-06-07 15:02:29.484: [iter 28 : loss : 0.6445 = 0.1723 + 0.4685 + 0.0036, time: 22.574028]
2023-06-07 15:02:29.870: epoch 28:	0.03187793  	0.09016439  	0.07233731  
2023-06-07 15:02:29.870: Find a better model.
2023-06-07 15:02:52.725: [iter 29 : loss : 0.6328 = 0.1612 + 0.4678 + 0.0038, time: 22.850088]
2023-06-07 15:02:53.028: epoch 29:	0.03190014  	0.09050135  	0.07283400  
2023-06-07 15:02:53.028: Find a better model.
2023-06-07 15:03:16.133: [iter 30 : loss : 0.6219 = 0.1511 + 0.4668 + 0.0040, time: 23.100650]
2023-06-07 15:03:16.399: epoch 30:	0.03198157  	0.09057783  	0.07330398  
2023-06-07 15:03:16.399: Find a better model.
2023-06-07 15:03:39.691: [iter 31 : loss : 0.6138 = 0.1436 + 0.4660 + 0.0042, time: 23.288068]
2023-06-07 15:03:40.096: epoch 31:	0.03213703  	0.09119949  	0.07355236  
2023-06-07 15:03:40.096: Find a better model.
2023-06-07 15:04:03.616: [iter 32 : loss : 0.6049 = 0.1355 + 0.4650 + 0.0044, time: 23.516071]
2023-06-07 15:04:03.941: epoch 32:	0.03212964  	0.09103394  	0.07359625  
2023-06-07 15:04:27.327: [iter 33 : loss : 0.5975 = 0.1287 + 0.4643 + 0.0046, time: 23.381382]
2023-06-07 15:04:27.605: epoch 33:	0.03221107  	0.09126679  	0.07384898  
2023-06-07 15:04:27.605: Find a better model.
2023-06-07 15:04:50.366: [iter 34 : loss : 0.5912 = 0.1229 + 0.4636 + 0.0048, time: 22.758061]
2023-06-07 15:04:50.639: epoch 34:	0.03213704  	0.09119109  	0.07398382  
2023-06-07 15:05:13.240: [iter 35 : loss : 0.5849 = 0.1171 + 0.4628 + 0.0050, time: 22.598402]
2023-06-07 15:05:13.534: epoch 35:	0.03215185  	0.09087779  	0.07406473  
2023-06-07 15:05:37.005: [iter 36 : loss : 0.5795 = 0.1123 + 0.4621 + 0.0051, time: 23.468033]
2023-06-07 15:05:37.341: epoch 36:	0.03217407  	0.09090695  	0.07419226  
2023-06-07 15:06:00.608: [iter 37 : loss : 0.5744 = 0.1077 + 0.4614 + 0.0053, time: 23.261274]
2023-06-07 15:06:00.910: epoch 37:	0.03224069  	0.09140643  	0.07432145  
2023-06-07 15:06:00.910: Find a better model.
2023-06-07 15:06:24.451: [iter 38 : loss : 0.5697 = 0.1034 + 0.4608 + 0.0055, time: 23.536059]
2023-06-07 15:06:24.861: epoch 38:	0.03227770  	0.09151554  	0.07443742  
2023-06-07 15:06:24.861: Find a better model.
2023-06-07 15:06:48.473: [iter 39 : loss : 0.5664 = 0.1006 + 0.4602 + 0.0056, time: 23.608086]
2023-06-07 15:06:48.760: epoch 39:	0.03218886  	0.09104425  	0.07429457  
2023-06-07 15:07:11.907: [iter 40 : loss : 0.5618 = 0.0963 + 0.4597 + 0.0058, time: 23.142080]
2023-06-07 15:07:12.178: epoch 40:	0.03230732  	0.09125533  	0.07438298  
2023-06-07 15:07:34.684: [iter 41 : loss : 0.5582 = 0.0930 + 0.4593 + 0.0059, time: 22.502388]
2023-06-07 15:07:34.989: epoch 41:	0.03215184  	0.09049319  	0.07415392  
2023-06-07 15:07:56.872: [iter 42 : loss : 0.5546 = 0.0898 + 0.4587 + 0.0061, time: 21.879331]
2023-06-07 15:07:57.145: epoch 42:	0.03218886  	0.09028225  	0.07428440  
2023-06-07 15:08:21.239: [iter 43 : loss : 0.5520 = 0.0875 + 0.4583 + 0.0062, time: 24.090461]
2023-06-07 15:08:21.496: epoch 43:	0.03217405  	0.09047765  	0.07442288  
2023-06-07 15:08:43.195: [iter 44 : loss : 0.5485 = 0.0842 + 0.4580 + 0.0063, time: 21.695643]
2023-06-07 15:08:43.454: epoch 44:	0.03217404  	0.09040999  	0.07443696  
2023-06-07 15:09:05.203: [iter 45 : loss : 0.5464 = 0.0824 + 0.4575 + 0.0065, time: 21.745229]
2023-06-07 15:09:05.461: epoch 45:	0.03218144  	0.09026607  	0.07459603  
2023-06-07 15:09:27.589: [iter 46 : loss : 0.5436 = 0.0798 + 0.4572 + 0.0066, time: 22.124182]
2023-06-07 15:09:27.871: epoch 46:	0.03225547  	0.09013323  	0.07453848  
2023-06-07 15:09:50.050: [iter 47 : loss : 0.5413 = 0.0777 + 0.4568 + 0.0068, time: 22.175377]
2023-06-07 15:09:50.319: epoch 47:	0.03215183  	0.08972714  	0.07428895  
2023-06-07 15:10:11.832: [iter 48 : loss : 0.5387 = 0.0754 + 0.4564 + 0.0069, time: 21.509106]
2023-06-07 15:10:12.100: epoch 48:	0.03203338  	0.08948786  	0.07423487  
2023-06-07 15:10:33.837: [iter 49 : loss : 0.5363 = 0.0733 + 0.4561 + 0.0070, time: 21.733024]
2023-06-07 15:10:34.107: epoch 49:	0.03198156  	0.08923370  	0.07411702  
2023-06-07 15:10:56.489: [iter 50 : loss : 0.5347 = 0.0717 + 0.4558 + 0.0071, time: 22.378135]
2023-06-07 15:10:56.766: epoch 50:	0.03190012  	0.08892237  	0.07410245  
2023-06-07 15:11:21.317: [iter 51 : loss : 0.5330 = 0.0703 + 0.4555 + 0.0073, time: 24.547746]
2023-06-07 15:11:21.616: epoch 51:	0.03198895  	0.08928689  	0.07419252  
2023-06-07 15:11:45.711: [iter 52 : loss : 0.5309 = 0.0684 + 0.4551 + 0.0074, time: 24.089779]
2023-06-07 15:11:46.071: epoch 52:	0.03189272  	0.08886539  	0.07398404  
2023-06-07 15:12:10.496: [iter 53 : loss : 0.5294 = 0.0670 + 0.4550 + 0.0075, time: 24.418642]
2023-06-07 15:12:10.859: epoch 53:	0.03196675  	0.08879506  	0.07397547  
2023-06-07 15:12:35.084: [iter 54 : loss : 0.5275 = 0.0651 + 0.4547 + 0.0076, time: 24.220150]
2023-06-07 15:12:35.377: epoch 54:	0.03187791  	0.08872630  	0.07393850  
2023-06-07 15:13:00.053: [iter 55 : loss : 0.5265 = 0.0642 + 0.4546 + 0.0077, time: 24.671283]
2023-06-07 15:13:00.644: epoch 55:	0.03180386  	0.08837351  	0.07384979  
2023-06-07 15:13:25.670: [iter 56 : loss : 0.5253 = 0.0632 + 0.4543 + 0.0078, time: 25.020138]
2023-06-07 15:13:25.967: epoch 56:	0.03194454  	0.08875843  	0.07384446  
2023-06-07 15:13:50.306: [iter 57 : loss : 0.5234 = 0.0614 + 0.4541 + 0.0079, time: 24.334482]
2023-06-07 15:13:50.656: epoch 57:	0.03180388  	0.08876765  	0.07375530  
2023-06-07 15:14:15.274: [iter 58 : loss : 0.5224 = 0.0605 + 0.4539 + 0.0081, time: 24.614098]
2023-06-07 15:14:15.564: epoch 58:	0.03164840  	0.08830959  	0.07363930  
2023-06-07 15:14:40.151: [iter 59 : loss : 0.5213 = 0.0594 + 0.4537 + 0.0082, time: 24.582685]
2023-06-07 15:14:40.515: epoch 59:	0.03162619  	0.08832841  	0.07361414  
2023-06-07 15:15:02.912: [iter 60 : loss : 0.5191 = 0.0574 + 0.4535 + 0.0083, time: 22.391680]
2023-06-07 15:15:03.182: epoch 60:	0.03162620  	0.08812518  	0.07365181  
2023-06-07 15:15:25.112: [iter 61 : loss : 0.5186 = 0.0570 + 0.4532 + 0.0084, time: 21.926767]
2023-06-07 15:15:25.379: epoch 61:	0.03154476  	0.08789306  	0.07348692  
2023-06-07 15:15:47.390: [iter 62 : loss : 0.5172 = 0.0556 + 0.4532 + 0.0085, time: 22.006675]
2023-06-07 15:15:47.663: epoch 62:	0.03144852  	0.08756790  	0.07336517  
2023-06-07 15:16:09.892: [iter 63 : loss : 0.5159 = 0.0545 + 0.4529 + 0.0086, time: 22.223700]
2023-06-07 15:16:10.156: epoch 63:	0.03139670  	0.08793133  	0.07323375  
2023-06-07 15:16:10.156: Early stopping is trigger at epoch: 63
2023-06-07 15:16:10.156: best_result@epoch 38:

2023-06-07 15:16:10.156: 		0.0323      	0.0915      	0.0744      
2023-06-07 16:36:35.741: my pid: 13980
2023-06-07 16:36:35.741: model: model.general_recommender.SGL
2023-06-07 16:36:35.741: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-07 16:36:35.741: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=2
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-07 16:36:40.213: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-07 16:37:02.064: [iter 1 : loss : 1.1326 = 0.6930 + 0.4396 + 0.0000, time: 21.849495]
2023-06-07 16:37:02.346: epoch 1:	0.00225052  	0.00375559  	0.00345041  
2023-06-07 16:37:02.346: Find a better model.
2023-06-07 16:37:24.393: [iter 2 : loss : 1.1324 = 0.6928 + 0.4396 + 0.0000, time: 22.043023]
2023-06-07 16:37:24.725: epoch 2:	0.00310187  	0.00558861  	0.00493883  
2023-06-07 16:37:24.725: Find a better model.
2023-06-07 16:37:46.833: [iter 3 : loss : 1.1324 = 0.6925 + 0.4399 + 0.0000, time: 22.105792]
2023-06-07 16:37:47.163: epoch 3:	0.00410128  	0.00823327  	0.00716531  
2023-06-07 16:37:47.164: Find a better model.
2023-06-07 16:38:09.036: [iter 4 : loss : 1.1322 = 0.6919 + 0.4403 + 0.0000, time: 21.857404]
2023-06-07 16:38:09.331: epoch 4:	0.00516730  	0.01204311  	0.00999523  
2023-06-07 16:38:09.331: Find a better model.
2023-06-07 16:38:31.105: [iter 5 : loss : 1.1314 = 0.6906 + 0.4408 + 0.0000, time: 21.770505]
2023-06-07 16:38:31.448: epoch 5:	0.00638878  	0.01588893  	0.01365293  
2023-06-07 16:38:31.448: Find a better model.
2023-06-07 16:38:53.188: [iter 6 : loss : 1.1299 = 0.6885 + 0.4414 + 0.0000, time: 21.733019]
2023-06-07 16:38:53.545: epoch 6:	0.00888358  	0.02429049  	0.02018769  
2023-06-07 16:38:53.545: Find a better model.
2023-06-07 16:39:15.252: [iter 7 : loss : 1.1271 = 0.6847 + 0.4424 + 0.0000, time: 21.702066]
2023-06-07 16:39:15.668: epoch 7:	0.01277761  	0.03555107  	0.03024414  
2023-06-07 16:39:15.668: Find a better model.
2023-06-07 16:39:37.447: [iter 8 : loss : 1.1200 = 0.6764 + 0.4436 + 0.0001, time: 21.774334]
2023-06-07 16:39:37.833: epoch 8:	0.01730835  	0.04883830  	0.04152511  
2023-06-07 16:39:37.833: Find a better model.
2023-06-07 16:39:59.449: [iter 9 : loss : 1.1030 = 0.6576 + 0.4453 + 0.0001, time: 21.610809]
2023-06-07 16:39:59.842: epoch 9:	0.02276454  	0.06258280  	0.05385272  
2023-06-07 16:39:59.842: Find a better model.
2023-06-07 16:40:21.653: [iter 10 : loss : 1.0674 = 0.6195 + 0.4477 + 0.0002, time: 21.807277]
2023-06-07 16:40:22.037: epoch 10:	0.02688074  	0.07336777  	0.06206882  
2023-06-07 16:40:22.037: Find a better model.
2023-06-07 16:40:43.493: [iter 11 : loss : 1.0109 = 0.5592 + 0.4513 + 0.0003, time: 21.452520]
2023-06-07 16:40:43.885: epoch 11:	0.02938304  	0.07999956  	0.06689399  
2023-06-07 16:40:43.885: Find a better model.
2023-06-07 16:41:03.955: [iter 12 : loss : 0.9401 = 0.4838 + 0.4558 + 0.0006, time: 20.065793]
2023-06-07 16:41:04.234: epoch 12:	0.03056016  	0.08332213  	0.06847548  
2023-06-07 16:41:04.234: Find a better model.
2023-06-07 16:41:24.184: [iter 13 : loss : 0.8702 = 0.4094 + 0.4601 + 0.0008, time: 19.945459]
2023-06-07 16:41:24.482: epoch 13:	0.03070085  	0.08426863  	0.06879110  
2023-06-07 16:41:24.482: Find a better model.
2023-06-07 16:41:44.604: [iter 14 : loss : 0.8091 = 0.3449 + 0.4632 + 0.0011, time: 20.119000]
2023-06-07 16:41:44.899: epoch 14:	0.03087853  	0.08466040  	0.06903707  
2023-06-07 16:41:44.899: Find a better model.
2023-06-07 16:42:04.966: [iter 15 : loss : 0.7611 = 0.2949 + 0.4650 + 0.0013, time: 20.064153]
2023-06-07 16:42:05.257: epoch 15:	0.03109323  	0.08594964  	0.06969304  
2023-06-07 16:42:05.258: Find a better model.
2023-06-07 16:42:25.359: [iter 16 : loss : 0.7220 = 0.2548 + 0.4657 + 0.0015, time: 20.098164]
2023-06-07 16:42:25.649: epoch 16:	0.03118948  	0.08657686  	0.07011630  
2023-06-07 16:42:25.649: Find a better model.
2023-06-07 16:42:45.740: [iter 17 : loss : 0.6923 = 0.2249 + 0.4656 + 0.0017, time: 20.088032]
2023-06-07 16:42:46.027: epoch 17:	0.03133754  	0.08700794  	0.07038536  
2023-06-07 16:42:46.027: Find a better model.
2023-06-07 16:43:07.238: [iter 18 : loss : 0.6684 = 0.2014 + 0.4651 + 0.0019, time: 21.207191]
2023-06-07 16:43:07.513: epoch 18:	0.03147078  	0.08768612  	0.07067049  
2023-06-07 16:43:07.513: Find a better model.
2023-06-07 16:43:27.673: [iter 19 : loss : 0.6489 = 0.1823 + 0.4645 + 0.0021, time: 20.156988]
2023-06-07 16:43:27.960: epoch 19:	0.03157443  	0.08821368  	0.07106528  
2023-06-07 16:43:27.960: Find a better model.
2023-06-07 16:43:48.362: [iter 20 : loss : 0.6327 = 0.1668 + 0.4636 + 0.0023, time: 20.399524]
2023-06-07 16:43:48.653: epoch 20:	0.03175210  	0.08845322  	0.07136866  
2023-06-07 16:43:48.653: Find a better model.
2023-06-07 16:44:09.105: [iter 21 : loss : 0.6187 = 0.1536 + 0.4626 + 0.0025, time: 20.447866]
2023-06-07 16:44:09.386: epoch 21:	0.03179652  	0.08856145  	0.07154480  
2023-06-07 16:44:09.386: Find a better model.
2023-06-07 16:44:30.013: [iter 22 : loss : 0.6073 = 0.1428 + 0.4619 + 0.0026, time: 20.622498]
2023-06-07 16:44:30.294: epoch 22:	0.03189277  	0.08881511  	0.07185555  
2023-06-07 16:44:30.294: Find a better model.
2023-06-07 16:44:52.587: [iter 23 : loss : 0.5973 = 0.1335 + 0.4611 + 0.0028, time: 22.288413]
2023-06-07 16:44:52.943: epoch 23:	0.03192238  	0.08894286  	0.07205182  
2023-06-07 16:44:52.943: Find a better model.
2023-06-07 16:45:14.955: [iter 24 : loss : 0.5878 = 0.1247 + 0.4602 + 0.0029, time: 22.009066]
2023-06-07 16:45:15.310: epoch 24:	0.03190017  	0.08926242  	0.07213837  
2023-06-07 16:45:15.310: Find a better model.
2023-06-07 16:45:37.453: [iter 25 : loss : 0.5798 = 0.1174 + 0.4594 + 0.0030, time: 22.133299]
2023-06-07 16:45:37.805: epoch 25:	0.03186315  	0.08936978  	0.07220704  
2023-06-07 16:45:37.805: Find a better model.
2023-06-07 16:46:00.039: [iter 26 : loss : 0.5726 = 0.1108 + 0.4587 + 0.0032, time: 22.229599]
2023-06-07 16:46:00.386: epoch 26:	0.03179651  	0.08897488  	0.07209578  
2023-06-07 16:46:22.785: [iter 27 : loss : 0.5666 = 0.1053 + 0.4580 + 0.0033, time: 22.395226]
2023-06-07 16:46:23.133: epoch 27:	0.03180392  	0.08915751  	0.07225315  
2023-06-07 16:46:45.502: [iter 28 : loss : 0.5616 = 0.1008 + 0.4573 + 0.0034, time: 22.361526]
2023-06-07 16:46:45.816: epoch 28:	0.03184834  	0.08953819  	0.07244916  
2023-06-07 16:46:45.816: Find a better model.
2023-06-07 16:47:07.850: [iter 29 : loss : 0.5565 = 0.0962 + 0.4568 + 0.0035, time: 22.031511]
2023-06-07 16:47:08.151: epoch 29:	0.03189277  	0.08934478  	0.07251775  
2023-06-07 16:47:30.432: [iter 30 : loss : 0.5514 = 0.0915 + 0.4562 + 0.0037, time: 22.274096]
2023-06-07 16:47:30.781: epoch 30:	0.03192239  	0.08951424  	0.07272133  
2023-06-07 16:47:53.383: [iter 31 : loss : 0.5480 = 0.0886 + 0.4557 + 0.0038, time: 22.597533]
2023-06-07 16:47:53.716: epoch 31:	0.03187056  	0.08959977  	0.07277332  
2023-06-07 16:47:53.716: Find a better model.
2023-06-07 16:48:16.225: [iter 32 : loss : 0.5439 = 0.0849 + 0.4551 + 0.0039, time: 22.504175]
2023-06-07 16:48:16.581: epoch 32:	0.03175951  	0.08910967  	0.07244153  
2023-06-07 16:48:39.015: [iter 33 : loss : 0.5401 = 0.0815 + 0.4547 + 0.0040, time: 22.425858]
2023-06-07 16:48:39.343: epoch 33:	0.03170769  	0.08877623  	0.07249137  
2023-06-07 16:49:01.941: [iter 34 : loss : 0.5372 = 0.0788 + 0.4543 + 0.0041, time: 22.593549]
2023-06-07 16:49:02.317: epoch 34:	0.03161144  	0.08831941  	0.07222979  
2023-06-07 16:49:25.654: [iter 35 : loss : 0.5336 = 0.0756 + 0.4539 + 0.0042, time: 23.332977]
2023-06-07 16:49:25.934: epoch 35:	0.03169286  	0.08798991  	0.07221287  
2023-06-07 16:49:47.623: [iter 36 : loss : 0.5311 = 0.0734 + 0.4535 + 0.0043, time: 21.683813]
2023-06-07 16:49:47.901: epoch 36:	0.03161883  	0.08768588  	0.07200636  
2023-06-07 16:50:09.869: [iter 37 : loss : 0.5284 = 0.0710 + 0.4531 + 0.0044, time: 21.964828]
2023-06-07 16:50:10.146: epoch 37:	0.03158181  	0.08837603  	0.07207884  
2023-06-07 16:50:31.836: [iter 38 : loss : 0.5260 = 0.0687 + 0.4528 + 0.0045, time: 21.686279]
2023-06-07 16:50:32.117: epoch 38:	0.03144114  	0.08761433  	0.07191037  
2023-06-07 16:50:54.351: [iter 39 : loss : 0.5245 = 0.0675 + 0.4524 + 0.0046, time: 22.228266]
2023-06-07 16:50:54.663: epoch 39:	0.03140413  	0.08785428  	0.07205932  
2023-06-07 16:51:18.309: [iter 40 : loss : 0.5216 = 0.0648 + 0.4522 + 0.0046, time: 23.641535]
2023-06-07 16:51:18.617: epoch 40:	0.03144114  	0.08789942  	0.07192432  
2023-06-07 16:51:42.216: [iter 41 : loss : 0.5197 = 0.0630 + 0.4519 + 0.0047, time: 23.590046]
2023-06-07 16:51:42.524: epoch 41:	0.03146336  	0.08772606  	0.07174192  
2023-06-07 16:52:06.339: [iter 42 : loss : 0.5177 = 0.0612 + 0.4516 + 0.0048, time: 23.811250]
2023-06-07 16:52:06.642: epoch 42:	0.03133751  	0.08774746  	0.07168277  
2023-06-07 16:52:30.625: [iter 43 : loss : 0.5165 = 0.0602 + 0.4514 + 0.0049, time: 23.977918]
2023-06-07 16:52:31.019: epoch 43:	0.03138193  	0.08759404  	0.07170179  
2023-06-07 16:52:55.024: [iter 44 : loss : 0.5144 = 0.0582 + 0.4512 + 0.0050, time: 24.001004]
2023-06-07 16:52:55.320: epoch 44:	0.03138192  	0.08741020  	0.07153734  
2023-06-07 16:53:19.251: [iter 45 : loss : 0.5132 = 0.0572 + 0.4510 + 0.0051, time: 23.926512]
2023-06-07 16:53:19.549: epoch 45:	0.03135971  	0.08734915  	0.07130603  
2023-06-07 16:53:43.079: [iter 46 : loss : 0.5118 = 0.0558 + 0.4508 + 0.0051, time: 23.526727]
2023-06-07 16:53:43.362: epoch 46:	0.03120424  	0.08666919  	0.07116272  
2023-06-07 16:54:06.679: [iter 47 : loss : 0.5106 = 0.0547 + 0.4506 + 0.0052, time: 23.312167]
2023-06-07 16:54:06.976: epoch 47:	0.03106359  	0.08598488  	0.07091606  
2023-06-07 16:54:30.524: [iter 48 : loss : 0.5090 = 0.0533 + 0.4504 + 0.0053, time: 23.531954]
2023-06-07 16:54:30.823: epoch 48:	0.03107839  	0.08636589  	0.07104617  
2023-06-07 16:54:55.369: [iter 49 : loss : 0.5072 = 0.0516 + 0.4502 + 0.0054, time: 24.541400]
2023-06-07 16:54:55.688: epoch 49:	0.03095994  	0.08583129  	0.07070792  
2023-06-07 16:55:20.403: [iter 50 : loss : 0.5065 = 0.0510 + 0.4501 + 0.0054, time: 24.710064]
2023-06-07 16:55:20.738: epoch 50:	0.03076005  	0.08531432  	0.07052805  
2023-06-07 16:55:44.370: [iter 51 : loss : 0.5054 = 0.0501 + 0.4498 + 0.0055, time: 23.627576]
2023-06-07 16:55:44.663: epoch 51:	0.03086370  	0.08521629  	0.07040118  
2023-06-07 16:56:08.449: [iter 52 : loss : 0.5044 = 0.0491 + 0.4497 + 0.0056, time: 23.781044]
2023-06-07 16:56:08.824: epoch 52:	0.03075265  	0.08495618  	0.07032639  
2023-06-07 16:56:33.439: [iter 53 : loss : 0.5032 = 0.0480 + 0.4495 + 0.0057, time: 24.611320]
2023-06-07 16:56:33.750: epoch 53:	0.03078226  	0.08494858  	0.07033700  
2023-06-07 16:56:58.377: [iter 54 : loss : 0.5020 = 0.0468 + 0.4494 + 0.0057, time: 24.624050]
2023-06-07 16:56:58.710: epoch 54:	0.03079708  	0.08481103  	0.07027110  
2023-06-07 16:57:22.498: [iter 55 : loss : 0.5014 = 0.0463 + 0.4493 + 0.0058, time: 23.782741]
2023-06-07 16:57:22.819: epoch 55:	0.03064160  	0.08422943  	0.07010837  
2023-06-07 16:57:46.693: [iter 56 : loss : 0.5009 = 0.0458 + 0.4492 + 0.0058, time: 23.871368]
2023-06-07 16:57:46.980: epoch 56:	0.03061200  	0.08390925  	0.06993670  
2023-06-07 16:57:46.981: Early stopping is trigger at epoch: 56
2023-06-07 16:57:46.981: best_result@epoch 31:

2023-06-07 16:57:46.981: 		0.0319      	0.0896      	0.0728      
2023-06-08 10:10:28.030: my pid: 14748
2023-06-08 10:10:28.030: model: model.general_recommender.SGL
2023-06-08 10:10:28.030: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-08 10:10:28.030: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=1
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 10:10:32.228: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 10:10:51.227: [iter 1 : loss : 1.1319 = 0.6926 + 0.4392 + 0.0000, time: 18.998431]
2023-06-08 10:10:51.493: epoch 1:	0.00287978  	0.00592787  	0.00473810  
2023-06-08 10:10:51.493: Find a better model.
2023-06-08 10:11:10.587: [iter 2 : loss : 1.1305 = 0.6913 + 0.4392 + 0.0000, time: 19.090892]
2023-06-08 10:11:10.893: epoch 2:	0.00582616  	0.01598278  	0.01233761  
2023-06-08 10:11:10.893: Find a better model.
2023-06-08 10:11:30.167: [iter 3 : loss : 1.1274 = 0.6875 + 0.4399 + 0.0000, time: 19.270706]
2023-06-08 10:11:30.455: epoch 3:	0.01140063  	0.03228983  	0.02755799  
2023-06-08 10:11:30.455: Find a better model.
2023-06-08 10:11:49.492: [iter 4 : loss : 1.1163 = 0.6748 + 0.4415 + 0.0000, time: 19.034312]
2023-06-08 10:11:49.780: epoch 4:	0.01867054  	0.05220012  	0.04403278  
2023-06-08 10:11:49.780: Find a better model.
2023-06-08 10:12:09.139: [iter 5 : loss : 1.0821 = 0.6382 + 0.4438 + 0.0001, time: 19.353219]
2023-06-08 10:12:09.427: epoch 5:	0.02405272  	0.06519711  	0.05550481  
2023-06-08 10:12:09.427: Find a better model.
2023-06-08 10:12:30.980: [iter 6 : loss : 1.0135 = 0.5665 + 0.4469 + 0.0002, time: 21.548140]
2023-06-08 10:12:31.451: epoch 6:	0.02702884  	0.07292897  	0.06131921  
2023-06-08 10:12:31.451: Find a better model.
2023-06-08 10:12:52.497: [iter 7 : loss : 0.9262 = 0.4752 + 0.4507 + 0.0003, time: 21.041708]
2023-06-08 10:12:53.013: epoch 7:	0.02822818  	0.07555796  	0.06346066  
2023-06-08 10:12:53.014: Find a better model.
2023-06-08 10:13:14.971: [iter 8 : loss : 0.8438 = 0.3897 + 0.4536 + 0.0005, time: 21.951104]
2023-06-08 10:13:15.437: epoch 8:	0.02877601  	0.07802206  	0.06477444  
2023-06-08 10:13:15.437: Find a better model.
2023-06-08 10:13:37.304: [iter 9 : loss : 0.7776 = 0.3214 + 0.4555 + 0.0007, time: 21.857373]
2023-06-08 10:13:37.779: epoch 9:	0.02894627  	0.07863829  	0.06541951  
2023-06-08 10:13:37.779: Find a better model.
2023-06-08 10:14:00.834: [iter 10 : loss : 0.7262 = 0.2691 + 0.4563 + 0.0008, time: 23.046757]
2023-06-08 10:14:01.355: epoch 10:	0.02909434  	0.07933602  	0.06575230  
2023-06-08 10:14:01.355: Find a better model.
2023-06-08 10:14:23.545: [iter 11 : loss : 0.6894 = 0.2317 + 0.4566 + 0.0010, time: 22.184190]
2023-06-08 10:14:23.974: epoch 11:	0.02938307  	0.07993586  	0.06617156  
2023-06-08 10:14:23.975: Find a better model.
2023-06-08 10:14:46.270: [iter 12 : loss : 0.6598 = 0.2022 + 0.4564 + 0.0011, time: 22.289457]
2023-06-08 10:14:46.558: epoch 12:	0.02923500  	0.08005735  	0.06616522  
2023-06-08 10:14:46.558: Find a better model.
2023-06-08 10:15:08.102: [iter 13 : loss : 0.6369 = 0.1796 + 0.4560 + 0.0013, time: 21.540704]
2023-06-08 10:15:08.597: epoch 13:	0.02931644  	0.08015541  	0.06646646  
2023-06-08 10:15:08.597: Find a better model.
2023-06-08 10:15:31.215: [iter 14 : loss : 0.6175 = 0.1606 + 0.4555 + 0.0014, time: 22.613233]
2023-06-08 10:15:31.666: epoch 14:	0.02947190  	0.08053126  	0.06672142  
2023-06-08 10:15:31.666: Find a better model.
2023-06-08 10:15:53.440: [iter 15 : loss : 0.6029 = 0.1465 + 0.4549 + 0.0015, time: 21.770175]
2023-06-08 10:15:53.958: epoch 15:	0.02942749  	0.08069146  	0.06693561  
2023-06-08 10:15:53.959: Find a better model.
2023-06-08 10:16:16.876: [iter 16 : loss : 0.5891 = 0.1332 + 0.4543 + 0.0016, time: 22.913167]
2023-06-08 10:16:17.339: epoch 16:	0.02950151  	0.08122316  	0.06701906  
2023-06-08 10:16:17.339: Find a better model.
2023-06-08 10:16:39.433: [iter 17 : loss : 0.5790 = 0.1236 + 0.4537 + 0.0017, time: 22.087758]
2023-06-08 10:16:39.995: epoch 17:	0.02936826  	0.08052659  	0.06683527  
2023-06-08 10:17:02.661: [iter 18 : loss : 0.5697 = 0.1148 + 0.4531 + 0.0018, time: 22.656659]
2023-06-08 10:17:03.113: epoch 18:	0.02924981  	0.08066835  	0.06678457  
2023-06-08 10:17:25.359: [iter 19 : loss : 0.5619 = 0.1074 + 0.4525 + 0.0019, time: 22.241635]
2023-06-08 10:17:25.669: epoch 19:	0.02939788  	0.08101079  	0.06691369  
2023-06-08 10:17:47.574: [iter 20 : loss : 0.5551 = 0.1011 + 0.4520 + 0.0020, time: 21.899078]
2023-06-08 10:17:48.051: epoch 20:	0.02936088  	0.08067185  	0.06674255  
2023-06-08 10:18:10.528: [iter 21 : loss : 0.5487 = 0.0953 + 0.4513 + 0.0020, time: 22.473050]
2023-06-08 10:18:11.050: epoch 21:	0.02920538  	0.07960784  	0.06643195  
2023-06-08 10:18:34.881: [iter 22 : loss : 0.5431 = 0.0901 + 0.4509 + 0.0021, time: 23.824518]
2023-06-08 10:18:35.345: epoch 22:	0.02896109  	0.07904346  	0.06617676  
2023-06-08 10:18:58.159: [iter 23 : loss : 0.5385 = 0.0859 + 0.4504 + 0.0022, time: 22.810819]
2023-06-08 10:18:58.580: epoch 23:	0.02907954  	0.07947544  	0.06643699  
2023-06-08 10:19:21.523: [iter 24 : loss : 0.5334 = 0.0811 + 0.4500 + 0.0023, time: 22.937198]
2023-06-08 10:19:22.050: epoch 24:	0.02894626  	0.07920763  	0.06622657  
2023-06-08 10:19:44.165: [iter 25 : loss : 0.5293 = 0.0774 + 0.4496 + 0.0023, time: 22.110254]
2023-06-08 10:19:44.644: epoch 25:	0.02879081  	0.07886901  	0.06591038  
2023-06-08 10:20:05.383: [iter 26 : loss : 0.5256 = 0.0739 + 0.4493 + 0.0024, time: 20.734668]
2023-06-08 10:20:05.664: epoch 26:	0.02876117  	0.07862231  	0.06569067  
2023-06-08 10:20:26.946: [iter 27 : loss : 0.5222 = 0.0708 + 0.4489 + 0.0025, time: 21.278714]
2023-06-08 10:20:27.224: epoch 27:	0.02851688  	0.07798950  	0.06552161  
2023-06-08 10:20:47.729: [iter 28 : loss : 0.5196 = 0.0685 + 0.4486 + 0.0026, time: 20.501019]
2023-06-08 10:20:48.039: epoch 28:	0.02845024  	0.07779093  	0.06520325  
2023-06-08 10:21:08.410: [iter 29 : loss : 0.5172 = 0.0662 + 0.4484 + 0.0026, time: 20.368340]
2023-06-08 10:21:08.710: epoch 29:	0.02833179  	0.07748573  	0.06504870  
2023-06-08 10:21:29.958: [iter 30 : loss : 0.5141 = 0.0634 + 0.4481 + 0.0027, time: 21.244957]
2023-06-08 10:21:30.231: epoch 30:	0.02832438  	0.07759013  	0.06495150  
2023-06-08 10:21:50.682: [iter 31 : loss : 0.5124 = 0.0619 + 0.4478 + 0.0027, time: 20.448029]
2023-06-08 10:21:51.023: epoch 31:	0.02814671  	0.07668361  	0.06454156  
2023-06-08 10:22:11.487: [iter 32 : loss : 0.5098 = 0.0595 + 0.4475 + 0.0028, time: 20.459197]
2023-06-08 10:22:11.775: epoch 32:	0.02808008  	0.07644631  	0.06440558  
2023-06-08 10:22:33.077: [iter 33 : loss : 0.5079 = 0.0576 + 0.4474 + 0.0028, time: 21.298117]
2023-06-08 10:22:33.439: epoch 33:	0.02816151  	0.07639232  	0.06445977  
2023-06-08 10:22:54.189: [iter 34 : loss : 0.5060 = 0.0559 + 0.4472 + 0.0029, time: 20.745423]
2023-06-08 10:22:54.478: epoch 34:	0.02804307  	0.07594591  	0.06413114  
2023-06-08 10:23:15.263: [iter 35 : loss : 0.5040 = 0.0541 + 0.4470 + 0.0030, time: 20.781564]
2023-06-08 10:23:15.550: epoch 35:	0.02799864  	0.07563461  	0.06403128  
2023-06-08 10:23:37.288: [iter 36 : loss : 0.5024 = 0.0526 + 0.4468 + 0.0030, time: 21.733871]
2023-06-08 10:23:37.560: epoch 36:	0.02799123  	0.07577796  	0.06393080  
2023-06-08 10:23:59.229: [iter 37 : loss : 0.5009 = 0.0512 + 0.4466 + 0.0031, time: 21.663964]
2023-06-08 10:23:59.585: epoch 37:	0.02802825  	0.07569199  	0.06379896  
2023-06-08 10:24:21.276: [iter 38 : loss : 0.4993 = 0.0497 + 0.4465 + 0.0031, time: 21.686501]
2023-06-08 10:24:21.550: epoch 38:	0.02784315  	0.07532622  	0.06348818  
2023-06-08 10:24:43.183: [iter 39 : loss : 0.4986 = 0.0491 + 0.4464 + 0.0032, time: 21.630380]
2023-06-08 10:24:43.482: epoch 39:	0.02799863  	0.07553405  	0.06356056  
2023-06-08 10:25:04.954: [iter 40 : loss : 0.4964 = 0.0469 + 0.4462 + 0.0032, time: 21.467268]
2023-06-08 10:25:05.246: epoch 40:	0.02779134  	0.07521632  	0.06339776  
2023-06-08 10:25:27.037: [iter 41 : loss : 0.4957 = 0.0463 + 0.4461 + 0.0033, time: 21.786729]
2023-06-08 10:25:27.342: epoch 41:	0.02774692  	0.07486365  	0.06313463  
2023-06-08 10:25:27.342: Early stopping is trigger at epoch: 41
2023-06-08 10:25:27.342: best_result@epoch 16:

2023-06-08 10:25:27.342: 		0.0295      	0.0812      	0.0670      
2023-06-08 10:36:15.092: my pid: 8068
2023-06-08 10:36:15.092: model: model.general_recommender.SGL
2023-06-08 10:36:15.092: Dataset statistics:
Name: book-crossing
The number of users: 6754
The number of items: 13670
The number of ratings: 374325
Average actions of users: 55.42
Average actions of items: 27.38
The sparsity of the dataset: 99.594567%

The number of training: 305679
The number of validation: 0
The number of testing: 68646
2023-06-08 10:36:15.092: NeuRec:[NeuRec]:
recommender=SGL
dataset=book-crossing
file_column=UI
sep=','
gpu_id=0
gpu_mem=0.99
metric=["Precision", "Recall", "NDCG"]
top_k=[20]
test_thread=8
test_batch_size=128
seed=2021
start_testing_epoch=0

SGL:[hyperparameters]:
aug_type=ED
reg=1e-4
embed_size=64
n_layers=4
ssl_reg=0.05
ssl_ratio=0.5
ssl_temp=0.2
ssl_mode=both_side
lr=0.001
learner=adam
adj_type=pre
epochs=1000
batch_size=2048
num_negatives=1
param_init=xavier_uniform
stddev=0.01
verbose=1
stop_cnt=25
pretrain_flag=0
save_flag=0
2023-06-08 10:36:19.690: metrics:	Precision@20	Recall@20   	NDCG@20     
2023-06-08 10:36:43.253: [iter 1 : loss : 1.1351 = 0.6931 + 0.4420 + 0.0000, time: 23.563715]
2023-06-08 10:36:43.571: epoch 1:	0.00212467  	0.00368967  	0.00302718  
2023-06-08 10:36:43.571: Find a better model.
2023-06-08 10:37:13.270: [iter 2 : loss : 1.1339 = 0.6931 + 0.4408 + 0.0000, time: 29.695681]
2023-06-08 10:37:13.734: epoch 2:	0.00244300  	0.00426596  	0.00376621  
2023-06-08 10:37:13.734: Find a better model.
2023-06-08 10:37:43.227: [iter 3 : loss : 1.1339 = 0.6930 + 0.4409 + 0.0000, time: 29.487654]
2023-06-08 10:37:43.744: epoch 3:	0.00279094  	0.00443594  	0.00421960  
2023-06-08 10:37:43.744: Find a better model.
2023-06-08 10:38:13.061: [iter 4 : loss : 1.1340 = 0.6930 + 0.4411 + 0.0000, time: 29.311499]
2023-06-08 10:38:13.554: epoch 4:	0.00345721  	0.00613700  	0.00563667  
2023-06-08 10:38:13.554: Find a better model.
2023-06-08 10:38:42.760: [iter 5 : loss : 1.1342 = 0.6929 + 0.4413 + 0.0000, time: 29.200518]
2023-06-08 10:38:43.246: epoch 5:	0.00425673  	0.00790592  	0.00723317  
2023-06-08 10:38:43.246: Find a better model.
2023-06-08 10:39:12.073: [iter 6 : loss : 1.1343 = 0.6928 + 0.4414 + 0.0000, time: 28.823554]
2023-06-08 10:39:12.467: epoch 6:	0.00472312  	0.00905466  	0.00865968  
2023-06-08 10:39:12.467: Find a better model.
2023-06-08 10:39:41.365: [iter 7 : loss : 1.1345 = 0.6927 + 0.4417 + 0.0000, time: 28.887563]
2023-06-08 10:39:41.850: epoch 7:	0.00521172  	0.01104428  	0.00972894  
2023-06-08 10:39:41.850: Find a better model.
2023-06-08 10:40:10.105: [iter 8 : loss : 1.1345 = 0.6926 + 0.4419 + 0.0000, time: 28.249104]
2023-06-08 10:40:10.417: epoch 8:	0.00547082  	0.01124995  	0.00997274  
2023-06-08 10:40:10.417: Find a better model.
2023-06-08 10:40:39.618: [iter 9 : loss : 1.1346 = 0.6925 + 0.4421 + 0.0000, time: 29.188172]
2023-06-08 10:40:40.123: epoch 9:	0.00546342  	0.01046130  	0.00996348  
2023-06-08 10:41:09.815: [iter 10 : loss : 1.1344 = 0.6921 + 0.4423 + 0.0000, time: 29.680091]
2023-06-08 10:41:10.313: epoch 10:	0.00548563  	0.01074695  	0.00970059  
2023-06-08 10:41:39.510: [iter 11 : loss : 1.1340 = 0.6914 + 0.4426 + 0.0000, time: 29.189076]
2023-06-08 10:41:40.019: epoch 11:	0.00591500  	0.01174798  	0.01074380  
2023-06-08 10:41:40.020: Find a better model.
2023-06-08 10:42:04.315: [iter 12 : loss : 1.1339 = 0.6910 + 0.4429 + 0.0000, time: 24.292478]
2023-06-08 10:42:04.617: epoch 12:	0.00600383  	0.01303029  	0.01180209  
2023-06-08 10:42:04.617: Find a better model.
2023-06-08 10:42:28.355: [iter 13 : loss : 1.1337 = 0.6906 + 0.4431 + 0.0000, time: 23.733378]
2023-06-08 10:42:28.648: epoch 13:	0.00661088  	0.01456951  	0.01302444  
2023-06-08 10:42:28.648: Find a better model.
2023-06-08 10:42:51.634: [iter 14 : loss : 1.1334 = 0.6900 + 0.4434 + 0.0000, time: 22.978014]
2023-06-08 10:42:52.035: epoch 14:	0.00712908  	0.01612832  	0.01415885  
2023-06-08 10:42:52.035: Find a better model.
2023-06-08 10:43:15.755: [iter 15 : loss : 1.1333 = 0.6895 + 0.4437 + 0.0000, time: 23.716034]
2023-06-08 10:43:16.077: epoch 15:	0.00855045  	0.02039818  	0.01778432  
2023-06-08 10:43:16.077: Find a better model.
2023-06-08 10:43:39.731: [iter 16 : loss : 1.1328 = 0.6887 + 0.4440 + 0.0000, time: 23.650030]
2023-06-08 10:43:40.055: epoch 16:	0.00941660  	0.02341793  	0.02056888  
2023-06-08 10:43:40.056: Find a better model.
2023-06-08 10:44:03.475: [iter 17 : loss : 1.1322 = 0.6877 + 0.4445 + 0.0000, time: 23.413112]
2023-06-08 10:44:03.759: epoch 17:	0.01049746  	0.02743608  	0.02401299  
2023-06-08 10:44:03.759: Find a better model.
2023-06-08 10:44:27.290: [iter 18 : loss : 1.1309 = 0.6860 + 0.4448 + 0.0001, time: 23.527029]
2023-06-08 10:44:27.574: epoch 18:	0.01210391  	0.03260127  	0.02762224  
2023-06-08 10:44:27.574: Find a better model.
2023-06-08 10:44:50.302: [iter 19 : loss : 1.1289 = 0.6835 + 0.4454 + 0.0001, time: 22.725531]
2023-06-08 10:44:50.617: epoch 19:	0.01390289  	0.03789483  	0.03251601  
2023-06-08 10:44:50.617: Find a better model.
2023-06-08 10:45:13.650: [iter 20 : loss : 1.1257 = 0.6796 + 0.4460 + 0.0001, time: 23.027174]
2023-06-08 10:45:14.019: epoch 20:	0.01596096  	0.04415677  	0.03799634  
2023-06-08 10:45:14.025: Find a better model.
2023-06-08 10:45:38.358: [iter 21 : loss : 1.1199 = 0.6731 + 0.4466 + 0.0001, time: 24.329272]
2023-06-08 10:45:38.745: epoch 21:	0.01856687  	0.05041654  	0.04396550  
2023-06-08 10:45:38.745: Find a better model.
2023-06-08 10:46:03.021: [iter 22 : loss : 1.1109 = 0.6632 + 0.4475 + 0.0002, time: 24.272478]
2023-06-08 10:46:03.312: epoch 22:	0.02154300  	0.05841282  	0.05021841  
2023-06-08 10:46:03.312: Find a better model.
2023-06-08 10:46:26.644: [iter 23 : loss : 1.0965 = 0.6475 + 0.4487 + 0.0003, time: 23.327169]
2023-06-08 10:46:26.993: epoch 23:	0.02390463  	0.06484141  	0.05575458  
2023-06-08 10:46:26.993: Find a better model.
2023-06-08 10:46:51.288: [iter 24 : loss : 1.0748 = 0.6240 + 0.4504 + 0.0004, time: 24.291220]
2023-06-08 10:46:51.666: epoch 24:	0.02642914  	0.07213149  	0.06124263  
2023-06-08 10:46:51.666: Find a better model.
2023-06-08 10:47:15.826: [iter 25 : loss : 1.0448 = 0.5914 + 0.4527 + 0.0006, time: 24.156998]
2023-06-08 10:47:16.137: epoch 25:	0.02796161  	0.07525870  	0.06392768  
2023-06-08 10:47:16.137: Find a better model.
2023-06-08 10:47:40.429: [iter 26 : loss : 1.0067 = 0.5498 + 0.4560 + 0.0009, time: 24.288067]
2023-06-08 10:47:40.715: epoch 26:	0.02896844  	0.07819061  	0.06611174  
2023-06-08 10:47:40.715: Find a better model.
2023-06-08 10:48:04.192: [iter 27 : loss : 0.9634 = 0.5021 + 0.4601 + 0.0012, time: 23.470995]
2023-06-08 10:48:04.572: epoch 27:	0.02976060  	0.08013037  	0.06732657  
2023-06-08 10:48:04.572: Find a better model.
2023-06-08 10:48:28.910: [iter 28 : loss : 0.9174 = 0.4513 + 0.4645 + 0.0015, time: 24.333474]
2023-06-08 10:48:29.313: epoch 28:	0.03019739  	0.08207408  	0.06819976  
2023-06-08 10:48:29.313: Find a better model.
2023-06-08 10:48:53.950: [iter 29 : loss : 0.8734 = 0.4027 + 0.4688 + 0.0019, time: 24.632792]
2023-06-08 10:48:54.233: epoch 29:	0.03050832  	0.08307997  	0.06844162  
2023-06-08 10:48:54.234: Find a better model.
2023-06-08 10:49:17.787: [iter 30 : loss : 0.8334 = 0.3592 + 0.4719 + 0.0023, time: 23.549258]
2023-06-08 10:49:18.167: epoch 30:	0.03056015  	0.08426374  	0.06870098  
2023-06-08 10:49:18.167: Find a better model.
2023-06-08 10:49:42.759: [iter 31 : loss : 0.7991 = 0.3224 + 0.4740 + 0.0027, time: 24.587893]
2023-06-08 10:49:43.066: epoch 31:	0.03075264  	0.08470872  	0.06897413  
2023-06-08 10:49:43.066: Find a better model.
2023-06-08 10:50:07.545: [iter 32 : loss : 0.7685 = 0.2906 + 0.4749 + 0.0030, time: 24.474365]
2023-06-08 10:50:07.874: epoch 32:	0.03084149  	0.08514836  	0.06906676  
2023-06-08 10:50:07.874: Find a better model.
2023-06-08 10:50:32.054: [iter 33 : loss : 0.7433 = 0.2648 + 0.4752 + 0.0034, time: 24.173393]
2023-06-08 10:50:32.403: epoch 33:	0.03088590  	0.08532544  	0.06934768  
2023-06-08 10:50:32.403: Find a better model.
2023-06-08 10:50:57.549: [iter 34 : loss : 0.7216 = 0.2428 + 0.4751 + 0.0037, time: 25.141333]
2023-06-08 10:50:57.852: epoch 34:	0.03108579  	0.08654914  	0.07000113  
2023-06-08 10:50:57.853: Find a better model.
2023-06-08 10:51:21.937: [iter 35 : loss : 0.7031 = 0.2245 + 0.4746 + 0.0040, time: 24.070404]
2023-06-08 10:51:22.320: epoch 35:	0.03120424  	0.08738071  	0.07053880  
2023-06-08 10:51:22.320: Find a better model.
2023-06-08 10:51:48.210: [iter 36 : loss : 0.6869 = 0.2087 + 0.4738 + 0.0043, time: 25.885464]
2023-06-08 10:51:48.521: epoch 36:	0.03122644  	0.08679213  	0.07054996  
2023-06-08 10:52:19.886: [iter 37 : loss : 0.6726 = 0.1950 + 0.4730 + 0.0046, time: 31.358618]
2023-06-08 10:52:20.273: epoch 37:	0.03143373  	0.08765125  	0.07108434  
2023-06-08 10:52:20.273: Find a better model.
2023-06-08 10:52:50.372: [iter 38 : loss : 0.6605 = 0.1834 + 0.4721 + 0.0049, time: 30.091321]
2023-06-08 10:52:50.893: epoch 38:	0.03152257  	0.08755147  	0.07137245  
2023-06-08 10:53:22.032: [iter 39 : loss : 0.6502 = 0.1738 + 0.4712 + 0.0052, time: 31.135014]
2023-06-08 10:53:22.330: epoch 39:	0.03158179  	0.08779902  	0.07154746  
2023-06-08 10:53:22.331: Find a better model.
2023-06-08 10:53:52.964: [iter 40 : loss : 0.6395 = 0.1637 + 0.4703 + 0.0055, time: 30.627548]
2023-06-08 10:53:53.443: epoch 40:	0.03156698  	0.08763942  	0.07134425  
2023-06-08 10:54:24.030: [iter 41 : loss : 0.6308 = 0.1555 + 0.4696 + 0.0057, time: 30.582766]
2023-06-08 10:54:24.498: epoch 41:	0.03169284  	0.08799276  	0.07157692  
2023-06-08 10:54:24.498: Find a better model.
2023-06-08 10:54:52.350: [iter 42 : loss : 0.6226 = 0.1479 + 0.4687 + 0.0060, time: 27.846672]
2023-06-08 10:54:52.846: epoch 42:	0.03181870  	0.08826914  	0.07196632  
2023-06-08 10:54:52.846: Find a better model.
2023-06-08 10:55:16.825: [iter 43 : loss : 0.6158 = 0.1416 + 0.4680 + 0.0062, time: 23.974977]
2023-06-08 10:55:17.095: epoch 43:	0.03181871  	0.08849792  	0.07223804  
2023-06-08 10:55:17.095: Find a better model.
2023-06-08 10:55:40.602: [iter 44 : loss : 0.6091 = 0.1353 + 0.4673 + 0.0065, time: 23.503389]
2023-06-08 10:55:40.886: epoch 44:	0.03197416  	0.08915151  	0.07269273  
2023-06-08 10:55:40.886: Find a better model.
2023-06-08 10:56:04.392: [iter 45 : loss : 0.6032 = 0.1301 + 0.4664 + 0.0067, time: 23.500730]
2023-06-08 10:56:04.663: epoch 45:	0.03203339  	0.08903940  	0.07268223  
2023-06-08 10:56:28.423: [iter 46 : loss : 0.5973 = 0.1245 + 0.4658 + 0.0069, time: 23.756042]
2023-06-08 10:56:28.693: epoch 46:	0.03207039  	0.08917767  	0.07301059  
2023-06-08 10:56:28.693: Find a better model.
2023-06-08 10:56:52.597: [iter 47 : loss : 0.5925 = 0.1202 + 0.4652 + 0.0072, time: 23.899502]
2023-06-08 10:56:52.901: epoch 47:	0.03214443  	0.08881538  	0.07295186  
2023-06-08 10:57:16.778: [iter 48 : loss : 0.5876 = 0.1158 + 0.4644 + 0.0074, time: 23.871048]
2023-06-08 10:57:17.054: epoch 48:	0.03237393  	0.08956298  	0.07349601  
2023-06-08 10:57:17.054: Find a better model.
2023-06-08 10:57:40.961: [iter 49 : loss : 0.5828 = 0.1114 + 0.4638 + 0.0076, time: 23.902423]
2023-06-08 10:57:41.232: epoch 49:	0.03235913  	0.08957779  	0.07360977  
2023-06-08 10:57:41.233: Find a better model.
2023-06-08 10:58:07.647: [iter 50 : loss : 0.5791 = 0.1080 + 0.4633 + 0.0078, time: 26.411446]
2023-06-08 10:58:07.928: epoch 50:	0.03234432  	0.08970363  	0.07357509  
2023-06-08 10:58:07.928: Find a better model.
2023-06-08 10:58:32.169: [iter 51 : loss : 0.5756 = 0.1049 + 0.4626 + 0.0080, time: 24.235654]
2023-06-08 10:58:32.437: epoch 51:	0.03235913  	0.08960624  	0.07365907  
2023-06-08 10:58:56.511: [iter 52 : loss : 0.5716 = 0.1013 + 0.4622 + 0.0082, time: 24.070450]
2023-06-08 10:58:56.780: epoch 52:	0.03241836  	0.08980584  	0.07391302  
2023-06-08 10:58:56.780: Find a better model.
2023-06-08 10:59:20.902: [iter 53 : loss : 0.5686 = 0.0986 + 0.4617 + 0.0084, time: 24.118042]
2023-06-08 10:59:21.171: epoch 53:	0.03247019  	0.08971350  	0.07394812  
2023-06-08 10:59:45.338: [iter 54 : loss : 0.5650 = 0.0952 + 0.4612 + 0.0086, time: 24.162414]
2023-06-08 10:59:45.607: epoch 54:	0.03233693  	0.08941694  	0.07379166  
2023-06-08 11:00:09.884: [iter 55 : loss : 0.5627 = 0.0932 + 0.4608 + 0.0087, time: 24.273525]
2023-06-08 11:00:10.154: epoch 55:	0.03242576  	0.08953544  	0.07388479  
2023-06-08 11:00:34.372: [iter 56 : loss : 0.5602 = 0.0909 + 0.4604 + 0.0089, time: 24.213466]
2023-06-08 11:00:34.640: epoch 56:	0.03239614  	0.08916955  	0.07392325  
2023-06-08 11:00:58.901: [iter 57 : loss : 0.5570 = 0.0880 + 0.4600 + 0.0091, time: 24.257559]
2023-06-08 11:00:59.168: epoch 57:	0.03238134  	0.08927037  	0.07389975  
2023-06-08 11:01:23.326: [iter 58 : loss : 0.5550 = 0.0861 + 0.4595 + 0.0093, time: 24.154200]
2023-06-08 11:01:23.602: epoch 58:	0.03236654  	0.08924936  	0.07369434  
2023-06-08 11:01:48.938: [iter 59 : loss : 0.5529 = 0.0842 + 0.4593 + 0.0094, time: 25.328889]
2023-06-08 11:01:49.398: epoch 59:	0.03242576  	0.08917087  	0.07373608  
2023-06-08 11:02:19.998: [iter 60 : loss : 0.5498 = 0.0813 + 0.4589 + 0.0096, time: 30.594552]
2023-06-08 11:02:20.303: epoch 60:	0.03240355  	0.08902442  	0.07372957  
2023-06-08 11:02:49.686: [iter 61 : loss : 0.5482 = 0.0800 + 0.4584 + 0.0098, time: 29.372108]
2023-06-08 11:02:50.149: epoch 61:	0.03237394  	0.08897513  	0.07359529  
2023-06-08 11:03:19.941: [iter 62 : loss : 0.5460 = 0.0778 + 0.4582 + 0.0099, time: 29.779867]
2023-06-08 11:03:20.381: epoch 62:	0.03227770  	0.08876342  	0.07360379  
2023-06-08 11:03:50.111: [iter 63 : loss : 0.5438 = 0.0759 + 0.4578 + 0.0101, time: 29.719528]
2023-06-08 11:03:50.555: epoch 63:	0.03227030  	0.08894414  	0.07369185  
2023-06-08 11:04:20.763: [iter 64 : loss : 0.5422 = 0.0744 + 0.4576 + 0.0103, time: 30.203782]
2023-06-08 11:04:21.080: epoch 64:	0.03218145  	0.08860049  	0.07342874  
2023-06-08 11:04:50.435: [iter 65 : loss : 0.5408 = 0.0731 + 0.4573 + 0.0104, time: 29.349925]
2023-06-08 11:04:50.912: epoch 65:	0.03221847  	0.08836699  	0.07334246  
2023-06-08 11:05:20.520: [iter 66 : loss : 0.5385 = 0.0709 + 0.4571 + 0.0106, time: 29.601547]
2023-06-08 11:05:20.988: epoch 66:	0.03216665  	0.08821125  	0.07320116  
2023-06-08 11:05:50.253: [iter 67 : loss : 0.5383 = 0.0707 + 0.4569 + 0.0107, time: 29.259890]
2023-06-08 11:05:50.701: epoch 67:	0.03210002  	0.08760611  	0.07300419  
2023-06-08 11:06:21.187: [iter 68 : loss : 0.5365 = 0.0690 + 0.4566 + 0.0108, time: 30.481703]
2023-06-08 11:06:21.550: epoch 68:	0.03201118  	0.08749671  	0.07282224  
2023-06-08 11:06:51.796: [iter 69 : loss : 0.5348 = 0.0674 + 0.4564 + 0.0110, time: 30.238062]
2023-06-08 11:06:52.241: epoch 69:	0.03198897  	0.08746819  	0.07277562  
2023-06-08 11:07:21.653: [iter 70 : loss : 0.5336 = 0.0663 + 0.4562 + 0.0111, time: 29.398390]
2023-06-08 11:07:22.101: epoch 70:	0.03201859  	0.08767228  	0.07282692  
2023-06-08 11:07:51.571: [iter 71 : loss : 0.5323 = 0.0651 + 0.4560 + 0.0113, time: 29.465597]
2023-06-08 11:07:51.966: epoch 71:	0.03198158  	0.08742407  	0.07273255  
2023-06-08 11:08:21.295: [iter 72 : loss : 0.5312 = 0.0640 + 0.4558 + 0.0114, time: 29.317899]
2023-06-08 11:08:21.748: epoch 72:	0.03207041  	0.08780586  	0.07289626  
2023-06-08 11:08:51.080: [iter 73 : loss : 0.5295 = 0.0624 + 0.4556 + 0.0115, time: 29.319055]
2023-06-08 11:08:51.522: epoch 73:	0.03187053  	0.08755706  	0.07280713  
2023-06-08 11:09:21.433: [iter 74 : loss : 0.5295 = 0.0623 + 0.4555 + 0.0117, time: 29.899542]
2023-06-08 11:09:21.910: epoch 74:	0.03198898  	0.08795540  	0.07285622  
2023-06-08 11:09:52.859: [iter 75 : loss : 0.5283 = 0.0614 + 0.4551 + 0.0118, time: 30.942540]
2023-06-08 11:09:53.226: epoch 75:	0.03194456  	0.08780084  	0.07267255  
2023-06-08 11:10:23.143: [iter 76 : loss : 0.5263 = 0.0595 + 0.4549 + 0.0119, time: 29.911680]
2023-06-08 11:10:23.688: epoch 76:	0.03187053  	0.08763474  	0.07268126  
2023-06-08 11:10:53.413: [iter 77 : loss : 0.5261 = 0.0593 + 0.4548 + 0.0120, time: 29.718735]
2023-06-08 11:10:53.903: epoch 77:	0.03172986  	0.08717238  	0.07247426  
2023-06-08 11:10:53.903: Early stopping is trigger at epoch: 77
2023-06-08 11:10:53.903: best_result@epoch 52:

2023-06-08 11:10:53.903: 		0.0324      	0.0898      	0.0739      
